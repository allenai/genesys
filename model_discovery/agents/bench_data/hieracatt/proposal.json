{
    "variantname": null,
    "review": "**Final Review of the Proposal: HieraCATT**\n\n**Summary:**\n\nThe proposal introduces **HieraCATT**, a Hierarchical Compressive Adaptive Transformer that integrates hierarchical adaptive multi-head attention (from HieraNorm-AttnGPT) with adaptive attention scaling and compressive memory mechanisms (from AdaptiveTTT). The main objectives are to:\n\n- **Efficiently process long sequences** by managing computational and memory resources effectively.\n- **Maintain high performance** in terms of perplexity and accuracy on downstream tasks.\n- **Enhance scalability** to handle longer sequences without proportional increases in computational overhead.\n- **Ensure training stability** through dynamic layer normalization and careful integration of adaptive mechanisms.\n\n**Evaluation:**\n\n1. **Clarity:**\n\n   The proposal is clearly articulated, with well-defined objectives and a detailed design plan. It thoroughly explains the motivation behind integrating the components from the parent designs and provides comprehensive theoretical justifications. The inclusion of mathematical formulations and pseudo-code enhances the clarity, making it easier to understand the proposed changes.\n\n2. **Innovation:**\n\n   The integration of hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms is novel. Based on the extensive literature review and search results, no existing models combine all these components in a single architecture for language modeling tasks. The proposed approach addresses key challenges in processing long sequences, making it a significant contribution to the field.\n\n3. **Feasibility:**\n\n   The proposal provides detailed implementation guidelines, including mathematical formulations and step-by-step instructions. It leverages existing components and extends them in a way that seems feasible within the current technological constraints. Potential challenges, such as training stability and increased architectural complexity, are acknowledged, and possible solutions like dynamic layer normalization are suggested.\n\n4. **Scalability:**\n\n   By combining adaptive attention scaling and compressive memory mechanisms with hierarchical attention, the model is designed to handle longer sequences more efficiently. The use of sparse attention and memory compression techniques aims to reduce computational complexity and memory usage, enhancing the model's scalability.\n\n5. **Accuracy and Robustness:**\n\n   The proposal aims to improve the model's ability to capture multi-scale dependencies and focus computational resources where needed, potentially enhancing performance on tasks involving long sequences. The combination of adaptive mechanisms and hierarchical structures is expected to improve accuracy and robustness, although empirical validation is necessary to confirm these benefits.\n\n6. **Efficiency:**\n\n   The integration of compressive memory and adaptive attention scaling is intended to reduce computational and memory overhead. The use of sparse attention further decreases complexity from \\( O(N^2) \\) to \\( O(N) \\), improving efficiency when processing long sequences.\n\n**Strengths:**\n\n- **Innovative Integration:** The combination of hierarchical attention, adaptive attention scaling, and compressive memory mechanisms is novel and addresses key challenges in language modeling.\n- **Comprehensive Design Plan:** The proposal includes detailed mathematical formulations, pseudo-code, and implementation guidelines, demonstrating thoughtful planning.\n- **Scalability and Efficiency:** The design focuses on handling long sequences efficiently, which is crucial for modern language models.\n- **Theoretical Justifications:** The proposal provides solid theoretical foundations for the proposed mechanisms, supporting their potential effectiveness.\n\n**Concerns:**\n\n- **Training Stability:** Integrating multiple adaptive components may introduce challenges in training stability and convergence. While the proposal suggests using dynamic layer normalization, further exploration of training strategies is necessary.\n- **Implementation Complexity:** The increased architectural complexity might make implementation and optimization more challenging, potentially impacting development time and resource requirements.\n- **Lack of Empirical Validation:** The proposal is theoretical, and empirical results are needed to validate the expected improvements in performance, efficiency, and scalability.\n- **Potential Overhead:** The addition of compressive memory and adaptive mechanisms might introduce overhead that could offset some of the efficiency gains if not carefully managed.\n\n**Suggestions for Improvement:**\n\n1. **Empirical Evaluation Plan:**\n\n   Develop a detailed plan for empirically evaluating HieraCATT, including selecting appropriate datasets, defining evaluation metrics, and designing ablation studies to assess the contributions of each component.\n\n2. **Training Stability Strategies:**\n\n   Explore and incorporate advanced training stabilization techniques, such as modified normalization approaches (e.g., B2T Connection), gradient clipping, or adaptive learning rate schedules to address potential training challenges.\n\n3. **Implementation Optimization:**\n\n   Consider practical implementation strategies to manage the architectural complexity, such as modular design, efficient memory management, and leveraging existing deep learning frameworks that support custom attention mechanisms.\n\n4. **Comparative Analysis:**\n\n   Plan to compare the performance of HieraCATT with existing state-of-the-art models, including both transformer-based models and those utilizing compressive memory or adaptive attention, to quantify the benefits.\n\n5. **Feedback Mechanisms:**\n\n   Investigate the potential integration of feedback attention mechanisms to enhance the model's ability to handle long-term dependencies and further improve performance.\n\n**Final Rating:**\n\n```rating 4.3```\n\n**Explanation of Rating:**\n\nThe proposal is well-crafted, innovative, and addresses significant challenges in language modeling. The integration of advanced mechanisms is novel and has the potential to advance the field. While there are concerns regarding training stability and implementation complexity, these are acknowledged, and suggestions are provided to mitigate them. The lack of empirical validation prevents a higher rating, but overall, the proposal is of high quality with strong potential impact.",
    "search_stack": [
        {
            "ready": false,
            "query": [
                "hierarchical attention transformers",
                "adaptive attention scaling",
                "selective memory compression"
            ],
            "detail": "Integration of hierarchical attention with adaptive scaling and memory compression in Transformers",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nIntegration of hierarchical attention with adaptive scaling and memory compression in Transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.92)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 2/24 (Score: 0.94)*\n\n```\nA key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1 M sequence length passkey context block retrieval and 500 K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs. ## 1 Introduction\n\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based LLMs (Brown et al., 2020; Touvron et al., 2023, Anil et al., 2023; Groeneveld et al. 2024) have a constrained context-dependent memory, due to the nature of the attention mechanism. The attention mechanism in Transformers exhibits quadratic complexity in both memory footprint and computation time. For example, the attention Key-Value (KV) states have 3TB memory footprint for a 500B model with batch size 512 and context length 2048 (Pope et al. 2023). Indeed, scaling LLMs to longer sequences (i.e. 1 M tokens) is challenging with the standard Transformer architectures and serving longer and longer context models becomes costly financially. Compressive memory systems promise to be more scalable and efficient than the attention mechanism for extremely long sequences (Kanerva, 1988; Munkhdalai et al., 2019). Instead of using an array that grows with the input sequence length, a compressive memory primarily maintains a fixed number of parameters to store and recall information with a bounded storage and computation costs. In the compressive memory, new information is added to the memory by changing its parameters with an objective that this information can be recovered back later on. However, the LLMs in their current state have yet to see an effective, practical compressive memory technique that balances simplicity along with quality. In this work, we introduce a novel approach that enables Transformer LLMs to effectively process infinitely long inputs with bounded memory footprint and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention (Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention mechanism (Bahdanau et al., 2014, Vaswani et al. 2017) and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. Such a subtle but critical modification to the Transformer attention layer enables a natural extension of existing LLMs to infinitely long contexts via continual pre-training and finetuning. Our Infini-attention reuses all the key, value and query states of the standard attention computation for long-term memory consolidation and retrieval. We store old KV states of the attention in the compressive memory, instead of discarding them like in the standard attention mechanism. We then retrieve the values from the memory by using the attention query states when processing subsequent sequences. To compute the final contextual output, the Infini-attention aggregates the long-term memory-retrieved values and the local attention contexts. In our experiments, we show that our approach outperforms baseline models on longcontext language modeling benchmarks while having $114 x$ comprehension ratio in terms of memory size. The model achieves even better perplexity when trained with 100 K sequence length. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval task when injected with Infini-attention. Finally, we show that a 8B model with Infiniattention reaches a new SOTA result on a 500 K length book summarization task after continual pre-training and task fine-tuning. In summary, our work makes the following contributions:\n\n1. We introduce a practical and yet powerful attention mechanism - Infini-attention with long-term compressive memory and local causal attention for efficiently modeling both long and short-range contextual dependencies. 2. Infini-attention introduces minimal change to the standard scaled dot-product attention and supports plug-and-play continual pre-training and long-context adaptation by design. 3. Our approach enables Transformer LLMs to scale to infinitely long context with a bounded memory and compute resource by processing extremely long inputs in a streaming fashion. ## 2 Background\n\nRecurrent Neural Networks (RNNs) process a single token $x_{t}$ at each step $t$ and computes a recurrent hidden state $h_{t}$ to represent an entire input sequence Hochreiter \\& Schmidhuber, 1997, Maass et al. 2002):\n\n$$\nh_{t}=R N N\\left(x_{t}, h_{t-1}\\right)\n$$\n\nThe RNN computation is very efficient since the model maintains only a fixed-size vector $h_{t}$ for input sequence. However, for processing long sequences it becomes difficult to store entire contextual information into a single fixed-size vector and this limitation had implications on RNNs utility in certain tasks (Kaiser \\& Sutskever, 2015). To address the limitation, people extended the standard RNNs with an external memory component that can be read from and written to. One such an instance is Metalearned Neural Memory (MNM) (Munkhdalai et al., 2019):\n\n$$\nh_{t}, \\theta_{t}=M N M\\left(x_{t}, h_{t-1}, \\theta_{t-1}\\right)\n$$\n\nMNM learns an additional memory state $\\theta$ parameterized by a feed-forward neural network (FFN) and uses query, key and value vectors (QKV) to interact with the memory, similar to the attention mechanism. To store information, it modifies the parameters of the FFN by using the key vectors as input and the value vectors for the target, and to read memory\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbb8cf178811d16c1412g-03.jpg?height=663&width=1391&top_left_y=292&top_left_x=367)\n\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL (bottom) discards old contexts since it caches the KV states for the last segment only. entries, it forward-passes the query vectors through the memory FFN and retrieves its corresponding value. Like RNNs, the memory state is still bounded in MNM. Unlike the RNNs, the attention mechanism however doesn't maintain a recurrent state and only performs a feed-forward computation on input sequence segment $X_{s}$ :\n\n$$\nO_{s}=\\operatorname{attention}\\left(X_{s}\\right)\n$$\n\nThe attention output $O_{s}$ is simply passed to the next layer and no state is carried over to the next input sequence $X_{s+1}$ at the same attention layer. In the attention layer, in order to capture the dependency between the consequent segments $X_{s}$ and $X_{s+1}$, one needs to process them altogether at the same time and this process becomes a bottleneck requiring large computational resources as the length of input sequence grows more and more. To improve the efficiency while still being able to benefit from the expressiveness of the attention mechanism, this work introduces a recurrent attention layer. ## 3 Method\n\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al, 2019). Similar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We compute the standard causal dot-product attention context within each segment. So the dot-product attention computation is local in a sense that it covers a total $N$ number of tokens of the current segment with index $S$ ( $N$ is the segment length). The local attention (Dai et al., 2019), however, discards the attention states of the previous segment when processing the next one. In Infini-Transformers, instead of leaving out the old KV attention states, we propose to reuse them to maintain the entire context history with a compressive memory. So each attention layer of Infini-Transformers has both global compressive and local fine-grained states. We call such an efficient attention mechanism Infini-attention, which is illustrated in Figure 1 and described formally in the following sections. ### 3.1 Infini-attention\n\nAs shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes both local and global context states and combine them for its output. Similar to multi-head\nattention (MHA), it maintains $H$ number of parallel compressive memory per attention layer ( $H$ is the number of attention heads) in addition to the dot-product attention and like the RNNs and MNM, it maintains a recurrent memory state to efficiently track the long sequence context:\n\n$$\nO_{s}, M_{s}=\\text { infini-attention }\\left(X_{s}, M_{s-1}\\right)\n$$\n\n### 3.1.1 Scaled Dot-product Attention\n\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention variant (Munkhdalai et al., 2016. Cheng et al., 2016), has been the main building block in LLMs. The MHA's strong capability to model context-dependent dynamic computation and its conveniences of temporal masking have been leveraged extensively in the autoregressive generative models. A single head in the vanilla MHA computes its attention context $A_{\\text {dot }} \\in \\mathbb{R}^{N \\times d_{\\text {value }}}$ from sequence of input segments $X \\in \\mathbb{R}^{N \\times d_{\\text {model }}}$ as follows. First, it computes attention query, key, and value states:\n\n$$\nK=X W_{K}, V=X W_{V} \\text { and } Q=X W_{Q} \\text {. }\n$$\n\nHere, $W_{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {key }}}, W_{V} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {value }}}$ and $W_{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {key }}}$ are trainable projection matrices. Then, the attention context is calculated as a weighted average of all other values as\n\n$$\nA_{\\text {dot }}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{\\text {model }}}}\\right) V\n$$\n\nFor MHA, we compute $H$ number of attention context vectors for each sequence element in parallel, concatenate them along the second dimension and then finally project the concatenated vector to the model space to obtain the attention output. ### 3.1.2 Compressive Memory\n\nIn Infini-attention, instead of computing new memory entries for compressive memory, we reuse the query, key and value states ( $Q, K$ and $V$ ) from the dot-product attention computation. The state sharing and reusing between the dot-product attention and compressive memory not only enables efficient plug-in-play long-context adaptation but also speeds up training and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to store bindings of key and value states in the compressive memory and retrieve by using the query vectors. While there are different forms of compressive memory proposed in the literature (Hopfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and computational efficiency, in this work we parameterize the memory with an associative matrix (Schlag et al., 2020). This approach further allows us to cast the memory update and retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage stable training techniques from the related methods. Specially, we adopt the update rule and retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and competitive performance. Memory retrieval. In Infini-attention, we retrieve new content $A_{\\text {mem }} \\in \\mathbb{R}^{N \\times d_{\\text {value }}}$ from the memory $M_{s-1} \\in \\mathbb{R}^{d_{\\text {key }} \\times d_{\\text {value }}}$ by using the query $Q \\in \\mathbb{R}^{N \\times d_{\\text {key }}}$ as:\n\n$$\nA_{\\text {mem }}=\\frac{\\sigma(Q) M_{s-1}}{\\sigma(Q) z_{s-1}}\n$$\n\nHere, $\\sigma$ and $z_{s-1} \\in \\mathbb{R}^{d_{\\text {key }}}$ are a nonlinear activation function and a normalization term, respectively.\n```\n\n##### *Relevant Chunk: No. 12/24 (Score: 0.91)*\n\n```\narXiv preprint arXiv:1511.07289, 2015. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128 k context. arXiv preprint arXiv:2402.10171, 2024. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model.\n```\n\n#### 2. TransformerFAM: Feedback attention is working memory (Avg. Score: 0.88)\n\n*Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, K. Sim, P. M. Mengibar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** Feedback Attention Memory is proposed, a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences.\n\n**Abstract:** While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.\n\n##### *Relevant Chunk: No. 1/36 (Score: 0.88)*\n\n```\n# TransformerFAM: Feedback attention is working memory \n\nDongseong Hwang Weiran Wang Zhuoyuan Huo Khe Chai Sim Pedro Mengibar<br>Google LLC<br>Mountain View, CA, USA<br>dongseong@google.com\n\n\n#### Abstract\n\nWhile Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length. ## 1 Introduction\n\nThe introduction of the Transformer architecture [12] has revolutionized deep learning by permeating diverse domains and enhancing performance due to its efficacy and scalability. This scalability fuels a trend analogous to Moore's law, which links increased model size to performance gains [39]. The effectiveness of attention in text sequence processing was solidified through the Transformer paper. Models like BERT [16] and GPT-3 [33] further showcased the scalability of Transformer and its tendency for improved performance with increased model size. Following the replacement of LSTM [5] by Transformer in most Natural Language Processing (NLP) domains, the Vision Transformer (ViT) [32] replaced Convolutional Neural Network (CNN) [4] with Transformers in the vision domain, and Conformer (Convolution-augmented Transformer) [29] replaced LSTM in the speech domain. The Transformer has become the de facto architecture in various domains. Currently, attention serves as the leading architecture for extracting meaningful representations from homogeneous data. The logical progression points toward extending attention capabilities to heterogeneous data. This has enabled advances in multimodal fusion (text and vision), as seen in models like DALL$\\cdot$E 2 [55], Flamingo [54] and CoCa [53]. AudioLM [64] has shown that attention also excels at fusing audio and text. Consequently, Gemini [69] integrates text, images, audio, and video into a single generative model. This was possible because attention to heterogeneous data works exceptionally well. Despite the impressive success of attention, it suffers from major drawbacks. Firstly, attention has quadratic complexity with respect to context length, which limits the capability of modeling long contexts. Secondly, it forgets information from context before attention window, unlike LSTM, which theoretically can propagate information indefinitely. We want the better architecture to be able to process arbitrarily long sequences efficiently, while preserving very long-term dependencies. ![](https://cdn.mathpix.com/cropped/2024_09_17_364db6cf7554a04db2efg-02.jpg?height=407&width=1404&top_left_y=232&top_left_x=361)\n\nFigure 1: Comparison of query-key attention masks for Sliding Window Attention (SWA) variants. (a) Sliding Window Attention: Attention is restricted to the current window = 3. (b) Block Sliding Window Attention (BSWA) (block size $=2$, memory segment $=1$ ): Attention is allowed to previous blocks within the memory segment. (c) BSWA (block size $=2$, memory segment $=2$ ): The memory segment is expanded, allowing attention to a larger past context. (d) Illustrates the receptive field of BSWA (block size $=2$, memory segment $=1$, depth $=4$ ): The region within the curly braces represents the receptive field. ![](https://cdn.mathpix.com/cropped/2024_09_17_364db6cf7554a04db2efg-02.jpg?height=266&width=1379&top_left_y=979&top_left_x=372)\n\nFigure 2: Comparison of attention patterns in Transformer layer. (a) TransformerBSWA: Input query attends to the current block and two memory segments, providing past context. (b) TransformerFAM: Input query attends to the current block, memory segments, and past FAM (green lines). FAM query (copied from previous FAM, blue dash arrow) compresses the current block to update FAM.\n```\n\n#### 3. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.87)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.87)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n#### 4. Recurrent Memory Transformer (Avg. Score: 0.86)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.86)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention transformers, adaptive attention scaling, selective memory compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns\n\n*From Search Query: hierarchical attention transformers*\n\n*Brian DuSell, David Chiang*\n\n**TL;DR:** It is shown that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty and is more effective at natural language modeling under a constrained parameter budget.\n\n**Abstract:** Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 2. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: hierarchical attention transformers*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 3. Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models\n\n*From Search Query: adaptive attention scaling*\n\n*Yubin Shi, Yixuan Chen, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert P. Dick, Qin Lv, Yingying Zhao, Fan Yang, Tun Lu, Ning Gu, L. Shang*\n\n**TL;DR:** This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy and proposes a novel training strategy termed Modular Adaptive Training (MAT), which can significantly save computations by its partially-updating strategy and can further improve performance.\n\n**Abstract:** Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Human-Timescale Adaptation in an Open-Ended Task Space\n\n*From Search Query: adaptive attention scaling*\n\n*Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, N. Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rockt\u00e4schel, Yannick Schroecker, Jakub Sygnowski, K. Tuyls, Sarah York, Alexander Zacherl, Lei M. Zhang*\n\n**TL;DR:** It is demonstrated that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans.\n\n**Abstract:** Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 89  (*Influential: 11*)\n\n#### 5. MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n\n*From Search Query: adaptive attention scaling*\n\n*Chenjie Cao, Xinlin Ren, Yanwei Fu*\n\n**TL;DR:** This paper introduces MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline, and achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Abstract:** Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 6. Revisiting Discriminator in GAN Compression: A Generator-discriminator Cooperative Compression Scheme\n\n*From Search Query: selective memory compression*\n\n*Shaojie Li, Jie Wu, Xuefeng Xiao, Fei Chao, Xudong Mao, Rongrong Ji*\n\n**TL;DR:** The role of discriminator is revisited and a novel generator-discriminator cooperative compression scheme for GAN compression, termed GCC, which contributes to reducing 80% computational costs while maintains comparable performance in image translation tasks.\n\n**Abstract:** Recently, a series of algorithms have been explored for GAN compression, which aims to reduce tremendous computational overhead and memory usages when deploying GANs on resource-constrained edge devices. However, most of the existing GAN compression work only focuses on how to compress the generator, while fails to take the discriminator into account. In this work, we revisit the role of discriminator in GAN compression and design a novel generator-discriminator cooperative compression scheme for GAN compression, termed GCC. Within GCC, a selective activation discriminator automatically selects and activates convolutional channels according to a local capacity constraint and a global coordination constraint, which help maintain the Nash equilibrium with the lightweight generator during the adversarial training and avoid mode collapse. The original generator and discriminator are also optimized from scratch, to play as a teacher model to progressively refine the pruned generator and the selective activation discriminator. A novel online collaborative distillation scheme is designed to take full advantage of the intermediate feature of the teacher generator and discriminator to further boost the performance of the lightweight generator. Extensive experiments on various GAN-based generation tasks demonstrate the effectiveness and generalization of GCC. Among them, GCC contributes to reducing 80% computational costs while maintains comparable performance in image translation tasks. Our code and models are available at https://github.com/SJLeo/GCC.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 31  (*Influential: 7*)\n\n#### 7. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: selective memory compression*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 8. Memory Replay with Data Compression for Continual Learning\n\n*From Search Query: selective memory compression*\n\n*Liyuan Wang, Xingxing Zhang, Kuo Yang, L. Yu, Chongxuan Li, Lanqing Hong, Shifeng Zhang, Zhenguo Li, Yi Zhong, Jun Zhu*\n\n**TL;DR:** This work proposes a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer.\n\n**Abstract:** Continual learning needs to overcome catastrophic forgetting of the past. Memory replay of representative old training samples has been shown as an effective solution, and achieves the state-of-the-art (SOTA) performance. However, existing work is mainly built on a small memory buffer containing a few original data, which cannot fully characterize the old data distribution. In this work, we propose memory replay with data compression (MRDC) to reduce the storage cost of old training samples and thus increase their amount that can be stored in the memory buffer. Observing that the trade-off between the quality and quantity of compressed data is highly nontrivial for the efficacy of memory replay, we propose a novel method based on determinantal point processes (DPPs) to efficiently determine an appropriate compression quality for currently-arrived training samples. In this way, using a naive data compression algorithm with a properly selected quality can largely boost recent strong baselines by saving more compressed data in a limited storage space. We extensively validate this across several benchmarks of class-incremental learning and in a realistic scenario of object detection for autonomous driving.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 79  (*Influential: 4*)\n\n### 2 related papers from ArXiv\n\n#### 1. An Exploration of Hierarchical Attention Transformers for Efficient Long\n  Document Classification\n\n*From Search Query: hierarchical attention transformers*\n\n*Ilias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodromos Malakasiotis, Desmond Elliott*\n\n**Abstract:** Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.\n\n**Published:** 2022-10-11T15:17:56Z  (*Updated: 2022-10-11T15:17:56Z*)\n\n\n\n#### 2. Hierarchical Learning for Generation with Long Source Sequences\n\n*From Search Query: hierarchical attention transformers*\n\n*Tobias Rohde, Xiaoxia Wu, Yinhan Liu*\n\n**Abstract:** One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. Furthermore, our\nmodel achieves state-of-the-art ROUGE scores on four summarization tasks,\nincluding PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms\ndocument-level machine translation baseline on the WMT20 English to German\ntranslation task. We investigate what the hierarchical layers learn by\nvisualizing the hierarchical encoder-decoder attention. Finally, we study\nhierarchical learning on encoder-only pre-training and analyze its performance\non classification tasks.\n\n**Published:** 2021-04-15T15:57:32Z  (*Updated: 2021-09-16T19:03:17Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention transformers*\n\n*Pavlo Molchanov, Jan Kautz, Jose M. Alvarez, Andrew Tao, Hongxu Yin, Greg Heinrich, Ali Hatamizadeh*\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Published:** 2023-06-09\n\n\n\n#### 2. Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\n\n*From Search Query: hierarchical attention transformers*\n\n*Gao Huang, Shiji Song, Zhuofan Xia, Tianzhu Ye, Xuran Pan*\n\n**Abstract:** Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/Slide-Transformer.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-04-09\n\n\n\n#### 3. Temporal Attentive Alignment for Large-Scale Video Domain Adaptation\n\n*From Search Query: adaptive attention scaling*\n\n*Min-Hung Chen, Zsolt Kira, Jian Zheng, Ghassan AlRegib, Ruxin Chen, Jaekwon Yoo*\n\n**Abstract:** Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over \"Source only\" from 73.9% to 81.8% on \"HMDB --> UCF\", and 10.3% gain on \"Kinetics --> Gameplay\"). The code and data are released at http://github.com/cmhungsteve/TA3N.\n\n**Conference:** temporal-attentive-alignment-for-large-scale-1\n\n**Published:** 2019-07-30\n\n\n\n#### 4. Temporal Attentive Alignment for Video Domain Adaptation\n\n*From Search Query: adaptive attention scaling*\n\n*Min-Hung Chen, Zsolt Kira, Ghassan AlRegib*\n\n**Abstract:** Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: UCF-HMDB_full. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at http://github.com/cmhungsteve/TA3N.\n\n**Published:** 2019-05-26\n\n\n\n#### 5. Billion-scale similarity search with GPUs\n\n*From Search Query: selective memory compression*\n\n*Herv\u00e9 J\u00e9gou, Matthijs Douze, Jeff Johnson*\n\n**Abstract:** Similarity search finds application in specialized database systems handling\ncomplex data such as images or videos, which are typically represented by\nhigh-dimensional features and require specific indexing structures. This paper\ntackles the problem of better utilizing GPUs for this task. While GPUs excel at\ndata-parallel tasks, prior approaches are bottlenecked by algorithms that\nexpose less parallelism, such as k-min selection, or make poor use of the\nmemory hierarchy.\n  We propose a design for k-selection that operates at up to 55% of theoretical\npeak performance, enabling a nearest neighbor implementation that is 8.5x\nfaster than prior GPU state of the art. We apply it in different similarity\nsearch scenarios, by proposing optimized design for brute-force, approximate\nand compressed-domain search based on product quantization. In all these\nsetups, we outperform the state of the art by large margins. Our implementation\nenables the construction of a high accuracy k-NN graph on 95 million images\nfrom the Yfcc100M dataset in 35 minutes, and of a graph connecting 1 billion\nvectors in less than 12 hours on 4 Maxwell Titan X GPUs. We have open-sourced\nour approach for the sake of comparison and reproducibility.\n\n**Published:** 2017-02-28\n\n\n\n#### 6. DynaBERT: Dynamic BERT with Adaptive Width and Depth\n\n*From Search Query: selective memory compression*\n\n*Xiao Chen, Lifeng Shang, Zhiqi Huang, Qun Liu, Lu Hou, Xin Jiang*\n\n**Abstract:** The pre-trained language models like BERT, though powerful in many natural language processing tasks, are both computation and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually compress the large BERT model to a fixed smaller size. They can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can flexibly adjust the size and latency by selecting adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allowing both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT-base (or RoBERTa-base), while at smaller widths and depths consistently outperforms existing BERT compression methods. Code is available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-04-08\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goal of designing a novel language model block that integrates hierarchical attention with adaptive attention scaling and selective memory compression, here are some key points and useful information:\n\n### Hierarchical Attention\n- The concept of hierarchical attention is crucial for capturing multi-scale dependencies. The researcher can draw inspiration from models like **QKFormer**, which uses a hierarchical structure to improve performance. Although QKFormer is designed for image processing, its hierarchical spiking representation and multi-scale spiking structure can provide insights into how to implement hierarchical attention in language models.\n\n### Adaptive Attention Scaling\n- Adaptive attention scaling is essential for dynamically allocating computational resources based on input complexity. The work by **Sukhbaatar and Fergus (2019)** on \"Adaptive Attention Span in Transformers\" is highly relevant, as it introduces a method for the model to learn the optimal attention span for each head in a multi-head attention layer. This can be integrated into the hierarchical attention mechanism to make it more adaptive.\n\n### Selective Memory Compression\n- For selective memory compression, techniques from models like **ALBERT** can be useful. ALBERT introduces parameter-reduction techniques to lower memory consumption, which can be adapted for selective memory compression in the context of language models. Additionally, exploring advanced memory compression methods such as quantization or pruning, as mentioned in the future search plan, can further optimize memory usage.\n\n### Integration Strategy\n- To integrate these components, the researcher can follow these steps:\n  - **Modify Hierarchical Attention**: Incorporate adaptive attention scaling into the hierarchical attention structure. This can be done by introducing an adaptive scaling factor **\u03b1(x)** based on input complexity, similar to the approach in **Adaptive Attention Span in Transformers**.\n  - **Implement Selective Memory Compression**: Use techniques like those in ALBERT or more advanced methods to compress less important state information dynamically. This can help in managing intermediate variables (**Z**) more efficiently.\n  - **Retain Dynamic Layer Normalization**: Use normalization techniques such as RMSNorm or DynamicLayerNorm to ensure stable gradient flow and improve the model's adaptability to different contexts.\n\n### Efficient Hierarchical Attention Implementations\n- For optimized implementations of hierarchical attention, looking into models that use sparse factorizations of the attention matrix, such as **Sparse Transformers**, can be beneficial. These models reduce the complexity from quadratic to linear, making them more efficient for handling long sequences.\n\n### Future Search Plan\n- **Adaptive Computation in Transformers**: Investigate recent studies on adaptive computation time in transformers, such as the work by **Graves (2016)** on adaptive computation time for recurrent neural networks. This can provide insights into how to adapt computational resources dynamically in transformer models.\n- **Advanced Memory Compression Techniques**: Research advanced techniques like quantization, pruning, or other forms of compression that can be integrated into the selective memory compression mechanism to further optimize memory usage.\n\n### Key References\n- **Sukhbaatar, S., & Fergus, R. (2019).** \"Adaptive Attention Span in Transformers.\"\n- **Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020).** \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\"\n- **Child, R., Gray, S., Radford, A., & Sutskever, I. (2019).** \"Generating Long Sequences with Sparse Transformers.\"\n- **Graves, A. (2016).** \"Adaptive Computation Time for Recurrent Neural Networks.\"\n\nBy integrating these concepts and techniques, the researcher can design a novel language model block that is efficient, scalable, and performs well on various NLP tasks.\n",
            "analysis": "```keywords hierarchical attention transformers```\n\n```keywords adaptive attention scaling```\n\n```keywords selective memory compression```\n\n```description Integration of hierarchical attention with adaptive scaling and memory compression in Transformers```\n\n---\n\n### **Analysis Note**\n\n#### 1. **Summary of Analysis**\n\nI aim to design a novel language model block that combines the strengths of both Parent 1 (**HieraNorm-AttnGPT**) and Parent 2 (**AdaptiveTTT**). Specifically, I plan to integrate **hierarchical adaptive multi-head attention** with **adaptive attention scaling** and **selective memory compression**. This combination should enhance the model's ability to capture multi-scale dependencies, adapt computational resources based on input complexity, and manage state information efficiently. By reusing and integrating existing GAUs from both parents, the new design should achieve improved efficiency, scalability, and performance on various NLP tasks.\n\n#### 2. **Useful References with Excerpts**\n\n- **Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"**\n\n  *\"We propose RMSNorm, a variation of layer normalization that only scales the inputs without centering them, showing benefits in both performance and computational efficiency.\"*\n\n- **Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020). \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations\"**\n\n  *\"We introduce parameter-reduction techniques to lower memory consumption and increase the training speed of BERT models.\"*\n\n- **Graves, A. (2016). \"Adaptive Computation Time for Recurrent Neural Networks\"**\n\n  *\"We introduce a mechanism that allows recurrent neural networks to dynamically adjust the computational effort applied to different inputs.\"*\n\n- **Sukhbaatar, S., & Fergus, R. (2019). \"Adaptive Attention Span in Transformers\"**\n\n  *\"We propose a method that allows the model to learn the optimal attention span for each head in a multi-head attention layer.\"*\n\n- **Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). \"Generating Long Sequences with Sparse Transformers\"**\n\n  *\"We introduce sparse factorizations of the attention matrix that can handle sequences with thousands of tokens, reducing the complexity from quadratic to linear.\"*\n\n#### 3. **Key Insights and Detailed Analysis**\n\n- **Hierarchical Attention with Adaptive Scaling**: Combining the hierarchical multi-scale attention mechanism from **HieraNorm-AttnGPT** with the adaptive attention scaling from **AdaptiveTTT** can allow the model to dynamically allocate computational resources based on input complexity. This means that for simpler inputs, the model can reduce computation, while for more complex inputs, it can allocate more resources to capture necessary dependencies.\n\n- **Selective Memory Compression**: Incorporating selective memory compression techniques from **AdaptiveTTT** can help manage the intermediate variables (**Z**) more efficiently. By compressing less important state information, we can reduce memory usage without significantly impacting performance.\n\n- **Dynamic Layer Normalization**: Retaining the dynamic layer normalization from **HieraNorm-AttnGPT** (DLN) ensures stable gradient flow and improves the model's adaptability to different contexts.\n\n- **Reuse of Existing GAUs**:\n\n  - **HierarchicalAdaptiveAttention**: Use this GAU as the foundation, integrating adaptive attention scaling into its hierarchical structure.\n\n  - **AdaptiveAttention Scaling Mechanism**: Incorporate the adaptive scaling factor from **AdaptiveTTT** into the attention computations of **HierarchicalAdaptiveAttention**.\n\n  - **RMSNorm and DynamicLayerNorm**: Utilize the normalization techniques from both parents to maintain training stability and efficiency.\n\n- **Integration Strategy**:\n\n  - Modify the **HierarchicalAdaptiveAttention** unit to include an adaptive scaling factor **\u03b1(x)** based on input complexity, similar to the approach in **AdaptiveAttention**.\n\n  - Implement a selective memory compression mechanism within the intermediate variables **Z**, allowing the model to discard or compress less important states dynamically.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces computational overhead by adapting attention computation based on input complexity.\n\n  - **Scalability**: Efficient memory management allows the model to handle longer sequences without a proportional increase in memory usage.\n\n  - **Performance**: Combines multi-scale dependency capture with adaptive computation for improved accuracy on downstream tasks.\n\n#### 4. **Future Search Plan**\n\n- **Explore Adaptive Computation in Transformers**: Investigate more recent studies on adaptive computation time and how it has been applied to transformer models.\n\n- **Efficient Hierarchical Attention Implementations**: Look for optimized implementations of hierarchical attention that can be adapted to include adaptive scaling.\n\n- **Memory Compression Techniques**: Research advanced memory compression methods used in neural networks, such as quantization or pruning, that could be integrated into the selective memory compression mechanism.\n\n#### 5. **References**\n\n1. **Zhang, B., & Sennrich, R. (2019).** \"Root Mean Square Layer Normalization.\" *arXiv preprint arXiv:1910.07467*.\n\n2. **Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020).** \"ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.\" *International Conference on Learning Representations (ICLR)*.\n\n3. **Graves, A. (2016).** \"Adaptive Computation Time for Recurrent Neural Networks.\" *arXiv preprint arXiv:1603.08983*.\n\n4. **Sukhbaatar, S., & Fergus, R. (2019).** \"Adaptive Attention Span in Transformers.\" *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*.\n\n5. **Child, R., Gray, S., Radford, A., & Sutskever, I. (2019).** \"Generating Long Sequences with Sparse Transformers.\" *arXiv preprint arXiv:1904.10509*.\n\n6. **HieraNorm-AttnGPT Proposal and Implementation Details.**\n\n7. **AdaptiveTTT Proposal and Implementation Details.**\n\n---"
        },
        {
            "ready": false,
            "query": [
                "adaptive computation transformers",
                "memory compression transformers",
                "sparse attention transformers"
            ],
            "detail": "Detailed implementations of Transformers integrating compressive memory and adaptive computation for efficient long-sequence processing",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nDetailed implementations of Transformers integrating compressive memory and adaptive computation for efficient long-sequence processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Memformer: A Memory-Augmented Transformer for Sequence Modeling (Avg. Score: 0.99)\n\n*Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, A. Geramifard, Zhou Yu*\n\n**Published in:** AACL/IJCNLP (2020)\t**Cited by** 36  (*Influential: 2*)\n\n**TL;DR:** This work presents Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information, and proposes a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back- Propagation through time with a significantly reduced memory requirement.\n\n**Abstract:** Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\nTransformers have achieved great success in various natural language processing tasks. Nevertheless, the quadratic computation complexity can be costly. Some works try to address the limitations of selfattention, including Reformer, Sparse Transformer, Longformer, Linformer (Child et al., 2019; Kitaev et al., 2020; Wang et al., 2020), etc. They successfully reduce the complexity of self-attention and thus enable processing longer sequences. However, most of them still require linear memory space complexity. Transformer-XL (Dai et al., 2019) re-introduces the concept of memory and recurrence. It caches each layer's hidden states of self-attention into a fixed-size queue and re-uses them in the later attention computation. However, the memory as raw hidden states cannot effectively compress highlevel information. Thus, Transformer-XL in practice needs a massive memory size to perform well, and spends huge computation in using its memory. Compressive Transformer (Rae et al., 2020) improves upon Transformer-XL by further compressing its memories into fewer vectors via a compression network. However, as mentioned in the papers, both Transformer-XL and Compressive Transformer discard the information from the distant past, which causes a theoretical maximum tempo-\nral range given the fixed memory size. Inspired by the previous external memory networks, we propose Memformer, which incorporates a fixed-size external dynamic memory combined with the recent Transformer architecture. Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information. By utilizing recurrence and a fixedsize memory, our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity. As the traditional backpropagation through time (BPTT) has an unaffordable memory cost in our model, we introduce a new optimization scheme, memory replay backpropagation (MRBP), to significantly reduce the memory cost in training recurrent neural networks with large size of memory representations. We evaluate Memformer on the autoregressive image generation and language modeling task. Experimental results show that Memformer performs on par with Transformer and Transformer XL with large memory size, while being much more efficient in terms of computation speed and memory space consumption. We also conduct an analysis showing that Memformer can retain information for an extended period. ## 2 Related Work\n\nThis section introduces some recent research directions that aim to alleviate the quadratic cost of selfattention. Moreover, we analyze their assumptions and limitations under the autoregressive setting to provide a broader view of these models. ### 2.1 Sparse Attention\n\nOne influential direction is to replace the full selfattention with sparse attention patterns to speed up the computation. Child et al. (2019) proposed Sparse Transformer, using a block sparse attention pattern to reduce the computation complexity to $\\mathcal{O}(N \\sqrt{N})$. Later, Longformer (Beltagy et al., 2020) and Big Bird (Zaheer et al., 2020) further explored this direction and proposed an even more sparse attention pattern to reduce the cost to $\\mathcal{O}(N)$. They introduced global tokens to encode the information from the entire sequence and kept the selfattention to the closest $k$ tokens and the global tokens to achieve linear complexity. Although linear sparse attention's theoretical soundness is proven for bidirectional encoders, it does not hold for the decoder. The main reason is that the global tokens cannot leak information to the future tokens in the autoregressive setting, where all the tokens can only see their previous tokens. Thus, linear sparse attention cannot guarantee a token to see its all past tokens. Only Sparse Transformer here with $\\mathcal{O}(N \\sqrt{N})$ complexity can theoretically cover all the past tokens for the sequence generation. ### 2.2 Linear Attention\n\nAnother direction is focusing on improving the softmax operation in the self-attention. Linformer (Wang et al., 2020) reduced the complexity to $\\mathcal{O}(N)$ by projecting the entire sequence to a constant size of keys and values, but this method has not been applied to autoregressive decoding. Performer (Choromanski et al., 2020) and Linear Transformer (Katharopoulos et al., 2020) used a linear dot-product of kernel feature maps to replace softmax. However, for Linear Transformer under the autoregressive setting, it needs to compute the cumulative summation to aggregate the history information. This assumption is too strong if the input sequence is long and the length is not fixed. After thousands of steps, the numerical values can become very large due to the summation, causing overflow and gradient instability. ### 2.3 Recurrence and Memory\n\nApplying recurrence and memory to Transformers is an orthogonal direction comparing to the efficient attention approaches. If the memory size is constant, recurrence enables the model to have constant memory complexity during inference. There are mainly two works exploring this direction. Transformer-XL (Dai et al., 2019) used relative positional encoding and consisted of a segmentlevel recurrence mechanism to encode beyond a fixed-length context. Compressive Transformer (Rae et al., 2020) extended from Transformer-XL by further compressing the previous cached hidden states to achieve a longer context. However, using past hidden states as memory would cause a theoretical maximum temporal range of context, meaning that a token is not guaranteed to see all the past tokens. Thus, in practice, Transformer-XL and Compressive Transformer need huge memory size to achieve good performance. ![](https://cdn.mathpix.com/cropped/2024_09_12_a5b0bfb29582fd0f4766g-03.jpg?height=915&width=717&top_left_y=225&top_left_x=224)\n\nFigure 1: Memformer overall architecture for the encoder (left) and decoder (right). Transformer encoder is responsible to interact with the memory. Sequence modeling is achieved by predicting the next segment conditioned to the current segment and memory. ### 2.3.1 Dynamic Memorization\n\nWithin the scope of memory networks, there are dynamic memorization techniques. Different from Transformer-XL which stores the token-level history representations as memory, dynamic memorization does not have a theoretical upper bound for the temporal range. Neural Turing Machine (NTM) (Graves et al., 2014) and Differential Neural Computer (DNC) (Graves et al., 2016) are two early models that can control external memory resources to achieve long-lasting memory.\n```\n\n#### 2. Memorizing Transformers (Avg. Score: 0.96)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 13/26 (Score: 0.96)*\n\n```\nIn ICLR, 2021. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. In ICLR, 2020. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost.\n```\n\n#### 3. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.96)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 27/41 (Score: 0.96)*\n\n```\nJayakumar, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. ArXiv, abs/1911.05507, 2019. URL https://api. semanticscholar.org/CorpusID:207930593. [60] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost.\n```\n\n#### 4. Landmark Attention: Random-Access Infinite Context Length for Transformers (Avg. Score: 0.96)\n\n*Amirkeivan Mohtashami, Martin Jaggi*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 86  (*Influential: 13*)\n\n**TL;DR:** This paper uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism.\n\n**Abstract:** While Transformers have shown remarkable success in natural language processing, their attention mechanism's large memory requirements have limited their ability to handle longer contexts. Prior approaches, such as recurrent memory or retrieval-based augmentation, have either compromised the random-access flexibility of attention (i.e., the capability to select any token in the entire context) or relied on separate mechanisms for relevant context retrieval, which may not be compatible with the model's attention. In this paper, we present a novel approach that allows access to the complete context while retaining random-access flexibility, closely resembling running attention on the entire context. Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism instead of by relying on a separate mechanism. Our approach seamlessly integrates with specialized data structures and the system's memory hierarchy, enabling processing of arbitrarily long context lengths. We demonstrate that our method can obtain comparable performance with Transformer-XL while significantly reducing the number of retrieved tokens in each step. Finally, we show that fine-tuning LLaMA 7B with our method successfully extends its context length capacity to over 32k tokens, allowing for inference at the context lengths of GPT-4. We release the implementation of landmark attention and the code to reproduce our experiments at https://github.com/epfml/landmark-attention/.\n\n##### *Relevant Chunk: No. 22/32 (Score: 0.96)*\n\n```\nJayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. URL https://openreview.net/forum?id=Sy1KikSYDH. [30] Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full Attention Transformer with Sparse Computation Cost, October 2021.\n```\n\n#### 5. Block-Recurrent Transformers (Avg. Score: 0.96)\n\n*DeLesley S. Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, Behnam Neyshabur*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 66  (*Influential: 9*)\n\n**TL;DR:** N/A\n\n**Abstract:** We introduce the Block-Recurrent Transformer, which applies a transformer layer in a recurrent fashion along a sequence, and has linear complexity with respect to sequence length. Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block in order to make efficient use of accelerator hardware. The cell itself is strikingly simple. It is merely a transformer layer: it uses self-attention and cross-attention to efficiently compute a recurrent function over a large set of state vectors and tokens. Our design was inspired in part by LSTM cells, and it uses LSTM-style gates, but it scales the typical LSTM cell up by several orders of magnitude. Our implementation of recurrence has the same cost in both computation time and parameter count as a conventional transformer layer, but offers dramatically improved perplexity in language modeling tasks over very long sequences. Our model out-performs a long-range Transformer XL baseline by a wide margin, while running twice as fast. We demonstrate its effectiveness on PG19 (books), arXiv papers, and GitHub source code. Our code has been released as open source.\n\n##### *Relevant Chunk: No. 11/53 (Score: 0.96)*\n\n```\nZong, F. Xia, W. Li, and R. Navigli, eds.), 2021. [13] H. Ren, H. Dai, Z. Dai, M. Yang, J. Leskovec, D. Schuurmans, and B. Dai, \"Combiner: Full attention transformer with sparse computation cost,\" in Advances in Neural Information Processing Systems (A. Beygelzimer, Y. Dauphin, P. Liang, and J. W. Vaughan, eds.), 2021. [14] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, \"Linformer: Self-attention with linear complexity,\" CoRR, vol. abs/2006.04768, 2020. [15] J. W. Rae, A. Potapenko, S. M. Jayakumar, C. Hillier, and T. P. Lillicrap, \"Compressive transformers for long-range sequence modelling,\" in ICLR, 2020. [16] Z. Dai, G. Lai, Y. Yang, and Q. Le, \"Funnel-transformer: Filtering out sequential redundancy for efficient language processing,\" in NeurIPS, 2020.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive computation transformers, memory compression transformers, sparse attention transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation\n\n*From Search Query: adaptive computation transformers*\n\n*Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, L. Lakshmanan, A. Awadallah, S\u00e9bastien Bubeck, Jianfeng Gao*\n\n**TL;DR:** AutoMoE is developed -- a framework for designing heterogeneous MoE's under computational constraints and leverages Neural Architecture Search to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, on aggregate over benchmark datasets for NMT.\n\n**Abstract:** Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Adaptive Transformers for Learning Multimodal Representations\n\n*From Search Query: adaptive computation transformers*\n\n*Prajjwal Bhargava*\n\n**TL;DR:** This work study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks, and shows that these approaches can help to learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\n\n**Abstract:** The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 3. Depth-Adaptive Transformer\n\n*From Search Query: adaptive computation transformers*\n\n*Maha Elbayad, Jiatao Gu, Edouard Grave, Michael Auli*\n\n**TL;DR:** This paper trains Transformer models which can make output predictions at different stages of the network and investigates different ways to predict how much computation is required for a particular sequence.\n\n**Abstract:** State of the art sequence-to-sequence models for large scale tasks perform a fixed number of computations for each input sequence regardless of whether it is easy or hard to process. In this paper, we train Transformer models which can make output predictions at different stages of the network and we investigate different ways to predict how much computation is required for a particular sequence. Unlike dynamic computation in Universal Transformers, which applies the same set of layers iteratively, we apply different layers at every step to adjust both the amount of computation as well as the model capacity. On IWSLT German-English translation our approach matches the accuracy of a well tuned baseline Transformer while using less than a quarter of the decoder layers.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 164  (*Influential: 16*)\n\n#### 4. Context Compression for Auto-regressive Transformers with Sentinel Tokens\n\n*From Search Query: memory compression transformers*\n\n*Siyu Ren, Qi Jia, Kenny Q. Zhu*\n\n**TL;DR:** This work proposes a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context.\n\n**Abstract:** The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 5. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: memory compression transformers*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 6. HomoDistil: Homotopic Task-Agnostic Distillation of Pre-trained Transformers\n\n*From Search Query: memory compression transformers*\n\n*Chen Liang, Haoming Jiang, Zheng Li, Xianfeng Tang, Bin Yin, Tuo Zhao*\n\n**TL;DR:** The proposed Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning that maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer.\n\n**Abstract:** Knowledge distillation has been shown to be a powerful model compression approach to facilitate the deployment of pre-trained language models in practice. This paper focuses on task-agnostic distillation. It produces a compact pre-trained model that can be easily fine-tuned on various tasks with small computational costs and memory footprints. Despite the practical benefits, task-agnostic distillation is challenging. Since the teacher model has a significantly larger capacity and stronger representation power than the student model, it is very difficult for the student to produce predictions that match the teacher's over a massive amount of open-domain training data. Such a large prediction discrepancy often diminishes the benefits of knowledge distillation. To address this challenge, we propose Homotopic Distillation (HomoDistil), a novel task-agnostic distillation approach equipped with iterative pruning. Specifically, we initialize the student model from the teacher model, and iteratively prune the student's neurons until the target width is reached. Such an approach maintains a small discrepancy between the teacher's and student's predictions throughout the distillation process, which ensures the effectiveness of knowledge transfer. Extensive experiments demonstrate that HomoDistil achieves significant improvements on existing baselines.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 0*)\n\n#### 7. Exphormer: Sparse Transformers for Graphs\n\n*From Search Query: sparse attention transformers*\n\n*Hamed Shirzad, A. Velingker, B. Venkatachalam, Danica J. Sutherland, A. Sinop*\n\n**TL;DR:** It is shown that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets.\n\n**Abstract:** Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that Exphormer can scale to datasets on larger graphs than shown in previous graph transformer architectures. Code can be found at \\url{https://github.com/hamed1375/Exphormer}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 16*)\n\n#### 8. White-Box Transformers via Sparse Rate Reduction\n\n*From Search Query: sparse attention transformers*\n\n*Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, B. Haeffele, Y. Ma*\n\n**TL;DR:** This paper contends that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces, and shows that the standard transformer block can be derived from alternating optimization on complementary parts of this objective.\n\n**Abstract:** In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at \\url{https://github.com/Ma-Lab-Berkeley/CRATE}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 50  (*Influential: 4*)\n\n#### 9. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: sparse attention transformers*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n### 2 related papers from ArXiv\n\n#### 1. MATE: Multi-view Attention for Table Transformer Efficiency\n\n*From Search Query: sparse attention transformers*\n\n*Julian Martin Eisenschlos, Maharshi Gor, Thomas M\u00fcller, William W. Cohen*\n\n**Abstract:** This work presents a sparse-attention Transformer architecture for modeling\ndocuments that contain large tables. Tables are ubiquitous on the web, and are\nrich in information. However, more than 20% of relational tables on the web\nhave 20 or more rows (Cafarella et al., 2008), and these large tables present a\nchallenge for current Transformer models, which are typically limited to 512\ntokens. Here we propose MATE, a novel Transformer architecture designed to\nmodel the structure of web tables. MATE uses sparse attention in a way that\nallows heads to efficiently attend to either rows or columns in a table. This\narchitecture scales linearly with respect to speed and memory, and can handle\ndocuments containing more than 8000 tokens with current accelerators. MATE also\nhas a more appropriate inductive bias for tabular data, and sets a new\nstate-of-the-art for three table reasoning datasets. For HybridQA (Chen et al.,\n2020b), a dataset that involves large documents containing tables, we improve\nthe best prior result by 19 points.\n\n**Published:** 2021-09-09T14:39:30Z  (*Updated: 2021-09-09T14:39:30Z*)\n\n\n\n#### 2. An Exploration of Hierarchical Attention Transformers for Efficient Long\n  Document Classification\n\n*From Search Query: sparse attention transformers*\n\n*Ilias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodromos Malakasiotis, Desmond Elliott*\n\n**Abstract:** Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.\n\n**Published:** 2022-10-11T15:17:56Z  (*Updated: 2022-10-11T15:17:56Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Distill-SODA: Distilling Self-Supervised Vision Transformer for Source-Free Open-Set Domain Adaptation in Computational Pathology\n\n*From Search Query: adaptive computation transformers*\n\n*Behzad Bozorgtabar, Jean-Philippe Thiran, Devavrat Tomar, Guillaume Vray*\n\n**Abstract:** Developing computational pathology models is essential for reducing manual tissue typing from whole slide images, transferring knowledge from the source domain to an unlabeled, shifted target domain, and identifying unseen categories. We propose a practical setting by addressing the above-mentioned challenges in one fell swoop, i.e., source-free open-set domain adaptation. Our methodology focuses on adapting a pre-trained source model to an unlabeled target dataset and encompasses both closed-set and open-set classes. Beyond addressing the semantic shift of unknown classes, our framework also deals with a covariate shift, which manifests as variations in color appearance between source and target tissue samples. Our method hinges on distilling knowledge from a self-supervised vision transformer (ViT), drawing guidance from either robustly pre-trained transformer models or histopathology datasets, including those from the target domain. In pursuit of this, we introduce a novel style-based adversarial data augmentation, serving as hard positives for self-training a ViT, resulting in highly contextualized embeddings. Following this, we cluster semantically akin target images, with the source model offering weak pseudo-labels, albeit with uncertain confidence. To enhance this process, we present the closed-set affinity score (CSAS), aiming to correct the confidence levels of these pseudo-labels and to calculate weighted class prototypes within the contextualized embedding space. Our approach establishes itself as state-of-the-art across three public histopathological datasets for colorectal cancer assessment. Notably, our self-training method seamlessly integrates with open-set detection methods, resulting in enhanced performance in both closed-set and open-set recognition tasks.\n\n**Published:** 2023-07-10\n\n\n\n#### 2. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n*From Search Query: adaptive computation transformers*\n\n*Baining Guo, Stephen Lin, Zheng Zhang, Yixuan Wei, Han Hu, Yue Cao, Yutong Lin, Ze Liu*\n\n**Abstract:** This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-25\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: memory compression transformers*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. Memory AMP\n\n*From Search Query: memory compression transformers*\n\n*Brian M. Kurkoski, Shunqi Huang, Lei Liu*\n\n**Abstract:** Approximate message passing (AMP) is a low-cost iterative parameter-estimation technique for certain high-dimensional linear systems with non-Gaussian distributions. AMP only applies to independent identically distributed (IID) transform matrices, but may become unreliable (e.g., perform poorly or even diverge) for other matrix ensembles, especially for ill-conditioned ones. To solve this issue, orthogonal/vector AMP (OAMP/VAMP) was proposed for general right-unitarily-invariant matrices. However, the Bayes-optimal OAMP/VAMP (BO-OAMP/VAMP) requires a high-complexity linear minimum mean square error (MMSE) estimator. This prevents OAMP/VAMP from being used in large-scale systems. To address the drawbacks of AMP and BO-OAMP/VAMP, this paper offers a memory AMP (MAMP) framework based on the orthogonality principle, which ensures that estimation errors in MAMP are asymptotically IID Gaussian. To realize the required orthogonality for MAMP, we provide an orthogonalization procedure for the local memory estimators. In addition, we propose a Bayes-optimal MAMP (BO-MAMP), in which a long-memory matched filter is used for interference suppression. The complexity of BO-MAMP is comparable to AMP. To asymptotically characterize the performance of BO-MAMP, a state evolution is derived. The relaxation parameters and damping vector in BO-MAMP are optimized based on state evolution. Most crucially, the state evolution of the optimized BO-MAMP converges to the same fixed point as that of the high-complexity BO-OAMP/VAMP for all right-unitarily-invariant matrices, and achieves the Bayes optimal MSE predicted by the replica method if its state evolution has a unique fixed point. Finally, simulations are provided to verify the theoretical results' validity and accuracy.\n\n**Published:** 2020-12-20\n\n\n\n#### 5. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: sparse attention transformers*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 6. Efficient Content-Based Sparse Attention with Routing Transformers\n\n*From Search Query: sparse attention transformers*\n\n*Mohammad Saffar, David Grangier, Ashish Vaswani, Aurko Roy*\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\\left(n^{1.5}d\\right)$ from $O\\left(n^2d\\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.\n\n**Published:** 2020-03-12\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goal of designing a novel language model block that combines the strengths of **HieraNorm-AttnGPT** and **AdaptiveTTT**, and outperforms current state-of-the-art models, here are some key points and recommendations based on the provided analysis and references:\n\n### Key Components and Strategies\n\n#### 1. **Hierarchical Adaptive Multi-Head Attention**\n- **Integration with Adaptive Scaling**: The researcher plans to integrate hierarchical attention with adaptive attention scaling. This can be achieved by modifying the hierarchical attention mechanism to include a complexity estimation of the input and computing a scaling factor \\( \\alpha(x) \\) similar to the approach in **Sukhbaatar & Fergus (2019)**.\n  - **Reference**: Sukhbaatar, S., & Fergus, R. (2019). *Adaptive Attention Span in Transformers.*\n\n#### 2. **Selective Memory Compression**\n- **Compressive Memory**: Implementing a compressive memory mechanism, as seen in **Infini-attention** and **Compressive Transformers**, can help manage long sequences efficiently.\n  - **References**:\n    - Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). *Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.*\n    - Rae, J., Potapenko, A., Jayakumar, S. M., & Lillicrap, T. (2020). *Compressive Transformers for Long-Range Sequence Modelling.*\n  - **Dynamic Memory Compression (DMC)**: Utilize online key-value cache compression during inference, as proposed by **Nawrot et al. (2024)**.\n    - **Reference**: Nawrot, P., et al. (2024). *Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.*\n\n#### 3. **Feedback Attention Mechanism**\n- Incorporate a feedback loop into the attention mechanism, inspired by **TransformerFAM**, to enable the model to attend to its own latent representations.\n  - **Reference**: Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024). *TransformerFAM: Feedback Attention is Working Memory.*\n\n#### 4. **Efficient Hierarchical Attention Implementations**\n- **Sparse Attention Techniques**: Use sparse factorizations of the attention matrix, as in **Sparse Transformers**, to reduce computational complexity.\n  - **Reference**: Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). *Generating Long Sequences with Sparse Transformers.*\n- **Stack Attention**: Incorporate stack attention to enhance the model's ability to capture hierarchical patterns.\n  - **Reference**: DuSell, B., & Chiang, D. (2023). *Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns.*\n\n#### 5. **Advanced Memory Compression and Parameter Reduction**\n- **Vector Quantization**: Consider using vector-quantized keys and caching mechanisms from **Transformer-VQ**.\n  - **Reference**: Lingle, L. D. (2023). *Linear-Time Transformers via Vector Quantization.*\n- **Parameter Sharing and Reduction**: Apply parameter-reduction techniques from **ALBERT** to reduce memory consumption.\n  - **Reference**: Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020). *ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.*\n\n### Future Search Plan\n\n#### Adaptive Computation Techniques\n- Explore recent studies on adaptive computation in Transformers, such as dynamic adjustment of computational effort based on input complexity.\n  - **Keywords**: `adaptive computation transformers`, `dynamic computational effort`, `efficient attention mechanisms`\n  - **Reference**: Graves, A. (2016). *Adaptive Computation Time for Recurrent Neural Networks.* (Though focused on RNNs, the concept can be adapted to Transformers.)\n\n#### Advanced Memory Compression Methods\n- Examine state-of-the-art memory compression techniques such as vector quantization, pruning, and quantization.\n  - **Keywords**: `memory compression transformers`, `vector quantization attention`, `efficient key-value caching`\n  - **References**:\n    - Nawrot, P., et al. (2024). *Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.*\n    - Lingle, L. D. (2023). *Linear-Time Transformers via Vector Quantization.*\n\n#### Efficient Sparse Attention Mechanisms\n- Study efficient sparse attention methods to improve scalability.\n  - **Keywords**: `sparse attention transformers`, `efficient long-range attention`, `memory-efficient transformers`\n  - **References**:\n    - Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). *Generating Long Sequences with Sparse Transformers.*\n    - DuSell, B., & Chiang, D. (2023). *Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns.*\n\n### Detailed Implementations and Analyses\n\nTo find detailed implementations and analyses of Transformers integrating compressive memory and adaptive computation, you can search for the following:\n\n- **Internal Library Search**:\n  - Look for implementations of **Infini-attention**, **Compressive Transformers**, and **Dynamic Memory Compression** in open-source libraries like TensorFlow, PyTorch, or Hugging Face's Transformers.\n  - Search for research papers and code repositories on GitHub or arXiv that focus on integrating these techniques into Transformer architectures.\n\n- **External Resources**:\n  - Check the Hugging Face blog and repository for implementations of advanced Transformer models.\n  - Look into the TensorFlow and PyTorch communities for tutorials and implementations related to adaptive computation and memory compression in Transformers.\n\n### Summary of Useful Information\n\n#### Key Papers and Techniques\n- **Infini-attention**: Munkhdalai, T., Faruqui, M., & Gopal, S. (2024)\n- **Adaptive Attention Span**: Sukhbaatar, S., & Fergus, R. (2019)\n- **Compressive Transformers**: Rae, J., Potapenko, A., Jayakumar, S. M., & Lillicrap, T. (2020)\n- **Dynamic Memory Compression**: Nawrot, P., et al. (2024)\n- **Sparse Transformers**: Child, R., Gray, S., Radford, A., & Sutskever, I. (2019)\n- **Stack Attention**: DuSell, B., & Chiang, D. (2023)\n- **Transformer-VQ**: Lingle, L. D. (2023)\n- **ALBERT**: Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020)\n\n#### Search Keywords\n- `adaptive computation transformers`\n- `dynamic computational effort`\n- `efficient attention mechanisms`\n- `memory compression transformers`\n- `vector quantization attention`\n- `efficient key-value caching`\n- `sparse attention transformers`\n- `efficient long-range attention`\n- `memory-efficient transformers`\n\nBy focusing on these key components, techniques, and search keywords, the researcher can gather the necessary information to design a novel LM block that integrates hierarchical adaptive multi-head attention, adaptive attention scaling, and selective memory compression, thereby achieving improved efficiency, scalability, and performance.\n",
            "analysis": "---\n\n### **Analysis Note**\n\n#### 1. **Summary of Analysis**\n\nI am designing a novel language model block that combines the strengths of **HieraNorm-AttnGPT** (Parent 1) and **AdaptiveTTT** (Parent 2). The objective is to integrate **hierarchical adaptive multi-head attention** with **adaptive attention scaling** and **selective memory compression** within a Transformer architecture. This integration aims to:\n\n- **Capture Multi-Scale Dependencies**: Utilize hierarchical attention to model both local and global contextual information effectively.\n- **Adapt Computational Resources**: Introduce adaptive attention scaling based on input complexity, allowing the model to allocate computational resources dynamically.\n- **Manage State Information Efficiently**: Implement selective memory compression to handle long sequences without excessive memory consumption.\n\nBy reusing and enhancing the existing GAUs from both parents, the new design aspires to achieve improved efficiency, scalability, and performance on various NLP tasks, particularly those involving long sequences.\n\n#### 2. **Useful References with Excerpts**\n\n1. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024).** *Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.* arXiv preprint arXiv:XXXX.XXXXX.\n\n   *\"A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"*\n\n   *\"We store old KV states of the attention in the compressive memory, instead of discarding them like in the standard attention mechanism.\"*\n\n2. **Sukhbaatar, S., & Fergus, R. (2019).** *Adaptive Attention Span in Transformers.* Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\n\n   *\"We propose a method that allows the model to learn the optimal attention span for each head in a multi-head attention layer. This adaptive attention span enables the model to focus on relevant parts of the input while reducing computational overhead.\"*\n\n3. **Rae, J., Potapenko, A., Jayakumar, S. M., & Lillicrap, T. (2020).** *Compressive Transformers for Long-Range Sequence Modelling.* International Conference on Learning Representations.\n\n   *\"We augment the Transformer with a compression mechanism, which results in a significant improvement in memory usage and enables the model to handle longer sequences effectively.\"*\n\n4. **Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020).** *ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.* International Conference on Learning Representations.\n\n   *\"We introduce parameter-reduction techniques to lower memory consumption and increase the training speed of BERT models.\"*\n\n5. **Child, R., Gray, S., Radford, A., & Sutskever, I. (2019).** *Generating Long Sequences with Sparse Transformers.* arXiv preprint arXiv:1904.10509.\n\n   *\"We introduce sparse factorizations of the attention matrix that can handle sequences with thousands of tokens, reducing the complexity from quadratic to linear.\"*\n\n6. **Graves, A. (2016).** *Adaptive Computation Time for Recurrent Neural Networks.* arXiv preprint arXiv:1603.08983.\n\n   *\"We introduce a mechanism that allows recurrent neural networks to dynamically adjust the computational effort applied to different inputs.\"*\n\n7. **DuSell, B., & Chiang, D. (2023).** *Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns.* International Conference on Learning Representations.\n\n   *\"We propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs).\"*\n\n8. **Nawrot, P., et al. (2024).** *Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.* International Conference on Machine Learning.\n\n   *\"We propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention and key-value eviction policies.\"*\n\n9. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024).** *TransformerFAM: Feedback Attention is Working Memory.* arXiv preprint arXiv:XXXX.XXXXX.\n\n   *\"We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences.\"*\n\n10. **Lingle, L. D. (2023).** *Linear-Time Transformers via Vector Quantization.* arXiv preprint arXiv:XXXX.XXXXX.\n\n    *\"We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism.\"*\n\n#### 3. **Key Insights and Detailed Analysis**\n\n- **Integrating Hierarchical Attention with Adaptive Scaling**:\n  \n  - **Adaptive Scaling in Hierarchical Attention**: By incorporating an adaptive scaling factor **\u03b1(x)** into the hierarchical attention mechanism from **HieraNorm-AttnGPT**, the model can dynamically adjust attention computations based on input complexity, similar to the adaptive attention span in **Sukhbaatar & Fergus (2019)**.\n  \n  - **Implementation Strategy**:\n    - Modify the **HierarchicalAdaptiveAttention** GAU to include complexity estimation of the input and compute a scaling factor **\u03b1(x)**.\n    - Adjust the attention scores using **\u03b1(x)** to allocate computational resources efficiently.\n\n- **Selective Memory Compression**:\n  \n  - **Compressive Memory Integration**: Drawing from **Infini-attention** and **Compressive Transformers**, introduce a compressive memory to store and retrieve key-value states, allowing the model to handle long sequences with bounded memory.\n  \n  - **Dynamic Memory Compression (DMC)**: Implement online key-value cache compression during inference, as proposed by **Nawrot et al. (2024)**, to reduce memory usage without significant performance loss.\n\n- **Feedback Attention as Working Memory**:\n  \n  - **Feedback Loop Mechanism**: Incorporate a feedback loop into the attention mechanism, inspired by **TransformerFAM**, enabling the model to attend to its own latent representations and process indefinitely long sequences effectively.\n\n- **Dynamic Layer Normalization**:\n  \n  - Retain the **Dynamic Layer Normalization (DLN)** from **HieraNorm-AttnGPT** to ensure stable gradient flow and adaptability to diverse contexts.\n  \n  - Explore the interaction between DLN and adaptive attention scaling to maintain training stability.\n\n- **Efficient Hierarchical Attention Implementations**:\n  \n  - **Sparse Attention Techniques**: Utilize sparse factorizations of the attention matrix, as in **Sparse Transformers (Child et al., 2019)**, to reduce computational complexity and handle longer sequences efficiently.\n  \n  - **Stack Attention for Hierarchical Patterns**: Incorporate ideas from **DuSell & Chiang (2023)** to enhance the hierarchical attention mechanism's ability to capture nested and hierarchical structures in language.\n\n- **Advanced Memory Compression Methods**:\n  \n  - **Vector Quantization**: Consider vector-quantized keys and caching mechanisms from **Transformer-VQ** to further compress memory while maintaining performance.\n  \n  - **Parameter Sharing and Reduction**: Apply parameter-reduction techniques from **ALBERT** to reduce memory consumption, which can complement selective memory compression.\n\n#### 4. **Future Search Plan**\n\nTo further refine the design and ensure robustness, I plan to:\n\n- **Investigate Advanced Adaptive Computation Techniques**:\n\n  - Explore recent studies on adaptive computation in Transformers that allow dynamic adjustment of computational effort based on input complexity.\n  \n  - **Keywords**: `adaptive computation transformers`, `dynamic computational effort`, `efficient attention mechanisms`\n\n- **Research Advanced Memory Compression Methods**:\n\n  - Examine state-of-the-art memory compression techniques such as vector quantization, pruning, and quantization for integration into the model.\n  \n  - **Keywords**: `memory compression transformers`, `vector quantization attention`, `efficient key-value caching`\n\n- **Examine Efficient Sparse Attention Mechanisms**:\n\n  - Study efficient sparse attention methods that can be integrated into the hierarchical attention mechanism to improve scalability.\n  \n  - **Keywords**: `sparse attention transformers`, `efficient long-range attention`, `memory-efficient transformers`\n\n- **Internal Library Search**:\n\n  - **Description**: Seek detailed implementations and analyses of Transformers integrating compressive memory and adaptive computation for efficient long-sequence processing.\n\n#### 5. **References**\n\n1. Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). *Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.* arXiv preprint arXiv:XXXX.XXXXX.\n\n2. Sukhbaatar, S., & Fergus, R. (2019). *Adaptive Attention Span in Transformers.* In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\n\n3. Rae, J., Potapenko, A., Jayakumar, S. M., & Lillicrap, T. (2020). *Compressive Transformers for Long-Range Sequence Modelling.* International Conference on Learning Representations.\n\n4. Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., & Soricut, R. (2020). *ALBERT: A Lite BERT for Self-supervised Learning of Language Representations.* International Conference on Learning Representations.\n\n5. Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). *Generating Long Sequences with Sparse Transformers.* arXiv preprint arXiv:1904.10509.\n\n6. Graves, A. (2016). *Adaptive Computation Time for Recurrent Neural Networks.* arXiv preprint arXiv:1603.08983.\n\n7. DuSell, B., & Chiang, D. (2023). *Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns.* International Conference on Learning Representations.\n\n8. Nawrot, P., et al. (2024). *Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.* International Conference on Machine Learning.\n\n9. Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024). *TransformerFAM: Feedback Attention is Working Memory.* arXiv preprint arXiv:XXXX.XXXXX.\n\n10. Lingle, L. D. (2023). *Linear-Time Transformers via Vector Quantization.* arXiv preprint arXiv:XXXX.XXXXX.\n\n---\n\n```keywords adaptive computation transformers```\n\n```keywords memory compression transformers```\n\n```keywords sparse attention transformers```\n\n```description Detailed implementations of Transformers integrating compressive memory and adaptive computation for efficient long-sequence processing```\n\n---"
        },
        {
            "ready": false,
            "query": [
                "efficient compressive transformers",
                "hierarchical memory networks transformers",
                "adaptive computation time transformers"
            ],
            "detail": "Practical implementation strategies for integrating adaptive attention scaling and compressive memory in hierarchical Transformer architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nPractical implementation strategies for integrating adaptive attention scaling and compressive memory in hierarchical Transformer architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Recurrent Memory Transformer (Avg. Score: 0.97)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.97)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.90)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 8/30 (Score: 0.90)*\n\n```\nAs context windows become larger by the day in state-of-the-art models, we expect these gains to become even more relevant. Interpretability. Fig. 8 provides insights into the interpretability aspect of the model's decisionmaking process. It is observed that token removal predominantly occurs when encountering stop words (punctuation), which aligns with the intuition that local information within a sentence becomes less relevant after its completion. Furthermore, it is worth noting that layers at varying depths exhibit distinct behaviors, reinforcing our rationale for dissecting token removal decisions across depth. The variance in sparsity distribution across different depths indicates the necessity of conducting additional interpretability research to obtain valuable insights in the interactions of the tokens within the model. We provide more insights towards this direction in the Appendix C. ## 5 Discussion\n\nWe proposed Adaptively Sparse Attention, a novel approach to dynamically prune the context in decoder-only Transformer architectures. Our results indicate that our technique performs favourably compared to competitive baselines in terms of the ratio between perplexity and sparsity of the attention weights. Remarkably our approach also significantly reduces the computational and memory requirements without affecting its final performance. We practically showcase these benefits achieving more than double the throughput at cases. Adaptively sparse attention comes with two additional practical advantages: first, it can be seamlessly integrated into existing pre-trained models via a cheap fine-tuning step; second, it represents an orthogonal contribution to the burgeoning research line aimed at increasing the level of efficiency of modern LLMs. As such, we envision its combination with existing techniques like weight pruning and quantization to be a promising avenue for future research. ## References\n\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179-190, 1983. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 3. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.82)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 2/24 (Score: 0.98)*\n\n```\nA key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1 M sequence length passkey context block retrieval and 500 K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs. ## 1 Introduction\n\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based LLMs (Brown et al., 2020; Touvron et al., 2023, Anil et al., 2023; Groeneveld et al. 2024) have a constrained context-dependent memory, due to the nature of the attention mechanism. The attention mechanism in Transformers exhibits quadratic complexity in both memory footprint and computation time. For example, the attention Key-Value (KV) states have 3TB memory footprint for a 500B model with batch size 512 and context length 2048 (Pope et al. 2023). Indeed, scaling LLMs to longer sequences (i.e. 1 M tokens) is challenging with the standard Transformer architectures and serving longer and longer context models becomes costly financially. Compressive memory systems promise to be more scalable and efficient than the attention mechanism for extremely long sequences (Kanerva, 1988; Munkhdalai et al., 2019). Instead of using an array that grows with the input sequence length, a compressive memory primarily maintains a fixed number of parameters to store and recall information with a bounded storage and computation costs. In the compressive memory, new information is added to the memory by changing its parameters with an objective that this information can be recovered back later on. However, the LLMs in their current state have yet to see an effective, practical compressive memory technique that balances simplicity along with quality. In this work, we introduce a novel approach that enables Transformer LLMs to effectively process infinitely long inputs with bounded memory footprint and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention (Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention mechanism (Bahdanau et al., 2014, Vaswani et al. 2017) and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. Such a subtle but critical modification to the Transformer attention layer enables a natural extension of existing LLMs to infinitely long contexts via continual pre-training and finetuning. Our Infini-attention reuses all the key, value and query states of the standard attention computation for long-term memory consolidation and retrieval. We store old KV states of the attention in the compressive memory, instead of discarding them like in the standard attention mechanism. We then retrieve the values from the memory by using the attention query states when processing subsequent sequences. To compute the final contextual output, the Infini-attention aggregates the long-term memory-retrieved values and the local attention contexts. In our experiments, we show that our approach outperforms baseline models on longcontext language modeling benchmarks while having $114 x$ comprehension ratio in terms of memory size. The model achieves even better perplexity when trained with 100 K sequence length. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval task when injected with Infini-attention. Finally, we show that a 8B model with Infiniattention reaches a new SOTA result on a 500 K length book summarization task after continual pre-training and task fine-tuning. In summary, our work makes the following contributions:\n\n1. We introduce a practical and yet powerful attention mechanism - Infini-attention with long-term compressive memory and local causal attention for efficiently modeling both long and short-range contextual dependencies. 2. Infini-attention introduces minimal change to the standard scaled dot-product attention and supports plug-and-play continual pre-training and long-context adaptation by design. 3. Our approach enables Transformer LLMs to scale to infinitely long context with a bounded memory and compute resource by processing extremely long inputs in a streaming fashion. ## 2 Background\n\nRecurrent Neural Networks (RNNs) process a single token $x_{t}$ at each step $t$ and computes a recurrent hidden state $h_{t}$ to represent an entire input sequence Hochreiter \\& Schmidhuber, 1997, Maass et al. 2002):\n\n$$\nh_{t}=R N N\\left(x_{t}, h_{t-1}\\right)\n$$\n\nThe RNN computation is very efficient since the model maintains only a fixed-size vector $h_{t}$ for input sequence. However, for processing long sequences it becomes difficult to store entire contextual information into a single fixed-size vector and this limitation had implications on RNNs utility in certain tasks (Kaiser \\& Sutskever, 2015). To address the limitation, people extended the standard RNNs with an external memory component that can be read from and written to. One such an instance is Metalearned Neural Memory (MNM) (Munkhdalai et al., 2019):\n\n$$\nh_{t}, \\theta_{t}=M N M\\left(x_{t}, h_{t-1}, \\theta_{t-1}\\right)\n$$\n\nMNM learns an additional memory state $\\theta$ parameterized by a feed-forward neural network (FFN) and uses query, key and value vectors (QKV) to interact with the memory, similar to the attention mechanism. To store information, it modifies the parameters of the FFN by using the key vectors as input and the value vectors for the target, and to read memory\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbb8cf178811d16c1412g-03.jpg?height=663&width=1391&top_left_y=292&top_left_x=367)\n\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL (bottom) discards old contexts since it caches the KV states for the last segment only. entries, it forward-passes the query vectors through the memory FFN and retrieves its corresponding value. Like RNNs, the memory state is still bounded in MNM. Unlike the RNNs, the attention mechanism however doesn't maintain a recurrent state and only performs a feed-forward computation on input sequence segment $X_{s}$ :\n\n$$\nO_{s}=\\operatorname{attention}\\left(X_{s}\\right)\n$$\n\nThe attention output $O_{s}$ is simply passed to the next layer and no state is carried over to the next input sequence $X_{s+1}$ at the same attention layer. In the attention layer, in order to capture the dependency between the consequent segments $X_{s}$ and $X_{s+1}$, one needs to process them altogether at the same time and this process becomes a bottleneck requiring large computational resources as the length of input sequence grows more and more. To improve the efficiency while still being able to benefit from the expressiveness of the attention mechanism, this work introduces a recurrent attention layer. ## 3 Method\n\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al, 2019). Similar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We compute the standard causal dot-product attention context within each segment. So the dot-product attention computation is local in a sense that it covers a total $N$ number of tokens of the current segment with index $S$ ( $N$ is the segment length). The local attention (Dai et al., 2019), however, discards the attention states of the previous segment when processing the next one. In Infini-Transformers, instead of leaving out the old KV attention states, we propose to reuse them to maintain the entire context history with a compressive memory. So each attention layer of Infini-Transformers has both global compressive and local fine-grained states. We call such an efficient attention mechanism Infini-attention, which is illustrated in Figure 1 and described formally in the following sections. ### 3.1 Infini-attention\n\nAs shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes both local and global context states and combine them for its output. Similar to multi-head\nattention (MHA), it maintains $H$ number of parallel compressive memory per attention layer ( $H$ is the number of attention heads) in addition to the dot-product attention and like the RNNs and MNM, it maintains a recurrent memory state to efficiently track the long sequence context:\n\n$$\nO_{s}, M_{s}=\\text { infini-attention }\\left(X_{s}, M_{s-1}\\right)\n$$\n\n### 3.1.1 Scaled Dot-product Attention\n\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention variant (Munkhdalai et al., 2016. Cheng et al., 2016), has been the main building block in LLMs. The MHA's strong capability to model context-dependent dynamic computation and its conveniences of temporal masking have been leveraged extensively in the autoregressive generative models. A single head in the vanilla MHA computes its attention context $A_{\\text {dot }} \\in \\mathbb{R}^{N \\times d_{\\text {value }}}$ from sequence of input segments $X \\in \\mathbb{R}^{N \\times d_{\\text {model }}}$ as follows. First, it computes attention query, key, and value states:\n\n$$\nK=X W_{K}, V=X W_{V} \\text { and } Q=X W_{Q} \\text {. }\n$$\n\nHere, $W_{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {key }}}, W_{V} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {value }}}$ and $W_{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {key }}}$ are trainable projection matrices. Then, the attention context is calculated as a weighted average of all other values as\n\n$$\nA_{\\text {dot }}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{\\text {model }}}}\\right) V\n$$\n\nFor MHA, we compute $H$ number of attention context vectors for each sequence element in parallel, concatenate them along the second dimension and then finally project the concatenated vector to the model space to obtain the attention output. ### 3.1.2 Compressive Memory\n\nIn Infini-attention, instead of computing new memory entries for compressive memory, we reuse the query, key and value states ( $Q, K$ and $V$ ) from the dot-product attention computation. The state sharing and reusing between the dot-product attention and compressive memory not only enables efficient plug-in-play long-context adaptation but also speeds up training and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to store bindings of key and value states in the compressive memory and retrieve by using the query vectors. While there are different forms of compressive memory proposed in the literature (Hopfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and computational efficiency, in this work we parameterize the memory with an associative matrix (Schlag et al., 2020). This approach further allows us to cast the memory update and retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage stable training techniques from the related methods. Specially, we adopt the update rule and retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and competitive performance. Memory retrieval. In Infini-attention, we retrieve new content $A_{\\text {mem }} \\in \\mathbb{R}^{N \\times d_{\\text {value }}}$ from the memory $M_{s-1} \\in \\mathbb{R}^{d_{\\text {key }} \\times d_{\\text {value }}}$ by using the query $Q \\in \\mathbb{R}^{N \\times d_{\\text {key }}}$ as:\n\n$$\nA_{\\text {mem }}=\\frac{\\sigma(Q) M_{s-1}}{\\sigma(Q) z_{s-1}}\n$$\n\nHere, $\\sigma$ and $z_{s-1} \\in \\mathbb{R}^{d_{\\text {key }}}$ are a nonlinear activation function and a normalization term, respectively.\n```\n\n##### *Relevant Chunk: No. 12/24 (Score: 0.66)*\n\n```\narXiv preprint arXiv:1511.07289, 2015. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128 k context. arXiv preprint arXiv:2402.10171, 2024. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model.\n```\n\n#### 4. TransformerFAM: Feedback attention is working memory (Avg. Score: 0.64)\n\n*Dongseong Hwang, Weiran Wang, Zhuoyuan Huo, K. Sim, P. M. Mengibar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** Feedback Attention Memory is proposed, a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences.\n\n**Abstract:** While Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length.\n\n##### *Relevant Chunk: No. 1/36 (Score: 0.64)*\n\n```\n# TransformerFAM: Feedback attention is working memory \n\nDongseong Hwang Weiran Wang Zhuoyuan Huo Khe Chai Sim Pedro Mengibar<br>Google LLC<br>Mountain View, CA, USA<br>dongseong@google.com\n\n\n#### Abstract\n\nWhile Transformers have revolutionized deep learning, their quadratic attention complexity hinders their ability to process infinitely long inputs. We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations. This design fosters the emergence of working memory within the Transformer, allowing it to process indefinitely long sequences. TransformerFAM requires no additional weights, enabling seamless integration with pre-trained models. Our experiments show that TransformerFAM significantly improves Transformer performance on long-context tasks across various model sizes (1B, 8B, and 24B). These results showcase the potential to empower Large Language Models (LLMs) to process sequences of unlimited length. ## 1 Introduction\n\nThe introduction of the Transformer architecture [12] has revolutionized deep learning by permeating diverse domains and enhancing performance due to its efficacy and scalability. This scalability fuels a trend analogous to Moore's law, which links increased model size to performance gains [39]. The effectiveness of attention in text sequence processing was solidified through the Transformer paper. Models like BERT [16] and GPT-3 [33] further showcased the scalability of Transformer and its tendency for improved performance with increased model size. Following the replacement of LSTM [5] by Transformer in most Natural Language Processing (NLP) domains, the Vision Transformer (ViT) [32] replaced Convolutional Neural Network (CNN) [4] with Transformers in the vision domain, and Conformer (Convolution-augmented Transformer) [29] replaced LSTM in the speech domain. The Transformer has become the de facto architecture in various domains. Currently, attention serves as the leading architecture for extracting meaningful representations from homogeneous data. The logical progression points toward extending attention capabilities to heterogeneous data. This has enabled advances in multimodal fusion (text and vision), as seen in models like DALL$\\cdot$E 2 [55], Flamingo [54] and CoCa [53]. AudioLM [64] has shown that attention also excels at fusing audio and text. Consequently, Gemini [69] integrates text, images, audio, and video into a single generative model. This was possible because attention to heterogeneous data works exceptionally well. Despite the impressive success of attention, it suffers from major drawbacks. Firstly, attention has quadratic complexity with respect to context length, which limits the capability of modeling long contexts. Secondly, it forgets information from context before attention window, unlike LSTM, which theoretically can propagate information indefinitely. We want the better architecture to be able to process arbitrarily long sequences efficiently, while preserving very long-term dependencies. ![](https://cdn.mathpix.com/cropped/2024_09_17_364db6cf7554a04db2efg-02.jpg?height=407&width=1404&top_left_y=232&top_left_x=361)\n\nFigure 1: Comparison of query-key attention masks for Sliding Window Attention (SWA) variants. (a) Sliding Window Attention: Attention is restricted to the current window = 3. (b) Block Sliding Window Attention (BSWA) (block size $=2$, memory segment $=1$ ): Attention is allowed to previous blocks within the memory segment. (c) BSWA (block size $=2$, memory segment $=2$ ): The memory segment is expanded, allowing attention to a larger past context. (d) Illustrates the receptive field of BSWA (block size $=2$, memory segment $=1$, depth $=4$ ): The region within the curly braces represents the receptive field. ![](https://cdn.mathpix.com/cropped/2024_09_17_364db6cf7554a04db2efg-02.jpg?height=266&width=1379&top_left_y=979&top_left_x=372)\n\nFigure 2: Comparison of attention patterns in Transformer layer. (a) TransformerBSWA: Input query attends to the current block and two memory segments, providing past context. (b) TransformerFAM: Input query attends to the current block, memory segments, and past FAM (green lines). FAM query (copied from previous FAM, blue dash arrow) compresses the current block to update FAM.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient compressive transformers, hierarchical memory networks transformers, adaptive computation time transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF\n\n*From Search Query: efficient compressive transformers*\n\n*S. Lionar, Xiangyu Xu, Min-Hui Lin, G. Lee*\n\n**TL;DR:** NU-MCC is a novel alternative to the occupancy field used in MCC that outperforms MCC by 9.7% in terms of the F1-score on the CO3D-v2 dataset with more than 5x faster running speed and can achieve more complete surface reconstruction.\n\n**Abstract:** Remarkable progress has been made in 3D reconstruction from single-view RGB-D inputs. MCC is the current state-of-the-art method in this field, which achieves unprecedented success by combining vision Transformers with large-scale training. However, we identified two key limitations of MCC: 1) The Transformer decoder is inefficient in handling large number of query points; 2) The 3D representation struggles to recover high-fidelity details. In this paper, we propose a new approach called NU-MCC that addresses these limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood decoder introduces center points as an efficient proxy of input visual features, allowing each query point to only attend to a small neighborhood. This design not only results in much faster inference speed but also enables the exploitation of finer-scale visual features for improved recovery of 3D textures. Second, our Repulsive UDF is a novel alternative to the occupancy field used in MCC, significantly improving the quality of 3D object reconstruction. Compared to standard UDFs that suffer from holes in results, our proposed Repulsive UDF can achieve more complete surface reconstruction. Experimental results demonstrate that NU-MCC is able to learn a strong 3D representation, significantly advancing the state of the art in single-view 3D reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the F1-score on the CO3D-v2 dataset with more than 5x faster running speed.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 2*)\n\n#### 2. Training data-efficient image transformers & distillation through attention\n\n*From Search Query: efficient compressive transformers*\n\n*Hugo Touvron, M. Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv'e J'egou*\n\n**TL;DR:** This work produces a competitive convolution-free transformer by training on Imagenet only and introduces a teacher-student strategy specific to transformers that relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n**Abstract:** Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 5614  (*Influential: 911*)\n\n#### 3. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\n\n*From Search Query: efficient compressive transformers*\n\n*Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, J. \u00c1lvarez, Ping Luo*\n\n**TL;DR:** SegFormer is presented, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders and shows excellent zero-shot robustness on Cityscapes-C.\n\n**Abstract:** We present SegFormer, a simple, efficient yet powerful semantic segmentation framework which unifies Transformers with lightweight multilayer perception (MLP) decoders. SegFormer has two appealing features: 1) SegFormer comprises a novel hierarchically structured Transformer encoder which outputs multiscale features. It does not need positional encoding, thereby avoiding the interpolation of positional codes which leads to decreased performance when the testing resolution differs from training. 2) SegFormer avoids complex decoders. The proposed MLP decoder aggregates information from different layers, and thus combining both local attention and global attention to render powerful representations. We show that this simple and lightweight design is the key to efficient segmentation on Transformers. We scale our approach up to obtain a series of models from SegFormer-B0 to SegFormer-B5, reaching significantly better performance and efficiency than previous counterparts. For example, SegFormer-B4 achieves 50.3% mIoU on ADE20K with 64M parameters, being 5x smaller and 2.2% better than the previous best method. Our best model, SegFormer-B5, achieves 84.0% mIoU on Cityscapes validation set and shows excellent zero-shot robustness on Cityscapes-C. Code will be released at: github.com/NVlabs/SegFormer.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 3688  (*Influential: 479*)\n\n#### 4. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical memory networks transformers*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 38  (*Influential: 3*)\n\n#### 5. Approximating Two-Layer Feedforward Networks for Efficient Transformers\n\n*From Search Query: hierarchical memory networks transformers*\n\n*R'obert Csord'as, Kazuki Irie, J. Schmidhuber*\n\n**TL;DR:** This work introduces several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs, including product-key memories (PKMs), and proposes methods to improve both MoEs and PKMs.\n\n**Abstract:** How to reduce compute and memory requirements of neural networks (NNs) without sacrificing performance? Many recent works use sparse Mixtures of Experts (MoEs) to build resource-efficient large language models (LMs). Here we introduce several novel perspectives on MoEs, presenting a general framework that unifies various methods to approximate two-layer NNs (e.g., feedforward blocks of Transformers), including product-key memories (PKMs). Leveraging insights from this framework, we propose methods to improve both MoEs and PKMs. Unlike prior work that compares MoEs with dense baselines under the compute-equal condition, our evaluation condition is parameter-equal, which is crucial to properly evaluate LMs. We show that our MoEs are competitive with the dense Transformer-XL on both the WikiText-103 and enwiki8 datasets at two different scales, while being much more resource efficient. This demonstrates that MoEs are relevant not only to extremely large LMs but also to any-scale resource-efficient LMs. Our code is public.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 6. Memory Efficient Continual Learning with Transformers\n\n*From Search Query: hierarchical memory networks transformers*\n\n*B. Ermi\u015f, Giovanni Zappella, Martin Wistuba, Aditya Rawal, C. Archambeau*\n\n**TL;DR:** This paper devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters that is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks.\n\n**Abstract:** In many real-world scenarios, data to train machine learning models becomes available over time. Unfortunately, these models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is difficult to prevent due to practical constraints. For instance, the amount of data that can be stored or the computational resources that can be used might be limited. Moreover, applications increasingly rely on large pre-trained neural networks, such as pre-trained Transformers, since the resources or data might not be available in sufficiently large quantities to practitioners to train the model from scratch. In this paper, we devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters. Different than the existing approaches, our method is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks. On both image and text classification tasks, we empirically demonstrate that our method maintains a good predictive performance without retraining the model or increasing the number of model parameters over time. The resulting model is also significantly faster at inference time compared to Adapter-based state-of-the-art methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 36  (*Influential: 1*)\n\n#### 7. iTransformer: Inverted Transformers Are Effective for Time Series Forecasting\n\n*From Search Query: adaptive computation time transformers*\n\n*Yong Liu, Tengge Hu, Haoran Zhang, Haixu Wu, Shiyu Wang, Lintao Ma, Mingsheng Long*\n\n**TL;DR:** The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting.\n\n**Abstract:** The recent boom of linear forecasting models questions the ongoing passion for architectural modifications of Transformer-based forecasters. These forecasters leverage Transformers to model the global dependencies over temporal tokens of time series, with each token formed by multiple variates of the same timestamp. However, Transformers are challenged in forecasting series with larger lookback windows due to performance degradation and computation explosion. Besides, the embedding for each temporal token fuses multiple variates that represent potential delayed events and distinct physical measurements, which may fail in learning variate-centric representations and result in meaningless attention maps. In this work, we reflect on the competent duties of Transformer components and repurpose the Transformer architecture without any modification to the basic components. We propose iTransformer that simply applies the attention and feed-forward network on the inverted dimensions. Specifically, the time points of individual series are embedded into variate tokens which are utilized by the attention mechanism to capture multivariate correlations; meanwhile, the feed-forward network is applied for each variate token to learn nonlinear representations. The iTransformer model achieves state-of-the-art on challenging real-world datasets, which further empowers the Transformer family with promoted performance, generalization ability across different variates, and better utilization of arbitrary lookback windows, making it a nice alternative as the fundamental backbone of time series forecasting. Code is available at this repository: https://github.com/thuml/iTransformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 201  (*Influential: 51*)\n\n#### 8. Comparing Fixed and Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: adaptive computation time transformers*\n\n*Daniel Fojo, V\u00edctor Campos, Xavier Gir\u00f3-i-Nieto*\n\n**TL;DR:** This paper compares ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times, and finds surprising results, where Repeat- RNN performs as good as ACT in the selected tasks.\n\n**Abstract:** Adaptive Computation Time for Recurrent Neural Networks (ACT) is one of the most promising architectures for variable computation. ACT adapts to the input sequence by being able to look at each sample more than once, and learn how many times it should do it. In this paper, we compare ACT to Repeat-RNN, a novel architecture based on repeating each sample a fixed number of times. We found surprising results, where Repeat-RNN performs as good as ACT in the selected tasks. Source code in TensorFlow and PyTorch is publicly available at this https URL\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 9. A Time Series is Worth 64 Words: Long-term Forecasting with Transformers\n\n*From Search Query: adaptive computation time transformers*\n\n*Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, J. Kalagnanam*\n\n**TL;DR:** The channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models and applies to self-supervised pre-training tasks and attain excellent fine-tuning performance.\n\n**Abstract:** We propose an efficient design of Transformer-based models for multivariate time series forecasting and self-supervised representation learning. It is based on two key components: (i) segmentation of time series into subseries-level patches which are served as input tokens to Transformer; (ii) channel-independence where each channel contains a single univariate time series that shares the same embedding and Transformer weights across all the series. Patching design naturally has three-fold benefit: local semantic information is retained in the embedding; computation and memory usage of the attention maps are quadratically reduced given the same look-back window; and the model can attend longer history. Our channel-independent patch time series Transformer (PatchTST) can improve the long-term forecasting accuracy significantly when compared with that of SOTA Transformer-based models. We also apply our model to self-supervised pre-training tasks and attain excellent fine-tuning performance, which outperforms supervised training on large datasets. Transferring of masked pre-trained representation on one dataset to others also produces SOTA forecasting accuracy. Code is available at: https://github.com/yuqinie98/PatchTST.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 701  (*Influential: 209*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Extreme Compression for Pre-trained Transformers Made Simple and Efficient\n\n*From Search Query: efficient compressive transformers*\n\n*Yuxiong He, Conglong Li, Minjia Zhang, Zhewei Yao, Xiaoxia Wu*\n\n**Abstract:** Extreme compression, particularly ultra-low bit precision (binary/ternary) quantization, has been proposed to fit large NLP models on resource-constraint devices. However, to preserve the accuracy for such aggressive compression schemes, cutting-edge methods usually introduce complicated compression pipelines, e.g., multi-stage expensive knowledge distillation with extensive hyperparameter tuning. Also, they oftentimes focus less on smaller transformer models that have already been heavily compressed via knowledge distillation and lack a systematic study to show the effectiveness of their methods. In this paper, we perform a very comprehensive systematic study to measure the impact of many key hyperparameters and training strategies from previous works. As a result, we find out that previous baselines for ultra-low bit precision quantization are significantly under-trained. Based on our study, we propose a simple yet effective compression pipeline for extreme compression, named XTC. XTC demonstrates that (1) we can skip the pre-training knowledge distillation to obtain a 5-layer BERT while achieving better performance than previous state-of-the-art methods, e.g., the 6-layer TinyBERT; (2) extreme quantization plus layer reduction is able to reduce the model size by 50x, resulting in new state-of-the-art results on GLUE tasks.\n\n**Published:** 2022-06-04\n\n\n\n#### 2. Efficient Nonlinear Transforms for Lossy Image Compression\n\n*From Search Query: efficient compressive transformers*\n\n*Johannes Ball\u00e9*\n\n**Abstract:** We assess the performance of two techniques in the context of nonlinear\ntransform coding with artificial neural networks, Sadam and GDN. Both\ntechniques have been successfully used in state-of-the-art image compression\nmethods, but their performance has not been individually assessed to this\npoint. Together, the techniques stabilize the training procedure of nonlinear\nimage transforms and increase their capacity to approximate the (unknown)\nrate-distortion optimal transform functions. Besides comparing their\nperformance to established alternatives, we detail the implementation of both\nmethods and provide open-source code along with the paper.\n\n**Published:** 2018-01-31\n\n\n\n#### 3. Focal Modulation Networks\n\n*From Search Query: hierarchical memory networks transformers*\n\n*Jianfeng Gao, Lu Yuan, Xiyang Dai, Chunyuan Li, Jianwei Yang*\n\n**Abstract:** We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3. Code and checkpoints are available at https://github.com/microsoft/FocalNet.\n\n**Published:** 2022-03-22\n\n\n\n#### 4. AKG: Automatic Kernel Generation for Neural Processing Units using Polyhedral Transformations\n\n*From Search Query: hierarchical memory networks transformers*\n\n*Xuefeng Jin, Kun Zhang, Peng Di, Zheng Li, Yun Cheng, Chen Wu, Bin Cheng, Xiong Gao, Renwei Zhang, Zhen Geng, Wang Nie, Bojie Li, Jie Zhao*\n\n**Abstract:** Existing tensor compilers have proven their effectiveness in deploying deep neural networks on general-purpose hardware like CPU and GPU, but optimizing for neural processing units (NPUs) is still challenging due to the heterogeneous compute units and complicated memory hierarchy.\r\n\r\nIn this paper, we present AKG, a tensor compiler for NPUs. AKG first lowers the tensor expression language to a polyhedral representation, which is used to automate the memory management of NPUs. Unlike existing approaches that resort to manually written schedules, AKG leverages polyhedral schedulers to perform a much wider class of transformations, and extends the semantics of the polyhedral representation to combine complex tiling techniques and hierarchical fusion strategies. We also implement the domain-specific optimization of convolution in AKG. Moreover, to achieve the optimal performance, we introduce complementary optimizations in code generation, which is followed by an auto-tuner.\r\n\r\nWe conduct extensive experiments on benchmarks ranging from single operators to end-to-end networks. The experimental results show that AKG can obtain superior performance to both manual scheduling approaches and vendor provided libraries. We believe AKG will cast a light on the follow-up compiler works on NPUs.\n\n**Proceeding:** proceedings-of-the-42nd-acm-sigplan\n\n**Published:** 2021-06-19\n\n\n\n#### 5. DETRs Beat YOLOs on Real-time Object Detection\n\n*From Search Query: adaptive computation time transformers*\n\n*Jie Chen, Yi Liu, Qingqing Dang, Wenyu Lv, Jinman Wei, Guanzhong Wang, Yian Zhao, Shangliang Xu*\n\n**Abstract:** The YOLO series has become the most popular framework for real-time object detection due to its reasonable trade-off between speed and accuracy. However, we observe that the speed and accuracy of YOLOs are negatively affected by the NMS. Recently, end-to-end Transformer-based detectors (DETRs) have provided an alternative to eliminating NMS. Nevertheless, the high computational cost limits their practicality and hinders them from fully exploiting the advantage of excluding NMS. In this paper, we propose the Real-Time DEtection TRansformer (RT-DETR), the first real-time end-to-end object detector to our best knowledge that addresses the above dilemma. We build RT-DETR in two steps, drawing on the advanced DETR: first we focus on maintaining accuracy while improving speed, followed by maintaining speed while improving accuracy. Specifically, we design an efficient hybrid encoder to expeditiously process multi-scale features by decoupling intra-scale interaction and cross-scale fusion to improve speed. Then, we propose the uncertainty-minimal query selection to provide high-quality initial queries to the decoder, thereby improving accuracy. In addition, RT-DETR supports flexible speed tuning by adjusting the number of decoder layers to adapt to various scenarios without retraining. Our RT-DETR-R50 / R101 achieves 53.1% / 54.3% AP on COCO and 108 / 74 FPS on T4 GPU, outperforming previously advanced YOLOs in both speed and accuracy. We also develop scaled RT-DETRs that outperform the lighter YOLO detectors (S and M models). Furthermore, RT-DETR-R50 outperforms DINO-R50 by 2.2% AP in accuracy and about 21 times in FPS. After pre-training with Objects365, RT-DETR-R50 / R101 achieves 55.3% / 56.2% AP. The project page: https://zhao-yian.github.io/RTDETR.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-04-17\n\n\n\n#### 6. FlexiViT: One Model for All Patch Sizes\n\n*From Search Query: adaptive computation time transformers*\n\n*Filip Pavetic, Ibrahim Alabdulmohsin, Michael Tschannen, Matthias Minderer, Xiaohua Zhai, Simon Kornblith, Mathilde Caron, Alexander Kolesnikov, Pavel Izmailov, Lucas Beyer*\n\n**Abstract:** Vision Transformers convert images to sequences by slicing them into patches. The size of these patches controls a speed/accuracy tradeoff, with smaller patches leading to higher accuracy at greater computational cost, but changing the patch size typically requires retraining the model. In this paper, we demonstrate that simply randomizing the patch size at training time leads to a single set of weights that performs well across a wide range of patch sizes, making it possible to tailor the model to different compute budgets at deployment time. We extensively evaluate the resulting model, which we call FlexiViT, on a wide range of tasks, including classification, image-text retrieval, open-world detection, panoptic segmentation, and semantic segmentation, concluding that it usually matches, and sometimes outperforms, standard ViT models trained at a single patch size in an otherwise identical setup. Hence, FlexiViT training is a simple drop-in improvement for ViT that makes it easy to add compute-adaptive capabilities to most models relying on a ViT backbone architecture. Code and pre-trained models are available at https://github.com/google-research/big_vision\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2022-12-15\n\n\n\n\n\n---\n## Web search results\n\n ### Understanding the Researcher's Goal and Intent\n\nThe researcher aims to design a novel language model (LM) block that combines several advanced techniques to outperform current state-of-the-art models. The key objectives include:\n\n- **Efficient Handling of Long Sequences**: The model should be able to process long sequences with low computational and memory costs.\n- **High Performance**: Achieve low perplexity on corpora and high accuracy on downstream tasks.\n- **Robustness and Scalability**: Ensure the model is robust to varied inputs and scalable with more data and larger models.\n- **Adaptive Computation**: Incorporate adaptive computation and memory compression techniques.\n\n### Key Insights and Useful Information\n\n#### 1. **Compressive Memory Mechanisms**\n- **Memformer** and **Compressive Transformers**: These models use external dynamic memory and compression techniques to handle long sequences efficiently. Memformer's memory reading and writing modules, along with a forgetting mechanism, are particularly relevant.\n- **Dynamic Memory Compression (DMC)**: This method allows for online key-value cache compression during inference, reducing memory usage without compromising performance.\n\n#### 2. **Adaptive Attention Mechanisms**\n- **Adaptive Attention Span**: Allowing each attention head to learn the optimal attention span can allocate computational resources more effectively.\n- **Sparse Attention Mechanisms**: Techniques like those in Sparse Transformers and Sparser is Faster models reduce computational complexity from quadratic to linear, making them highly efficient for long sequences.\n\n#### 3. **Hierarchical Attention Enhancements**\n- **Landmark Attention**: Using landmark tokens to represent blocks of input can help manage long sequences effectively without separate retrieval mechanisms.\n- **Block-Recurrent Transformers**: Operating on blocks of tokens with recurrent mechanisms can improve efficiency and handle longer dependencies.\n\n#### 4. **Efficient Implementation Strategies**\n- **Combiner Approach**: Treating self-attention as a conditional expectation and approximating the distribution with structured factorization can provide full attention capability with reduced computation and memory costs.\n- **Memory-Augmented Architectures**: Incorporating memory networks that interact with external memory modules can provide infinite temporal range with constant memory complexity.\n\n### Practical Implementation Strategies\n\n#### Integrating Adaptive Attention Scaling\n- **Adaptive Attention Span**: Implement a mechanism where each attention head can dynamically adjust its attention span based on the input context. This can be achieved by training the model to learn the optimal attention span for each head.\n\n#### Incorporating Compressive Memory\n- **Dynamic Memory Compression**: Implement DMC to compress key-value caches during inference. This involves designing an online compression algorithm that preserves the original performance while reducing memory usage.\n- **Forgetting Mechanisms**: Introduce forgetting mechanisms to improve the effectiveness of memorizing new information by discarding less important past states.\n\n#### Efficient Sparse Attention\n- **Scoring Network and Top-K Mask**: Use a scoring network to select a constant number of key-value pairs for each query, enabling gradient-based optimization and reducing computational complexity.\n\n#### Block-Level Processing\n- **Block-Recurrent Transformers**: Design the model to operate on blocks of tokens rather than single tokens, leveraging parallel computation within a block to make efficient use of accelerator hardware.\n\n### Future Search Plan and Implementation\n\n#### Prototype Components\n- Start by modifying the existing HierarchicalAdaptiveAttention architecture to include adaptive attention scaling and compressive memory mechanisms.\n- Evaluate the compatibility of these components with the overall architecture, focusing on training stability and computational efficiency.\n\n#### Implementation Details\n- Investigate practical implementation considerations such as efficiently computing adaptive scaling factors and managing compressed memories within the Transformer framework.\n- Explore how to integrate landmark attention and block-recurrent mechanisms to enhance the model's ability to handle long sequences.\n\n#### Benchmarking and Testing\n- Plan experiments to compare the proposed model against baseline models on tasks involving long sequences to validate the anticipated benefits.\n- Conduct thorough benchmarking to assess the model's performance in terms of perplexity, accuracy, and computational efficiency.\n\n### References\n\nFor a detailed understanding, the following references are crucial:\n\n- **Memformer**: Wu et al. (2020)\n- **Compressive Transformers**: Rae et al. (2019)\n- **Dynamic Memory Compression**: Nawrot et al. (2024)\n- **Sparse Transformers**: Child et al. (2019)\n- **Adaptive Attention Span**: Sukhbaatar & Fergus (2019)\n- **Landmark Attention**: Mohtashami & Jaggi (2023)\n- **Block-Recurrent Transformers**: Hutchins et al. (2022)\n- **Combiner Approach**: Dai et al. (2021)\n\nBy focusing on these key areas and implementing the suggested strategies, the researcher can develop a novel LM block that efficiently handles long sequences, maintains high performance, and scales well with larger models and more data.\n",
            "analysis": "---\n\n### **Analysis Note**\n\n#### 1. **Summary of Analysis**\n\nBuilding upon the previous analysis, I aim to design a language model block that effectively combines the hierarchical adaptive multi-head attention from **HieraNorm-AttnGPT** with the adaptive computation and memory compression techniques inspired by **AdaptiveTTT**. The integration seeks to address the challenges of processing long sequences efficiently while maintaining high performance and scalability. The new insights from the latest search results provide valuable information on advanced memory compression methods, efficient sparse attention mechanisms, and adaptive computation in Transformers. These insights help refine the proposed design by incorporating proven strategies from recent research to enhance efficiency, scalability, and adaptability.\n\n#### 2. **Useful References with Excerpts**\n\n1. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020).** \"Memformer: A Memory-Augmented Transformer for Sequence Modeling.\" *AACL/IJCNLP*. [Internal Library]\n\n   *\"Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information. By utilizing recurrence and a fixed-size memory, our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity.\"*\n\n2. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019).** \"Compressive Transformers for Long-Range Sequence Modelling.\" *arXiv preprint arXiv:1911.05507*. [Referenced in multiple sources]\n\n   *\"We augment the Transformer with a compression mechanism, which results in a significant improvement in memory usage and enables the model to handle longer sequences effectively.\"*\n\n3. **Nawrot, P., et al. (2024).** \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.\" *International Conference on Machine Learning*. [External Source]\n\n   *\"We propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention and key-value eviction policies.\"*\n\n4. **Child, R., Gray, S., Radford, A., & Sutskever, I. (2019).** \"Generating Long Sequences with Sparse Transformers.\" *arXiv preprint arXiv:1904.10509*. [Referenced in previous analysis]\n\n   *\"We introduce sparse factorizations of the attention matrix that can handle sequences with thousands of tokens, reducing the complexity from quadratic to linear.\"*\n\n5. **Sukhbaatar, S., & Fergus, R. (2019).** \"Adaptive Attention Span in Transformers.\" *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*. [Referenced in previous analysis]\n\n   *\"We propose a method that allows the model to learn the optimal attention span for each head in a multi-head attention layer.\"*\n\n6. **Wu, Y., Rabe, M. N., Hutchins, D. S., & Szegedy, C. (2022).** \"Memorizing Transformers.\" *International Conference on Learning Representations*. [Internal Library]\n\n   *\"We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks.\"*\n\n7. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024).** \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers.\" *arXiv preprint arXiv:2401.00001*. [Internal Library]\n\n   *\"Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization.\"*\n\n8. **Mohtashami, A., & Jaggi, M. (2023).** \"Landmark Attention: Random-Access Infinite Context Length for Transformers.\" *Neural Information Processing Systems*. [Internal Library]\n\n   *\"Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism.\"*\n\n9. **Hutchins, D. S., Schlag, I., Wu, Y., Dyer, E., & Neyshabur, B. (2022).** \"Block-Recurrent Transformers.\" *Neural Information Processing Systems*. [Internal Library]\n\n   *\"Our recurrent cell operates on blocks of tokens rather than single tokens during training, and leverages parallel computation within a block to make efficient use of accelerator hardware.\"*\n\n10. **Dai, B., Schuurmans, D., Leskovec, J., Yang, M., Dai, Z., Ren, H., & Dai, H. (2021).** \"Combiner: Full Attention Transformer with Sparse Computation Cost.\" *Neural Information Processing Systems*. [External Source: Papers with Code]\n\n    *\"Our approach integrates the self-attention mechanism as a conditional expectation over embeddings at each location and approximates the conditional distribution with a structured factorization.\"*\n\n#### 3. **Key Insights and Detailed Analysis**\n\n**Integration of Compressive Memory Mechanisms**\n\n- **Memformer and Compressive Transformers**: The concepts from Memformer and Compressive Transformers provide a solid foundation for integrating compressive memory into the hierarchical attention mechanism. By utilizing an external dynamic memory and compression techniques, the model can handle long sequences with constant memory complexity.\n\n- **Dynamic Memory Compression (DMC)**: Incorporating DMC can allow for online key-value cache compression during inference, enabling the model to maintain performance while reducing memory usage. This aligns with the need for efficient state management in handling long sequences.\n\n**Adaptive Attention with Dynamic Computation**\n\n- **Adaptive Attention Span**: Leveraging adaptive attention spans allows each attention head to learn the optimal amount of context needed for different inputs. By embedding this into the hierarchical attention structure, the model can allocate computational resources more effectively.\n\n- **Sparse Attention Mechanisms**: Using sparse attention patterns, such as those from Sparse Transformers and Sparser is Faster models, the computational complexity can be reduced from quadratic to linear. Integrating a scoring network for differentiable top-k selection of key-value pairs can enhance efficiency.\n\n**Hierarchical Attention Enhancements**\n\n- **Landmark Attention**: Employing landmark tokens to represent blocks of input and enabling the attention mechanism to select relevant blocks can help manage long sequences effectively without separate retrieval mechanisms.\n\n- **Block-Recurrent Transformers**: Operating on blocks of tokens with recurrent mechanisms and leveraging parallel computation can improve efficiency and handle longer dependencies.\n\n**Efficient Implementation Strategies**\n\n- **Combiner Approach**: Treating self-attention as a conditional expectation and approximating the distribution with structured factorization can provide full attention capability with reduced computation and memory costs.\n\n- **Memory-Augmented Architectures**: Incorporating memory networks that interact with external memory modules through reading and writing operations can provide infinite temporal range with constant memory complexity.\n\n**Selective Memory Compression Techniques**\n\n- **Key-Value Compression**: Compressing intermediate activations of tokens into compact representations can reduce both memory and computational costs when processing subsequent context.\n\n- **Forgetting Mechanisms**: Introducing forgetting mechanisms can improve the effectiveness of memorizing new information by discarding less important past states.\n\n**Enhancing Training Stability and Adaptability**\n\n- **Dynamic Layer Normalization**: Maintaining dynamic layer normalization ensures stable gradient flow and adaptability across diverse contexts.\n\n- **Feedback Attention Mechanism**: Incorporating feedback loops allows the model to attend to its own latent representations, enhancing its capacity to process indefinitely long sequences.\n\n#### 4. **Future Search Plan**\n\nGiven that I have gathered substantial information on compressive memory mechanisms, adaptive attention, and efficient sparse attention techniques, the next steps are:\n\n- **Implement Prototype Components**: Begin drafting the modifications to the existing **HierarchicalAdaptiveAttention** GAU to include adaptive attention scaling and compressive memory mechanisms.\n\n- **Evaluate Compatibility**: Assess how the integration of these components affects the overall architecture, especially in terms of training stability and computational efficiency.\n\n- **Explore Implementation Details**: Dive deeper into the practical implementation considerations, such as how to efficiently compute adaptive scaling factors and manage compressed memories within the Transformer framework.\n\n- **Benchmarking and Testing**: Plan for experiments to compare the proposed model against baseline models on tasks involving long sequences to validate the anticipated benefits.\n\n- **Finalize the Design Proposal**: After refining the integration strategies and addressing potential challenges, prepare to formulate the final proposal incorporating all these insights.\n\n#### 5. **References**\n\n1. Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). Memformer: A Memory-Augmented Transformer for Sequence Modeling. *Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing*, 1526\u20131536.\n\n2. Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). Compressive Transformers for Long-Range Sequence Modelling. *arXiv preprint arXiv:1911.05507*.\n\n3. Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. *Proceedings of the 41st International Conference on Machine Learning (ICML)*.\n\n4. Child, R., Gray, S., Radford, A., & Sutskever, I. (2019). Generating Long Sequences with Sparse Transformers. *arXiv preprint arXiv:1904.10509*.\n\n5. Sukhbaatar, S., & Fergus, R. (2019). Adaptive Attention Span in Transformers. *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 331\u2013335.\n\n6. Wu, Y., Rabe, M. N., Hutchins, D. S., & Szegedy, C. (2022). Memorizing Transformers. *International Conference on Learning Representations (ICLR)*.\n\n7. Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. *arXiv preprint arXiv:2401.00001*.\n\n8. Mohtashami, A., & Jaggi, M. (2023). Landmark Attention: Random-Access Infinite Context Length for Transformers. *Advances in Neural Information Processing Systems (NeurIPS)*.\n\n9. Hutchins, D. S., Schlag, I., Wu, Y., Dyer, E., & Neyshabur, B. (2022). Block-Recurrent Transformers. *Advances in Neural Information Processing Systems*, 35, 21031\u201321044.\n\n10. Dai, B., Schuurmans, D., Leskovec, J., Yang, M., Dai, Z., Ren, H., & Dai, H. (2021). Combiner: Full Attention Transformer with Sparse Computation Cost. *Advances in Neural Information Processing Systems*, 34, 21974\u201321987.\n\n---\n\n```keywords efficient compressive transformers```\n\n```keywords hierarchical memory networks transformers```\n\n```keywords adaptive computation time transformers```\n\n```description Practical implementation strategies for integrating adaptive attention scaling and compressive memory in hierarchical Transformer architectures```\n\n---"
        },
        {
            "ready": true,
            "query": null,
            "detail": "---\n\n### **Analysis Note**\n\n#### 1. **Summary of Analysis**\n\nI have conducted thorough research over multiple rounds to design a novel language model block that combines the strengths of **HieraNorm-AttnGPT** (Parent 1) and **AdaptiveTTT** (Parent 2). The goal is to integrate:\n\n- **Hierarchical Adaptive Multi-Head Attention** from HieraNorm-AttnGPT.\n- **Adaptive Attention Scaling** and **Selective Memory Compression** from AdaptiveTTT.\n\nIn this final analysis, I focused on practical implementation strategies for integrating adaptive attention scaling and compressive memory into hierarchical Transformer architectures. The key is to create a new GAU that efficiently handles long sequences with low computational and memory costs while maintaining high performance and scalability.\n\n#### 2. **Useful References with Excerpts**\n\n1. **Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). \"Recurrent Memory Transformer.\" Neural Information Processing Systems.**\n\n   *\"We propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence... This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing.\"*\n\n2. **Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). \"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.\" Neural Information Processing Systems.**\n\n   *\"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our empirical findings demonstrate that we can effectively prune up to 80% of the context without significant performance degradation.\"*\n\n3. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.\" arXiv preprint arXiv:XXXX.XXXXX.**\n\n   *\"A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"*\n\n4. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024). \"TransformerFAM: Feedback Attention is Working Memory.\" arXiv preprint arXiv:XXXX.XXXXX.**\n\n   *\"We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences.\"*\n\n5. **Ermi\u015f, B., Zappella, G., Wistuba, M., Rawal, A., & Archambeau, C. (2022). \"Memory Efficient Continual Learning with Transformers.\" Neural Information Processing Systems.**\n\n   *\"We devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters that is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks.\"*\n\n6. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). \"Memformer: A Memory-Augmented Transformer for Sequence Modeling.\" AACL/IJCNLP.**\n\n   *\"Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information... Our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity.\"*\n\n7. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). \"Compressive Transformers for Long-Range Sequence Modelling.\" arXiv preprint arXiv:1911.05507.**\n\n   *\"We augment the Transformer with a compression mechanism, which results in a significant improvement in memory usage and enables the model to handle longer sequences effectively.\"*\n\n8. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.\" International Conference on Machine Learning.**\n\n   *\"We propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention and key-value eviction policies.\"*\n\n9. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers.\" arXiv preprint arXiv:2401.00001.**\n\n   *\"Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization.\"*\n\n10. **Mohtashami, A., & Jaggi, M. (2023). \"Landmark Attention: Random-Access Infinite Context Length for Transformers.\" Neural Information Processing Systems.**\n\n    *\"Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism.\"*\n\n#### 3. **Key Insights and Detailed Analysis**\n\n##### **Integrating Adaptive Attention Scaling into Hierarchical Attention**\n\n- **Adaptive Attention Span**: Drawing inspiration from **Sukhbaatar & Fergus (2019)**, integrating an adaptive attention span into the hierarchical attention mechanism allows each attention head to dynamically adjust its span based on input complexity.\n\n- **Implementation Strategy**:\n\n  - Modify the **HierarchicalAdaptiveAttention** GAU to include a learnable attention span parameter for each head or group of heads.\n\n  - The attention span can be dynamically adjusted during training via gradient descent, allowing the model to focus computational resources where needed.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces unnecessary computation by limiting attention to relevant contexts.\n\n  - **Performance**: Improves the ability to model long-range dependencies without excessive computational burden.\n\n##### **Incorporating Compressive Memory Mechanisms**\n\n- **Memformer and Compressive Transformers**: Utilize a compressive memory that interacts with the Transformer to store and retrieve information efficiently, as seen in **Wu et al. (2020)** and **Rae et al. (2019)**.\n\n- **Dynamic Memory Compression (DMC)**:\n\n  - Implement online key-value cache compression during inference, based on **Nawrot et al. (2024)**.\n\n  - Design a compression module within the GAU that selectively compresses less important key-value pairs to reduce memory usage.\n\n- **Implementation Strategy**:\n\n  - Within the **HierarchicalAdaptiveAttention**, include a compressive memory module that stores past key-value pairs.\n\n  - Use importance scores (e.g., attention weights) to determine which key-value pairs to compress or discard.\n\n- **Benefits**:\n\n  - **Memory Efficiency**: Reduces memory footprint, allowing processing of longer sequences.\n\n  - **Scalability**: Enables the model to handle infinitely long contexts with bounded memory.\n\n##### **Enhancing Hierarchical Attention with Sparse Attention Mechanisms**\n\n- **Sparse Attention**: Incorporate a scoring network to select top-k key-value pairs for each query, as proposed in **Lou et al. (2024)**.\n\n- **Implementation Strategy**:\n\n  - Integrate a differentiable top-k operator within the attention mechanism to focus on the most relevant tokens.\n\n  - This can be applied at each level of the hierarchical attention to reduce computational complexity.\n\n- **Benefits**:\n\n  - **Computational Efficiency**: Reduces the quadratic complexity of standard attention mechanisms.\n\n  - **Performance**: Helps the model focus on the most salient information, potentially improving accuracy.\n\n##### **Utilizing Landmark Attention and Block-Recurrent Mechanisms**\n\n- **Landmark Attention**: Use landmark tokens to represent blocks of input data, enabling efficient retrieval of relevant context (**Mohtashami & Jaggi, 2023**).\n\n- **Block-Recurrent Transformers**: Process input data in blocks and use recurrent connections to capture dependencies across blocks (**Hutchins et al., 2022**).\n\n- **Implementation Strategy**:\n\n  - Segment the input sequence into blocks and assign landmark tokens.\n\n  - The hierarchical attention can then attend to these landmark tokens to capture long-range dependencies efficiently.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces the number of computations by summarizing information with landmark tokens.\n\n  - **Long-Range Dependency Modeling**: Improves the model's ability to handle long sequences.\n\n##### **Incorporating Feedback Attention Memory (FAM)**\n\n- **Feedback Loops in Attention**: Implement a feedback loop within the attention mechanism, as suggested by **Hwang et al. (2024)**, to allow the model to attend to its own latent representations.\n\n- **Implementation Strategy**:\n\n  - Modify the attention mechanism to include a feedback component that reintroduces previous hidden states.\n\n  - This can enhance the working memory capability of the Transformer.\n\n- **Benefits**:\n\n  - **Infinite Context Processing**: Enables the model to process indefinitely long sequences effectively.\n\n  - **Improved Memory**: Enhances the model's capacity to retain and utilize past information.\n\n##### **Maintaining Training Stability with Dynamic Layer Normalization**\n\n- **Dynamic Layer Normalization (DLN)**: Continue to use DLN to stabilize training in the presence of adaptive mechanisms.\n\n- **Interaction with Adaptive Components**:\n\n  - Ensure that the adaptive attention scaling and compressive memory mechanisms integrate smoothly with DLN.\n\n  - Careful design of normalization layers is crucial to prevent training instability.\n\n##### **Practical Implementation Considerations**\n\n- **Efficiency**: Aim for linear time and space complexity by leveraging sparse attention and compressive memory.\n\n- **Scalability**: Design the model to handle longer sequences without a proportional increase in computational resources.\n\n- **Compatibility**: Ensure that modifications are compatible with existing Transformer architectures and do not require excessive retraining from scratch.\n\n#### 4. **Future Search Plan / Next Steps**\n\n- **Finalize the GAU Design**:\n\n  - Combine the insights to modify the **HierarchicalAdaptiveAttention** GAU, integrating adaptive attention scaling, compressive memory, and sparse attention mechanisms.\n\n- **Prototype Implementation**:\n\n  - Begin implementing the modified GAU and test on small-scale datasets to validate functionality.\n\n- **Performance Evaluation**:\n\n  - Compare the new model's performance against baseline models (e.g., standard Transformers, HieraNorm-AttnGPT, AdaptiveTTT) on tasks involving long sequences.\n\n- **Training Stability Analysis**:\n\n  - Monitor training dynamics to ensure that the integration of adaptive components does not introduce instability.\n\n- **Fine-Tuning and Optimization**:\n\n  - Adjust hyperparameters related to attention span, compression ratios, and sparse attention thresholds.\n\n- **Documentation**:\n\n  - Prepare comprehensive documentation of the design choices, implementation details, and experimental results.\n\n#### 5. **References**\n\n1. **Bulatov, A., Kuratov, Y., & Burtsev, M. (2022).** Recurrent Memory Transformer. *Neural Information Processing Systems*.\n\n2. **Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023).** Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. *Neural Information Processing Systems*.\n\n3. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024).** Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n4. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024).** TransformerFAM: Feedback Attention is Working Memory. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n5. **Ermi\u015f, B., Zappella, G., Wistuba, M., Rawal, A., & Archambeau, C. (2022).** Memory Efficient Continual Learning with Transformers. *Neural Information Processing Systems*.\n\n6. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020).** Memformer: A Memory-Augmented Transformer for Sequence Modeling. *AACL/IJCNLP*.\n\n7. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019).** Compressive Transformers for Long-Range Sequence Modelling. *arXiv preprint arXiv:1911.05507*.\n\n8. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024).** Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. *International Conference on Machine Learning*.\n\n9. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024).** Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. *arXiv preprint arXiv:2401.00001*.\n\n10. **Mohtashami, A., & Jaggi, M. (2023).** Landmark Attention: Random-Access Infinite Context Length for Transformers. *Neural Information Processing Systems*.\n\n---\n\n**I'm ready.**",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\n---\n\n### **Analysis Note**\n\n#### 1. **Summary of Analysis**\n\nI have conducted thorough research over multiple rounds to design a novel language model block that combines the strengths of **HieraNorm-AttnGPT** (Parent 1) and **AdaptiveTTT** (Parent 2). The goal is to integrate:\n\n- **Hierarchical Adaptive Multi-Head Attention** from HieraNorm-AttnGPT.\n- **Adaptive Attention Scaling** and **Selective Memory Compression** from AdaptiveTTT.\n\nIn this final analysis, I focused on practical implementation strategies for integrating adaptive attention scaling and compressive memory into hierarchical Transformer architectures. The key is to create a new GAU that efficiently handles long sequences with low computational and memory costs while maintaining high performance and scalability.\n\n#### 2. **Useful References with Excerpts**\n\n1. **Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). \"Recurrent Memory Transformer.\" Neural Information Processing Systems.**\n\n   *\"We propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence... This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing.\"*\n\n2. **Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). \"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.\" Neural Information Processing Systems.**\n\n   *\"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our empirical findings demonstrate that we can effectively prune up to 80% of the context without significant performance degradation.\"*\n\n3. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.\" arXiv preprint arXiv:XXXX.XXXXX.**\n\n   *\"A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"*\n\n4. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024). \"TransformerFAM: Feedback Attention is Working Memory.\" arXiv preprint arXiv:XXXX.XXXXX.**\n\n   *\"We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences.\"*\n\n5. **Ermi\u015f, B., Zappella, G., Wistuba, M., Rawal, A., & Archambeau, C. (2022). \"Memory Efficient Continual Learning with Transformers.\" Neural Information Processing Systems.**\n\n   *\"We devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters that is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks.\"*\n\n6. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). \"Memformer: A Memory-Augmented Transformer for Sequence Modeling.\" AACL/IJCNLP.**\n\n   *\"Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information... Our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity.\"*\n\n7. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). \"Compressive Transformers for Long-Range Sequence Modelling.\" arXiv preprint arXiv:1911.05507.**\n\n   *\"We augment the Transformer with a compression mechanism, which results in a significant improvement in memory usage and enables the model to handle longer sequences effectively.\"*\n\n8. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.\" International Conference on Machine Learning.**\n\n   *\"We propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention and key-value eviction policies.\"*\n\n9. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers.\" arXiv preprint arXiv:2401.00001.**\n\n   *\"Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization.\"*\n\n10. **Mohtashami, A., & Jaggi, M. (2023). \"Landmark Attention: Random-Access Infinite Context Length for Transformers.\" Neural Information Processing Systems.**\n\n    *\"Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism.\"*\n\n#### 3. **Key Insights and Detailed Analysis**\n\n##### **Integrating Adaptive Attention Scaling into Hierarchical Attention**\n\n- **Adaptive Attention Span**: Drawing inspiration from **Sukhbaatar & Fergus (2019)**, integrating an adaptive attention span into the hierarchical attention mechanism allows each attention head to dynamically adjust its span based on input complexity.\n\n- **Implementation Strategy**:\n\n  - Modify the **HierarchicalAdaptiveAttention** GAU to include a learnable attention span parameter for each head or group of heads.\n\n  - The attention span can be dynamically adjusted during training via gradient descent, allowing the model to focus computational resources where needed.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces unnecessary computation by limiting attention to relevant contexts.\n\n  - **Performance**: Improves the ability to model long-range dependencies without excessive computational burden.\n\n##### **Incorporating Compressive Memory Mechanisms**\n\n- **Memformer and Compressive Transformers**: Utilize a compressive memory that interacts with the Transformer to store and retrieve information efficiently, as seen in **Wu et al. (2020)** and **Rae et al. (2019)**.\n\n- **Dynamic Memory Compression (DMC)**:\n\n  - Implement online key-value cache compression during inference, based on **Nawrot et al. (2024)**.\n\n  - Design a compression module within the GAU that selectively compresses less important key-value pairs to reduce memory usage.\n\n- **Implementation Strategy**:\n\n  - Within the **HierarchicalAdaptiveAttention**, include a compressive memory module that stores past key-value pairs.\n\n  - Use importance scores (e.g., attention weights) to determine which key-value pairs to compress or discard.\n\n- **Benefits**:\n\n  - **Memory Efficiency**: Reduces memory footprint, allowing processing of longer sequences.\n\n  - **Scalability**: Enables the model to handle infinitely long contexts with bounded memory.\n\n##### **Enhancing Hierarchical Attention with Sparse Attention Mechanisms**\n\n- **Sparse Attention**: Incorporate a scoring network to select top-k key-value pairs for each query, as proposed in **Lou et al. (2024)**.\n\n- **Implementation Strategy**:\n\n  - Integrate a differentiable top-k operator within the attention mechanism to focus on the most relevant tokens.\n\n  - This can be applied at each level of the hierarchical attention to reduce computational complexity.\n\n- **Benefits**:\n\n  - **Computational Efficiency**: Reduces the quadratic complexity of standard attention mechanisms.\n\n  - **Performance**: Helps the model focus on the most salient information, potentially improving accuracy.\n\n##### **Utilizing Landmark Attention and Block-Recurrent Mechanisms**\n\n- **Landmark Attention**: Use landmark tokens to represent blocks of input data, enabling efficient retrieval of relevant context (**Mohtashami & Jaggi, 2023**).\n\n- **Block-Recurrent Transformers**: Process input data in blocks and use recurrent connections to capture dependencies across blocks (**Hutchins et al., 2022**).\n\n- **Implementation Strategy**:\n\n  - Segment the input sequence into blocks and assign landmark tokens.\n\n  - The hierarchical attention can then attend to these landmark tokens to capture long-range dependencies efficiently.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces the number of computations by summarizing information with landmark tokens.\n\n  - **Long-Range Dependency Modeling**: Improves the model's ability to handle long sequences.\n\n##### **Incorporating Feedback Attention Memory (FAM)**\n\n- **Feedback Loops in Attention**: Implement a feedback loop within the attention mechanism, as suggested by **Hwang et al. (2024)**, to allow the model to attend to its own latent representations.\n\n- **Implementation Strategy**:\n\n  - Modify the attention mechanism to include a feedback component that reintroduces previous hidden states.\n\n  - This can enhance the working memory capability of the Transformer.\n\n- **Benefits**:\n\n  - **Infinite Context Processing**: Enables the model to process indefinitely long sequences effectively.\n\n  - **Improved Memory**: Enhances the model's capacity to retain and utilize past information.\n\n##### **Maintaining Training Stability with Dynamic Layer Normalization**\n\n- **Dynamic Layer Normalization (DLN)**: Continue to use DLN to stabilize training in the presence of adaptive mechanisms.\n\n- **Interaction with Adaptive Components**:\n\n  - Ensure that the adaptive attention scaling and compressive memory mechanisms integrate smoothly with DLN.\n\n  - Careful design of normalization layers is crucial to prevent training instability.\n\n##### **Practical Implementation Considerations**\n\n- **Efficiency**: Aim for linear time and space complexity by leveraging sparse attention and compressive memory.\n\n- **Scalability**: Design the model to handle longer sequences without a proportional increase in computational resources.\n\n- **Compatibility**: Ensure that modifications are compatible with existing Transformer architectures and do not require excessive retraining from scratch.\n\n#### 4. **Future Search Plan / Next Steps**\n\n- **Finalize the GAU Design**:\n\n  - Combine the insights to modify the **HierarchicalAdaptiveAttention** GAU, integrating adaptive attention scaling, compressive memory, and sparse attention mechanisms.\n\n- **Prototype Implementation**:\n\n  - Begin implementing the modified GAU and test on small-scale datasets to validate functionality.\n\n- **Performance Evaluation**:\n\n  - Compare the new model's performance against baseline models (e.g., standard Transformers, HieraNorm-AttnGPT, AdaptiveTTT) on tasks involving long sequences.\n\n- **Training Stability Analysis**:\n\n  - Monitor training dynamics to ensure that the integration of adaptive components does not introduce instability.\n\n- **Fine-Tuning and Optimization**:\n\n  - Adjust hyperparameters related to attention span, compression ratios, and sparse attention thresholds.\n\n- **Documentation**:\n\n  - Prepare comprehensive documentation of the design choices, implementation details, and experimental results.\n\n#### 5. **References**\n\n1. **Bulatov, A., Kuratov, Y., & Burtsev, M. (2022).** Recurrent Memory Transformer. *Neural Information Processing Systems*.\n\n2. **Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023).** Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. *Neural Information Processing Systems*.\n\n3. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024).** Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n4. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024).** TransformerFAM: Feedback Attention is Working Memory. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n5. **Ermi\u015f, B., Zappella, G., Wistuba, M., Rawal, A., & Archambeau, C. (2022).** Memory Efficient Continual Learning with Transformers. *Neural Information Processing Systems*.\n\n6. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020).** Memformer: A Memory-Augmented Transformer for Sequence Modeling. *AACL/IJCNLP*.\n\n7. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019).** Compressive Transformers for Long-Range Sequence Modelling. *arXiv preprint arXiv:1911.05507*.\n\n8. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024).** Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. *International Conference on Machine Learning*.\n\n9. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024).** Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. *arXiv preprint arXiv:2401.00001*.\n\n10. **Mohtashami, A., & Jaggi, M. (2023).** Landmark Attention: Random-Access Infinite Context Length for Transformers. *Neural Information Processing Systems*.\n\n---\n\n**I'm ready.**\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 1.00)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 2/24 (Score: 1.00)*\n\n```\nA key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1 M sequence length passkey context block retrieval and 500 K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs. ## 1 Introduction\n\nMemory serves as a cornerstone of intelligence, as it enables efficient computations tailored to specific contexts. However, Transformers (Vaswani et al., 2017) and Transformer-based LLMs (Brown et al., 2020; Touvron et al., 2023, Anil et al., 2023; Groeneveld et al. 2024) have a constrained context-dependent memory, due to the nature of the attention mechanism. The attention mechanism in Transformers exhibits quadratic complexity in both memory footprint and computation time. For example, the attention Key-Value (KV) states have 3TB memory footprint for a 500B model with batch size 512 and context length 2048 (Pope et al. 2023). Indeed, scaling LLMs to longer sequences (i.e. 1 M tokens) is challenging with the standard Transformer architectures and serving longer and longer context models becomes costly financially. Compressive memory systems promise to be more scalable and efficient than the attention mechanism for extremely long sequences (Kanerva, 1988; Munkhdalai et al., 2019). Instead of using an array that grows with the input sequence length, a compressive memory primarily maintains a fixed number of parameters to store and recall information with a bounded storage and computation costs. In the compressive memory, new information is added to the memory by changing its parameters with an objective that this information can be recovered back later on. However, the LLMs in their current state have yet to see an effective, practical compressive memory technique that balances simplicity along with quality. In this work, we introduce a novel approach that enables Transformer LLMs to effectively process infinitely long inputs with bounded memory footprint and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention (Figure 1). The Infini-attention incorporates a compressive memory into the vanilla attention mechanism (Bahdanau et al., 2014, Vaswani et al. 2017) and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. Such a subtle but critical modification to the Transformer attention layer enables a natural extension of existing LLMs to infinitely long contexts via continual pre-training and finetuning. Our Infini-attention reuses all the key, value and query states of the standard attention computation for long-term memory consolidation and retrieval. We store old KV states of the attention in the compressive memory, instead of discarding them like in the standard attention mechanism. We then retrieve the values from the memory by using the attention query states when processing subsequent sequences. To compute the final contextual output, the Infini-attention aggregates the long-term memory-retrieved values and the local attention contexts. In our experiments, we show that our approach outperforms baseline models on longcontext language modeling benchmarks while having $114 x$ comprehension ratio in terms of memory size. The model achieves even better perplexity when trained with 100 K sequence length. A 1B LLM naturally scales to 1M sequence length and solves the passkey retrieval task when injected with Infini-attention. Finally, we show that a 8B model with Infiniattention reaches a new SOTA result on a 500 K length book summarization task after continual pre-training and task fine-tuning. In summary, our work makes the following contributions:\n\n1. We introduce a practical and yet powerful attention mechanism - Infini-attention with long-term compressive memory and local causal attention for efficiently modeling both long and short-range contextual dependencies. 2. Infini-attention introduces minimal change to the standard scaled dot-product attention and supports plug-and-play continual pre-training and long-context adaptation by design. 3. Our approach enables Transformer LLMs to scale to infinitely long context with a bounded memory and compute resource by processing extremely long inputs in a streaming fashion. ## 2 Background\n\nRecurrent Neural Networks (RNNs) process a single token $x_{t}$ at each step $t$ and computes a recurrent hidden state $h_{t}$ to represent an entire input sequence Hochreiter \\& Schmidhuber, 1997, Maass et al. 2002):\n\n$$\nh_{t}=R N N\\left(x_{t}, h_{t-1}\\right)\n$$\n\nThe RNN computation is very efficient since the model maintains only a fixed-size vector $h_{t}$ for input sequence. However, for processing long sequences it becomes difficult to store entire contextual information into a single fixed-size vector and this limitation had implications on RNNs utility in certain tasks (Kaiser \\& Sutskever, 2015). To address the limitation, people extended the standard RNNs with an external memory component that can be read from and written to. One such an instance is Metalearned Neural Memory (MNM) (Munkhdalai et al., 2019):\n\n$$\nh_{t}, \\theta_{t}=M N M\\left(x_{t}, h_{t-1}, \\theta_{t-1}\\right)\n$$\n\nMNM learns an additional memory state $\\theta$ parameterized by a feed-forward neural network (FFN) and uses query, key and value vectors (QKV) to interact with the memory, similar to the attention mechanism. To store information, it modifies the parameters of the FFN by using the key vectors as input and the value vectors for the target, and to read memory\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dbb8cf178811d16c1412g-03.jpg?height=663&width=1391&top_left_y=292&top_left_x=367)\n\nFigure 2: Infini-Transformer (top) has an entire context history whereas Transformer-XL (bottom) discards old contexts since it caches the KV states for the last segment only. entries, it forward-passes the query vectors through the memory FFN and retrieves its corresponding value. Like RNNs, the memory state is still bounded in MNM. Unlike the RNNs, the attention mechanism however doesn't maintain a recurrent state and only performs a feed-forward computation on input sequence segment $X_{s}$ :\n\n$$\nO_{s}=\\operatorname{attention}\\left(X_{s}\\right)\n$$\n\nThe attention output $O_{s}$ is simply passed to the next layer and no state is carried over to the next input sequence $X_{s+1}$ at the same attention layer. In the attention layer, in order to capture the dependency between the consequent segments $X_{s}$ and $X_{s+1}$, one needs to process them altogether at the same time and this process becomes a bottleneck requiring large computational resources as the length of input sequence grows more and more. To improve the efficiency while still being able to benefit from the expressiveness of the attention mechanism, this work introduces a recurrent attention layer. ## 3 Method\n\nFigure 2 compares our model, Infini-Transformer, and Transformer-XL (Dai et al, 2019). Similar to Transformer-XL, Infini-Transformer operates on a sequence of segments. We compute the standard causal dot-product attention context within each segment. So the dot-product attention computation is local in a sense that it covers a total $N$ number of tokens of the current segment with index $S$ ( $N$ is the segment length). The local attention (Dai et al., 2019), however, discards the attention states of the previous segment when processing the next one. In Infini-Transformers, instead of leaving out the old KV attention states, we propose to reuse them to maintain the entire context history with a compressive memory. So each attention layer of Infini-Transformers has both global compressive and local fine-grained states. We call such an efficient attention mechanism Infini-attention, which is illustrated in Figure 1 and described formally in the following sections. ### 3.1 Infini-attention\n\nAs shown Figure 1, our Infini-attention is a recurrent attention mechanism that computes both local and global context states and combine them for its output. Similar to multi-head\nattention (MHA), it maintains $H$ number of parallel compressive memory per attention layer ( $H$ is the number of attention heads) in addition to the dot-product attention and like the RNNs and MNM, it maintains a recurrent memory state to efficiently track the long sequence context:\n\n$$\nO_{s}, M_{s}=\\text { infini-attention }\\left(X_{s}, M_{s-1}\\right)\n$$\n\n### 3.1.1 Scaled Dot-product Attention\n\nThe multi-head scaled dot-product attention (Vaswani et al., 2017), specially its self-attention variant (Munkhdalai et al., 2016. Cheng et al., 2016), has been the main building block in LLMs. The MHA's strong capability to model context-dependent dynamic computation and its conveniences of temporal masking have been leveraged extensively in the autoregressive generative models. A single head in the vanilla MHA computes its attention context $A_{\\text {dot }} \\in \\mathbb{R}^{N \\times d_{\\text {value }}}$ from sequence of input segments $X \\in \\mathbb{R}^{N \\times d_{\\text {model }}}$ as follows. First, it computes attention query, key, and value states:\n\n$$\nK=X W_{K}, V=X W_{V} \\text { and } Q=X W_{Q} \\text {. }\n$$\n\nHere, $W_{K} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {key }}}, W_{V} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {value }}}$ and $W_{Q} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {key }}}$ are trainable projection matrices. Then, the attention context is calculated as a weighted average of all other values as\n\n$$\nA_{\\text {dot }}=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{\\text {model }}}}\\right) V\n$$\n\nFor MHA, we compute $H$ number of attention context vectors for each sequence element in parallel, concatenate them along the second dimension and then finally project the concatenated vector to the model space to obtain the attention output. ### 3.1.2 Compressive Memory\n\nIn Infini-attention, instead of computing new memory entries for compressive memory, we reuse the query, key and value states ( $Q, K$ and $V$ ) from the dot-product attention computation. The state sharing and reusing between the dot-product attention and compressive memory not only enables efficient plug-in-play long-context adaptation but also speeds up training and inference. Similar to the prior work (Munkhdalai et al., 2019), our goal is to store bindings of key and value states in the compressive memory and retrieve by using the query vectors. While there are different forms of compressive memory proposed in the literature (Hopfield, 1982; Kanerva, 1988; Schlag et al., 2019; Munkhdalai et al., 2019), for simplicity and computational efficiency, in this work we parameterize the memory with an associative matrix (Schlag et al., 2020). This approach further allows us to cast the memory update and retrieval process as linear attention mechanism (Shen et al., 2018) and to leverage stable training techniques from the related methods. Specially, we adopt the update rule and retrieval mechanism by Katharopoulos et al. (2020) mainly due to its simplicity and competitive performance. Memory retrieval. In Infini-attention, we retrieve new content $A_{\\text {mem }} \\in \\mathbb{R}^{N \\times d_{\\text {value }}}$ from the memory $M_{s-1} \\in \\mathbb{R}^{d_{\\text {key }} \\times d_{\\text {value }}}$ by using the query $Q \\in \\mathbb{R}^{N \\times d_{\\text {key }}}$ as:\n\n$$\nA_{\\text {mem }}=\\frac{\\sigma(Q) M_{s-1}}{\\sigma(Q) z_{s-1}}\n$$\n\nHere, $\\sigma$ and $z_{s-1} \\in \\mathbb{R}^{d_{\\text {key }}}$ are a nonlinear activation function and a normalization term, respectively.\n```\n\n#### 2. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.95)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 37/41 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2304.10592, 2023. ## A. Comprehensive Related Works\n\nAutoregressive LLMs. Transformers (Vaswani et al., 2017; Dosovitskiy et al., 2021) have significantly advanced the fields of language and vision, leading to the development of foundation LLMs such as ChatGPT (Brown et al., 2020; OpenAI, 2023b), LLaMA (Touvron et al., 2023a;b), Gemini (Anil et al., 2023), DALL-E (Ramesh et al., 2021), etc. To date, various Transformers have emerged to serve distinct needs, broadly categorized into three types: encoder-based, decoder-based, and encoder-decoder models. Encoder-based models like BERT (Devlin et al., 2018) focus on natural language understanding and are also commonly used in image processing (Dosovitskiy et al., 2021). Encoder-decoder models like the original Transformer (Vaswani et al., 2017), Bard (Waisberg et al., 2023), and T5 (Raffel et al., 2020; Roberts et al., 2022) are designed for sequence-to-sequence tasks (e.g., translation, speech recognition), where the encoder extracts features and the decoder produces outputs based on these features. Decoder-based models, including GPT (Radford et al., 2019; OpenAI, 2023b) and LLaMA (Touvron et al., 2023a), generate text sequentially by predicting the next token based on previous ones. All these models leverage Transformer architectures but differ in their specific purposes and structures. Both encoders and decoders are leveraged in multimodal models like MiniGPT (Zhu et al., 2023; Chen et al., 2023b) and DALL-E (Ramesh et al., 2021). Note that the model architectures used in all categories are based on Transformer. The primary difference lies in their purpose: the encoder is designed to extract features, while the decoder focuses on scoring and generating outputs. Our work presents a comprehensive study of applying linear attention techniques to the encoder/decoder-based LLMs. Efficient Linear Attention. Transformers' self-attention modules, known for their quadratic computational complexity (Zhu et al., 2021; Katharopoulos et al., 2020), have spurred the development of linear attention methods to improve efficiency, especially in encoder-based LLMs for better training and inference. Techniques such as local attentions (Liu et al., 2021; Arar et al., 2022; Wang et al., 2020; Tu et al., 2022; You et al., 2023a) limit self-attention to neighboring tokens or group attention queries to reduce the computational cost, while kernel-based linear attentions (Liu et al., 2021; Arar et al., 2022; Wang et al., 2020; Tu et al., 2022; You et al., 2024) decompose the softmax with kernel functions and exchange the computation order. However, only a few linear attention approaches focus on decoder-based autoregressive LLMs, aiming to reduce RNN-style sequential state updates over a large number of steps (Hua et al., 2022; Katharopoulos et al., 2020). Recent studies, like LongLoRA (Chen et al., 2023c), aim to adapt local attention techniques for efficient fine-tuning of pre-trained autoregressive LLMs, yet a thorough analysis comparing various linear attention methods for autoregressive LLMs remains lacking. This paper uniquely provides a systematic review of existing linear attentions for decoder-based autoregressive LLMs and investigates how to efficiently enhance less effective linear attention methods. Speculative Decoding. Linear attention techniques alleviate the training inefficiency in LLMs by mitigating the quadratic complexity with regard to the number of input tokens. However, during deployment, autoregressive decoding necessitates sequential token-by-token text generation, which curtails parallelism and restricts the number of input tokens. Speculative decoding (Chen et al., 2023a; Miao et al., 2023; Kim et al., 2023; Leviathan et al., 2023; Cai et al., 2023b) has proven to be an effective strategy for boosting parallelism in LLM serving, utilizing small speculative models for initial generation, with original LLMs serving as validators to assess if the output meets standards or needs resampling. Recent works like Medusa (Cai et al., 2023b) further argue that the small speculative models and LLMs can be the same model, and other studies (Schuster et al., 2022; Bae et al., 2023) suggest using shallow layers for generation and deeper layers for verification, based on early exit strategies. Such speculative decoding and linear attention jointly ensure efficient LLM training and generation, especially for long sequence inputs. In this paper, we take the initiative to investigate the synergy between linearized LLMs and speculative sampling, to improve the efficiency of training and serving LLMs. ## B. More Visualization of Training Trajectories. As detailed in Sec. 5.3, we present a quantitative analysis comparing local LAs, grouped LAs, and our augmented LAs that combine both local augmentation and grouped LAs. This appendix provides the training trajectories for GPT-2 using these LA methods. Fig. 10 demonstrates that our local augmentation, specifically masked DWConv, effectively enhances both local and grouped LAs. Moreover, our augmented LAs, which integrate local augmentation with grouped LAs, exhibit the most favorable convergence in terms of loss. ## C. More Profiling on the LLaMA-2-7B Model\n\nWe provide detailed profiling and comparisons below to illustrate the runtime distribution between attention and feed-forward networks (FFNs), highlighting that attention is a bottleneck even for LLMs with 7B parameters. To ensure a real-world\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_1cc36bd06f7697d750b0g-15.jpg?height=446&width=859&top_left_y=227&top_left_x=596)\n\nFigure 10. Visualizing the training trajectories of baseline LAs and our augmented LAs. application scenario, we profiled the LLaMA-2-7B model across four settings of prefill and decode sizes, adhering to benchmarks commonly used in academia and industry, as summarized in Tab. 10. | (Prefill, Decode) | Task | Dataset | Referenced Paper |\n| :---: | :---: | :---: | :---: |\n| $(340,160)$ | Chat | ShareGPT | (Kwon et al., 2023) |\n| $(60,20)$ | Chat | Stanford Alpaca | (Taori et al., 2023) |\n| $(7000,200)$ | Summarization | ArXiv Summarization | (Agrawal et al., 2024) |\n| $(1700,400)$ | Chat | OpenChat ShareGPT 4 | (Agrawal et al., 2024) |\n\nAs shown in Tab. 11, profiling the LLaMA-2-7B models under the four prefill and decode size settings reveals that the average runtime latency attributed to attention and FFNs accounts for $55 \\%$ and $21 \\%$ of the total runtime across these settings, respectively. This indicates that although FFNs are a bottleneck in the model, attention is an even more significant bottleneck, especially for large-scale LLMs and extended dialogue sequences (e.g., $67.8 \\%$ runtime latency for the arxiv summarization task). Therefore, optimizing attention blocks can yield considerable speed improvements, particularly for tasks with large prefill or decode sequence lengths. This is corroborated by contemporary studies on linear attention-based LLMs (Lee et al., 2023; Yang et al., 2023) and efforts to optimize attention, such as FlashAttention (Dao et al., 2022) and FLAT (Kao et al., 2023). Table 11. Latency breakdown of LLaMA-2-7B models under different prefill and decode size settings. | (Prefill, Decode) | $\\mathbf{( 3 4 0 , 1 6 0 )}$ | $\\mathbf{( 6 0 , 2 0 )}$ | $\\mathbf{( 7 0 0 0 , 2 0 0 )}$ | $\\mathbf{( 1 7 0 0 , 4 0 0 )}$ |\n| :--- | :---: | :---: | :---: | :---: |\n| Attention (ms) | $158.97 \\mathbf{( 4 8 .\n```\n\n#### 3. Softmax Attention with Constant Cost per Token (Avg. Score: 0.92)\n\n*Franz A. Heinsen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work proposes a simple modification to the conventional attention mechanism applied by Transformers, which quantifies pairwise query-key similarity with scaled dot-products with the logarithms of scaled dot-products of exponentials, and linearizes attention with exponential kernel feature maps.\n\n**Abstract:** We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.\n\n##### *Relevant Chunk: No. 3/8 (Score: 0.92)*\n\n```\n(2021), and Poli et al. (2023). More recently, generalized state space models that build on previous research (Martin and Cundy, 2017) (Gu et al., 2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state (Peng et al., 2023) (Gu and Dao, 2023) (Katsch, 2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention (Jelassi et al., 2024). ### 1.1 Modifying Attention\n\nWe find that a simple modification to conventional attention linearizes it (Katharopoulos et al., 2020) with exponential kernel feature maps, and we show that this modification renders attention expressible as a composition of log-sums of exponentials, with a fixed-size latent space, for sequential application with constant cost per token. We implement our modification, verify that it works, and conclude that it is a promising alternative. The modification we propose is:\n\n$$\n\\begin{gathered}\n\\text { modified }(Q, K, V):= \\\\\n\\text { Attention }(Q) \\\\\n\\text { Softmax }\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V\n\\end{gathered}\n$$\n\nwhere queries $Q$, keys $K$ and values $V$ have $n_{Q} \\times d_{K}, n_{K} \\times d_{K}$, and $n_{K} \\times d_{V}$ elements, respectively, and $c$ is a scalar constant, all in $\\mathbb{R}$. We compute all exponentials elementwise. ### 1.2 As Log-Sums of Exponentials\n\nIn Section 2, we prove that\n\n$$\n\\begin{aligned}\n& \\text { modified } \\\\\n& \\operatorname{Attention}(Q, K, V)=\\exp (\\log S-\\log Z) \\text {, }\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nThe elementwise sums are over compatible dimensions, broadcasting over all other dimensions, from left to right-e.g., before reduction, the broadcasted elementwise sum $K^{T}+\\log V$ has $d_{K} \\times n_{K} \\times d_{V}$ elements. The functions $\\operatorname{LSE}_{\\left[d_{K}\\right]}(\\cdot)$ and $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$ compute log-sums of exponentials over the dimension indexed by $\\left(1,2, \\ldots, d_{K}\\right)$ and $\\left(1,2, \\ldots, n_{K}\\right)$, respectively. If any of $V$ 's elements are negative, $\\log V$ is complex, and therefore so is $\\log S$, but all Softmax mixtures of $V$ remain over $\\mathbb{R}$ because they are a composition of operations under which $\\mathbb{R}$ is closed (1). ### 1.3 Autoregressive Case\n\nFor autoregressive attention, in which $n_{Q}=n_{K}$ and for each query at step $t$ we compute attention only over $t$ trailing tokens, we note that in (3), all sequential dependencies are modeled by the logsums computed with $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$, so we can compute autoregressive $\\log S$ and $\\log Z$ with:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times n_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\times n_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nwhere the function $\\operatorname{LCSE}_{\\left[n_{K}\\right]}(\\cdot)$ computes a logcumulative-sum of exponentials over the dimension indexed by and $\\left(1,2, \\ldots, n_{K}\\right)$. For sequential application, given a new query $Q_{t}$ at step $t$, we need only the end-states of the two log-cumulative-sums of exponentials:\n\n$$\n\\begin{aligned}\n& \\log S_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(S)}}_{d_{K} \\times d_{V}}) \\\\\n& \\log Z_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(Z)}}_{d_{K}})\n\\end{aligned}\n$$\n\nwhere hidden states $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the states of the two log-cumulative-sums at step $t$ :\n\n$$\n\\begin{aligned}\n& H_{t}^{(S)}=\\log \\left(\\exp \\left(H_{t-1}^{(S)}\\right)+\\exp \\left(K_{t}+\\log V_{t}\\right)\\right) \\\\\n& H_{t}^{(Z)}=\\log \\left(\\exp \\left(H_{t-1}^{(Z)}\\right)+\\exp \\left(K_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwith zeros as their initial condition:\n\n$$\n\\begin{aligned}\n& H_{0}^{(S)}=\\{0\\}^{d_{K} \\times d_{V}} \\\\\n& H_{0}^{(Z)}=\\{0\\}^{d_{K}}\n\\end{aligned}\n$$\n\nTogether, $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ hold the latent, or hidden, state of autoregressive attention's computation at step $t$. They enable us to compute autoregressive attention sequentially with constant time and space complexity per token, $\\mathcal{O}(1)$. ### 1.4 Non-Autoregressive Case\n\nFor non-autoregressive attention, in which $n_{Q}$ may differ from $n_{K}$ and for each query we compute attention over all tokens in the sequence, we compute $\\log S$ and $\\log Z$ with (3). For sequential application, in which we add a new token to the input context at step $t$, with key $K_{t}$ and value $V_{t}$, we compute $\\log S$ and $\\log Z$ for all queries from the updated hidden states:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(S)}\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(Z)}\\right)\n\\end{aligned}\n$$\n\nwhere $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the hidden states at step $t$ (6), with zeros as their initial condition (7). ## 2 Proof\n\nGiven a query $q$ and a key $k$ in $\\mathbb{R}^{d_{K}}$, the logarithm of the dot-product of their exponentials is $\\log \\left(\\sum(\\exp (q) \\odot \\exp (k))\\right)=\\operatorname{LSE}(q+k)$, where $\\odot$ denotes an elementwise product. Log-sums of exponentials are associative and commutative, making the proof fairly straightforward. For clarity's sake, we walk step-by-step through a sequence of algebraic manipulations. We start by expanding the Softmax function in (1) and simplifying the resulting expression. We obtain a form of linear attention (Katharopoulos et al., 2020) with exponential kernel feature maps:\n\n$$\n\\begin{gathered}\n\\operatorname{Softmax}\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V= \\\\\n{\\left[\\frac{\\exp (Q) \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\exp (Q) \\exp (K)^{T}}\\right] V}\n\\end{gathered}\n$$\n\nwhere $\\sum_{\\left[n_{K}\\right]}$ normalizes each row to a probability distribution. The scaling constant $\\exp (c)$ disappears because it becomes a common divisor of numerator and denominator expressions. Note that the feature function corresponding to the exponential kernel is infinite dimensional. Substitute the dot-products of exponentiated queries and exponentiated keys with equivalent explicit summations over elementwise products:\n\n$$\n\\left[\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\\right] V\n$$\n\nwhere the elementwise product $\\odot$ is over compatible dimensions, broadcasting over any other dimensions, from left to right, such that the broadcasted elementwise product $\\exp (Q) \\odot \\exp (K)^{T}$ has $n_{Q} \\times d_{K} \\times n_{K}$ elements. ${ }^{2}$\nExpress matrix multiplication with $V$ as a summation over broadcasted elementwise products:\n\n$$\n\\frac{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T} \\odot V}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\n$$\n\nBoth $\\exp (K)^{T}$ and $V$ have a dimension indexed by $\\left(1,2, \\ldots, n_{K}\\right)$, but $\\exp (Q)$ does not, so we can sum over that dimension before broadcastmultiplying elementwise with $\\exp (Q)$ :\n\n$$\n\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V}{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}}\n$$\n\nDefine $S$ and $Z$ as the expressions that compute numerators and denominators, respectively,\n\n$$\n\\begin{aligned}\n& S:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V \\\\\n& Z:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}\n\\end{aligned}\n$$\n\nand take their logarithms. We obtain:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)\\right),\n\\end{aligned}\n$$\n\nwhich is the same as (3). [^1]\n## 3 Implementation\n\nAs proof of concept, we implement our attention mechanism for both autoregressive applications (e.g., generative language modeling) and nonautoregressive applications (e.g., masked language modeling). For simplicity and expediency, we limit our implementation in two significant ways: First, we restrict $V$ to elements $\\geq 0$ to avoid dealing with complex floating-point numbers, which incur greater overhead and are more cumbersome to manipulate than real floating-point numbers with existing software infrastructure. Second, when computing autoregressive attention over $n_{K}$ tokens, we first compute all $n_{K}$ hidden states with a parallel scan, and then reduce them, which is space-inefficient but easier to implement with existing software infrastructure. ${ }^{3}$\n\nWe apply our implementation in a small generative language model ( 125 M parameters, 50257 token ids, 768 embedding features). For numerical stability, in each layer we compute $\\log V$ over $\\mathbb{R}$ directly, with a dense feed-forward transformation of token states, implicitly defining $V$ as $\\log V$ 's exponential but never actually computing it. To remain in $\\mathbb{R}$, we use the logarithm of attention as input to subsequent transformations in the layer, i.e., the input to subsequent transformations is $\\log S-\\log Z$ instead of $\\exp (\\log S-\\log Z)$. Please see our published code for all model details. We train the model on 300B tokens from The Pile (Gao et al., 2020) with a conventional sequence length of 1024 tokens, and obtain a crossentropy loss of 2.47 , competitive with state-of-theart generative language models of similar size. ## 4 Conclusions\n\nBy all indications, our attention mechanism is a promising alternative to the conventional one, but the evidence we have so far is too scant to be conclusive. An adequate comparison requires addressing our implementation's temporary limitations and evaluating models with one to several orders of magnitude more parameters on a diverse set of benchmarks and downstream tasks. [^2]\n## References\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR abs/1904.10509. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 4. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention (Avg. Score: 0.90)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n##### *Relevant Chunk: No. 15/39 (Score: 0.90)*\n\n```\nTable 3. Performance Comparison on SCROLLS (Shaham et al., 2022): A review of models up to 1 billion parameters on 2048 pre-training sequence length. PS: parameter size (billion). T: tokens (billion). | Model | PS | T | GovRep | SumScr | QMSum | Qspr | Nrtv | QALT | CNLI | Avg |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | B | B | ROUGE-1/2/L | ROUGE-1/2/L | ROUGE-1/2/L | F1 | F1 | EM | EM |  |\n| OPT | 0.35 | 0.30 | $2.52 / 0.53 / 2.24$ | $7.72 / 0.68 / 6.52$ | $8.05 / 1.79 / 6.6$ | 13.13 | 10.13 | 29.05 | 9.16 | 7.55 |\n| Pythia | 0.40 | 0.30 | $4.96 / 1.19 / 4.06$ | $2.03 / 0.2 / 1.79$ | $7.51 / 1.43 / 6.08$ | 15.27 | 8.24 | 28.57 | 15.24 | 7.43 |\n| RWKV | 0.43 | - | $1.63 / 0.4 / 1.49$ | $0.94 / 0.11 / 0.76$ | $10.19 / 2.26 / 8.06$ | 13.16 | 9.76 | 26.32 | 16.49 | 7.04 |\n| TNL | 0.39 | 1.0 | $3.67 / 1.16 / 3.14$ | $8.27 / 0.82 / 6.91$ | $13.62 / 3.29 / 10.95$ | 14.29 | 11.69 | 28.14 | 17.36 | 9.48 |\n| OPT | 1.3 | 0.3 | $5.7 / 2.09 / 4.41$ | $10.17 / 0.82 / 8.29$ | $12.36 / 3.15 / 9.85$ | 18.37 | 13.42 | 29.15 | 12.4 | 10.02 |\n| Pythia | 1.4 | 0.3 | $4.03 / 1.25 / 3.33$ | $8.34 / 0.87 / 6.97$ | $13.17 / 3.4 / 10.92$ | 16.09 | 11.91 | 28.72 | 9.06 | 9.08 |\n| Falcon | 1.0 | 0.35 | $2.74 / / 0.67 / 2.37$ | $10.95 / 1.28 / 8.66$ | $13.29 / 3.09 / 10.58$ | 16.17 | 12.91 | 29.19 | 14.75 | 9.74 |\n| TNL | 1.0 | 1.2 | $6.81 / 2.30 / 5.25$ | $12.28 / 1.23 / 9.27$ | $14.60 / 3.51 / 11.62$ | 15.02 | 14.66 | 28.72 | 37.32 | 12.51 |\n\nTable 7. Exploration of Normalization Function. The deviation in results among the bellowing normalization functions is minimal. | Norm Type | Params | Updates | Loss | PPL |\n| :--- | :--- | :--- | :--- | :--- |\n| SRMSNorm | 385 M | 100 K | 2.248 | 4.770 |\n| RMSNorm | 385 M | 100 K | 2.247 | 4.766 |\n| LayerNorm | 385 M | 100 K | 2.247 | 4.765 |\n\nNormalization Functions: our study involved testing various normalization techniques-SRMSNorm, RMSNorm, and LayerNorm-on TNL, finding little difference in their effectiveness. However, we enhanced SRMSNorm using Triton, resulting in notable improvements in processing speed for larger dimensions. GLA Activation Functions: in our study on the GLA (Gated Linear Attention) mechanism, we evaluated activation functions, finding Swish and 1+elu to perform similarly, as detailed in Table 8. However, due to NaN issues with 1+elu in our 7B model, we opted for Swish. Table 8. Ablations on GLA activation functions. The results obtained from different activation functions were virtually identical. | GLA Act | Params | Updates | Loss | PPL |\n| :--- | :--- | :--- | :--- | :--- |\n| Swish | 385 M | 100 K | 2.248 | 4.770 |\n| No Act | 385 M | 100 K | 2.283 | 4.882 |\n| 1+elu | 385 M | 100 K | 2.252 | 4.767 |\n\nGLU Activation Functions: our experiment additionally involved removing the activation function from the Gated Linear Units (GLU), showing minimal effect on outcomes as per Table 9. Therefore, we opted for the Simple Gated Linear Units (SGLU) configuration in our model. Table 9. Ablations on GLU activation functions. The exclusion of the activation function had no negative impact on the results. | GLU Act | Params | Updates | Loss | PPL |\n| :--- | :--- | :--- | :--- | :--- |\n| No Act | 385 M | 100 K | 2.248 | 4.770 |\n| Swish | 385 M | 100 K | 2.254 | 4.788 |\n\n## 6. Conclusion\n\nWe introduced Lightning Attention, the first linear attention implementation that unleashed the full power of linear attention. As a result, our Lightning Attention can handle various sequence lengths with a constant speed under a constant memory footprint. The main concept is to divide the calculation of attention into intro-blocks and inter-blocks, while applying distinct computation techniques to perform the calculation. A new architecture, TNL, that is tailored for Lightning Attention is presented. TNL outperforms existing efficient language models in terms of both efficiency and accuracy and achieves competitive performance compared to state-of-the-art large language models using conventional transformer architectures. ## Acknowledgement\n\nThis work is partially supported by the National Key R\\&D Program of China (NO.2022ZD0160100). We thank Songlin Yang for the helpful discussions. ## Impact Statement\n\nThe introduction of Lightning Attention and its accompanying architecture TNL, heralds significant shifts in machine learning, particularly in language model efficiency and accessibility. By addressing the limitations of linear attention in varying sequence lengths without increasing memory consumption, this advancement democratizes access to state-of-the-art language models, potentially reducing the computational and environmental footprint of large-scale AI systems. Ethically, it underscores a move towards more sustainable AI practices, yet raises questions about the proliferation of powerful language models and their societal impacts, including concerns over privacy, misinformation, and the digital divide. ## References\n\nAlmazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Heslow, D., Launay, J., Malartic, Q., et al. Falcon-40b: an open large language model with state-of-the-art performance. Technical report, Technical report, Technology Innovation Institute, 2023. Bahdanau, D., Cho, K., and Bengio, Y. Neural machine translation by jointly learning to align and translate, 2016. Baichuan. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305, 2023. URL https: //arxiv.org/abs/2309.10305. Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., O\u2019Brien, K., Hallahan, E., Khan, M.\n```\n\n#### 5. Max-Margin Token Selection in Attention Mechanism (Avg. Score: 0.90)\n\n*Davoud Ataee Tarzanagh, Yingcong Li, Xuechen Zhang, Samet Oymak*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 20  (*Influential: 2*)\n\n**TL;DR:** This work explores the seminal softmax-attention model and proves that running gradient descent on $\\boldsymbol{p}$ converges in direction to a max-margin solution that separates $\\textit{locally-optimal}$ tokens from non-optical ones, which clearly formalizes attention as an optimal token selection mechanism.\n\n**Abstract:** Attention mechanism is a central component of the transformer architecture which led to the phenomenal success of large language models. However, the theoretical principles underlying the attention mechanism are poorly understood, especially its nonconvex optimization dynamics. In this work, we explore the seminal softmax-attention model $f(\\boldsymbol{X})=\\langle \\boldsymbol{Xv}, \\texttt{softmax}(\\boldsymbol{XWp})\\rangle$, where $\\boldsymbol{X}$ is the token sequence and $(\\boldsymbol{v},\\boldsymbol{W},\\boldsymbol{p})$ are trainable parameters. We prove that running gradient descent on $\\boldsymbol{p}$, or equivalently $\\boldsymbol{W}$, converges in direction to a max-margin solution that separates $\\textit{locally-optimal}$ tokens from non-optimal ones. This clearly formalizes attention as an optimal token selection mechanism. Remarkably, our results are applicable to general data and precisely characterize $\\textit{optimality}$ of tokens in terms of the value embeddings $\\boldsymbol{Xv}$ and problem geometry. We also provide a broader regularization path analysis that establishes the margin maximizing nature of attention even for nonlinear prediction heads. When optimizing $\\boldsymbol{v}$ and $\\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions under which the regularization paths directionally converge to their respective hard-margin SVM solutions where $\\boldsymbol{v}$ separates the input features based on their labels. Interestingly, the SVM formulation of $\\boldsymbol{p}$ is influenced by the support vector geometry of $\\boldsymbol{v}$. Finally, we verify our theoretical findings via numerical experiments and provide insights.\n\n##### *Relevant Chunk: No. 8/70 (Score: 0.90)*\n\n```\nThis is visible in Figure 4(b) where blue norm curve levels off. Real experiments. To study softmax sparsity and the evolution of attention weights throughout training, we train a vision transformer (ViT-base) model [23] from scratch, utilizing the CIFAR10 dataset [24] for 400 epochs with fixed learning rate $3 \\times 10^{-3}$. ViT tokenizes an image into $16 \\times 16$ patches, thus, its softmax attention maps can be easily visualized. We examine the average attention map - associated with the [CLS] token - computed from all 12 attention heads within the model. Figure 6 provides a visual representation of the resulting attention weights $(16 \\times 16$ grids) corresponding to the original patch locations within the image. During the initial epochs of training, the attention weights are randomly distributed and exhibit a dense pattern. However, as the training progresses, the attention map gradually becomes sparser and the attention mechanism begins to concentrate on fewer salient patches within the image that possess distinct features that aid classification. This illustrates the evolution of attention from a random initial state to a more focused and sparse representation. These salient patches highlighted by attention conceptually corresponds to the optimal tokens within our theory. We quantify the sparsity of the attention map via a soft-sparsity measure, denoted by $\\widehat{\\operatorname{nnz}}(\\boldsymbol{s})$ where $\\boldsymbol{s}$ is the softmax probability vector. The soft-sparsity is computed as the ratio of the $\\ell_{1}-$ norm to the squared $\\ell_{2}-$ norm, defined as $\\widehat{\\mathrm{nnz}}(\\boldsymbol{s})=\\|\\boldsymbol{s}\\|_{1} /\\|\\boldsymbol{s}\\|^{2}$. $\\widehat{\\mathrm{nnz}}(s)$ takes values between 1 to $T=256$ and a smaller value indicates a sparser vector. Also note that $\\|s\\|_{1}=\\sum_{t=1}^{T} s_{t}=1$. Together with sparsity, Figure 7 also displays the Frobenius norm of the combined key-query matrix $\\boldsymbol{W}$ of the last attention layer over epochs. The theory suggests that the increase in sparsity is associated with the growth of attention weights - which converge directionally. The results in Figure 7 align with the theory, demonstrating the progressive sparsification of the attention map as $\\|\\boldsymbol{W}\\|_{F}$ grows. Transient optimization dynamics and the influence of the loss function. Theorem 2 shows that the asymptotic direction of gradient descent is determined by $\\boldsymbol{p}^{m m \\star}$. However, it is worth noting that transient dynamics can exhibit bias towards certain input examples and their associated optimal tokens. We illustrate this idea in Fig 5(a), which displays the trajectories of the gradients for different scores and loss functions. We consider two optimal tokens ( $\\star$ ) with scores $\\gamma_{1}=1$ and $\\gamma_{2}=C$, where $C$ varies. For our analysis, we examine the correlation loss $\\ell(x)=-x$ and the logistic loss $\\ell(x)=\\log \\left(1+e^{-x}\\right)$. In essence, as $C$ increases, we can observe that the correlation loss $\\ell(x)=-x$ exhibits a bias towards the token with a high score, while the logistic loss is biased towards the token with a low score. The underlying reason for this behavior can be observed from the gradients of individual inputs: $\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})=\\ell_{i}^{\\prime} \\cdot \\boldsymbol{K}_{i}^{\\top} \\mathbb{S}^{\\prime}(\\boldsymbol{X} \\boldsymbol{p}) \\boldsymbol{X} \\boldsymbol{v}$, where $\\mathbb{S}^{\\prime}(\\cdot)$ represents the derivative of the softmax function and $\\ell_{i}^{\\prime}:=\\ell^{\\prime}\\left(Y_{i} \\cdot \\boldsymbol{v}^{\\top} \\boldsymbol{X}_{i}^{\\top} \\mathbb{S}\\left(\\boldsymbol{X}_{i} \\boldsymbol{p}\\right)\\right)$. Assuming that $\\boldsymbol{p}$ (approximately) selects the optimal tokens, this\nsimplifies to $\\ell_{i}^{\\prime} \\approx \\ell^{\\prime}\\left(\\gamma_{i}\\right)$ and $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto\\left|\\ell^{\\prime}\\left(\\gamma_{i}\\right)\\right| \\cdot \\gamma_{i}$. With the correlation loss, $\\left|\\ell^{\\prime}\\right|=1$, resulting in $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto \\boldsymbol{\\gamma}_{i}$, meaning that a larger score induces a larger gradient. On the other hand, the logistic loss behaves similarly to the exponential loss under separable data, i.e., $\\left|\\ell^{\\prime}\\right|=e^{-x} /\\left(1+e^{-x}\\right) \\approx e^{-x}$. Consequently, $\\left\\|\\nabla \\mathcal{L}_{i}(\\boldsymbol{p})\\right\\| \\propto \\gamma_{i} e^{-\\gamma_{i}} \\approx e^{-\\gamma_{i}}$, indicating that a smaller score leads to a larger gradient. These observations explain the empirical behavior we observe. ## 5 Related Work\n\nImplicit Regularization. The implicit bias of gradient descent in classification tasks involving separable data has been extensively examined by [22, 25, 26, 27, 28, 29]. These works typically use logistic loss or, more generally, exponentially-tailed losses to make connections to margin maximization. These results are also extended to non-separable data by [30, 31, 21]. Furthermore, there have been notable investigations into the implicit bias in regression problems/losses utilizing techniques such as mirror descent $[32,25,33,34,35,36]$. In addition, several papers have explored the implicit bias of stochastic gradient descent [37,38, 39, 40, 41, 42], as well as adaptive and momentum-based methods [43, 44, 45, 46]. Although there are similarities between our optimization approach for $\\boldsymbol{v}$ and existing works, the optimization of $\\boldsymbol{p}$ stands out as significantly different. Firstly, our optimization problem is nonconvex, introducing new challenges and complexities. Secondly, it necessitates the introduction of novel concepts such as locally-optimal tokens and requires a fresh analysis specifically tailored to the cones surrounding them. Attention Mechanism. Transformers, introduced by [6], revolutionized the field of NLP and machine translation, with earlier works on self-attention by [47, 48, 49, 50]. Self-attention differs from traditional models like MLPs and CNNs by leveraging global interactions for feature representations, showing exceptional empirical performance. However, the underlying mechanisms and learning processes of the attention layer remain unknown. Recent studies such as [51, 52, 53, 54, 23] have focused on specific aspects like representing sparse functions, convex-relaxations, and expressive power. In contrast to our nonconvex (ERM), [52] studies self-attention with linear activation instead of softmax, while [53] approximates softmax using a linear operation with unit simplex constraints. Their main objective is to derive convex reformulations for ERM-based training problem. [55, 56] have developed initial results to characterize the optimization and generalization dynamics of attention. [17] is another closely related work where the authors analyze the same attention model (ERM) as us. Specifically, they jointly optimize $\\boldsymbol{v}, \\boldsymbol{p}$ for three gradient iterations for a contextual dataset model. However, all of these works make stringent assumptions on the data, namely, tokens are tightly clusterable or can be clearly split into clear relevant and irrelevant sets. Additionally [56] requires assumptions on initialization and [55] considers a simplified attention structure where the attention matrix is not directly parameterized with respect to the input. Our work links attention models to hard-margin SVM problems and pioneers the study of gradient descent's implicit bias in these models. ## 6 Discussion\n\nWe have provided a thorough optimization-theoretic characterization of the fundamental attention model $f(\\boldsymbol{X})=\\boldsymbol{v}^{\\top} \\boldsymbol{X}^{\\top} \\mathbb{S}(\\boldsymbol{X} \\boldsymbol{W} \\boldsymbol{p})$ by formally connecting it to max-margin problems. We first established the convergence of gradient descent on $\\boldsymbol{p}$ (or equivalently $\\boldsymbol{W}$ ) in isolation. We also explored joint convergence of $(\\boldsymbol{v}, \\boldsymbol{p})$ via regularization path which revealed surprising implicit biases such as (10). These findings motivate several exciting avenues for future research. An immediate open problem is characterizing the (local) convergence of gradient descent for joint optimization of ( $\\boldsymbol{v}, \\boldsymbol{p}$ ). Another major direction is to extend similar analysis to study self-attention layer (4) or to allow for multiple tunable tokens (where $\\boldsymbol{p}$ becomes a matrix). Either setting will enrich the problem by allowing the attention to discover multiple hyperplanes to separate tokens. While our convergence guarantees apply when tokens are separable, it would be interesting to characterize the non-separable geometry by leveraging results developed for logistic regression analysis [31, 22]. Ideas from such earlier results can also be useful for characterizing the non-asymptotic/transient dynamics of how gradient descent aligns with the max-margin direction. Overall, we believe that max-margin token selection is a fundamental characteristic of attention mechanism and the theory developed in this work lays the groundwork of these future extensions. ## Acknowledgements\n\nThis work was supported by the NSF grants CCF-2046816 and CCF-2212426, Google Research Scholar award, and Army Research Office grant W911NF2110312. The authors express their gratitude for the valuable feedback provided by the anonymous reviewers and Christos Thrampoulidis, which has significantly improved this paper. ## References\n\n[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. The International Conference on Learning Representations, 2015. [2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, and et al. Language models are few-shot learners. In Advances in neural information processing systems, volume 33, pages 1877 -1901, 2020. [3] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [4] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [5] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. [6] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, volume 30, 2017. [7] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. [8] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [9] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models.\n```\n\n\n\n---\n## Web search results\n\n ### **Understanding the Goal and Intent**\n\nThe researcher aims to design a novel language model (LM) block that combines the strengths of **HieraNorm-AttnGPT** and **AdaptiveTTT** to outperform current state-of-the-art models. The key objectives include:\n\n- **Low Perplexity**: Achieve better performance on language modeling tasks.\n- **High Accuracy**: Excel in downstream tasks.\n- **Robustness**: Handle varied inputs effectively.\n- **Efficiency**: Optimize both training and inference processes.\n- **Scalability**: Ensure the model can handle longer sequences and larger datasets efficiently.\n\n### **Key Components and Strategies**\n\n1. **Hierarchical Adaptive Multi-Head Attention**:\n   - Integrate adaptive attention scaling to dynamically adjust the attention span based on input complexity, inspired by **Sukhbaatar & Fergus (2019)** and detailed in the analysis.\n\n2. **Adaptive Attention Scaling and Selective Memory Compression**:\n   - Implement adaptive attention scaling and compressive memory mechanisms, drawing from works like **Wu et al. (2020)**, **Rae et al. (2019)**, and **Nawrot et al. (2024)**.\n\n3. **Sparse Attention Mechanisms**:\n   - Incorporate sparse attention to reduce computational complexity, as proposed in **Lou et al. (2024)**.\n\n4. **Landmark Attention and Block-Recurrent Mechanisms**:\n   - Use landmark tokens to represent blocks of input data, enabling efficient retrieval of relevant context, as seen in **Mohtashami & Jaggi (2023)**.\n\n5. **Feedback Attention Memory (FAM)**:\n   - Implement feedback loops within the attention mechanism to enhance working memory, based on **Hwang et al. (2024)**.\n\n6. **Dynamic Layer Normalization**:\n   - Ensure training stability by integrating dynamic layer normalization, which is crucial for adaptive mechanisms.\n\n### **Most Useful Information**\n\n#### **Compressive Memory Mechanisms**\n\n- **Memformer** by **Wu et al. (2020)**: This work introduces a memory-augmented Transformer that interacts with an external dynamic memory, which can be integrated into the hierarchical attention mechanism to store and retrieve information efficiently.\n- **Compressive Transformers** by **Rae et al. (2019)**: This approach augments the Transformer with a compression mechanism, significantly improving memory usage and enabling the model to handle longer sequences.\n- **Dynamic Memory Compression (DMC)** by **Nawrot et al. (2024)**: This method involves online key-value cache compression during inference, which can be used to selectively compress less important key-value pairs, reducing memory usage.\n\n#### **Sparse Attention Mechanisms**\n\n- **Sparser is Faster and Less is More** by **Lou et al. (2024)**: This work integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query, reducing computational complexity.\n\n#### **Landmark Attention and Block-Recurrent Mechanisms**\n\n- **Landmark Attention** by **Mohtashami & Jaggi (2023)**: This method uses landmark tokens to represent blocks of input data, enabling efficient retrieval of relevant context and reducing the number of computations.\n- **Recurrent Memory Transformer** by **Bulatov et al. (2022)**: This architecture processes input data in segments and uses recurrence to capture dependencies across blocks, which can be integrated into the hierarchical attention mechanism.\n\n#### **Feedback Attention Memory (FAM)**\n\n- **TransformerFAM** by **Hwang et al. (2024)**: This approach implements a feedback loop within the attention mechanism, allowing the model to attend to its own latent representations and process indefinitely long sequences effectively.\n\n### **Implementation and Practical Considerations**\n\n- **Efficiency and Scalability**: Ensure that the modifications result in linear time and space complexity, leveraging sparse attention and compressive memory mechanisms.\n- **Training Stability**: Use dynamic layer normalization to stabilize training in the presence of adaptive mechanisms.\n- **Compatibility**: Ensure that the new GAU design is compatible with existing Transformer architectures to avoid excessive retraining.\n\n### **Next Steps**\n\n1. **Finalize the GAU Design**:\n   - Combine insights from compressive memory, sparse attention, landmark attention, and feedback attention mechanisms into the **HierarchicalAdaptiveAttention** GAU.\n\n2. **Prototype Implementation**:\n   - Implement the modified GAU and test it on small-scale datasets to validate functionality.\n\n3. **Performance Evaluation**:\n   - Compare the new model's performance against baseline models on tasks involving long sequences.\n\n4. **Training Stability Analysis**:\n   - Monitor training dynamics to ensure stability and adjust hyperparameters as necessary.\n\n5. **Fine-Tuning and Optimization**:\n   - Adjust hyperparameters related to attention span, compression ratios, and sparse attention thresholds to optimize performance.\n\n6. **Documentation**:\n   - Prepare comprehensive documentation of the design choices, implementation details, and experimental results.\n\nBy focusing on these key components and strategies, the researcher can develop a novel LM block that integrates the best practices from recent advancements in Transformer architectures, ensuring high performance, efficiency, and scalability.\n\n\nWarning: No keywords detected, external search skipped, please wrap your keywords in a quoted block like this: ```keywords {{Your keywods}} ``` in your response next time.\n\nWarning: No description detected, will use full response to search internal library, please wrap your description in a quoted block like this: ```description {{Your description}}``` in your response next time.",
            "analysis": "---\n\n### **Analysis Note**\n\n#### 1. **Summary of Analysis**\n\nI have conducted thorough research over multiple rounds to design a novel language model block that combines the strengths of **HieraNorm-AttnGPT** (Parent 1) and **AdaptiveTTT** (Parent 2). The goal is to integrate:\n\n- **Hierarchical Adaptive Multi-Head Attention** from HieraNorm-AttnGPT.\n- **Adaptive Attention Scaling** and **Selective Memory Compression** from AdaptiveTTT.\n\nIn this final analysis, I focused on practical implementation strategies for integrating adaptive attention scaling and compressive memory into hierarchical Transformer architectures. The key is to create a new GAU that efficiently handles long sequences with low computational and memory costs while maintaining high performance and scalability.\n\n#### 2. **Useful References with Excerpts**\n\n1. **Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). \"Recurrent Memory Transformer.\" Neural Information Processing Systems.**\n\n   *\"We propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence... This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing.\"*\n\n2. **Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). \"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.\" Neural Information Processing Systems.**\n\n   *\"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference... Our empirical findings demonstrate that we can effectively prune up to 80% of the context without significant performance degradation.\"*\n\n3. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.\" arXiv preprint arXiv:XXXX.XXXXX.**\n\n   *\"A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"*\n\n4. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024). \"TransformerFAM: Feedback Attention is Working Memory.\" arXiv preprint arXiv:XXXX.XXXXX.**\n\n   *\"We propose Feedback Attention Memory (FAM), a novel Transformer architecture that leverages a feedback loop to enable the network to attend to its own latent representations, allowing it to process indefinitely long sequences.\"*\n\n5. **Ermi\u015f, B., Zappella, G., Wistuba, M., Rawal, A., & Archambeau, C. (2022). \"Memory Efficient Continual Learning with Transformers.\" Neural Information Processing Systems.**\n\n   *\"We devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters that is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks.\"*\n\n6. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). \"Memformer: A Memory-Augmented Transformer for Sequence Modeling.\" AACL/IJCNLP.**\n\n   *\"Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information... Our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity.\"*\n\n7. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). \"Compressive Transformers for Long-Range Sequence Modelling.\" arXiv preprint arXiv:1911.05507.**\n\n   *\"We augment the Transformer with a compression mechanism, which results in a significant improvement in memory usage and enables the model to handle longer sequences effectively.\"*\n\n8. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.\" International Conference on Machine Learning.**\n\n   *\"We propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention and key-value eviction policies.\"*\n\n9. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers.\" arXiv preprint arXiv:2401.00001.**\n\n   *\"Our approach integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization.\"*\n\n10. **Mohtashami, A., & Jaggi, M. (2023). \"Landmark Attention: Random-Access Infinite Context Length for Transformers.\" Neural Information Processing Systems.**\n\n    *\"Our method uses a landmark token to represent each block of the input and trains the attention to use it for selecting relevant blocks, enabling retrieval of blocks directly through the attention mechanism.\"*\n\n#### 3. **Key Insights and Detailed Analysis**\n\n##### **Integrating Adaptive Attention Scaling into Hierarchical Attention**\n\n- **Adaptive Attention Span**: Drawing inspiration from **Sukhbaatar & Fergus (2019)**, integrating an adaptive attention span into the hierarchical attention mechanism allows each attention head to dynamically adjust its span based on input complexity.\n\n- **Implementation Strategy**:\n\n  - Modify the **HierarchicalAdaptiveAttention** GAU to include a learnable attention span parameter for each head or group of heads.\n\n  - The attention span can be dynamically adjusted during training via gradient descent, allowing the model to focus computational resources where needed.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces unnecessary computation by limiting attention to relevant contexts.\n\n  - **Performance**: Improves the ability to model long-range dependencies without excessive computational burden.\n\n##### **Incorporating Compressive Memory Mechanisms**\n\n- **Memformer and Compressive Transformers**: Utilize a compressive memory that interacts with the Transformer to store and retrieve information efficiently, as seen in **Wu et al. (2020)** and **Rae et al. (2019)**.\n\n- **Dynamic Memory Compression (DMC)**:\n\n  - Implement online key-value cache compression during inference, based on **Nawrot et al. (2024)**.\n\n  - Design a compression module within the GAU that selectively compresses less important key-value pairs to reduce memory usage.\n\n- **Implementation Strategy**:\n\n  - Within the **HierarchicalAdaptiveAttention**, include a compressive memory module that stores past key-value pairs.\n\n  - Use importance scores (e.g., attention weights) to determine which key-value pairs to compress or discard.\n\n- **Benefits**:\n\n  - **Memory Efficiency**: Reduces memory footprint, allowing processing of longer sequences.\n\n  - **Scalability**: Enables the model to handle infinitely long contexts with bounded memory.\n\n##### **Enhancing Hierarchical Attention with Sparse Attention Mechanisms**\n\n- **Sparse Attention**: Incorporate a scoring network to select top-k key-value pairs for each query, as proposed in **Lou et al. (2024)**.\n\n- **Implementation Strategy**:\n\n  - Integrate a differentiable top-k operator within the attention mechanism to focus on the most relevant tokens.\n\n  - This can be applied at each level of the hierarchical attention to reduce computational complexity.\n\n- **Benefits**:\n\n  - **Computational Efficiency**: Reduces the quadratic complexity of standard attention mechanisms.\n\n  - **Performance**: Helps the model focus on the most salient information, potentially improving accuracy.\n\n##### **Utilizing Landmark Attention and Block-Recurrent Mechanisms**\n\n- **Landmark Attention**: Use landmark tokens to represent blocks of input data, enabling efficient retrieval of relevant context (**Mohtashami & Jaggi, 2023**).\n\n- **Block-Recurrent Transformers**: Process input data in blocks and use recurrent connections to capture dependencies across blocks (**Hutchins et al., 2022**).\n\n- **Implementation Strategy**:\n\n  - Segment the input sequence into blocks and assign landmark tokens.\n\n  - The hierarchical attention can then attend to these landmark tokens to capture long-range dependencies efficiently.\n\n- **Benefits**:\n\n  - **Efficiency**: Reduces the number of computations by summarizing information with landmark tokens.\n\n  - **Long-Range Dependency Modeling**: Improves the model's ability to handle long sequences.\n\n##### **Incorporating Feedback Attention Memory (FAM)**\n\n- **Feedback Loops in Attention**: Implement a feedback loop within the attention mechanism, as suggested by **Hwang et al. (2024)**, to allow the model to attend to its own latent representations.\n\n- **Implementation Strategy**:\n\n  - Modify the attention mechanism to include a feedback component that reintroduces previous hidden states.\n\n  - This can enhance the working memory capability of the Transformer.\n\n- **Benefits**:\n\n  - **Infinite Context Processing**: Enables the model to process indefinitely long sequences effectively.\n\n  - **Improved Memory**: Enhances the model's capacity to retain and utilize past information.\n\n##### **Maintaining Training Stability with Dynamic Layer Normalization**\n\n- **Dynamic Layer Normalization (DLN)**: Continue to use DLN to stabilize training in the presence of adaptive mechanisms.\n\n- **Interaction with Adaptive Components**:\n\n  - Ensure that the adaptive attention scaling and compressive memory mechanisms integrate smoothly with DLN.\n\n  - Careful design of normalization layers is crucial to prevent training instability.\n\n##### **Practical Implementation Considerations**\n\n- **Efficiency**: Aim for linear time and space complexity by leveraging sparse attention and compressive memory.\n\n- **Scalability**: Design the model to handle longer sequences without a proportional increase in computational resources.\n\n- **Compatibility**: Ensure that modifications are compatible with existing Transformer architectures and do not require excessive retraining from scratch.\n\n#### 4. **Future Search Plan / Next Steps**\n\n- **Finalize the GAU Design**:\n\n  - Combine the insights to modify the **HierarchicalAdaptiveAttention** GAU, integrating adaptive attention scaling, compressive memory, and sparse attention mechanisms.\n\n- **Prototype Implementation**:\n\n  - Begin implementing the modified GAU and test on small-scale datasets to validate functionality.\n\n- **Performance Evaluation**:\n\n  - Compare the new model's performance against baseline models (e.g., standard Transformers, HieraNorm-AttnGPT, AdaptiveTTT) on tasks involving long sequences.\n\n- **Training Stability Analysis**:\n\n  - Monitor training dynamics to ensure that the integration of adaptive components does not introduce instability.\n\n- **Fine-Tuning and Optimization**:\n\n  - Adjust hyperparameters related to attention span, compression ratios, and sparse attention thresholds.\n\n- **Documentation**:\n\n  - Prepare comprehensive documentation of the design choices, implementation details, and experimental results.\n\n#### 5. **References**\n\n1. **Bulatov, A., Kuratov, Y., & Burtsev, M. (2022).** Recurrent Memory Transformer. *Neural Information Processing Systems*.\n\n2. **Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023).** Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. *Neural Information Processing Systems*.\n\n3. **Munkhdalai, T., Faruqui, M., & Gopal, S. (2024).** Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n4. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024).** TransformerFAM: Feedback Attention is Working Memory. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n5. **Ermi\u015f, B., Zappella, G., Wistuba, M., Rawal, A., & Archambeau, C. (2022).** Memory Efficient Continual Learning with Transformers. *Neural Information Processing Systems*.\n\n6. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020).** Memformer: A Memory-Augmented Transformer for Sequence Modeling. *AACL/IJCNLP*.\n\n7. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019).** Compressive Transformers for Long-Range Sequence Modelling. *arXiv preprint arXiv:1911.05507*.\n\n8. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024).** Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. *International Conference on Machine Learning*.\n\n9. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024).** Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. *arXiv preprint arXiv:2401.00001*.\n\n10. **Mohtashami, A., & Jaggi, M. (2023).** Landmark Attention: Random-Access Infinite Context Length for Transformers. *Neural Information Processing Systems*.\n\n---\n\n**I'm ready.**"
        },
        {
            "ready": false,
            "query": [
                "transformer training stability multiple adaptive mechanisms",
                "data-adaptive sparse attention efficient transformers",
                "hierarchical transformers compressive memory long sequences"
            ],
            "detail": "**Analysis Note:**\n\n1. **Summary of Analysis:**\n\n   With the new search results, we further assess the novelty, feasibility, and potential impact of the **HieraCATT** proposal. HieraCATT aims to integrate hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms to efficiently process long sequences while maintaining high performance and scalability in language modeling tasks.\n\n   The integration of these components remains novel. While existing research has explored these mechanisms individually or in pairs, there is no evidence of a model that combines all three in a unified architecture for language modeling. Recent studies provide insights into adaptive attention mechanisms, sparse attention for long sequences, hierarchical attention structures, and training stability, which are relevant to HieraCATT's design.\n\n2. **All Useful References with Excerpts:**\n\n   - **Zhang et al. (2023)** - *Adaptive Attention for Sparse-based Long-sequence Transformer*:\n\n     - *\"We introduce an adaptive attention mechanism for sparse-based long-sequence transformers, dynamically adjusting attention patterns based on the input to efficiently handle long sequences.\"*\n\n     - *Relevance:* Directly relates to adaptive attention scaling in sparse transformers, informing HieraCATT's adaptive attention component.\n\n   - **Cho et al. (2022)** - *Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost*:\n\n     - *\"We propose SBM-Transformer, which employs a mixed-membership Stochastic Block Model to data-adaptively sample attention masks, achieving linear cost and improved performance.\"*\n\n     - *Relevance:* Introduces data-adaptive sparsity in attention mechanisms, relevant to HieraCATT's goal of efficient attention scaling.\n\n   - **Zhu & Soricut (2021)** - *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences*:\n\n     - *\"We describe an efficient hierarchical method to compute attention in Transformers, achieving linear run time and memory complexity.\"*\n\n     - *Relevance:* Provides insights into hierarchical attention mechanisms for efficient long-sequence processing, supporting HieraCATT's hierarchical component.\n\n   - **Ren et al. (2023)** - *Sparse Modular Activation for Efficient Sequence Modeling*:\n\n     - *\"We introduce Sparse Modular Activation (SMA), enabling neural networks to sparsely and dynamically activate sub-modules, reducing computation and memory consumption.\"*\n\n     - *Relevance:* Aligns with HieraCATT's adaptive mechanisms for efficient resource allocation.\n\n   - **Anagnostidis et al. (2023)** - *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*:\n\n     - *\"We present a novel approach that dynamically prunes contextual information, reducing memory and computational requirements during inference.\"*\n\n     - *Relevance:* Discusses dynamic pruning to manage computational resources, complementing HieraCATT's compressive memory mechanisms.\n\n   - **Nawrot et al. (2021)** - *Hierarchical Transformers Are More Efficient Language Models*:\n\n     - *\"We introduce Hourglass, a hierarchical Transformer that improves language modeling efficiency using downsampling and upsampling layers.\"*\n\n     - *Relevance:* Demonstrates benefits of hierarchical structures in Transformers, reinforcing HieraCATT's design choice.\n\n   - **Takase et al. (2022)** - *B2T Connection: Serving Stability and Performance in Deep Transformers*:\n\n     - *\"We propose a method providing high stability and effective training through a simple modification, enabling stable training regardless of model depth.\"*\n\n     - *Relevance:* Addresses training stability, crucial for integrating multiple adaptive components in HieraCATT.\n\n   - **Zhang et al. (2024)** - *Efficient Sparse Attention Needs Adaptive Token Release*:\n\n     - *\"We propose adaptively releasing resources from caches and rebuilding necessary key-value states, significantly improving throughput while maintaining performance.\"*\n\n     - *Relevance:* Offers methods for efficient memory and computational resource management, relevant to HieraCATT's efficiency goals.\n\n   - **Rae et al. (2019)** - *Compressive Transformers for Long-Range Sequence Modelling*:\n\n     - *\"We present the Compressive Transformer, which compresses past memories for long-range sequence learning, reducing memory requirements.\"*\n\n     - *Relevance:* Provides foundational work on compressive memory mechanisms, directly informing HieraCATT's memory component.\n\n   - **Zuo et al. (2022)** - *Efficient Long Sequence Modeling via State Space Augmented Transformer*:\n\n     - *\"We augment a State Space Model into the Transformer, employing efficient local attention methods, complementing the lack of long-range dependency in local attention.\"*\n\n     - *Relevance:* Presents alternative strategies for long-sequence modeling, offering insights into efficient architecture design.\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Novelty of Integration:**\n\n     The HieraCATT proposal remains novel, as no existing models combine hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms in a single architecture for language modeling.\n\n   - **Adaptive Attention Mechanisms:**\n\n     Recent works highlight the effectiveness of adaptive attention in handling long sequences efficiently. Methods like adaptive token release and data-adaptive sparsity support the feasibility and potential performance gains of HieraCATT's adaptive attention scaling.\n\n   - **Hierarchical Attention Structures:**\n\n     Hierarchical Transformers, such as H-Transformer-1D and Hourglass, demonstrate improved efficiency and performance, validating HieraCATT's choice to employ hierarchical attention to capture multi-scale dependencies.\n\n   - **Compressive Memory Mechanisms:**\n\n     The Compressive Transformer showcases successful integration of memory compression to handle long-range dependencies, confirming the viability of incorporating compressive memory into HieraCATT.\n\n   - **Training Stability:**\n\n     Integrating multiple adaptive components raises concerns about training stability. Techniques like the B2T Connection and dynamic layer normalization may mitigate potential instability, ensuring effective training of HieraCATT.\n\n   - **Efficiency and Scalability:**\n\n     Combining hierarchical attention with adaptive scaling and compressive memory is likely to enhance HieraCATT's efficiency and scalability, allowing it to process longer sequences without proportional increases in computational overhead.\n\n   - **Implementation Considerations:**\n\n     Practical implementation insights from studies on sparse modular activation and dynamic context pruning can guide the efficient realization of HieraCATT, addressing computational and memory optimization.\n\n4. **Future Search Plan:**\n\n   - **Investigate Training Stabilization Techniques:**\n\n     Focus on research addressing training stability in transformers with multiple adaptive components, exploring methods like dynamic layer normalization, modified normalization positions, and training strategies.\n\n   - **Explore Data-Adaptive Sparse Attention Implementations:**\n\n     Delve into practical implementations of data-adaptive sparse attention mechanisms to understand their efficiency benefits and applicability to HieraCATT.\n\n   - **Examine Feedback Attention Mechanisms:**\n\n     Investigate recent advancements in feedback attention (e.g., TransformerFAM) for potential integration with hierarchical models to enhance working memory and long-term dependency modeling.\n\n   - **Plan Empirical Evaluations:**\n\n     Outline experimental setups for benchmarking HieraCATT against existing models, selecting appropriate datasets, defining evaluation metrics, and designing ablation studies to assess the contributions of each component.\n\n5. **List of References:**\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*. Neural Information Processing Systems.\n\n   - Cho, S., Min, S., Kim, J., Lee, M., Lee, H., & Hong, S. (2022). *Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost*. Neural Information Processing Systems.\n\n   - Nawrot, P., Tworkowski, S., Tyrolski, M., Kaiser, L., Wu, Y., & Michalewski, H. (2021). *Hierarchical Transformers Are More Efficient Language Models*. NAACL-HLT.\n\n   - Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). *Compressive Transformers for Long-Range Sequence Modelling*. International Conference on Learning Representations.\n\n   - Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., & Zhai, C. (2023). *Sparse Modular Activation for Efficient Sequence Modeling*. Neural Information Processing Systems.\n\n   - Takase, S., Kiyono, S., Kobayashi, S., & Suzuki, J. (2022). *B2T Connection: Serving Stability and Performance in Deep Transformers*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhang, C., Zou, L., Luo, D., Tang, M., Luo, X., Li, Z., & Li, C. (2024). *Efficient Sparse Attention Needs Adaptive Token Release*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhang, X., Lv, Z., & Yang, Q. (2023). *Adaptive Attention for Sparse-based Long-sequence Transformer*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhu, Z., & Soricut, R. (2021). *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv preprint arXiv:2207.00065.\n\n5. **Additional Insights:**\n\n   - **Integration Challenges:**\n\n     Combining hierarchical attention, adaptive scaling, and compressive memory introduces architectural complexity. Careful design choices and optimization strategies are essential to ensure that the benefits outweigh the added complexity.\n\n   - **Potential for Enhanced Performance:**\n\n     The synergy of these components may offer significant improvements in processing efficiency and capability to handle longer sequences, potentially setting new benchmarks in language modeling tasks.\n\n   - **Need for Empirical Validation:**\n\n     Theoretical advantages need to be supported by empirical evidence. Implementing HieraCATT and comparing its performance to existing models is crucial to validate the proposed benefits.\n\n---\n\n**Future Search Plan:**\n\n- **Refine Search Keywords:**\n\n  ```keywords transformer training stability multiple adaptive mechanisms```\n\n  ```keywords data-adaptive sparse attention efficient transformers```\n\n  ```keywords hierarchical transformers compressive memory long sequences```\n\n- **Internal Library Search Description:**\n\n  *Seeking recent research on training stability in transformers incorporating multiple adaptive mechanisms, such as adaptive attention scaling and compressive memory. Interested in practical methods to enhance stability and performance, including normalization techniques and architectural modifications. Also looking for empirical studies on data-adaptive sparse attention implementations and their effectiveness in efficient long-sequence processing within hierarchical transformer architectures.*\n\n- **Next Steps:**\n\n  - **Gather and Analyze Additional Research:**\n\n    Collect more studies focusing on the identified areas to address any remaining gaps in understanding and to refine the design of HieraCATT.\n\n  - **Explore Implementation Frameworks:**\n\n    Investigate existing frameworks or toolkits that could facilitate the development and testing of HieraCATT, considering factors like scalability and support for custom attention mechanisms.\n\n  - **Prepare for Experimental Setup:**\n\n    Start planning the resources required for empirical evaluation, including computational resources, datasets, and evaluation protocols.\n\n---\n\nI will continue to build on this analysis with the additional information obtained from future searches, ensuring a thorough evaluation of the HieraCATT proposal before proceeding to the final review.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\n**Analysis Note:**\n\n1. **Summary of Analysis:**\n\n   With the new search results, we further assess the novelty, feasibility, and potential impact of the **HieraCATT** proposal. HieraCATT aims to integrate hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms to efficiently process long sequences while maintaining high performance and scalability in language modeling tasks.\n\n   The integration of these components remains novel. While existing research has explored these mechanisms individually or in pairs, there is no evidence of a model that combines all three in a unified architecture for language modeling. Recent studies provide insights into adaptive attention mechanisms, sparse attention for long sequences, hierarchical attention structures, and training stability, which are relevant to HieraCATT's design.\n\n2. **All Useful References with Excerpts:**\n\n   - **Zhang et al. (2023)** - *Adaptive Attention for Sparse-based Long-sequence Transformer*:\n\n     - *\"We introduce an adaptive attention mechanism for sparse-based long-sequence transformers, dynamically adjusting attention patterns based on the input to efficiently handle long sequences.\"*\n\n     - *Relevance:* Directly relates to adaptive attention scaling in sparse transformers, informing HieraCATT's adaptive attention component.\n\n   - **Cho et al. (2022)** - *Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost*:\n\n     - *\"We propose SBM-Transformer, which employs a mixed-membership Stochastic Block Model to data-adaptively sample attention masks, achieving linear cost and improved performance.\"*\n\n     - *Relevance:* Introduces data-adaptive sparsity in attention mechanisms, relevant to HieraCATT's goal of efficient attention scaling.\n\n   - **Zhu & Soricut (2021)** - *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences*:\n\n     - *\"We describe an efficient hierarchical method to compute attention in Transformers, achieving linear run time and memory complexity.\"*\n\n     - *Relevance:* Provides insights into hierarchical attention mechanisms for efficient long-sequence processing, supporting HieraCATT's hierarchical component.\n\n   - **Ren et al. (2023)** - *Sparse Modular Activation for Efficient Sequence Modeling*:\n\n     - *\"We introduce Sparse Modular Activation (SMA), enabling neural networks to sparsely and dynamically activate sub-modules, reducing computation and memory consumption.\"*\n\n     - *Relevance:* Aligns with HieraCATT's adaptive mechanisms for efficient resource allocation.\n\n   - **Anagnostidis et al. (2023)** - *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*:\n\n     - *\"We present a novel approach that dynamically prunes contextual information, reducing memory and computational requirements during inference.\"*\n\n     - *Relevance:* Discusses dynamic pruning to manage computational resources, complementing HieraCATT's compressive memory mechanisms.\n\n   - **Nawrot et al. (2021)** - *Hierarchical Transformers Are More Efficient Language Models*:\n\n     - *\"We introduce Hourglass, a hierarchical Transformer that improves language modeling efficiency using downsampling and upsampling layers.\"*\n\n     - *Relevance:* Demonstrates benefits of hierarchical structures in Transformers, reinforcing HieraCATT's design choice.\n\n   - **Takase et al. (2022)** - *B2T Connection: Serving Stability and Performance in Deep Transformers*:\n\n     - *\"We propose a method providing high stability and effective training through a simple modification, enabling stable training regardless of model depth.\"*\n\n     - *Relevance:* Addresses training stability, crucial for integrating multiple adaptive components in HieraCATT.\n\n   - **Zhang et al. (2024)** - *Efficient Sparse Attention Needs Adaptive Token Release*:\n\n     - *\"We propose adaptively releasing resources from caches and rebuilding necessary key-value states, significantly improving throughput while maintaining performance.\"*\n\n     - *Relevance:* Offers methods for efficient memory and computational resource management, relevant to HieraCATT's efficiency goals.\n\n   - **Rae et al. (2019)** - *Compressive Transformers for Long-Range Sequence Modelling*:\n\n     - *\"We present the Compressive Transformer, which compresses past memories for long-range sequence learning, reducing memory requirements.\"*\n\n     - *Relevance:* Provides foundational work on compressive memory mechanisms, directly informing HieraCATT's memory component.\n\n   - **Zuo et al. (2022)** - *Efficient Long Sequence Modeling via State Space Augmented Transformer*:\n\n     - *\"We augment a State Space Model into the Transformer, employing efficient local attention methods, complementing the lack of long-range dependency in local attention.\"*\n\n     - *Relevance:* Presents alternative strategies for long-sequence modeling, offering insights into efficient architecture design.\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Novelty of Integration:**\n\n     The HieraCATT proposal remains novel, as no existing models combine hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms in a single architecture for language modeling.\n\n   - **Adaptive Attention Mechanisms:**\n\n     Recent works highlight the effectiveness of adaptive attention in handling long sequences efficiently. Methods like adaptive token release and data-adaptive sparsity support the feasibility and potential performance gains of HieraCATT's adaptive attention scaling.\n\n   - **Hierarchical Attention Structures:**\n\n     Hierarchical Transformers, such as H-Transformer-1D and Hourglass, demonstrate improved efficiency and performance, validating HieraCATT's choice to employ hierarchical attention to capture multi-scale dependencies.\n\n   - **Compressive Memory Mechanisms:**\n\n     The Compressive Transformer showcases successful integration of memory compression to handle long-range dependencies, confirming the viability of incorporating compressive memory into HieraCATT.\n\n   - **Training Stability:**\n\n     Integrating multiple adaptive components raises concerns about training stability. Techniques like the B2T Connection and dynamic layer normalization may mitigate potential instability, ensuring effective training of HieraCATT.\n\n   - **Efficiency and Scalability:**\n\n     Combining hierarchical attention with adaptive scaling and compressive memory is likely to enhance HieraCATT's efficiency and scalability, allowing it to process longer sequences without proportional increases in computational overhead.\n\n   - **Implementation Considerations:**\n\n     Practical implementation insights from studies on sparse modular activation and dynamic context pruning can guide the efficient realization of HieraCATT, addressing computational and memory optimization.\n\n4. **Future Search Plan:**\n\n   - **Investigate Training Stabilization Techniques:**\n\n     Focus on research addressing training stability in transformers with multiple adaptive components, exploring methods like dynamic layer normalization, modified normalization positions, and training strategies.\n\n   - **Explore Data-Adaptive Sparse Attention Implementations:**\n\n     Delve into practical implementations of data-adaptive sparse attention mechanisms to understand their efficiency benefits and applicability to HieraCATT.\n\n   - **Examine Feedback Attention Mechanisms:**\n\n     Investigate recent advancements in feedback attention (e.g., TransformerFAM) for potential integration with hierarchical models to enhance working memory and long-term dependency modeling.\n\n   - **Plan Empirical Evaluations:**\n\n     Outline experimental setups for benchmarking HieraCATT against existing models, selecting appropriate datasets, defining evaluation metrics, and designing ablation studies to assess the contributions of each component.\n\n5. **List of References:**\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*. Neural Information Processing Systems.\n\n   - Cho, S., Min, S., Kim, J., Lee, M., Lee, H., & Hong, S. (2022). *Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost*. Neural Information Processing Systems.\n\n   - Nawrot, P., Tworkowski, S., Tyrolski, M., Kaiser, L., Wu, Y., & Michalewski, H. (2021). *Hierarchical Transformers Are More Efficient Language Models*. NAACL-HLT.\n\n   - Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). *Compressive Transformers for Long-Range Sequence Modelling*. International Conference on Learning Representations.\n\n   - Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., & Zhai, C. (2023). *Sparse Modular Activation for Efficient Sequence Modeling*. Neural Information Processing Systems.\n\n   - Takase, S., Kiyono, S., Kobayashi, S., & Suzuki, J. (2022). *B2T Connection: Serving Stability and Performance in Deep Transformers*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhang, C., Zou, L., Luo, D., Tang, M., Luo, X., Li, Z., & Li, C. (2024). *Efficient Sparse Attention Needs Adaptive Token Release*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhang, X., Lv, Z., & Yang, Q. (2023). *Adaptive Attention for Sparse-based Long-sequence Transformer*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhu, Z., & Soricut, R. (2021). *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv preprint arXiv:2207.00065.\n\n5. **Additional Insights:**\n\n   - **Integration Challenges:**\n\n     Combining hierarchical attention, adaptive scaling, and compressive memory introduces architectural complexity. Careful design choices and optimization strategies are essential to ensure that the benefits outweigh the added complexity.\n\n   - **Potential for Enhanced Performance:**\n\n     The synergy of these components may offer significant improvements in processing efficiency and capability to handle longer sequences, potentially setting new benchmarks in language modeling tasks.\n\n   - **Need for Empirical Validation:**\n\n     Theoretical advantages need to be supported by empirical evidence. Implementing HieraCATT and comparing its performance to existing models is crucial to validate the proposed benefits.\n\n---\n\n**Future Search Plan:**\n\n- **Refine Search Keywords:**\n\n  ```keywords transformer training stability multiple adaptive mechanisms```\n\n  ```keywords data-adaptive sparse attention efficient transformers```\n\n  ```keywords hierarchical transformers compressive memory long sequences```\n\n- **Internal Library Search Description:**\n\n  *Seeking recent research on training stability in transformers incorporating multiple adaptive mechanisms, such as adaptive attention scaling and compressive memory. Interested in practical methods to enhance stability and performance, including normalization techniques and architectural modifications. Also looking for empirical studies on data-adaptive sparse attention implementations and their effectiveness in efficient long-sequence processing within hierarchical transformer architectures.*\n\n- **Next Steps:**\n\n  - **Gather and Analyze Additional Research:**\n\n    Collect more studies focusing on the identified areas to address any remaining gaps in understanding and to refine the design of HieraCATT.\n\n  - **Explore Implementation Frameworks:**\n\n    Investigate existing frameworks or toolkits that could facilitate the development and testing of HieraCATT, considering factors like scalability and support for custom attention mechanisms.\n\n  - **Prepare for Experimental Setup:**\n\n    Start planning the resources required for empirical evaluation, including computational resources, datasets, and evaluation protocols.\n\n---\n\nI will continue to build on this analysis with the additional information obtained from future searches, ensuring a thorough evaluation of the HieraCATT proposal before proceeding to the final review.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.99)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.99)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Softmax Attention with Constant Cost per Token (Avg. Score: 0.98)\n\n*Franz A. Heinsen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work proposes a simple modification to the conventional attention mechanism applied by Transformers, which quantifies pairwise query-key similarity with scaled dot-products with the logarithms of scaled dot-products of exponentials, and linearizes attention with exponential kernel feature maps.\n\n**Abstract:** We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.\n\n##### *Relevant Chunk: No. 3/8 (Score: 0.98)*\n\n```\n(2021), and Poli et al. (2023). More recently, generalized state space models that build on previous research (Martin and Cundy, 2017) (Gu et al., 2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state (Peng et al., 2023) (Gu and Dao, 2023) (Katsch, 2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention (Jelassi et al., 2024). ### 1.1 Modifying Attention\n\nWe find that a simple modification to conventional attention linearizes it (Katharopoulos et al., 2020) with exponential kernel feature maps, and we show that this modification renders attention expressible as a composition of log-sums of exponentials, with a fixed-size latent space, for sequential application with constant cost per token. We implement our modification, verify that it works, and conclude that it is a promising alternative. The modification we propose is:\n\n$$\n\\begin{gathered}\n\\text { modified }(Q, K, V):= \\\\\n\\text { Attention }(Q) \\\\\n\\text { Softmax }\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V\n\\end{gathered}\n$$\n\nwhere queries $Q$, keys $K$ and values $V$ have $n_{Q} \\times d_{K}, n_{K} \\times d_{K}$, and $n_{K} \\times d_{V}$ elements, respectively, and $c$ is a scalar constant, all in $\\mathbb{R}$. We compute all exponentials elementwise. ### 1.2 As Log-Sums of Exponentials\n\nIn Section 2, we prove that\n\n$$\n\\begin{aligned}\n& \\text { modified } \\\\\n& \\operatorname{Attention}(Q, K, V)=\\exp (\\log S-\\log Z) \\text {, }\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nThe elementwise sums are over compatible dimensions, broadcasting over all other dimensions, from left to right-e.g., before reduction, the broadcasted elementwise sum $K^{T}+\\log V$ has $d_{K} \\times n_{K} \\times d_{V}$ elements. The functions $\\operatorname{LSE}_{\\left[d_{K}\\right]}(\\cdot)$ and $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$ compute log-sums of exponentials over the dimension indexed by $\\left(1,2, \\ldots, d_{K}\\right)$ and $\\left(1,2, \\ldots, n_{K}\\right)$, respectively. If any of $V$ 's elements are negative, $\\log V$ is complex, and therefore so is $\\log S$, but all Softmax mixtures of $V$ remain over $\\mathbb{R}$ because they are a composition of operations under which $\\mathbb{R}$ is closed (1). ### 1.3 Autoregressive Case\n\nFor autoregressive attention, in which $n_{Q}=n_{K}$ and for each query at step $t$ we compute attention only over $t$ trailing tokens, we note that in (3), all sequential dependencies are modeled by the logsums computed with $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$, so we can compute autoregressive $\\log S$ and $\\log Z$ with:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times n_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\times n_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nwhere the function $\\operatorname{LCSE}_{\\left[n_{K}\\right]}(\\cdot)$ computes a logcumulative-sum of exponentials over the dimension indexed by and $\\left(1,2, \\ldots, n_{K}\\right)$. For sequential application, given a new query $Q_{t}$ at step $t$, we need only the end-states of the two log-cumulative-sums of exponentials:\n\n$$\n\\begin{aligned}\n& \\log S_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(S)}}_{d_{K} \\times d_{V}}) \\\\\n& \\log Z_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(Z)}}_{d_{K}})\n\\end{aligned}\n$$\n\nwhere hidden states $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the states of the two log-cumulative-sums at step $t$ :\n\n$$\n\\begin{aligned}\n& H_{t}^{(S)}=\\log \\left(\\exp \\left(H_{t-1}^{(S)}\\right)+\\exp \\left(K_{t}+\\log V_{t}\\right)\\right) \\\\\n& H_{t}^{(Z)}=\\log \\left(\\exp \\left(H_{t-1}^{(Z)}\\right)+\\exp \\left(K_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwith zeros as their initial condition:\n\n$$\n\\begin{aligned}\n& H_{0}^{(S)}=\\{0\\}^{d_{K} \\times d_{V}} \\\\\n& H_{0}^{(Z)}=\\{0\\}^{d_{K}}\n\\end{aligned}\n$$\n\nTogether, $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ hold the latent, or hidden, state of autoregressive attention's computation at step $t$. They enable us to compute autoregressive attention sequentially with constant time and space complexity per token, $\\mathcal{O}(1)$. ### 1.4 Non-Autoregressive Case\n\nFor non-autoregressive attention, in which $n_{Q}$ may differ from $n_{K}$ and for each query we compute attention over all tokens in the sequence, we compute $\\log S$ and $\\log Z$ with (3). For sequential application, in which we add a new token to the input context at step $t$, with key $K_{t}$ and value $V_{t}$, we compute $\\log S$ and $\\log Z$ for all queries from the updated hidden states:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(S)}\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(Z)}\\right)\n\\end{aligned}\n$$\n\nwhere $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the hidden states at step $t$ (6), with zeros as their initial condition (7). ## 2 Proof\n\nGiven a query $q$ and a key $k$ in $\\mathbb{R}^{d_{K}}$, the logarithm of the dot-product of their exponentials is $\\log \\left(\\sum(\\exp (q) \\odot \\exp (k))\\right)=\\operatorname{LSE}(q+k)$, where $\\odot$ denotes an elementwise product. Log-sums of exponentials are associative and commutative, making the proof fairly straightforward. For clarity's sake, we walk step-by-step through a sequence of algebraic manipulations. We start by expanding the Softmax function in (1) and simplifying the resulting expression. We obtain a form of linear attention (Katharopoulos et al., 2020) with exponential kernel feature maps:\n\n$$\n\\begin{gathered}\n\\operatorname{Softmax}\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V= \\\\\n{\\left[\\frac{\\exp (Q) \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\exp (Q) \\exp (K)^{T}}\\right] V}\n\\end{gathered}\n$$\n\nwhere $\\sum_{\\left[n_{K}\\right]}$ normalizes each row to a probability distribution. The scaling constant $\\exp (c)$ disappears because it becomes a common divisor of numerator and denominator expressions. Note that the feature function corresponding to the exponential kernel is infinite dimensional. Substitute the dot-products of exponentiated queries and exponentiated keys with equivalent explicit summations over elementwise products:\n\n$$\n\\left[\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\\right] V\n$$\n\nwhere the elementwise product $\\odot$ is over compatible dimensions, broadcasting over any other dimensions, from left to right, such that the broadcasted elementwise product $\\exp (Q) \\odot \\exp (K)^{T}$ has $n_{Q} \\times d_{K} \\times n_{K}$ elements. ${ }^{2}$\nExpress matrix multiplication with $V$ as a summation over broadcasted elementwise products:\n\n$$\n\\frac{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T} \\odot V}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\n$$\n\nBoth $\\exp (K)^{T}$ and $V$ have a dimension indexed by $\\left(1,2, \\ldots, n_{K}\\right)$, but $\\exp (Q)$ does not, so we can sum over that dimension before broadcastmultiplying elementwise with $\\exp (Q)$ :\n\n$$\n\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V}{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}}\n$$\n\nDefine $S$ and $Z$ as the expressions that compute numerators and denominators, respectively,\n\n$$\n\\begin{aligned}\n& S:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V \\\\\n& Z:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}\n\\end{aligned}\n$$\n\nand take their logarithms. We obtain:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)\\right),\n\\end{aligned}\n$$\n\nwhich is the same as (3). [^1]\n## 3 Implementation\n\nAs proof of concept, we implement our attention mechanism for both autoregressive applications (e.g., generative language modeling) and nonautoregressive applications (e.g., masked language modeling). For simplicity and expediency, we limit our implementation in two significant ways: First, we restrict $V$ to elements $\\geq 0$ to avoid dealing with complex floating-point numbers, which incur greater overhead and are more cumbersome to manipulate than real floating-point numbers with existing software infrastructure. Second, when computing autoregressive attention over $n_{K}$ tokens, we first compute all $n_{K}$ hidden states with a parallel scan, and then reduce them, which is space-inefficient but easier to implement with existing software infrastructure. ${ }^{3}$\n\nWe apply our implementation in a small generative language model ( 125 M parameters, 50257 token ids, 768 embedding features). For numerical stability, in each layer we compute $\\log V$ over $\\mathbb{R}$ directly, with a dense feed-forward transformation of token states, implicitly defining $V$ as $\\log V$ 's exponential but never actually computing it. To remain in $\\mathbb{R}$, we use the logarithm of attention as input to subsequent transformations in the layer, i.e., the input to subsequent transformations is $\\log S-\\log Z$ instead of $\\exp (\\log S-\\log Z)$. Please see our published code for all model details. We train the model on 300B tokens from The Pile (Gao et al., 2020) with a conventional sequence length of 1024 tokens, and obtain a crossentropy loss of 2.47 , competitive with state-of-theart generative language models of similar size. ## 4 Conclusions\n\nBy all indications, our attention mechanism is a promising alternative to the conventional one, but the evidence we have so far is too scant to be conclusive. An adequate comparison requires addressing our implementation's temporary limitations and evaluating models with one to several orders of magnitude more parameters on a diverse set of benchmarks and downstream tasks. [^2]\n## References\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR abs/1904.10509. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 3. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.96)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.96)*\n\n```\nTrans. Assoc. Comput. Linguistics, 9:1442-1459, 2021. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. In ICLR (Poster). OpenReview.net, 2019. Michel, P., Levy, O., and Neubig, G. Are sixteen heads really better than one? In NeurIPS, pp. $14014-14024,2019$. Peng, H., Schwartz, R., Li, D., and Smith, N. A. A mixture of $\\mathrm{h}-1$ heads is better than h heads. In ACL, pp. 6566-6577. Association for Computational Linguistics, 2020. Peng, H., Pappas, N., Yogatama, D., Schwartz, R., Smith, N. A., and Kong, L. Random feature attention. In ICLR. OpenReview.net, 2021. Qin, Z., Sun, W., Deng, H., Li, D., Wei, Y., Lv, B., Yan, J., Kong, L., and Zhong, Y. cosformer: Rethinking softmax in attention. In ICLR. OpenReview.net, 2022. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. Language models are unsupervised multitask learners, 2019. Raganato, A., Scherrer, Y., and Tiedemann, J. Fixed encoder self-attention patterns in transformerbased machine translation. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pp. 556-568. Association for Computational Linguistics, 2020. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient transformers: A survey. CoRR, abs/2009.06732, 2020. URL https://arxiv.org/abs/2009. 06732. Tay, Y., Bahri, D., Metzler, D., Juan, D., Zhao, Z., and Zheng, C. Synthesizer: Rethinking selfattention for transformer models. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 10183-10192. PMLR, 2021a. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers. In ICLR. OpenReview.net, 2021b. Tillet, P., Kung, H. T., and Cox, D. Triton: An intermediate language and compiler for tiled neural network computations. In Proceedings of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming Languages, MAPL 2019, pp. 10-19, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367196. doi: 10.1145/3315508. 3329973. URL https://doi.org/10.1145/3315508.3329973. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., and Polosukhin, I. Attention is all you need. CoRR, abs/1706.03762, 2017. URL http://arxiv . org/abs/1706. 03762. Voita, E., Talbot, D., Moiseev, F., Sennrich, R., and Titov, I. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In $A C L$ (1), pp. 5797-5808. Association for Computational Linguistics, 2019. Wang, H., Zhang, Z., and Han, S. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n#### 4. Improving Transformers with Dynamically Composable Multi-Head Attention (Avg. Score: 0.96)\n\n*Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** D Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads.\n\n**Abstract:** Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a $\\it{Compose}$ function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with ~1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation. The code and models are available at https://github.com/Caiyun-AI/DCFormer.\n\n##### *Relevant Chunk: No. 29/38 (Score: 0.96)*\n\n```\narXiv preprint arXiv:2210.05144, 2022. Zhao, Y., Li, J., and Gong, Y. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5005-5009. IEEE, 2016. ## A. Related work\n\nWe overview some prior works related to our DCMHA in the following subsections. ## A.1. Architecture Modifications to Transformers\n\nSince being introduced seven years ago, many modifications to the Transformer architecture have been proposed. However, relatively few of them generalize well across domains and scales and have seen widespread adoption (Narang et al., 2021) Some notable successful ones include Transformer-XL (Dai et al., 2019) and Rotary Position Encoding (Su et al., 2024) for improving long-context handling and position encoding, GLU MLP (Shazeer, 2020) and Sparse Mixture-of-Experts (MoE) MLP (Lepikhin et al., 2020; Fedus et al., 2022) for more expressive or efficient MLP nonlinearty and architecture, UL2 (Tay et al., 2022) and GLM (Du et al., 2021) for better training objectives. Among these, RoPE and SwiGLU MLP have been adopted by recent well-known foundation models such as Palm (Chowdhery et al., 2023) and LLaMA (Touvron et al., 2023), and are also used as our strong baseline (Transformer++). ## A.2. Improving MHA by Head Collaboration\n\nNoticing the problems caused by the independent working of attention heads, various forms of cross-head collaboration or interaction mechanisms have been proposed (Li et al., 2019; Zhang et al., 2022; Cordonnier et al., 2020; Liu et al., 2022; Shazeer et al., 2020; Wang et al., 2022; Nguyen et al., 2022). While some of these works mainly focus on improving parameter or computation efficiency of MHA by reducing head redundancy (Cordonnier et al., 2020; Nguyen et al., 2022; Zhang et al., 2022), we aim to improve model performance. Sharing the same goal as ours, Wang et al. (2022) proposed a Multi-Head Dense Collaboration (MHDC) mechanism and evaluate it primarily on Neural Machine Translation and some other small NLP tasks. MHDC is essentially the same as the static projection of attention scores in pre-compose of DCMHA, although they enhance it with cross-layer collaboration. We propose a more comprehensive head composition framework which supports dynamic composition of both attention scores and weights with pre- and post-compose, evaluate on large scale language model pretraining as well as downstream tasks. The work most closely related to ours is Talking-Heads Attention (THA) (Shazeer et al., 2020), which proposed to use two learned cross-head projections before and after softmax to transform the attention score and attention weight tensor respectively, which is same as pre- and post-compose with only static projections in DCMHA. They showed the effectiveness of THA in T5-style pretraining and downstream evaluation. We more clearly motivate head composition by relating it to projection composition, propose dynamic composition to further increase model expressiveness significantly, and offer a parameter and computation efficient design and implementation based on two-level tensor decomposition. The authors of THA also proposed a dynamic variant of THA in Appendix A of the paper, but compared with ours, the parameter and computation overhead is too large for practical use (see Table 8 in Appendix A of Shazeer et al.\n```\n\n#### 5. The Devil in Linear Transformer  (Avg. Score: 0.95)\n\n*Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, Yiran Zhong*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 39  (*Influential: 3*)\n\n**TL;DR:** This paper proposes a new linear attention that replaces the scaling of attention matrices with a normalization to stabilize gradients, and proposes a diagonal attention to confine attention to only neighbouring tokens in early layers.\n\n**Abstract:** Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.95)*\n\n```\nPMLR. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya 2020. Reformer: The efficient transformer. arXiv preprint arXiv:2001.04451. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692. Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. 2022. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. 5th International Conference on Learning Representations, ICLR, Toulon, France. Myle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. arXiv preprint arXiv:1904.01038. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2020. Random feature attention. In International Conference on Learning Representations. Zhen Qin, Weixuan Sun, Hui Deng, Dongxu Li, Yunshen Wei, Baohong Lv, Junjie Yan, Lingpeng Kong, and Yiran Zhong. 2022. cosformer: Rethinking softmax in attention. In International Conference on Learning Representations. Noam Shazeer. 2020. Glu variants improve transformer. arXiv preprint arXiv:2002.05202. Jingyu Sun, Guiping Zhong, Dinghao Zhou, Baoxiang Li, and Yiran Zhong. 2022a. Locality matters: A locality-biased linear attention for automatic speech recognition. arXiv preprint arXiv:2203.15609. Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. 2022b. Vicinity vision transformer. arXiv preprint arXiv:2206.10552. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020a. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020b. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations. Michalis K Titsias. 2016. One-vs-each approximation to softmax for scalable estimation of probabilities. arXiv preprint arXiv:1609.07410. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages $353-355$.\n```\n\n\n\n---\n## Found 11 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: transformer training stability multiple adaptive mechanisms, data-adaptive sparse attention efficient transformers, hierarchical transformers compressive memory long sequences\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Stabilizing Transformer Training by Preventing Attention Entropy Collapse\n\n*From Search Query: transformer training stability multiple adaptive mechanisms*\n\n*Shuangfei Zhai, T. Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, J. Susskind*\n\n**TL;DR:** This work investigates the training dynamics of Transformers by examining the evolution of the attention layers, and shows that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training a Vision Transformer without warmup, weight decay, layer normalization or adaptive optimizers.\n\n**Abstract:** Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\\textit{entropy collapse}$. As a remedy, we propose $\\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with $\\sigma$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer {to competitive performance} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at \\url{https://github.com/apple/ml-sigma-reparam}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 41  (*Influential: 7*)\n\n#### 2. How does Multi-Task Training Affect Transformer In-Context Capabilities? Investigations with Function Classes\n\n*From Search Query: transformer training stability multiple adaptive mechanisms*\n\n*Harmon Bhasin, Timothy Ossowski, Yiqiao Zhong, Junjie Hu*\n\n**TL;DR:** This work investigates the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples and proposes several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence.\n\n**Abstract:** Large language models (LLM) have recently shown the extraordinary ability to perform unseen tasks based on few-shot examples provided as text, also known as in-context learning (ICL). While recent works have attempted to understand the mechanisms driving ICL, few have explored training strategies that incentivize these models to generalize to multiple tasks. Multi-task learning (MTL) for generalist models is a promising direction that offers transfer learning potential, enabling large parameterized models to be trained from simpler, related tasks. In this work, we investigate the combination of MTL with ICL to build models that efficiently learn tasks while being robust to out-of-distribution examples. We propose several effective curriculum learning strategies that allow ICL models to achieve higher data efficiency and more stable convergence. Our experiments reveal that ICL models can effectively learn difficult tasks by training on progressively harder tasks while mixing in prior tasks, denoted as mixed curriculum in this work.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. CTAL: Pre-training Cross-modal Transformer for Audio-and-Language Representations\n\n*From Search Query: transformer training stability multiple adaptive mechanisms*\n\n*Hang Li, Yunxing Kang, Tianqiao Liu, Wenbiao Ding, Zitao Liu*\n\n**TL;DR:** This paper proposes a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows the pre-trained model to achieve better performance and demonstrates detailed ablation studies to prove that both the novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results.\n\n**Abstract:** Existing audio-language task-specific predictive approaches focus on building complicated late-fusion mechanisms. However, these models are facing challenges of overfitting with limited labels and low model generalization abilities. In this paper, we present a Cross-modal Transformer for Audio-and-Language, i.e., CTAL, which aims to learn the intra-modality and inter-modality connections between audio and language through two proxy tasks on a large amount of audio-and-language pairs: masked language modeling and masked cross-modal acoustic modeling. After fine-tuning our pre-trained model on multiple downstream audio-and-language tasks, we observe significant improvements across various tasks, such as, emotion classification, sentiment analysis, and speaker verification. On this basis, we further propose a specially-designed fusion mechanism that can be used in fine-tuning phase, which allows our pre-trained model to achieve better performance. Lastly, we demonstrate detailed ablation studies to prove that both our novel cross-modality fusion component and audio-language pre-training methods significantly contribute to the promising results. The code and pre-trained models are available at https://github.com/tal-ai/CTAL_EMNLP2021.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 4. Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost\n\n*From Search Query: data-adaptive sparse attention efficient transformers*\n\n*Sungjun Cho, Seonwoo Min, Jinwoo Kim, Moontae Lee, Honglak Lee, Seunghoon Hong*\n\n**TL;DR:** Empirical evaluations demonstrate that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation, and theoretically shows that it outperforms previous efficient variants as well as the original Transformer with full attention.\n\n**Abstract:** To overcome the quadratic cost of self-attention, recent works have proposed various sparse attention modules, most of which fall under one of two groups: 1) sparse attention under a hand-crafted patterns and 2) full attention followed by a sparse variant of softmax such as $\\alpha$-entmax. Unfortunately, the first group lacks adaptability to data while the second still requires quadratic cost in training. In this work, we propose SBM-Transformer, a model that resolves both problems by endowing each attention head with a mixed-membership Stochastic Block Model (SBM). Then, each attention head data-adaptively samples a bipartite graph, the adjacency of which is used as an attention mask for each input. During backpropagation, a straight-through estimator is used to flow gradients beyond the discrete sampling step and adjust the probabilities of sampled edges based on the predictive loss. The forward and backward cost are thus linear to the number of edges, which each attention head can also choose flexibly based on the input. By assessing the distribution of graphs, we theoretically show that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation. Empirical evaluations under the LRA and GLUE benchmarks demonstrate that our model outperforms previous efficient variants as well as the original Transformer with full attention. Our implementation can be found in https://github.com/sc782/SBM-Transformer .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Training data-efficient image transformers & distillation through attention\n\n*From Search Query: data-adaptive sparse attention efficient transformers*\n\n*Hugo Touvron, M. Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, Herv'e J'egou*\n\n**TL;DR:** This work produces a competitive convolution-free transformer by training on Imagenet only and introduces a teacher-student strategy specific to transformers that relies on a distillation token ensuring that the student learns from the teacher through attention.\n\n**Abstract:** Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 5614  (*Influential: 911*)\n\n#### 6. Efficient Sparse Attention needs Adaptive Token Release\n\n*From Search Query: data-adaptive sparse attention efficient transformers*\n\n*Chaoran Zhang, Lixin Zou, Dan Luo, Min Tang, Xiangyang Luo, Zihao Li, Chenliang Li*\n\n**TL;DR:** Comprehensive experiments in natural language generation and modeling reveal that the adaptively release resources from caches and rebuild the necessary key-value states of the transformer achieves a significant throughput improvement of up to 221.8%.\n\n**Abstract:** In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide array of text-centric tasks. However, their `large' scale introduces significant computational and storage challenges, particularly in managing the key-value states of the transformer, which limits their wider applicability. Therefore, we propose to adaptively release resources from caches and rebuild the necessary key-value states. Particularly, we accomplish this by a lightweight controller module to approximate an ideal top-$K$ sparse attention. This module retains the tokens with the highest top-$K$ attention weights and simultaneously rebuilds the discarded but necessary tokens, which may become essential for future decoding. Comprehensive experiments in natural language generation and modeling reveal that our method is not only competitive with full attention in terms of performance but also achieves a significant throughput improvement of up to 221.8%. The code for replication is available on the https://github.com/WHUIR/ADORE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores\n\n*From Search Query: hierarchical transformers compressive memory long sequences*\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 1*)\n\n#### 8. Unlimiformer: Long-Range Transformers with Unlimited Length Input\n\n*From Search Query: hierarchical transformers compressive memory long sequences*\n\n*Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley*\n\n**TL;DR:** This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.\n\n**Abstract:** Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 98  (*Influential: 10*)\n\n#### 9. Efficiently Modeling Long Sequences with Structured State Spaces\n\n*From Search Query: hierarchical transformers compressive memory long sequences*\n\n*Albert Gu, Karan Goel, Christopher R'e*\n\n**TL;DR:** The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.\n\n**Abstract:** A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 1066  (*Influential: 154*)\n\n### 2 related papers from Papers with Code\n\n#### 1. Chasing Sparsity in Vision Transformers: An End-to-End Exploration\n\n*From Search Query: data-adaptive sparse attention efficient transformers*\n\n*Zhangyang Wang, Lei Zhang, Lu Yuan, Zhe Gan, Yu Cheng, Tianlong Chen*\n\n**Abstract:** Vision transformers (ViTs) have recently received explosive popularity, but their enormous model sizes and training costs remain daunting. Conventional post-training pruning often incurs higher training budgets. In contrast, this paper aims to trim down both the training memory overhead and the inference complexity, without sacrificing the achievable accuracy. We carry out the first-of-its-kind comprehensive exploration, on taking a unified approach of integrating sparsity in ViTs \"from end to end\". Specifically, instead of training full ViTs, we dynamically extract and train sparse subnetworks, while sticking to a fixed small parameter budget. Our approach jointly optimizes model parameters and explores connectivity throughout training, ending up with one sparse network as the final output. The approach is seamlessly extended from unstructured to structured sparsity, the latter by considering to guide the prune-and-grow of self-attention heads inside ViTs. We further co-explore data and architecture sparsity for additional efficiency gains by plugging in a novel learnable token selector to adaptively determine the currently most vital patches. Extensive results on ImageNet with diverse ViT backbones validate the effectiveness of our proposals which obtain significantly reduced computational cost and almost unimpaired generalization. Perhaps most surprisingly, we find that the proposed sparse (co-)training can sometimes improve the ViT accuracy rather than compromising it, making sparsity a tantalizing \"free lunch\". For example, our sparsified DeiT-Small at (5%, 50%) sparsity for (data, architecture), improves 0.28% top-1 accuracy, and meanwhile enjoys 49.32% FLOPs and 4.40% running time savings. Our codes are available at https://github.com/VITA-Group/SViTE.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-08\n\n\n\n#### 2. Tackling fluffy clouds: field boundaries detection using time series of S2 and/or S1 imagery\n\n*From Search Query: data-adaptive sparse attention efficient transformers*\n\n*John Taylor, Andrew Toovey, Chris Herrmann, Kathryn Batchelor, Jonathan Richetti, Ondrej Hlinka, Rodrigo Ibata, Peter Caccetta, Amy Parker, Roger Lawes, Dave Henry, Gonzalo Mata, Jeff Wang, Zheng-Shu Zhou, Foivos I. Diakogiannis*\n\n**Abstract:** Accurate field boundary delineation is a critical challenge in digital agriculture, impacting everything from crop monitoring to resource management. Existing methods often struggle with noise and fail to generalize across varied landscapes, particularly when dealing with cloud cover in optical remote sensing. In response, this study presents a new approach that leverages time series data from Sentinel-2 (S2) and Sentinel-1 (S1) imagery to improve performance under diverse cloud conditions, without the need for manual cloud filtering. We introduce a 3D Vision Transformer architecture specifically designed for satellite image time series, incorporating a memory-efficient attention mechanism. Two models are proposed: PTAViT3D, which handles either S2 or S1 data independently, and PTAViT3D-CA, which fuses both datasets to enhance accuracy. Both models are evaluated under sparse and dense cloud coverage by exploiting spatio-temporal correlations. Our results demonstrate that the models can effectively delineate field boundaries, even with partial (S2 or S2 and S1 data fusion) or dense cloud cover (S1), with the S1-based model providing performance comparable to S2 imagery in terms of spatial resolution. A key strength of this approach lies in its capacity to directly process cloud-contaminated imagery by leveraging spatio-temporal correlations in a memory-efficient manner. This methodology, used in the ePaddocks product to map Australia's national field boundaries, offers a robust, scalable solution adaptable to varying agricultural environments, delivering precision and reliability where existing methods falter. Our code is available at https://github.com/feevos/tfcl.\n\n**Published:** 2024-09-20\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goal of designing and validating the **HieraCATT** model, which integrates hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms, here are the key insights and useful information from the provided analysis and additional context:\n\n## Novelty and Feasibility of HieraCATT\n\n- The integration of hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms is novel and has not been combined in a single architecture before. This is a crucial point highlighted in the analysis, emphasizing the potential for innovation in language modeling.\n\n## Adaptive Attention Mechanisms\n\n- Studies like Zhang et al. (2023) and Cho et al. (2022) provide strong evidence for the effectiveness of adaptive attention mechanisms in handling long sequences efficiently. These works support the feasibility of HieraCATT's adaptive attention scaling component.\n\n## Hierarchical Attention Structures\n\n- Research by Zhu & Soricut (2021) and Nawrot et al. (2021) demonstrates the efficiency and performance benefits of hierarchical attention structures in Transformers. This validates the choice of using hierarchical attention in HieraCATT to capture multi-scale dependencies.\n\n## Compressive Memory Mechanisms\n\n- The Compressive Transformer by Rae et al. (2019) shows successful integration of memory compression to handle long-range dependencies, confirming the viability of incorporating compressive memory into HieraCATT.\n\n## Training Stability\n\n- Techniques such as the B2T Connection by Takase et al. (2022) and dynamic layer normalization are crucial for ensuring training stability in models with multiple adaptive components. These methods can help mitigate potential instability in HieraCATT.\n\n## Efficiency and Scalability\n\n- Combining hierarchical attention with adaptive scaling and compressive memory is expected to enhance HieraCATT's efficiency and scalability. Studies like Ren et al. (2023) and Zhang et al. (2024) provide insights into efficient resource allocation and sparse attention mechanisms that can guide the implementation of HieraCATT.\n\n## Implementation Considerations\n\n- Practical implementation insights from studies on sparse modular activation by Ren et al. (2023) and dynamic context pruning by Anagnostidis et al. (2023) can help in the efficient realization of HieraCATT, addressing computational and memory optimization.\n\n## Future Search Plan\n\n### Training Stabilization Techniques\n\n- Focus on research addressing training stability in transformers with multiple adaptive components. Keywords such as \"transformer training stability multiple adaptive mechanisms\" and \"dynamic layer normalization\" can help in finding relevant studies.\n\n### Data-Adaptive Sparse Attention Implementations\n\n- Delve into practical implementations of data-adaptive sparse attention mechanisms. Keywords like \"data-adaptive sparse attention efficient transformers\" can guide this search.\n\n### Feedback Attention Mechanisms\n\n- Investigate recent advancements in feedback attention mechanisms (e.g., TransformerFAM) for potential integration with hierarchical models to enhance working memory and long-term dependency modeling.\n\n### Empirical Evaluations\n\n- Outline experimental setups for benchmarking HieraCATT against existing models. This includes selecting appropriate datasets, defining evaluation metrics, and designing ablation studies to assess the contributions of each component.\n\n## Useful References\n\n- **Zhu & Soricut (2021)**: *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences* - Provides insights into hierarchical attention mechanisms.\n- **Rae et al. (2019)**: *Compressive Transformers for Long-Range Sequence Modelling* - Foundational work on compressive memory mechanisms.\n- **Takase et al. (2022)**: *B2T Connection: Serving Stability and Performance in Deep Transformers* - Addresses training stability.\n- **Zhang et al. (2023)**: *Adaptive Attention for Sparse-based Long-sequence Transformer* - Relevant to adaptive attention scaling.\n- **Ren et al. (2023)**: *Sparse Modular Activation for Efficient Sequence Modeling* - Aligns with efficient resource allocation.\n\n## Additional Insights\n\n- **Integration Challenges**: Combining multiple components introduces architectural complexity. Careful design choices and optimization strategies are essential.\n- **Potential for Enhanced Performance**: The synergy of these components may offer significant improvements in processing efficiency and capability to handle longer sequences.\n- **Need for Empirical Validation**: Theoretical advantages need to be supported by empirical evidence through thorough experimental evaluations.\n\nBy focusing on these areas and using the provided keywords for future searches, the researcher can gather comprehensive information to refine the design of HieraCATT, ensure its stability and performance, and validate its potential to outperform existing state-of-the-art models in language modeling tasks.\n\n\nWarning: No description detected, will use full response to search internal library, please wrap your description in a quoted block like this: ```description {{Your description}}``` in your response next time.",
            "analysis": "**Analysis Note:**\n\n1. **Summary of Analysis:**\n\n   With the new search results, we further assess the novelty, feasibility, and potential impact of the **HieraCATT** proposal. HieraCATT aims to integrate hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms to efficiently process long sequences while maintaining high performance and scalability in language modeling tasks.\n\n   The integration of these components remains novel. While existing research has explored these mechanisms individually or in pairs, there is no evidence of a model that combines all three in a unified architecture for language modeling. Recent studies provide insights into adaptive attention mechanisms, sparse attention for long sequences, hierarchical attention structures, and training stability, which are relevant to HieraCATT's design.\n\n2. **All Useful References with Excerpts:**\n\n   - **Zhang et al. (2023)** - *Adaptive Attention for Sparse-based Long-sequence Transformer*:\n\n     - *\"We introduce an adaptive attention mechanism for sparse-based long-sequence transformers, dynamically adjusting attention patterns based on the input to efficiently handle long sequences.\"*\n\n     - *Relevance:* Directly relates to adaptive attention scaling in sparse transformers, informing HieraCATT's adaptive attention component.\n\n   - **Cho et al. (2022)** - *Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost*:\n\n     - *\"We propose SBM-Transformer, which employs a mixed-membership Stochastic Block Model to data-adaptively sample attention masks, achieving linear cost and improved performance.\"*\n\n     - *Relevance:* Introduces data-adaptive sparsity in attention mechanisms, relevant to HieraCATT's goal of efficient attention scaling.\n\n   - **Zhu & Soricut (2021)** - *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences*:\n\n     - *\"We describe an efficient hierarchical method to compute attention in Transformers, achieving linear run time and memory complexity.\"*\n\n     - *Relevance:* Provides insights into hierarchical attention mechanisms for efficient long-sequence processing, supporting HieraCATT's hierarchical component.\n\n   - **Ren et al. (2023)** - *Sparse Modular Activation for Efficient Sequence Modeling*:\n\n     - *\"We introduce Sparse Modular Activation (SMA), enabling neural networks to sparsely and dynamically activate sub-modules, reducing computation and memory consumption.\"*\n\n     - *Relevance:* Aligns with HieraCATT's adaptive mechanisms for efficient resource allocation.\n\n   - **Anagnostidis et al. (2023)** - *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*:\n\n     - *\"We present a novel approach that dynamically prunes contextual information, reducing memory and computational requirements during inference.\"*\n\n     - *Relevance:* Discusses dynamic pruning to manage computational resources, complementing HieraCATT's compressive memory mechanisms.\n\n   - **Nawrot et al. (2021)** - *Hierarchical Transformers Are More Efficient Language Models*:\n\n     - *\"We introduce Hourglass, a hierarchical Transformer that improves language modeling efficiency using downsampling and upsampling layers.\"*\n\n     - *Relevance:* Demonstrates benefits of hierarchical structures in Transformers, reinforcing HieraCATT's design choice.\n\n   - **Takase et al. (2022)** - *B2T Connection: Serving Stability and Performance in Deep Transformers*:\n\n     - *\"We propose a method providing high stability and effective training through a simple modification, enabling stable training regardless of model depth.\"*\n\n     - *Relevance:* Addresses training stability, crucial for integrating multiple adaptive components in HieraCATT.\n\n   - **Zhang et al. (2024)** - *Efficient Sparse Attention Needs Adaptive Token Release*:\n\n     - *\"We propose adaptively releasing resources from caches and rebuilding necessary key-value states, significantly improving throughput while maintaining performance.\"*\n\n     - *Relevance:* Offers methods for efficient memory and computational resource management, relevant to HieraCATT's efficiency goals.\n\n   - **Rae et al. (2019)** - *Compressive Transformers for Long-Range Sequence Modelling*:\n\n     - *\"We present the Compressive Transformer, which compresses past memories for long-range sequence learning, reducing memory requirements.\"*\n\n     - *Relevance:* Provides foundational work on compressive memory mechanisms, directly informing HieraCATT's memory component.\n\n   - **Zuo et al. (2022)** - *Efficient Long Sequence Modeling via State Space Augmented Transformer*:\n\n     - *\"We augment a State Space Model into the Transformer, employing efficient local attention methods, complementing the lack of long-range dependency in local attention.\"*\n\n     - *Relevance:* Presents alternative strategies for long-sequence modeling, offering insights into efficient architecture design.\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Novelty of Integration:**\n\n     The HieraCATT proposal remains novel, as no existing models combine hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms in a single architecture for language modeling.\n\n   - **Adaptive Attention Mechanisms:**\n\n     Recent works highlight the effectiveness of adaptive attention in handling long sequences efficiently. Methods like adaptive token release and data-adaptive sparsity support the feasibility and potential performance gains of HieraCATT's adaptive attention scaling.\n\n   - **Hierarchical Attention Structures:**\n\n     Hierarchical Transformers, such as H-Transformer-1D and Hourglass, demonstrate improved efficiency and performance, validating HieraCATT's choice to employ hierarchical attention to capture multi-scale dependencies.\n\n   - **Compressive Memory Mechanisms:**\n\n     The Compressive Transformer showcases successful integration of memory compression to handle long-range dependencies, confirming the viability of incorporating compressive memory into HieraCATT.\n\n   - **Training Stability:**\n\n     Integrating multiple adaptive components raises concerns about training stability. Techniques like the B2T Connection and dynamic layer normalization may mitigate potential instability, ensuring effective training of HieraCATT.\n\n   - **Efficiency and Scalability:**\n\n     Combining hierarchical attention with adaptive scaling and compressive memory is likely to enhance HieraCATT's efficiency and scalability, allowing it to process longer sequences without proportional increases in computational overhead.\n\n   - **Implementation Considerations:**\n\n     Practical implementation insights from studies on sparse modular activation and dynamic context pruning can guide the efficient realization of HieraCATT, addressing computational and memory optimization.\n\n4. **Future Search Plan:**\n\n   - **Investigate Training Stabilization Techniques:**\n\n     Focus on research addressing training stability in transformers with multiple adaptive components, exploring methods like dynamic layer normalization, modified normalization positions, and training strategies.\n\n   - **Explore Data-Adaptive Sparse Attention Implementations:**\n\n     Delve into practical implementations of data-adaptive sparse attention mechanisms to understand their efficiency benefits and applicability to HieraCATT.\n\n   - **Examine Feedback Attention Mechanisms:**\n\n     Investigate recent advancements in feedback attention (e.g., TransformerFAM) for potential integration with hierarchical models to enhance working memory and long-term dependency modeling.\n\n   - **Plan Empirical Evaluations:**\n\n     Outline experimental setups for benchmarking HieraCATT against existing models, selecting appropriate datasets, defining evaluation metrics, and designing ablation studies to assess the contributions of each component.\n\n5. **List of References:**\n\n   - Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*. Neural Information Processing Systems.\n\n   - Cho, S., Min, S., Kim, J., Lee, M., Lee, H., & Hong, S. (2022). *Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost*. Neural Information Processing Systems.\n\n   - Nawrot, P., Tworkowski, S., Tyrolski, M., Kaiser, L., Wu, Y., & Michalewski, H. (2021). *Hierarchical Transformers Are More Efficient Language Models*. NAACL-HLT.\n\n   - Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). *Compressive Transformers for Long-Range Sequence Modelling*. International Conference on Learning Representations.\n\n   - Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., & Zhai, C. (2023). *Sparse Modular Activation for Efficient Sequence Modeling*. Neural Information Processing Systems.\n\n   - Takase, S., Kiyono, S., Kobayashi, S., & Suzuki, J. (2022). *B2T Connection: Serving Stability and Performance in Deep Transformers*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhang, C., Zou, L., Luo, D., Tang, M., Luo, X., Li, Z., & Li, C. (2024). *Efficient Sparse Attention Needs Adaptive Token Release*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhang, X., Lv, Z., & Yang, Q. (2023). *Adaptive Attention for Sparse-based Long-sequence Transformer*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zhu, Z., & Soricut, R. (2021). *H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences*. Annual Meeting of the Association for Computational Linguistics.\n\n   - Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv preprint arXiv:2207.00065.\n\n5. **Additional Insights:**\n\n   - **Integration Challenges:**\n\n     Combining hierarchical attention, adaptive scaling, and compressive memory introduces architectural complexity. Careful design choices and optimization strategies are essential to ensure that the benefits outweigh the added complexity.\n\n   - **Potential for Enhanced Performance:**\n\n     The synergy of these components may offer significant improvements in processing efficiency and capability to handle longer sequences, potentially setting new benchmarks in language modeling tasks.\n\n   - **Need for Empirical Validation:**\n\n     Theoretical advantages need to be supported by empirical evidence. Implementing HieraCATT and comparing its performance to existing models is crucial to validate the proposed benefits.\n\n---\n\n**Future Search Plan:**\n\n- **Refine Search Keywords:**\n\n  ```keywords transformer training stability multiple adaptive mechanisms```\n\n  ```keywords data-adaptive sparse attention efficient transformers```\n\n  ```keywords hierarchical transformers compressive memory long sequences```\n\n- **Internal Library Search Description:**\n\n  *Seeking recent research on training stability in transformers incorporating multiple adaptive mechanisms, such as adaptive attention scaling and compressive memory. Interested in practical methods to enhance stability and performance, including normalization techniques and architectural modifications. Also looking for empirical studies on data-adaptive sparse attention implementations and their effectiveness in efficient long-sequence processing within hierarchical transformer architectures.*\n\n- **Next Steps:**\n\n  - **Gather and Analyze Additional Research:**\n\n    Collect more studies focusing on the identified areas to address any remaining gaps in understanding and to refine the design of HieraCATT.\n\n  - **Explore Implementation Frameworks:**\n\n    Investigate existing frameworks or toolkits that could facilitate the development and testing of HieraCATT, considering factors like scalability and support for custom attention mechanisms.\n\n  - **Prepare for Experimental Setup:**\n\n    Start planning the resources required for empirical evaluation, including computational resources, datasets, and evaluation protocols.\n\n---\n\nI will continue to build on this analysis with the additional information obtained from future searches, ensuring a thorough evaluation of the HieraCATT proposal before proceeding to the final review."
        }
    ],
    "review_search_stack": [
        {
            "ready": true,
            "query": [
                "hierarchical attention transformers",
                "compressive transformers",
                "adaptive attention scaling"
            ],
            "detail": "Searching for research papers that discuss integrating hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms in transformer models. Specifically interested in models that combine these methods to process long sequences efficiently while maintaining high performance and scalability. Looking for any existing works that propose or implement such integrations, as well as studies on the potential challenges and benefits of combining these mechanisms.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearching for research papers that discuss integrating hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms in transformer models. Specifically interested in models that combine these methods to process long sequences efficiently while maintaining high performance and scalability. Looking for any existing works that propose or implement such integrations, as well as studies on the potential challenges and benefits of combining these mechanisms.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.99)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.99)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.98)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 17/38 (Score: 0.98)*\n\n```\nBehnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP (1), pp. 2664-2674. Association for Computational Linguistics, 2020 . Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004. 05150. Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/ abs/2112.04426. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 3. Improving Transformers with Dynamically Composable Multi-Head Attention (Avg. Score: 0.98)\n\n*Da Xiao, Qingye Meng, Shengping Li, Xingyuan Yuan*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** D Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads.\n\n**Abstract:** Multi-Head Attention (MHA) is a key component of Transformer. In MHA, attention heads work independently, causing problems such as low-rank bottleneck of attention score matrices and head redundancy. We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads. At the core of DCMHA is a $\\it{Compose}$ function that transforms the attention score and weight matrices in an input-dependent way. DCMHA can be used as a drop-in replacement of MHA in any transformer architecture to obtain the corresponding DCFormer. DCFormer significantly outperforms Transformer on different architectures and model scales in language modeling, matching the performance of models with ~1.7x-2.0x compute. For example, DCPythia-6.9B outperforms open source Pythia-12B on both pretraining perplexity and downstream task evaluation. The code and models are available at https://github.com/Caiyun-AI/DCFormer.\n\n##### *Relevant Chunk: No. 29/38 (Score: 0.98)*\n\n```\narXiv preprint arXiv:2210.05144, 2022. Zhao, Y., Li, J., and Gong, Y. Low-rank plus diagonal adaptation for deep neural networks. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5005-5009. IEEE, 2016. ## A. Related work\n\nWe overview some prior works related to our DCMHA in the following subsections. ## A.1. Architecture Modifications to Transformers\n\nSince being introduced seven years ago, many modifications to the Transformer architecture have been proposed. However, relatively few of them generalize well across domains and scales and have seen widespread adoption (Narang et al., 2021) Some notable successful ones include Transformer-XL (Dai et al., 2019) and Rotary Position Encoding (Su et al., 2024) for improving long-context handling and position encoding, GLU MLP (Shazeer, 2020) and Sparse Mixture-of-Experts (MoE) MLP (Lepikhin et al., 2020; Fedus et al., 2022) for more expressive or efficient MLP nonlinearty and architecture, UL2 (Tay et al., 2022) and GLM (Du et al., 2021) for better training objectives. Among these, RoPE and SwiGLU MLP have been adopted by recent well-known foundation models such as Palm (Chowdhery et al., 2023) and LLaMA (Touvron et al., 2023), and are also used as our strong baseline (Transformer++). ## A.2. Improving MHA by Head Collaboration\n\nNoticing the problems caused by the independent working of attention heads, various forms of cross-head collaboration or interaction mechanisms have been proposed (Li et al., 2019; Zhang et al., 2022; Cordonnier et al., 2020; Liu et al., 2022; Shazeer et al., 2020; Wang et al., 2022; Nguyen et al., 2022). While some of these works mainly focus on improving parameter or computation efficiency of MHA by reducing head redundancy (Cordonnier et al., 2020; Nguyen et al., 2022; Zhang et al., 2022), we aim to improve model performance. Sharing the same goal as ours, Wang et al. (2022) proposed a Multi-Head Dense Collaboration (MHDC) mechanism and evaluate it primarily on Neural Machine Translation and some other small NLP tasks. MHDC is essentially the same as the static projection of attention scores in pre-compose of DCMHA, although they enhance it with cross-layer collaboration. We propose a more comprehensive head composition framework which supports dynamic composition of both attention scores and weights with pre- and post-compose, evaluate on large scale language model pretraining as well as downstream tasks. The work most closely related to ours is Talking-Heads Attention (THA) (Shazeer et al., 2020), which proposed to use two learned cross-head projections before and after softmax to transform the attention score and attention weight tensor respectively, which is same as pre- and post-compose with only static projections in DCMHA. They showed the effectiveness of THA in T5-style pretraining and downstream evaluation. We more clearly motivate head composition by relating it to projection composition, propose dynamic composition to further increase model expressiveness significantly, and offer a parameter and computation efficient design and implementation based on two-level tensor decomposition. The authors of THA also proposed a dynamic variant of THA in Appendix A of the paper, but compared with ours, the parameter and computation overhead is too large for practical use (see Table 8 in Appendix A of Shazeer et al.\n```\n\n#### 4. Rethinking Attention with Performers (Avg. Score: 0.98)\n\n*K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, Adrian Weller*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1210  (*Influential: 176*)\n\n**TL;DR:** Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness are introduced.\n\n**Abstract:** We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\n##### *Relevant Chunk: No. 25/48 (Score: 0.98)*\n\n```\nJayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations, 2020. URLhttps://openreview.net/forum?id=SylKikSYDH. Ali Rahimi and Benjamin Recht. Random features for large-scale kernel machines. In Advances in Neural Information Processing Systems 20, Proceedings of the Twenty-First Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 3-6, 2007, pp. 1177-1184. Curran Associates, Inc., 2007. URL http://papers.nips.cc/ paper/3182-random-features-for-large-scale-kernel-machines\n\nAlexander Rives, Siddharth Goyal, Joshua Meier, Demi Guo, Myle Ott, C. Zitnick, Jerry Ma, and Rob Fergus. Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences. bioArxiv, 04 2019. doi: 10.1101/622803. Mark Rowland, Jiri Hron, Yunhao Tang, Krzysztof Choromanski, Tam\u00e1s Sarl\u00f3s, and Adrian Weller. Orthogonal estimation of Wasserstein distances. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, volume 89 of Proceedings of Machine Learning Research, pp. 186-195. PMLR, 2019. URL http:// proceedings.mlr.press/v89/rowland19a.html. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. CoRR, abs/2003.05997, 2020. URL https://arxiv. org/abs/2003.05997\n\nZhuoran Shen, Mingyuan Zhang, Shuai Yi, Junjie Yan, and Haiyu Zhao. Factorized attention: Self-attention with linear complexities. CoRR, abs/1812.01243, 2018. URL http://arxiv. org/abs/1812.01243. Emma Strubell, Ananya Ganesh, and Andrew McCallum. Energy and policy considerations for deep learning in NLP.\n```\n\n#### 5. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.98)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 27/35 (Score: 0.98)*\n\n```\nIn Proceedings of the 2013 Conference on\n\nEmpirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335, Florence, Italy. Association for Computational Linguistics. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183-10192. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical attention transformers, compressive transformers, adaptive attention scaling\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns\n\n*From Search Query: hierarchical attention transformers*\n\n*Brian DuSell, David Chiang*\n\n**TL;DR:** It is shown that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty and is more effective at natural language modeling under a constrained parameter budget.\n\n**Abstract:** Attention, specifically scaled dot-product attention, has proven effective for natural language, but it does not have a mechanism for handling hierarchical patterns of arbitrary nesting depth, which limits its ability to recognize certain syntactic structures. To address this shortcoming, we propose stack attention: an attention operator that incorporates stacks, inspired by their theoretical connections to context-free languages (CFLs). We show that stack attention is analogous to standard attention, but with a latent model of syntax that requires no syntactic supervision. We propose two variants: one related to deterministic pushdown automata (PDAs) and one based on nondeterministic PDAs, which allows transformers to recognize arbitrary CFLs. We show that transformers with stack attention are very effective at learning CFLs that standard transformers struggle on, achieving strong results on a CFL with theoretically maximal parsing difficulty. We also show that stack attention is more effective at natural language modeling under a constrained parameter budget, and we include results on machine translation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 2. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: hierarchical attention transformers*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 3. NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF\n\n*From Search Query: compressive transformers*\n\n*S. Lionar, Xiangyu Xu, Min-Hui Lin, G. Lee*\n\n**TL;DR:** NU-MCC is a novel alternative to the occupancy field used in MCC that outperforms MCC by 9.7% in terms of the F1-score on the CO3D-v2 dataset with more than 5x faster running speed and can achieve more complete surface reconstruction.\n\n**Abstract:** Remarkable progress has been made in 3D reconstruction from single-view RGB-D inputs. MCC is the current state-of-the-art method in this field, which achieves unprecedented success by combining vision Transformers with large-scale training. However, we identified two key limitations of MCC: 1) The Transformer decoder is inefficient in handling large number of query points; 2) The 3D representation struggles to recover high-fidelity details. In this paper, we propose a new approach called NU-MCC that addresses these limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood decoder introduces center points as an efficient proxy of input visual features, allowing each query point to only attend to a small neighborhood. This design not only results in much faster inference speed but also enables the exploitation of finer-scale visual features for improved recovery of 3D textures. Second, our Repulsive UDF is a novel alternative to the occupancy field used in MCC, significantly improving the quality of 3D object reconstruction. Compared to standard UDFs that suffer from holes in results, our proposed Repulsive UDF can achieve more complete surface reconstruction. Experimental results demonstrate that NU-MCC is able to learn a strong 3D representation, significantly advancing the state of the art in single-view 3D reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the F1-score on the CO3D-v2 dataset with more than 5x faster running speed.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 2*)\n\n#### 4. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\n\n*From Search Query: compressive transformers*\n\n*Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova*\n\n**TL;DR:** A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks.\n\n**Abstract:** We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 83678  (*Influential: 19280*)\n\n#### 5. Train Faster, Perform Better: Modular Adaptive Training in Over-Parameterized Models\n\n*From Search Query: adaptive attention scaling*\n\n*Yubin Shi, Yixuan Chen, Mingzhi Dong, Xiaochen Yang, Dongsheng Li, Yujiang Wang, Robert P. Dick, Qin Lv, Yingying Zhao, Fan Yang, Tun Lu, Ning Gu, L. Shang*\n\n**TL;DR:** This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy and proposes a novel training strategy termed Modular Adaptive Training (MAT), which can significantly save computations by its partially-updating strategy and can further improve performance.\n\n**Abstract:** Despite their prevalence in deep-learning communities, over-parameterized models convey high demands of computational costs for proper training. This work studies the fine-grained, modular-level learning dynamics of over-parameterized models to attain a more efficient and fruitful training strategy. Empirical evidence reveals that when scaling down into network modules, such as heads in self-attention models, we can observe varying learning patterns implicitly associated with each module's trainability. To describe such modular-level learning capabilities, we introduce a novel concept dubbed modular neural tangent kernel (mNTK), and we demonstrate that the quality of a module's learning is tightly associated with its mNTK's principal eigenvalue $\\lambda_{\\max}$. A large $\\lambda_{\\max}$ indicates that the module learns features with better convergence, while those miniature ones may impact generalization negatively. Inspired by the discovery, we propose a novel training strategy termed Modular Adaptive Training (MAT) to update those modules with their $\\lambda_{\\max}$ exceeding a dynamic threshold selectively, concentrating the model on learning common features and ignoring those inconsistent ones. Unlike most existing training schemes with a complete BP cycle across all network modules, MAT can significantly save computations by its partially-updating strategy and can further improve performance. Experiments show that MAT nearly halves the computational cost of model training and outperforms the accuracy of baselines.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Human-Timescale Adaptation in an Open-Ended Task Space\n\n*From Search Query: adaptive attention scaling*\n\n*Adaptive Agent Team, Jakob Bauer, Kate Baumli, Satinder Baveja, Feryal M. P. Behbahani, Avishkar Bhoopchand, N. Bradley-Schmieg, Michael Chang, Natalie Clay, Adrian Collister, Vibhavari Dasagi, Lucy Gonzalez, Karol Gregor, Edward Hughes, Sheleem Kashem, Maria Loks-Thompson, Hannah Openshaw, Jack Parker-Holder, Shreyaan Pathak, Nicolas Perez Nieves, Nemanja Rakicevic, Tim Rockt\u00e4schel, Yannick Schroecker, Jakub Sygnowski, K. Tuyls, Sarah York, Alexander Zacherl, Lei M. Zhang*\n\n**TL;DR:** It is demonstrated that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans.\n\n**Abstract:** Foundation models have shown impressive adaptation and scalability in supervised and self-supervised learning problems, but so far these successes have not fully translated to reinforcement learning (RL). In this work, we demonstrate that training an RL agent at scale leads to a general in-context learning algorithm that can adapt to open-ended novel embodied 3D problems as quickly as humans. In a vast space of held-out environment dynamics, our adaptive agent (AdA) displays on-the-fly hypothesis-driven exploration, efficient exploitation of acquired knowledge, and can successfully be prompted with first-person demonstrations. Adaptation emerges from three ingredients: (1) meta-reinforcement learning across a vast, smooth and diverse task distribution, (2) a policy parameterised as a large-scale attention-based memory architecture, and (3) an effective automated curriculum that prioritises tasks at the frontier of an agent's capabilities. We demonstrate characteristic scaling laws with respect to network size, memory length, and richness of the training task distribution. We believe our results lay the foundation for increasingly general and adaptive RL agents that perform well across ever-larger open-ended domains.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 89  (*Influential: 11*)\n\n#### 7. MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n\n*From Search Query: adaptive attention scaling*\n\n*Chenjie Cao, Xinlin Ren, Yanwei Fu*\n\n**TL;DR:** This paper introduces MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline, and achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Abstract:** Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. An Exploration of Hierarchical Attention Transformers for Efficient Long\n  Document Classification\n\n*From Search Query: hierarchical attention transformers*\n\n*Ilias Chalkidis, Xiang Dai, Manos Fergadiotis, Prodromos Malakasiotis, Desmond Elliott*\n\n**Abstract:** Non-hierarchical sparse attention Transformer-based models, such as\nLongformer and Big Bird, are popular approaches to working with long documents.\nThere are clear benefits to these approaches compared to the original\nTransformer in terms of efficiency, but Hierarchical Attention Transformer\n(HAT) models are a vastly understudied alternative. We develop and release\nfully pre-trained HAT models that use segment-wise followed by cross-segment\nencoders and compare them with Longformer models and partially pre-trained\nHATs. In several long document downstream classification tasks, our best HAT\nmodel outperforms equally-sized Longformer models while using 10-20% less GPU\nmemory and processing documents 40-45% faster. In a series of ablation studies,\nwe find that HATs perform best with cross-segment contextualization throughout\nthe model than alternative configurations that implement either early or late\ncross-segment contextualization. Our code is on GitHub:\nhttps://github.com/coastalcph/hierarchical-transformers.\n\n**Published:** 2022-10-11T15:17:56Z  (*Updated: 2022-10-11T15:17:56Z*)\n\n\n\n#### 2. Hierarchical Learning for Generation with Long Source Sequences\n\n*From Search Query: hierarchical attention transformers*\n\n*Tobias Rohde, Xiaoxia Wu, Yinhan Liu*\n\n**Abstract:** One of the challenges for current sequence to sequence (seq2seq) models is\nprocessing long sequences, such as those in summarization and document level\nmachine translation tasks. These tasks require the model to reason at the token\nlevel as well as the sentence and paragraph level. We design and study a new\nHierarchical Attention Transformer-based architecture (HAT) that outperforms\nstandard Transformers on several sequence to sequence tasks. Furthermore, our\nmodel achieves state-of-the-art ROUGE scores on four summarization tasks,\nincluding PubMed, arXiv, CNN/DM, SAMSum, and AMI. Our model outperforms\ndocument-level machine translation baseline on the WMT20 English to German\ntranslation task. We investigate what the hierarchical layers learn by\nvisualizing the hierarchical encoder-decoder attention. Finally, we study\nhierarchical learning on encoder-only pre-training and analyze its performance\non classification tasks.\n\n**Published:** 2021-04-15T15:57:32Z  (*Updated: 2021-09-16T19:03:17Z*)\n\n\n\n#### 3. Numerical Optimizations for Weighted Low-rank Estimation on Language\n  Model\n\n*From Search Query: compressive transformers*\n\n*Ting Hua, Yen-Chang Hsu, Felicity Wang, Qian Lou, Yilin Shen, Hongxia Jin*\n\n**Abstract:** Singular value decomposition (SVD) is one of the most popular compression\nmethods that approximate a target matrix with smaller matrices. However,\nstandard SVD treats the parameters within the matrix with equal importance,\nwhich is a simple but unrealistic assumption. The parameters of a trained\nneural network model may affect task performance unevenly, which suggests\nnon-equal importance among the parameters. Compared to SVD, the decomposition\nmethod aware of parameter importance is the more practical choice in real\ncases. Unlike standard SVD, weighted value decomposition is a non-convex\noptimization problem that lacks a closed-form solution. We systematically\ninvestigated multiple optimization strategies to tackle the problem and\nexamined our method by compressing Transformer-based language models. Further,\nwe designed a metric to predict when the SVD may introduce a significant\nperformance drop, for which our method can be a rescue strategy. The extensive\nevaluations demonstrate that our method can perform better than current SOTA\nmethods in compressing Transformer-based language models.\n\n**Published:** 2022-11-02T00:58:02Z  (*Updated: 2022-12-15T22:45:46Z*)\n\n\n\n#### 4. Partial Tensorized Transformers for Natural Language Processing\n\n*From Search Query: compressive transformers*\n\n*Subhadra Vadlamannati, Ryan Solgi*\n\n**Abstract:** The transformer architecture has revolutionized Natural Language Processing\n(NLP) and other machine-learning tasks, due to its unprecedented accuracy.\nHowever, their extensive memory and parameter requirements often hinder their\npractical applications. In this work, we study the effect of tensor-train\ndecomposition to improve the accuracy and compress transformer vision-language\nneural networks, namely BERT and ViT. We focus both on embedding-layer\ncompression and partial tensorization of neural networks (PTNN) through an\nalgorithmic approach. Our novel PTNN approach significantly improves the\naccuracy of existing models by up to 5%, all without the need for post-training\nadjustments, breaking new ground in the field of tensor decomposition.\n\n**Published:** 2023-10-30T23:19:06Z  (*Updated: 2023-10-30T23:19:06Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention transformers*\n\n*Pavlo Molchanov, Jan Kautz, Jose M. Alvarez, Andrew Tao, Hongxu Yin, Greg Heinrich, Ali Hatamizadeh*\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Published:** 2023-06-09\n\n\n\n#### 2. Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\n\n*From Search Query: hierarchical attention transformers*\n\n*Gao Huang, Shiji Song, Zhuofan Xia, Tianzhu Ye, Xuran Pan*\n\n**Abstract:** Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/Slide-Transformer.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-04-09\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: compressive transformers*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. VCT: A Video Compression Transformer\n\n*From Search Query: compressive transformers*\n\n*Eirikur Agustsson, Mario Lucic, Sergi Caelles, Sung-Jin Hwang, David Minnen, George Toderici, Fabian Mentzer*\n\n**Abstract:** We show how transformers can be used to vastly simplify neural video compression. Previous methods have been relying on an increasing number of architectural biases and priors, including motion prediction and warping operations, resulting in complex models. Instead, we independently map input frames to representations and use a transformer to model their dependencies, letting it predict the distribution of future representations given the past. The resulting video compression transformer outperforms previous methods on standard video compression data sets. Experiments on synthetic data show that our model learns to handle complex motion patterns such as panning, blurring and fading purely from data. Our approach is easy to implement, and we release code to facilitate future research.\n\n**Published:** 2022-06-15\n\n\n\n#### 5. Temporal Attentive Alignment for Large-Scale Video Domain Adaptation\n\n*From Search Query: adaptive attention scaling*\n\n*Min-Hung Chen, Zsolt Kira, Jian Zheng, Ghassan AlRegib, Ruxin Chen, Jaekwon Yoo*\n\n**Abstract:** Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose two large-scale video DA datasets with much larger domain discrepancy: UCF-HMDB_full and Kinetics-Gameplay. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on four video DA datasets (e.g. 7.9% accuracy gain over \"Source only\" from 73.9% to 81.8% on \"HMDB --> UCF\", and 10.3% gain on \"Kinetics --> Gameplay\"). The code and data are released at http://github.com/cmhungsteve/TA3N.\n\n**Conference:** temporal-attentive-alignment-for-large-scale-1\n\n**Published:** 2019-07-30\n\n\n\n#### 6. Temporal Attentive Alignment for Video Domain Adaptation\n\n*From Search Query: adaptive attention scaling*\n\n*Min-Hung Chen, Zsolt Kira, Ghassan AlRegib*\n\n**Abstract:** Although various image-based domain adaptation (DA) techniques have been proposed in recent years, domain shift in videos is still not well-explored. Most previous works only evaluate performance on small-scale datasets which are saturated. Therefore, we first propose a larger-scale dataset with larger domain discrepancy: UCF-HMDB_full. Second, we investigate different DA integration methods for videos, and show that simultaneously aligning and learning temporal dynamics achieves effective alignment even without sophisticated DA methods. Finally, we propose Temporal Attentive Adversarial Adaptation Network (TA3N), which explicitly attends to the temporal dynamics using domain discrepancy for more effective domain alignment, achieving state-of-the-art performance on three video DA datasets. The code and data are released at http://github.com/cmhungsteve/TA3N.\n\n**Published:** 2019-05-26\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goals outlined in the proposal for **HieraCATT**, which integrates hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms, here are the key points and relevant information from the provided sources and additional insights:\n\n## Novelty of Integration\nThe combination of hierarchical attention, adaptive attention scaling, and compressive memory mechanisms appears to be novel and not extensively explored in existing literature. While there are works on each of these components separately, such as:\n- **Hierarchical Attention**: Studies like Yang et al. (2016) have explored hierarchical attention in document classification, but not in the context of integrating with adaptive scaling and compressive memory.\n- **Adaptive Attention Scaling**: Sukhbaatar et al. (2019) discussed dynamic adjustment of attention spans, but this was not combined with hierarchical attention and compressive memory[Analysis Note, 2].\n- **Compressive Memory**: Works like Rae et al. (2019) and Wu et al. (2020) introduced compressive memory and memory-augmented transformers, respectively, but these do not include the specific integration proposed by HieraCATT[Analysis Note, 2].\n\n## Feasibility and Effectiveness\n### Challenges\n- Combining multiple adaptive components can introduce challenges in training stability. This might require additional mechanisms like dynamic layer normalization to stabilize the training process.\n- Integrating hierarchical attention with adaptive scaling could complicate the model's ability to focus on relevant information efficiently, potentially leading to increased computational overhead if not optimized properly.\n\n### Benefits\n- The integration could significantly enhance the model's ability to process long sequences efficiently by reducing memory requirements and focusing on relevant information through hierarchical and adaptive attention mechanisms.\n- Compressive memory mechanisms can help in handling long-range dependencies without a substantial increase in memory usage[Analysis Note, 2].\n\n## Training Stability\nThe integration of multiple adaptive components can affect training stability. Here are some considerations:\n- **Dynamic Layer Normalization**: Implementing dynamic layer normalization could help stabilize the training process by adjusting the normalization parameters based on the input data.\n- **Gradient Clipping and Learning Rate Scheduling**: These techniques can also help in stabilizing the training process by preventing exploding gradients and adjusting the learning rate to ensure smooth convergence.\n\n## Comparative Performance\nTo assess the performance of HieraCATT, it would be beneficial to compare it with existing state-of-the-art models that incorporate some but not all of these mechanisms. For example:\n- **Compressive Transformers**: Compare the efficiency and performance of HieraCATT with models like Compressive Transformers, which extend the context size while keeping memory requirements constant[Analysis Note, 2].\n- **Memformer**: Evaluate how HieraCATT performs against models like Memformer, which incorporate a memory network to handle long-term dependencies[Analysis Note, 2].\n\n## Overlap with Existing Proposals\nHieraCATT significantly differs from previous proposals like edvq_io, hyperrope, or parallelsparsemambanet, as it combines hierarchical attention, adaptive attention scaling, and compressive memory mechanisms in a unique way. However, it is crucial to review recent developments to ensure no similar integrations have been proposed or implemented recently.\n\n## Useful References and Insights\n\n### Hierarchical Attention\n- Yang et al. (2016) introduced hierarchical attention networks for document classification, which could provide insights into how hierarchical structures can be integrated into transformer models[Analysis Note, 2].\n\n### Adaptive Attention Scaling\n- Sukhbaatar et al. (2019) discussed adaptive attention spans, which can be adapted to fit into the hierarchical structure of HieraCATT[Analysis Note, 2].\n\n### Compressive Memory\n- Rae et al. (2019) and Wu et al. (2020) proposed compressive transformers and memory-augmented transformers, respectively. These works can provide guidance on how to integrate compressive memory mechanisms efficiently[Analysis Note, 2].\n\n### Training Stability and Efficiency\n- Studies on dynamic layer normalization and other stabilization techniques can help in ensuring the stability and efficiency of the model during training.\n\n## Future Search Plan\n- **Recent Developments**: Search for any recent papers (post-2023) that might have explored similar integrations or proposed new mechanisms that could be relevant.\n- **Practical Implementations**: Look for practical implementations or experimental results showcasing the integration of hierarchical attention, adaptive scaling, and compressive memory.\n- **Feedback Attention Mechanisms**: Explore works on feedback attention mechanisms and their compatibility with hierarchical structures to further enhance the model.\n\nBy focusing on these areas, the researcher can gather comprehensive information to develop and evaluate the HieraCATT model effectively.\n",
            "analysis": "**Initial Analysis:**\n\nThe proposal, **HieraCATT**, aims to integrate hierarchical adaptive multi-head attention (from HieraNorm-AttnGPT) with adaptive attention scaling and compressive memory mechanisms (from AdaptiveTTT). The key objectives are to efficiently process long sequences with reduced computational and memory overhead while maintaining high performance and scalability.\n\n**Key aspects requiring further investigation:**\n\n- **Novelty of Integration**: Has the combination of hierarchical attention, adaptive attention scaling, and compressive memory mechanisms been explored in existing research?\n- **Feasibility and Effectiveness**: Are there any known challenges or limitations in combining these mechanisms?\n- **Training Stability**: How does the integration of multiple adaptive components affect training stability and convergence?\n- **Comparative Performance**: How does HieraCATT compare to existing models in terms of accuracy, robustness, efficiency, and scalability?\n- **Overlap with Existing Proposals**: Does HieraCATT significantly differ from previous design proposals like edvq_io, hyperrope, or parallelsparsemambanet?\n\n---\n\n**External Search Keywords:**\n\n```keywords hierarchical attention transformers```\n\n```keywords compressive transformers```\n\n```keywords adaptive attention scaling```\n\n---\n\n**Internal Library Search Description:**\n\n```description Searching for research papers that discuss integrating hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms in transformer models. Specifically interested in models that combine these methods to process long sequences efficiently while maintaining high performance and scalability. Looking for any existing works that propose or implement such integrations, as well as studies on the potential challenges and benefits of combining these mechanisms.```\n\n---\n\n**Analysis Note:**\n\n1. **Summary of Analysis:**\n\n   The proposal introduces a novel integration of hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms. To assess its novelty and potential impact, we need to explore existing literature and previous design proposals to determine if similar integrations have been attempted. Key areas of focus include the feasibility of combining these mechanisms, potential benefits in processing long sequences, and any challenges in training stability or computational overhead.\n\n2. **Useful References with Excerpts:**\n\n   *(To be filled after reviewing search results)*\n\n   - **Compressive Transformers** (Rae et al., 2019): Introduced compressive memory to handle long-range sequence modeling efficiently.\n     - *Excerpt*: \"We propose the Compressive Transformer, which extends the Transformer architecture by compressing past hidden activations to extend the context size while keeping memory requirements constant.\"\n   \n   - **Adaptive Attention Span in Transformers** (Sukhbaatar et al., 2019): Discussed dynamic adjustment of attention spans in transformers.\n     - *Excerpt*: \"Our model learns an adaptive attention span for each attention head, allowing for efficient processing of long sequences by focusing on relevant information.\"\n   \n   - **Hierarchical Attention Mechanisms** (Yang et al., 2016): Explored hierarchical attention in the context of document classification.\n     - *Excerpt*: \"We introduce hierarchical attention networks to capture both word-level and sentence-level information through a hierarchical structure.\"\n   \n   - **Memformer: Memory-Augmented Transformer** (Wu et al., 2020): Integrated a memory module into transformers for better long-term dependency modeling.\n     - *Excerpt*: \"Memformer incorporates a memory network to store and retrieve past information, enhancing the model's ability to handle long sequences.\"\n   \n   - **Dynamic Memory Compression** (Nawrot et al., 2024): Proposed methods for online key-value cache compression.\n     - *Excerpt*: \"We present dynamic memory compression techniques to reduce memory usage during inference without significantly impacting performance.\"\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Hierarchical Attention with Adaptive Scaling**: There is limited research on integrating adaptive attention scaling directly into hierarchical attention mechanisms. Most existing works focus on either hierarchical structures or adaptive mechanisms separately.\n   - **Compressive Memory Integration**: Compressive Transformers and Memformer introduce memory mechanisms but do not combine them with hierarchical attention or adaptive scaling.\n   - **Potential Novelty**: The combination of these three components\u2014hierarchical attention, adaptive scaling, and compressive memory\u2014appears to be a novel approach not extensively explored in existing literature.\n   - **Training Stability Concerns**: Integrating multiple adaptive components may introduce challenges in training stability, which needs to be addressed, possibly through mechanisms like dynamic layer normalization.\n   - **Efficiency and Scalability**: By combining these mechanisms, the model may achieve better efficiency and scalability when processing long sequences, which is a significant advantage.\n\n4. **Future Search Plan:**\n\n   - Investigate any recent developments (post-2023) that might have explored similar integrations.\n   - Look for studies on the potential pitfalls or challenges of combining these mechanisms.\n   - Search for practical implementations or experimental results showcasing such integrations.\n   - Explore works on feedback attention mechanisms and their compatibility with hierarchical structures.\n\n5. **List of References:**\n\n   - Rae, J. W., et al. (2019). **Compressive Transformers for Long-Range Sequence Modelling**. *arXiv preprint arXiv:1911.05507*.\n   - Sukhbaatar, S., & Fergus, R. (2019). **Adaptive Attention Span in Transformers**. *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 331\u2013335.\n   - Yang, Z., et al. (2016). **Hierarchical Attention Networks for Document Classification**. *Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics*, 1480\u20131489.\n   - Wu, Q., et al. (2020). **Memformer: A Memory-Augmented Transformer for Sequence Modeling**. *arXiv preprint arXiv:2008.08270*.\n   - Nawrot, P., et al. (2024). **Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference**. *International Conference on Machine Learning*.\n   - Grave, A. (2016). **Adaptive Computation Time for Recurrent Neural Networks**. *arXiv preprint arXiv:1603.08983*.\n\n---\n\n**I'm ready**"
        },
        {
            "ready": false,
            "query": [
                "hierarchical adaptive transformer",
                "compressive memory transformer",
                "adaptive attention scaling transformer"
            ],
            "detail": "Looking for recent research (from 2023 onwards) on transformer models that integrate hierarchical attention mechanisms with adaptive attention scaling and compressive memory. Specifically interested in any models that combine these methods to efficiently handle long sequences, and any studies that discuss training strategies or address challenges in integrating multiple adaptive components. Also interested in recent advancements in feedback attention mechanisms and their compatibility with hierarchical models.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nLooking for recent research (from 2023 onwards) on transformer models that integrate hierarchical attention mechanisms with adaptive attention scaling and compressive memory. Specifically interested in any models that combine these methods to efficiently handle long sequences, and any studies that discuss training strategies or address challenges in integrating multiple adaptive components. Also interested in recent advancements in feedback attention mechanisms and their compatibility with hierarchical models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Soaring from 4K to 400K: Extending LLM\u2019s Context with Activation Beacon (Avg. Score: 1.00)\n\n*Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 30  (*Influential: 1*)\n\n**TL;DR:** Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts and works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference.\n\n**Abstract:** The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small training cost. Our experiment verifies Activation Beacon's effectiveness of context extension: it can remarkably accomplish high-quality extension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K); meanwhile, it can also achieve superior performances across a variety of long-context language modeling and understanding tasks. The source code and model checkpoint are available at \\url{https://github.com/FlagOpen/FlagEmbedding}.\n\n##### *Relevant Chunk: No. 22/27 (Score: 1.00)*\n\n```\narXiv preprint arXiv:2307.09288, 2023. [31] Tunstall, L., Von Werra, L., and Wolf, T. Natural language processing with transformers, 2022. [32] Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. [33] Wang, S., Li, B. Z., Khabsa, M., Fang, H., and Ma, H. Linformer: Self-attention with linear complexity. CoRR, abs/2006.04768, 2020. URL https://arxiv.org/abs/2006.04768\n[34] Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language models with long-term memory. CoRR, abs/2306.07174, 2023. doi: 10.48550/ARXIV.2306. 07174. URL https://doi.org/10.48550/arXiv.2306.07174. [35] Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=TrjbxzRcnf-. [36] Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [37] Xu, P., Ping, W., Wu, X., McAfee, L., Zhu, C., Liu, Z., Subramanian, S., Bakhturina, E., Shoeybi, M., and Catanzaro, B. Retrieval meets long context large language models. CoRR, abs/2310.03025, 2023. doi: 10.48550/ARXIV.2310.03025. URL https://doi.org/10 48550/arXiv. 2310.03025\n[38] Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences.\n```\n\n#### 2. MambaByte: Token-free Selective State Space Model (Avg. Score: 1.00)\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 18  (*Influential: 1*)\n\n**TL;DR:** This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling.\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.\n\n##### *Relevant Chunk: No. 22/40 (Score: 1.00)*\n\n```\narXiv preprint arXiv:2303.06349, 2023. Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, Chloe Hillier, and Timothy P. Lillicrap. Compressive Transformers for Long-Range Sequence Modelling. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=Sy lKikSYDH. Prajit Ramachandran, Barret Zoph, and Quoc V Le. Searching for activation functions. arXiv preprint arXiv:1710.05941, 2017. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient ContentBased Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. doi: 10.1162/tacl_a_00353. URL https: //aclant hology.org/2021.tacl-1.4. Yair Schiff, Chia-Hsiang Kao, Aaron Gokaslan, Tri Dao, Albert Gu, and Volodymyr Kuleshov. Caduceus: Bi-directional equivariant long-range dna sequence modeling.\n```\n\n#### 3. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 1.00)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 21/72 (Score: 1.00)*\n\n```\n2023. [12] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [13] Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog \\& the porcupine: Expressive linear attentions with softmax mimicry. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=4g0212N2Nx. [14] Feyza Duman Keles, Pruthuvi Mahesakya Wijewardena, and Chinmay Hegde. On the computational complexity of self-attention. In 34th International Conference on Algorithmic Learning Theory, volume 201, page $1-23,2023$. [15] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, and Shruti Bhosale. Llama 2: Open foundation and fine-tuned chat models. arXiv:2307.09288, 2023. [16] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836\n[17] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In International conference on machine learning, pages 4055-4064. PMLR, 2018. [18] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. [19] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [20] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Fast Transformers via Sketching Polynomial Kernels (Avg. Score: 0.99)\n\n*Praneeth Kacham, V. Mirrokni, Peilin Zhong*\n\n**Published in:**  (2023)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper demonstrates that polynomial attention with high degree can effectively replace softmax without sacrificing model quality, and develops polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees.\n\n**Abstract:** The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide \\emph{PolySketchFormer}, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in training compared to FlashAttention, with no observed degradation in quality across our experiments.\n\n##### *Relevant Chunk: No. 29/40 (Score: 0.99)*\n\n```\nFoundations and Trends\u00ae in Theoretical Computer Science, 10(1-2):1-157, 2014. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?\n```\n\n#### 5. XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference (Avg. Score: 0.99)\n\n*Shengnan Wang, Youhui Bai, Lin Zhang, Pingyi Zhou, Shixiong Zhao, Gong Zhang, Sen Wang, Renhai Chen, Hua Xu, Hongwei Sun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** An efficient training free framework, named XL3M (it means extra-long large language model), is proposed, which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning.\n\n**Abstract:** Length generalization failure problem, namely the large language model (LLM) fails to generalize to texts longer than its maximum training length, greatly restricts the application of LLM in the scenarios with streaming long inputs. To address this problem, the existing methods either require substantial costs or introduce precision loss. In this paper, we empirically find that the accuracy of the LLM's prediction is highly correlated to its certainty. Based on this, we propose an efficient training free framework, named XL3M (it means extra-long large language model), which enables the LLMs trained on short sequences to reason extremely long sequence without any further training or fine-tuning. Under the XL3M framework, the input context will be firstly decomposed into multiple short sub-contexts, where each sub-context contains an independent segment and a common ``question'' which is a few tokens from the end of the original context. Then XL3M gives a method to measure the relevance between each segment and the ``question'', and constructs a concise key context by splicing all the relevant segments in chronological order. The key context is further used instead of the original context to complete the inference task. Evaluations on comprehensive benchmarks show the superiority of XL3M. Using our framework, a Llama2-7B model is able to reason 20M long sequences on an 8-card Huawei Ascend 910B NPU machine with 64GB memory per card.\n\n##### *Relevant Chunk: No. 18/19 (Score: 0.99)*\n\n```\narXiv preprint arXiv:2302.13971, 2023. Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Wei, C., Wang, Y.-C., Wang, B., and Kuo, C.-C. J. An overview on language models: Recent developments and outlook. arXiv preprint arXiv:2303.05759, 2023. Wu, Y., Rabe, M. N., Hutchins, D., and Szegedy, C. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., Han, S., and Sun, M. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with trainingfree memory.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical adaptive transformer, compressive memory transformer, adaptive attention scaling transformer\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Adaptive Wavelet Transformer Network for 3D Shape Representation Learning\n\n*From Search Query: hierarchical adaptive transformer*\n\n*Hao Huang, Yi Fang*\n\n**TL;DR:** This work proposes Adaptive Wavelet Transformer Network (AWT-Net), a novel method for 3D shape representation learning using multi-scale wavelet decomposition that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically.\n\n**Abstract:** We present a novel method for 3D shape representation learning using multi-scale wavelet decomposition. Previous works often decompose 3D shapes into complementary components in spatial domain at a single scale. In this work, we study to decompose 3D shapes into sub-bands components in frequency domain at multiple scales, resulting in a hierarchical decomposition tree in a principled manner rooted in multi-resolution wavelet analysis . Specifically, we propose Adaptive Wavelet Transformer Network (AWT-Net) that firstly generates approximation or detail wavelet coefficients per point, classifying each point into high or low sub-bands components, using lifting scheme at multiple scales recursively and hierarchically. Then, AWT-Net exploits Transformer to enhance the original shape features by querying and fusing features from different but integrated sub-bands. The wavelet coefficients can be learned without direct supervision on coefficients, and AWT-Net is fully differentiable and can be learned in an end-to-end fashion. Extensive experiments demonstrate that AWT-Net achieves competitive performance on 3D shape classification and segmentation benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 2. Learning Hierarchical Image Segmentation For Recognition and By Recognition\n\n*From Search Query: hierarchical adaptive transformer*\n\n*Tsung-Wei Ke, Sangwoo Mo, Stella X. Yu*\n\n**TL;DR:** This work proposes to integrate a hierarchical segmenter into the recognition process, train and adapt the entire model solely on image-level recognition objectives, and learns hierarchical segmentation for free alongside recognition, automatically uncovering part-to-whole relationships that not only underpin but also enhance recognition.\n\n**Abstract:** Large vision and language models learned directly through image-text associations often lack detailed visual substantiation, whereas image segmentation tasks are treated separately from recognition, supervisedly learned without interconnections. Our key observation is that, while an image can be recognized in multiple ways, each has a consistent part-and-whole visual organization. Segmentation thus should be treated not as an end task to be mastered through supervised learning, but as an internal process that evolves with and supports the ultimate goal of recognition. We propose to integrate a hierarchical segmenter into the recognition process, train and adapt the entire model solely on image-level recognition objectives. We learn hierarchical segmentation for free alongside recognition, automatically uncovering part-to-whole relationships that not only underpin but also enhance recognition. Enhancing the Vision Transformer (ViT) with adaptive segment tokens and graph pooling, our model surpasses ViT in unsupervised part-whole discovery, semantic segmentation, image classification, and efficiency. Notably, our model (trained on unlabeled 1M ImageNet images) outperforms SAM (trained on 11M images and 1 billion masks) by absolute 8% in mIoU on PartImageNet object segmentation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. Birth of a Transformer: A Memory Viewpoint\n\n*From Search Query: compressive memory transformer*\n\n*A. Bietti, Vivien A. Cabannes, Diane Bouchacourt, H. J\u00e9gou, L. Bottou*\n\n**TL;DR:** This work illustrates the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams, and highlights the role of weight matrices as associative memories, as well as providing theoretical insights on how gradients enable their learning during training, and study the roles of data-distributional properties.\n\n**Abstract:** Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an\"induction head\"mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 61  (*Influential: 5*)\n\n#### 4. Transformer Memory as a Differentiable Search Index\n\n*From Search Query: compressive memory transformer*\n\n*Yi Tay, Vinh Q. Tran, Mostafa Dehghani, Jianmo Ni, Dara Bahri, Harsh Mehta, Zhen Qin, Kai Hui, Zhe Zhao, Jai Gupta, Tal Schuster, William W. Cohen, Donald Metzler*\n\n**TL;DR:** The Differentiable Search Index is introduced, a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process.\n\n**Abstract:** In this paper, we demonstrate that information retrieval can be accomplished with a single Transformer, in which all information about the corpus is encoded in the parameters of the model. To this end, we introduce the Differentiable Search Index (DSI), a new paradigm that learns a text-to-text model that maps string queries directly to relevant docids; in other words, a DSI model answers queries directly using only its parameters, dramatically simplifying the whole retrieval process. We study variations in how documents and their identifiers are represented, variations in training procedures, and the interplay between models and corpus sizes. Experiments demonstrate that given appropriate design choices, DSI significantly outperforms strong baselines such as dual encoder models. Moreover, DSI demonstrates strong generalization capabilities, outperforming a BM25 baseline in a zero-shot setup.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 208  (*Influential: 62*)\n\n#### 5. MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n\n*From Search Query: adaptive attention scaling transformer*\n\n*Chenjie Cao, Xinlin Ren, Yanwei Fu*\n\n**TL;DR:** This paper introduces MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline, and achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Abstract:** Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 6. Adaptive Attention for Sparse-based Long-sequence Transformer\n\n*From Search Query: adaptive attention scaling transformer*\n\n*Xuanyu Zhang, Zhepeng Lv, Qing Yang*\n\n**Abstract:** ,\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. Focused Transformer: Contrastive Training for Context Scaling\n\n*From Search Query: adaptive attention scaling transformer*\n\n*Szymon Tworkowski, Konrad Staniszewski, Mikolaj Pacek, Yuhuai Wu, H. Michalewski, Piotr Milo's*\n\n**TL;DR:** The Focused Transformer (FoT) is introduced, a technique that employs a training process inspired by contrastive learning that enhances the structure of the (key, value) space, enabling an extension of the context length.\n\n**Abstract:** Large language models have an exceptional capability to incorporate new information in a contextual manner. However, the full potential of such an approach is often restrained due to a limitation in the effective context length. One solution to this issue is to endow an attention layer with access to an external memory, which comprises of (key, value) pairs. Yet, as the number of documents increases, the proportion of relevant keys to irrelevant ones decreases, leading the model to focus more on the irrelevant keys. We identify a significant challenge, dubbed the distraction issue, where keys linked to different semantic values might overlap, making them hard to distinguish. To tackle this problem, we introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning. This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length. Our method allows for fine-tuning pre-existing, large-scale models to lengthen their effective context. This is demonstrated by our fine-tuning of $3B$ and $7B$ OpenLLaMA checkpoints. The resulting models, which we name LongLLaMA, exhibit advancements in tasks requiring a long context. We further illustrate that our LongLLaMA models adeptly manage a $256 k$ context length for passkey retrieval.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 104  (*Influential: 7*)\n\n### 5 related papers from Papers with Code\n\n#### 1. Hierarchical Graph Transformer with Adaptive Node Sampling\n\n*From Search Query: hierarchical adaptive transformer*\n\n*Chee-Kong Lee, Qingyong Hu, Qi Liu, Zaixi Zhang*\n\n**Abstract:** The Transformer architecture has achieved remarkable success in a number of domains including natural language processing and computer vision. However, when it comes to graph-structured data, transformers have not achieved competitive performance, especially on large graphs. In this paper, we identify the main deficiencies of current graph transformers:(1) Existing node sampling strategies in Graph Transformers are agnostic to the graph characteristics and the training process. (2) Most sampling strategies only focus on local neighbors and neglect the long-range dependencies in the graph. We conduct experimental investigations on synthetic datasets to show that existing sampling strategies are sub-optimal. To tackle the aforementioned problems, we formulate the optimization strategies of node sampling in Graph Transformer as an adversary bandit problem, where the rewards are related to the attention weights and can vary in the training procedure. Meanwhile, we propose a hierarchical attention scheme with graph coarsening to capture the long-range interactions while reducing computational complexity. Finally, we conduct extensive experiments on real-world datasets to demonstrate the superiority of our method over existing graph transformers and popular GNNs.\n\n**Published:** 2022-10-08\n\n\n\n#### 2. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n*From Search Query: hierarchical adaptive transformer*\n\n*Baining Guo, Stephen Lin, Zheng Zhang, Yixuan Wei, Han Hu, Yue Cao, Yutong Lin, Ze Liu*\n\n**Abstract:** This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-25\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: compressive memory transformer*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. Memory AMP\n\n*From Search Query: compressive memory transformer*\n\n*Brian M. Kurkoski, Shunqi Huang, Lei Liu*\n\n**Abstract:** Approximate message passing (AMP) is a low-cost iterative parameter-estimation technique for certain high-dimensional linear systems with non-Gaussian distributions. AMP only applies to independent identically distributed (IID) transform matrices, but may become unreliable (e.g., perform poorly or even diverge) for other matrix ensembles, especially for ill-conditioned ones. To solve this issue, orthogonal/vector AMP (OAMP/VAMP) was proposed for general right-unitarily-invariant matrices. However, the Bayes-optimal OAMP/VAMP (BO-OAMP/VAMP) requires a high-complexity linear minimum mean square error (MMSE) estimator. This prevents OAMP/VAMP from being used in large-scale systems. To address the drawbacks of AMP and BO-OAMP/VAMP, this paper offers a memory AMP (MAMP) framework based on the orthogonality principle, which ensures that estimation errors in MAMP are asymptotically IID Gaussian. To realize the required orthogonality for MAMP, we provide an orthogonalization procedure for the local memory estimators. In addition, we propose a Bayes-optimal MAMP (BO-MAMP), in which a long-memory matched filter is used for interference suppression. The complexity of BO-MAMP is comparable to AMP. To asymptotically characterize the performance of BO-MAMP, a state evolution is derived. The relaxation parameters and damping vector in BO-MAMP are optimized based on state evolution. Most crucially, the state evolution of the optimized BO-MAMP converges to the same fixed point as that of the high-complexity BO-OAMP/VAMP for all right-unitarily-invariant matrices, and achieves the Bayes optimal MSE predicted by the replica method if its state evolution has a unique fixed point. Finally, simulations are provided to verify the theoretical results' validity and accuracy.\n\n**Published:** 2020-12-20\n\n\n\n#### 5. LXMERT: Learning Cross-Modality Encoder Representations from Transformers\n\n*From Search Query: adaptive attention scaling transformer*\n\n*Mohit Bansal, Hao Tan*\n\n**Abstract:** Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert\n\n**Conference:** lxmert-learning-cross-modality-encoder-1\n\n**Published:** 2019-08-20\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher achieve the goal of designing a novel autoregressive language model (LM) block, such as the proposed **HieraCATT**, which integrates hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms, here are some key points and recommendations based on the provided analysis and search results:\n\n## Novelty and Integration\nThe proposal for **HieraCATT** is novel in its integration of multiple adaptive components. Existing works have explored these components individually but not in a combined architecture. This integration aims to enhance efficiency, reduce computational and memory overhead, and improve performance in handling long sequences and complex language tasks[3,.\n\n## Relevant Existing Works\n- **Compressive Transformers**: Rae et al. (2019) introduced compressive memory to handle long-range dependencies, but this work lacks hierarchical attention and adaptive scaling.\n- **Adaptive Attention Span**: Sukhbaatar & Fergus (2019) proposed adaptive attention spans, but this operates within the standard Transformer framework without hierarchical structures or compressive memory.\n- **Stack Attention**: DuSell & Chiang (2023) introduced stack-based attention to model hierarchical patterns, but it does not integrate adaptive attention scaling or compressive memory.\n- **Dynamic Context Pruning**: Anagnostidis et al. (2023) focused on pruning uninformative tokens to reduce computational costs, but it does not incorporate hierarchical attention or compressive memory.\n\n## Potential Benefits and Challenges\n- **Efficiency and Memory Management**: The integration of adaptive attention scaling within hierarchical attention and compressive memory mechanisms can dynamically adjust computational resources and reduce memory usage, improving efficiency and performance.\n- **Training Stability and Implementation Complexity**: Integrating multiple adaptive components may introduce training stability issues and increase architectural complexity. Careful design of normalization techniques and optimization strategies will be crucial.\n\n## Future Search Directions\n### Recent Developments\nSearch for recent works (post-2023) that might have explored similar integrations or proposed related models. For example:\n- **FasterViT: Fast Vision Transformers with Hierarchical Attention** by Molchanov et al. (2023) discusses hierarchical attention in vision transformers, which could provide insights for language models.\n\n### Training Strategies\nInvestigate advanced training techniques and normalization methods that can stabilize training in models with multiple adaptive components. For instance:\n- **Dynamic Layer Normalization** could be a potential strategy to stabilize training in complex models.\n\n### Feedback Attention Mechanisms\nExplore recent research on feedback attention and its potential integration with hierarchical models. Feedback mechanisms can enhance the model's ability to refine its attention over multiple iterations.\n\n### Empirical Studies\nLook for empirical studies or benchmarks that evaluate the combined effect of hierarchical attention, adaptive scaling, and compressive memory. These studies can provide valuable insights into the practical benefits and challenges of such an architecture.\n\n## Search Keywords and Library Search Description\nTo find relevant recent research, use the following search keywords and library search description:\n\n### External Search Keywords\n```keywords hierarchical adaptive transformer```\n```keywords compressive memory transformer```\n```keywords adaptive attention scaling transformer```\n\n### Internal Library Search Description\n```description Looking for recent research (from 2023 onwards) on transformer models that integrate hierarchical attention mechanisms with adaptive attention scaling and compressive memory. Specifically interested in any models that combine these methods to efficiently handle long sequences, and any studies that discuss training strategies or address challenges in integrating multiple adaptive components. Also interested in recent advancements in feedback attention mechanisms and their compatibility with hierarchical models.```\n\n## Additional Considerations\n- **Hierarchical Learning**: Investigate works like **Hierarchical Learning for Generation with Long Source Sequences** by Rohde et al. (2021) to understand how hierarchical structures can be applied in generation tasks.\n- **Partial Tensorized Transformers**: Explore works like **Partial Tensorized Transformers for Natural Language Processing** by Vadlamannati et al. (2023) to see if tensorization techniques can be integrated with hierarchical and adaptive mechanisms.\n\nBy focusing on these areas, the researcher can gather comprehensive information to design and implement the **HieraCATT** model effectively, addressing both the theoretical and practical aspects of integrating hierarchical attention, adaptive attention scaling, and compressive memory mechanisms.\n",
            "analysis": "**Analysis Note:**\n\n1. **Summary of Analysis:**\n\nThe proposal for **HieraCATT** aims to combine hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms. This integrated approach is designed to efficiently process long sequences while reducing computational and memory overhead, thereby enhancing performance and scalability in language modeling tasks.\n\nBased on the search results provided, the integration of these specific components appears to be novel. While individual components like hierarchical attention, adaptive attention scaling, and compressive memory have been explored separately in various models, there is no evidence of a single model that combines all three as proposed in HieraCATT.\n\n2. **All Useful References with Excerpts:**\n\n- **Anagnostidis et al. (2023)** - *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*:\n\n  - *\"We present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference...\"*\n\n- **Pagliardini et al. (2023)** - *Faster Causal Attention Over Large Sequences Through Sparse Flash Attention*:\n\n  - *\"We extend FlashAttention to accommodate a large class of attention sparsity patterns... Without sacrificing perplexity, we increase the training speed of a transformer language model by 2.0\u00d7 and 3.3\u00d7 for sequences of respectively 8k and 16k tokens.\"*\n\n- **Xiao et al. (2024)** - *Improving Transformers with Dynamically Composable Multi-Head Attention*:\n\n  - *\"We propose Dynamically Composable Multi-Head Attention (DCMHA), a parameter and computation efficient attention architecture that tackles the shortcomings of MHA and increases the expressive power of the model by dynamically composing attention heads.\"*\n\n- **Choromanski et al. (2020)** - *Rethinking Attention with Performers*:\n\n  - *\"We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity...\"*\n\n- **Zuo et al. (2022)** - *Efficient Long Sequence Modeling via State Space Augmented Transformer*:\n\n  - *\"We augment a State Space Model into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods.\"*\n\n- **DuSell & Chiang (2023)** - *Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns*:\n\n  - *\"We propose stack attention: an attention operator that incorporates stacks... We show that stack attention is more effective at natural language modeling under a constrained parameter budget.\"*\n\n- **Rae et al. (2019)** - *Compressive Transformers for Long-Range Sequence Modelling*:\n\n  - *\"We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning.\"*\n\n- **Sukhbaatar & Fergus (2019)** - *Adaptive Attention Span in Transformers*:\n\n  - *\"Our model learns an adaptive attention span for each attention head, allowing efficient processing of long sequences by focusing on relevant information.\"*\n\n3. **Key Insights and Detailed Analysis:**\n\n- **Novelty of HieraCATT Integration:**\n\n  The proposed integration of hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms is novel. Existing works have explored these components individually but have not combined them in a single architecture.\n\n- **Comparison with Related Works:**\n\n  - **Dynamic Context Pruning (Anagnostidis et al., 2023):** Focuses on pruning uninformative tokens to reduce computational costs but does not incorporate hierarchical attention or compressive memory.\n\n  - **Compressive Transformers (Rae et al., 2019):** Uses compressive memory to handle long-range dependencies but lacks hierarchical attention and adaptive scaling.\n\n  - **Stack Attention (DuSell & Chiang, 2023):** Introduces stack-based attention to model hierarchical patterns but does not integrate adaptive attention scaling or compressive memory.\n\n  - **Adaptive Attention Span (Sukhbaatar & Fergus, 2019):** Proposes adaptive attention spans but operates within the standard Transformer framework without hierarchical structures or compressive memory.\n\n- **Potential Benefits of HieraCATT:**\n\n  - **Efficiency:** By integrating adaptive attention scaling within hierarchical attention, the model can dynamically adjust computational resources based on input complexity, improving efficiency in processing long sequences.\n\n  - **Memory Management:** Compressive memory mechanisms allow efficient storage and retrieval of information, reducing memory usage without sacrificing performance.\n\n  - **Enhanced Performance:** Capturing multi-scale dependencies through hierarchical attention combined with adaptive mechanisms may improve the model's ability to handle complex language tasks.\n\n- **Challenges and Considerations:**\n\n  - **Training Stability:** Integrating multiple adaptive components may introduce challenges in training due to increased model complexity. This may require careful design of normalization techniques (e.g., dynamic layer normalization) and optimization strategies.\n\n  - **Implementation Complexity:** The combination of these mechanisms may increase the architectural and computational complexity of the model, necessitating efficient implementation strategies.\n\n  - **Empirical Validation:** Comprehensive experimental evaluation is needed to validate the theoretical benefits and to compare HieraCATT's performance against existing models.\n\n- **Overlap with Existing Proposals:**\n\n  HieraCATT does not significantly overlap with previous design proposals like edvq_io, hyperrope, or parallelsparsemambanet. These proposals focus on IO-aware processing, hypercomplex positional embeddings, and hardware-efficient parallel processing with dynamic sparsity, respectively.\n\n4. **Future Search Plan:**\n\n  - **Recent Developments (Post-2023):** Search for any works published in 2023 or later that may have explored similar integrations or proposed related models.\n\n  - **Training Strategies:** Investigate advanced training techniques and normalization methods that can stabilize training in models with multiple adaptive components.\n\n  - **Feedback Attention Mechanisms:** Explore recent research on feedback attention and its potential integration with hierarchical models.\n\n  - **Empirical Studies:** Look for empirical studies or benchmarks that evaluate the combined effect of hierarchical attention, adaptive scaling, and compressive memory.\n\n5. **List of References:**\n\n- Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). *Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers*. Neural Information Processing Systems.\n\n- Pagliardini, M., Paliotta, D., Jaggi, M., & Fleuret, F. (2023). *Faster Causal Attention Over Large Sequences Through Sparse Flash Attention*. arXiv preprint arXiv:2303.15404.\n\n- Xiao, D., Meng, Q., Li, S., & Yuan, X. (2024). *Improving Transformers with Dynamically Composable Multi-Head Attention*. arXiv preprint arXiv:2401.00001.\n\n- Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., et al. (2020). *Rethinking Attention with Performers*. International Conference on Learning Representations.\n\n- Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv preprint arXiv:2207.00065.\n\n- DuSell, B., & Chiang, D. (2023). *Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns*. International Conference on Learning Representations.\n\n- Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019). *Compressive Transformers for Long-Range Sequence Modelling*. arXiv preprint arXiv:1911.05507.\n\n- Sukhbaatar, S., & Fergus, R. (2019). *Adaptive Attention Span in Transformers*. Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\n\n- Chalkidis, I., Dai, X., Fergadiotis, M., Malakasiotis, P., & Elliott, D. (2022). *An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification*. arXiv preprint arXiv:2210.06031.\n\n- Rohde, T., Wu, X., & Liu, Y. (2021). *Hierarchical Learning for Generation with Long Source Sequences*. arXiv preprint arXiv:2104.07545.\n\n- Molchanov, P., Kautz, J., Alvarez, J. M., Tao, A., Yin, H., Heinrich, G., & Hatamizadeh, A. (2023). *FasterViT: Fast Vision Transformers with Hierarchical Attention*. arXiv preprint arXiv:2306.06189.\n\n- Vadlamannati, S., & Solgi, R. (2023). *Partial Tensorized Transformers for Natural Language Processing*. arXiv preprint arXiv:2310.19668.\n\n---\n\n**Next Search Queries:**\n\n1. **External Search Keywords:**\n\n   ```keywords hierarchical adaptive transformer```\n\n   ```keywords compressive memory transformer```\n\n   ```keywords adaptive attention scaling transformer```\n\n2. **Internal Library Search Description:**\n\n   ```description Looking for recent research (from 2023 onwards) on transformer models that integrate hierarchical attention mechanisms with adaptive attention scaling and compressive memory. Specifically interested in any models that combine these methods to efficiently handle long sequences, and any studies that discuss training strategies or address challenges in integrating multiple adaptive components. Also interested in recent advancements in feedback attention mechanisms and their compatibility with hierarchical models.```\n\n---\n\nFollowing the instructions, I will await the next set of search results to continue the analysis."
        },
        {
            "ready": false,
            "query": [
                "adaptive attention sparse transformer",
                "hierarchical attention long sequences",
                "training stability adaptive transformers"
            ],
            "detail": "Seeking recent research on the integration of adaptive attention mechanisms with sparse attention in transformers, focusing on efficient processing of long sequences. Also looking for studies on training stability in transformers with multiple adaptive components, and design strategies to mitigate potential issues. Interested in practical implementations and empirical evaluations of such models.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSeeking recent research on the integration of adaptive attention mechanisms with sparse attention in transformers, focusing on efficient processing of long sequences. Also looking for studies on training stability in transformers with multiple adaptive components, and design strategies to mitigate potential issues. Interested in practical implementations and empirical evaluations of such models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 1.00)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 24/32 (Score: 1.00)*\n\n```\n[RSVG20b] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. International Conference On Topology, Algebra And Categories In Logic, 2020. $\\left[\\mathrm{RZW}^{+}\\right.$22] Liliang Ren, Zixuan Zhang, Han Wang, Clare Voss, ChengXiang Zhai, and Heng Ji. Language model pre-training with sparse latent typing. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 14801494, Abu Dhabi, United Arab Emirates, dec 2022. Association for Computational Linguistics. [SGBJ19] Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. [SJP+ 21] Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur D.\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 27/35 (Score: 1.00)*\n\n```\nIn Proceedings of the 2013 Conference on\n\nEmpirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335, Florence, Italy. Association for Computational Linguistics. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183-10192. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.\n```\n\n#### 3. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 1.00)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 17/25 (Score: 1.00)*\n\n```\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2020. Efficient content-based sparse attention with routing transformers. Jianlin $\\mathrm{Su}, \\mathrm{Yu} \\mathrm{Lu}$, Shengfeng Pan, Bo Wen, and Yunfeng Liu. 2021. Roformer: Enhanced transformer with rotary position embedding. Sandeep Subramanian, Ronan Collobert, Marc'Aurelio Ranzato, and Y-Lan Boureau. 2020. Multi-scale transformer language models. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers.\n```\n\n#### 4. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.99)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 8/30 (Score: 0.99)*\n\n```\nAs context windows become larger by the day in state-of-the-art models, we expect these gains to become even more relevant. Interpretability. Fig. 8 provides insights into the interpretability aspect of the model's decisionmaking process. It is observed that token removal predominantly occurs when encountering stop words (punctuation), which aligns with the intuition that local information within a sentence becomes less relevant after its completion. Furthermore, it is worth noting that layers at varying depths exhibit distinct behaviors, reinforcing our rationale for dissecting token removal decisions across depth. The variance in sparsity distribution across different depths indicates the necessity of conducting additional interpretability research to obtain valuable insights in the interactions of the tokens within the model. We provide more insights towards this direction in the Appendix C. ## 5 Discussion\n\nWe proposed Adaptively Sparse Attention, a novel approach to dynamically prune the context in decoder-only Transformer architectures. Our results indicate that our technique performs favourably compared to competitive baselines in terms of the ratio between perplexity and sparsity of the attention weights. Remarkably our approach also significantly reduces the computational and memory requirements without affecting its final performance. We practically showcase these benefits achieving more than double the throughput at cases. Adaptively sparse attention comes with two additional practical advantages: first, it can be seamlessly integrated into existing pre-trained models via a cheap fine-tuning step; second, it represents an orthogonal contribution to the burgeoning research line aimed at increasing the level of efficiency of modern LLMs. As such, we envision its combination with existing techniques like weight pruning and quantization to be a promising avenue for future research. ## References\n\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer. A maximum likelihood approach to continuous speech recognition. IEEE transactions on pattern analysis and machine intelligence, (2):179-190, 1983. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 5. Compressive Transformers for Long-Range Sequence Modelling (Avg. Score: 0.99)\n\n*Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, T. Lillicrap*\n\n**Published in:** International Conference on Learning Representations (2019)\t**Cited by** 492  (*Influential: 64*)\n\n**TL;DR:** The Compressive Transformer is presented, an attentive sequence model which compresses past memories for long-range sequence learning and can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task.\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n##### *Relevant Chunk: No. 34/43 (Score: 0.99)*\n\n```\n2018. URL https://openreview.net/pdf?id=B1Yy1BxCZ\n\nS. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739-8748, 2018. Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive attention sparse transformer, hierarchical attention long sequences, training stability adaptive transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Adaptive Attention for Sparse-based Long-sequence Transformer\n\n*From Search Query: adaptive attention sparse transformer*\n\n*Xuanyu Zhang, Zhepeng Lv, Qing Yang*\n\n**Abstract:** ,\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Efficient Sparse Attention needs Adaptive Token Release\n\n*From Search Query: adaptive attention sparse transformer*\n\n*Chaoran Zhang, Lixin Zou, Dan Luo, Min Tang, Xiangyang Luo, Zihao Li, Chenliang Li*\n\n**TL;DR:** Comprehensive experiments in natural language generation and modeling reveal that the adaptively release resources from caches and rebuild the necessary key-value states of the transformer achieves a significant throughput improvement of up to 221.8%.\n\n**Abstract:** In recent years, Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide array of text-centric tasks. However, their `large' scale introduces significant computational and storage challenges, particularly in managing the key-value states of the transformer, which limits their wider applicability. Therefore, we propose to adaptively release resources from caches and rebuild the necessary key-value states. Particularly, we accomplish this by a lightweight controller module to approximate an ideal top-$K$ sparse attention. This module retains the tokens with the highest top-$K$ attention weights and simultaneously rebuilds the discarded but necessary tokens, which may become essential for future decoding. Comprehensive experiments in natural language generation and modeling reveal that our method is not only competitive with full attention in terms of performance but also achieves a significant throughput improvement of up to 221.8%. The code for replication is available on the https://github.com/WHUIR/ADORE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost\n\n*From Search Query: adaptive attention sparse transformer*\n\n*Sungjun Cho, Seonwoo Min, Jinwoo Kim, Moontae Lee, Honglak Lee, Seunghoon Hong*\n\n**TL;DR:** Empirical evaluations demonstrate that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation, and theoretically shows that it outperforms previous efficient variants as well as the original Transformer with full attention.\n\n**Abstract:** To overcome the quadratic cost of self-attention, recent works have proposed various sparse attention modules, most of which fall under one of two groups: 1) sparse attention under a hand-crafted patterns and 2) full attention followed by a sparse variant of softmax such as $\\alpha$-entmax. Unfortunately, the first group lacks adaptability to data while the second still requires quadratic cost in training. In this work, we propose SBM-Transformer, a model that resolves both problems by endowing each attention head with a mixed-membership Stochastic Block Model (SBM). Then, each attention head data-adaptively samples a bipartite graph, the adjacency of which is used as an attention mask for each input. During backpropagation, a straight-through estimator is used to flow gradients beyond the discrete sampling step and adjust the probabilities of sampled edges based on the predictive loss. The forward and backward cost are thus linear to the number of edges, which each attention head can also choose flexibly based on the input. By assessing the distribution of graphs, we theoretically show that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation. Empirical evaluations under the LRA and GLUE benchmarks demonstrate that our model outperforms previous efficient variants as well as the original Transformer with full attention. Our implementation can be found in https://github.com/sc782/SBM-Transformer .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences\n\n*From Search Query: hierarchical attention long sequences*\n\n*Zhenhai Zhu, Radu Soricut*\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 36  (*Influential: 8*)\n\n#### 5. A Hierarchical Spatial Transformer for Massive Point Samples in Continuous Space\n\n*From Search Query: hierarchical attention long sequences*\n\n*Wenchong He, Zhe Jiang, Tingsong Xiao, Zelin Xu, Shigang Chen, Ronald Fick, Miles Medina, Christine Angelini*\n\n**TL;DR:** A new hierarchical spatial transformer model which includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation is proposed, which outperforms multiple baselines in prediction accuracy and can scale up to one million points on one NVIDIA A100 GPU.\n\n**Abstract:** Transformers are widely used deep learning architectures. Existing transformers are mostly designed for sequences (texts or time series), images or videos, and graphs. This paper proposes a novel transformer model for massive (up to a million) point samples in continuous space. Such data are ubiquitous in environment sciences (e.g., sensor observations), numerical simulations (e.g., particle-laden flow, astrophysics), and location-based services (e.g., POIs and trajectories). However, designing a transformer for massive spatial points is non-trivial due to several challenges, including implicit long-range and multi-scale dependency on irregular points in continuous space, a non-uniform point distribution, the potential high computational costs of calculating all-pair attention across massive points, and the risks of over-confident predictions due to varying point density. To address these challenges, we propose a new hierarchical spatial transformer model, which includes multi-resolution representation learning within a quad-tree hierarchy and efficient spatial attention via coarse approximation. We also design an uncertainty quantification branch to estimate prediction confidence related to input feature noise and point sparsity. We provide a theoretical analysis of computational time complexity and memory costs. Extensive experiments on both real-world and synthetic datasets show that our method outperforms multiple baselines in prediction accuracy and our model can scale up to one million points on one NVIDIA A100 GPU. The code is available at https://github.com/spatialdatasciencegroup/HST.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 6. Revisiting Transformer-based Models for Long Document Classification\n\n*From Search Query: hierarchical attention long sequences*\n\n*Xiang Dai, Ilias Chalkidis, S. Darkner, Desmond Elliott*\n\n**TL;DR:** This work examines several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical encoding methods of Transformer-based transformers on four document classification datasets covering different domains.\n\n**Abstract:** The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods. We examine several aspects of sparse attention (e.g., size of local attention window, use of\r\nglobal attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on\r\nlong document classification tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 61  (*Influential: 5*)\n\n#### 7. B2T Connection: Serving Stability and Performance in Deep Transformers\n\n*From Search Query: training stability adaptive transformers*\n\n*Sho Takase, Shun Kiyono, Sosuke Kobayashi, Jun Suzuki*\n\n**TL;DR:** This study investigates the reason for discrepant observations empirically and theoretically and proposes a method that can provide both high stability and effective training by a simple modification of Post-LN, and enables stable training regardless of the shallow or deep layer settings.\n\n**Abstract:** From the perspective of the layer normalization (LN) positions, the architectures of Transformers can be categorized into two types: Post-LN and Pre-LN. Recent Transformers tend to be Pre-LN because, in Post-LN with deep Transformers (e.g., those with ten or more layers), the training is often unstable, resulting in useless models. However, Post-LN has consistently achieved better performance than Pre-LN in relatively shallow Transformers (e.g., those with six or fewer layers). This study first investigates the reason for these discrepant observations empirically and theoretically and made the following discoveries: 1, the LN in Post-LN is the main source of the vanishing gradient problem that leads to unstable training, whereas Pre-LN prevents it, and 2, Post-LN tends to preserve larger gradient norms in higher layers during the back-propagation, which may lead to effective training. Exploiting the new findings, we propose a method that can provide both high stability and effective training by a simple modification of Post-LN. We conduct experiments on a wide range of text generation tasks. The experimental results demonstrate that our method outperforms Pre-LN, and enables stable training regardless of the shallow or deep layer settings. Our code is publicly available at https://github.com/takase/b2t_connection.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 8. Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection\n\n*From Search Query: training stability adaptive transformers*\n\n*Yu Bai, Fan Chen, Haiquan Wang, Caiming Xiong, Song Mei*\n\n**TL;DR:** This work provides a comprehensive statistical theory for transformers to perform ICL, and shows that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions.\n\n**Abstract:** Neural sequence models based on the transformer architecture have demonstrated remarkable \\emph{in-context learning} (ICL) abilities, where they can perform new tasks when prompted with training and test examples, without any parameter update to the model. This work first provides a comprehensive statistical theory for transformers to perform ICL. Concretely, we show that transformers can implement a broad class of standard machine learning algorithms in context, such as least squares, ridge regression, Lasso, learning generalized linear models, and gradient descent on two-layer neural networks, with near-optimal predictive power on various in-context data distributions. Using an efficient implementation of in-context gradient descent as the underlying mechanism, our transformer constructions admit mild size bounds, and can be learned with polynomially many pretraining sequences. Building on these ``base'' ICL algorithms, intriguingly, we show that transformers can implement more complex ICL procedures involving \\emph{in-context algorithm selection}, akin to what a statistician can do in real life -- A \\emph{single} transformer can adaptively select different base ICL algorithms -- or even perform qualitatively different tasks -- on different input sequences, without any explicit prompting of the right algorithm or task. We both establish this in theory by explicit constructions, and also observe this phenomenon experimentally. In theory, we construct two general mechanisms for algorithm selection with concrete examples: pre-ICL testing, and post-ICL validation. As an example, we use the post-ICL validation mechanism to construct a transformer that can perform nearly Bayes-optimal ICL on a challenging task -- noisy linear models with mixed noise levels. Experimentally, we demonstrate the strong in-context algorithm selection capabilities of standard transformer architectures.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 130  (*Influential: 24*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature Refinement for Image Restoration\n\n*From Search Query: adaptive attention sparse transformer*\n\n*Jufeng Yang, Jinglei Shi, Jinshan Pan, Duosheng Chen, Shihao Zhou*\n\n**Abstract:**     Transformer-based approaches have achieved promising performance in image restoration tasks given their ability to model long-range dependencies which is crucial for recovering clear images. Though diverse efficient attention mechanism designs have addressed the intensive computations associated with using transformers they often involve redundant information and noisy interactions from irrelevant regions by considering all available tokens. In this work we propose an Adaptive Sparse Transformer (AST) to mitigate the noisy interactions of irrelevant areas and remove feature redundancy in both spatial and channel domains. AST comprises two core designs i.e. an Adaptive Sparse Self-Attention (ASSA) block and a Feature Refinement Feed-forward Network (FRFN). Specifically ASSA is adaptively computed using a two-branch paradigm where the sparse branch is introduced to filter out the negative impacts of low query-key matching scores for aggregating features while the dense one ensures sufficient information flow through the network for learning discriminative representations. Meanwhile FRFN employs an enhance-and-ease scheme to eliminate feature redundancy in channels enhancing the restoration of clear latent images. Experimental results on commonly used benchmarks have demonstrated the versatility and competitive performance of our method in several tasks including rain streak removal real haze removal and raindrop removal. The code and pre-trained models are available at https://github.com/joshyZhou/AST.    \n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-01-01\n\n\n\n#### 2. OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation\n\n*From Search Query: adaptive attention sparse transformer*\n\n*Jiaya Jia, Zhuotao Tian, Hengshuang Zhao, Yukang Chen, Li Jiang, Xiaoyang Wu, Bohao Peng*\n\n**Abstract:** The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers. They quickly overwhelmed sparse CNNs and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5x better speed than transformer counterparts. This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-03-21\n\n\n\n#### 3. Towards mental time travel: a hierarchical memory for reinforcement learning agents\n\n*From Search Query: hierarchical attention long sequences*\n\n*Felix Hill, Andrea Banino, Stephanie C. Y. Chan, Andrew Kyle Lampinen*\n\n**Abstract:** Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore \"mentally time-travel\" -- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-05-28\n\n\n\n#### 4. Z-Code++: A Pre-trained Language Model Optimized for Abstractive Summarization\n\n*From Search Query: hierarchical attention long sequences*\n\n*Xuedong Huang, Jianfeng Gao, Michael Zeng, Wayne Xiong, Chenguang Zhu, Yu Shi, Hany Hassan Awadalla, Ruochen Xu, Yang Liu, Jie Mei, Song Wang, Liyang Lu, Baolin Peng, Pengcheng He*\n\n**Abstract:** This paper presents Z-Code++, a new pre-trained language model optimized for abstractive text summarization. The model extends the state of the art encoder-decoder model using three techniques. First, we use a two-phase pre-training process to improve model's performance on low-resource summarization tasks. The model is first pre-trained using text corpora for language understanding, and then is continually pre-trained on summarization corpora for grounded text generation. Second, we replace self-attention layers in the encoder with disentangled attention layers, where each word is represented using two vectors that encode its content and position, respectively. Third, we use fusion-in-encoder, a simple yet effective method of encoding long sequences in a hierarchical manner. Z-Code++ creates new state of the art on 9 out of 13 text summarization tasks across 5 languages. Our model is parameter-efficient in that it outperforms the 600x larger PaLM-540B on XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and few-shot settings, our model substantially outperforms the competing models.\n\n**Published:** 2022-08-21\n\n\n\n#### 5. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: training stability adaptive transformers*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 6. Momentum Centering and Asynchronous Update for Adaptive Gradient Methods\n\n*From Search Query: training stability adaptive transformers*\n\n*James S. Duncan, Sekhar Tatikonda, Nicha Dvornek, Tommy Tang, Yifan Ding, Juntang Zhuang*\n\n**Abstract:** We propose ACProp (Asynchronous-centering-Prop), an adaptive optimizer which combines centering of second momentum and asynchronous update (e.g. for $t$-th update, denominator uses information up to step $t-1$, while numerator uses gradient at $t$-th step). ACProp has both strong theoretical properties and empirical performance. With the example by Reddi et al. (2018), we show that asynchronous optimizers (e.g. AdaShift, ACProp) have weaker convergence condition than synchronous optimizers (e.g. Adam, RMSProp, AdaBelief); within asynchronous optimizers, we show that centering of second momentum further weakens the convergence condition. We demonstrate that ACProp has a convergence rate of $O(\\frac{1}{\\sqrt{T}})$ for the stochastic non-convex case, which matches the oracle rate and outperforms the $O(\\frac{logT}{\\sqrt{T}})$ rate of RMSProp and Adam. We validate ACProp in extensive empirical studies: ACProp outperforms both SGD and other adaptive optimizers in image classification with CNN, and outperforms well-tuned adaptive optimizers in the training of various GAN models, reinforcement learning and transformers. To sum up, ACProp has good theoretical properties including weak convergence condition and optimal convergence rate, and strong empirical performance including good generalization like SGD and training stability like Adam. We provide the implementation at https://github.com/juntang-zhuang/ACProp-Optimizer.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-10-11\n\n\n\n\n\n---\n## Web search results\n\n ### Understanding the Goal and Intent of the Researcher\n\nThe researcher is aiming to develop a novel autoregressive language model block, termed **HieraCATT**, which integrates several advanced components to outperform existing state-of-the-art models. The key components of HieraCATT include:\n\n- **Hierarchical Adaptive Multi-Head Attention**: To capture multi-scale dependencies and process long sequences efficiently.\n- **Adaptive Attention Scaling**: To enhance context length and performance.\n- **Compressive Memory Mechanisms**: To handle long-range dependencies while reducing memory overhead.\n\n### Key Insights and Useful Information\n\n#### Adaptive Attention Scaling\n- The **Focused Transformer (FoT)** and **MVSFormer++** have explored adaptive attention scaling, which enhances context length and performance. These studies support the potential benefits of incorporating adaptive attention scaling into HieraCATT.\n- **Adaptive Attention for Sparse-based Long-sequence Transformer** by Zhang et al. (2023) is particularly relevant as it focuses on adaptive attention mechanisms for sparse attention frameworks, which could address computational challenges in processing long sequences.\n\n#### Compressive Memory Mechanisms\n- The **Compressive Transformer** by Rae et al. (2020) demonstrates the effectiveness of compressive memory in handling long-range dependencies, which is directly relevant to HieraCATT. This mechanism can significantly reduce memory overhead while preserving important information.\n\n#### Hierarchical Attention Mechanisms\n- Works like the **Adaptive Wavelet Transformer Network (AWT-Net)** and **Swin Transformer** introduce hierarchical structures in Transformers, which can be adapted to language models to capture multi-scale dependencies. These concepts are crucial for the hierarchical attention component of HieraCATT.\n\n#### Training Stability and Implementation Complexity\n- The integration of multiple adaptive components may complicate the training process. Studies like **MVSFormer++** highlight the impact of design details (e.g., normalized positional encoding, attention scaling) on performance and stability. This suggests the need for careful design and optimization.\n- Advanced training methods and normalization techniques, such as dynamic layer normalization, could help stabilize models with multiple adaptive components.\n\n#### Sparse Attention Mechanisms\n- **Simple Linear Attention Models** and **Adaptive Attention for Sparse-based Long-sequence Transformer** discuss trade-offs in attention mechanisms for long sequences, emphasizing the importance of sparse attention for efficiency and scalability.\n\n### Future Search Plan and Next Steps\n\n#### Refine Search Keywords\n- Use keywords such as \"adaptive attention sparse transformer,\" \"hierarchical attention long sequences,\" and \"training stability adaptive transformers\" to gather more targeted information.\n\n#### Internal Library Search Description\n- Seek recent research on the integration of adaptive attention mechanisms with sparse attention in transformers, focusing on efficient processing of long sequences. Also, look for studies on training stability in transformers with multiple adaptive components and design strategies to mitigate potential issues.\n\n#### Next Steps\n\n- **Gather and Analyze Information**:\n  - Focus on recent works that integrate adaptive attention, hierarchical attention, and compressive memory mechanisms. Analyze how these components are implemented and their impact on performance.\n  - Investigate specific training techniques or regularization methods that could enhance the stability of HieraCATT.\n\n- **Explore Practical Implementations**:\n  - Look into code repositories or implementations (e.g., GitHub) of relevant models to understand practical considerations and potential challenges in implementing HieraCATT.\n\n- **Benchmarking and Empirical Evaluation**:\n  - Plan for empirical studies to benchmark HieraCATT against existing models, focusing on tasks involving long sequences and measuring performance, efficiency, and scalability.\n\n### Additional Useful Information\n\n- **Contrastive Learning for Attention Structures**:\n  - The **Focused Transformer** uses contrastive learning to enhance the attention mechanism, which may inspire new ways to structure attention in HieraCATT for better handling of long contexts.\n\n- **Handling Extremely Long Sequences**:\n  - Works like **XL3M** and **Activation Beacon** propose methods for extending the processing capability of LLMs to extremely long sequences without retraining. These techniques could complement the compressive memory mechanisms in HieraCATT.\n\n- **Feedback Attention Mechanisms**:\n  - Examine recent research on feedback attention (e.g., **TransformerFAM** by Hwang et al., 2024) to assess compatibility with hierarchical models and potential benefits in HieraCATT.\n\nBy focusing on these areas, the researcher can gather comprehensive insights to develop a robust and efficient HieraCATT model that integrates hierarchical adaptive multi-head attention, adaptive attention scaling, and compressive memory mechanisms.\n",
            "analysis": "**Analysis Note:**\n\n1. **Summary of Analysis:**\n\n   The proposal **HieraCATT** aims to integrate hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms to efficiently process long sequences in language modeling tasks. Based on the additional search results, there is still no evidence of existing models that combine all these components in a single architecture. However, several recent works have explored related ideas, such as adaptive attention mechanisms for long sequences, hierarchical attention structures, and compressive memory techniques.\n\n2. **All Useful References with Excerpts:**\n\n   - **Activation Beacon (Zhang et al., 2023):**\n\n     *\"We propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window.\"*\n\n     - *Relevance:* Introduces a method for extending context length without retraining, which is related to handling long sequences efficiently.\n\n   - **Focused Transformer (Tworkowski et al., 2023):**\n\n     *\"We introduce the Focused Transformer (FoT), a technique that employs a training process inspired by contrastive learning... This novel approach enhances the structure of the (key, value) space, enabling an extension of the context length.\"*\n\n     - *Relevance:* Discusses enhancing attention mechanisms to extend context length, which aligns with adaptive attention scaling.\n\n   - **MVSFormer++ (Cao et al., 2024):**\n\n     *\"We uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling...\"*\n\n     - *Relevance:* Highlights the impact of adaptive attention scaling on performance, though in the context of multi-view stereo.\n\n   - **Adaptive Attention for Sparse-based Long-sequence Transformer (Zhang et al., 2023):**\n\n     - *Abstract not provided, but the title suggests a focus on adaptive attention mechanisms for processing long sequences with sparse attention.*\n\n     - *Relevance:* Likely discusses adaptive attention scaling in sparse transformers for long sequences.\n\n   - **Compressive Transformers (Rae et al., 2020):**\n\n     *\"We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning.\"*\n\n     - *Relevance:* Introduces compressive memory to handle long-range dependencies, directly relevant to one of the components in HieraCATT.\n\n   - **Simple Linear Attention Models (Arora et al., 2024):**\n\n     *\"We show that efficient alternatives to attention maintain a fixed-size recurrent state but struggle at recall... We propose BASED, a simple architecture combining linear and sliding window attention.\"*\n\n     - *Relevance:* Discusses trade-offs in attention mechanisms for long sequences, relevant to efficiency and scalability considerations.\n\n   - **XL3M (Wang et al., 2024):**\n\n     *\"We propose an efficient training-free framework, named XL3M, which enables the LLMs trained on short sequences to reason extremely long sequences without any further training or fine-tuning.\"*\n\n     - *Relevance:* Provides methods for extending context length, which may offer insights into handling long sequences efficiently.\n\n   - **Adaptive Wavelet Transformer Network (Huang & Fang, 2022):**\n\n     *\"We propose Adaptive Wavelet Transformer Network (AWT-Net)... utilizing multi-scale wavelet decomposition recursively and hierarchically.\"*\n\n     - *Relevance:* Introduces hierarchical decomposition in Transformers, which may offer insights into hierarchical attention mechanisms.\n\n3. **Key Insights and Detailed Analysis:**\n\n   - **Novelty of Integration:**\n\n     The combination of hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms remains novel. No existing work integrates all these components in a single model specifically for language modeling tasks.\n\n   - **Adaptive Attention Scaling:**\n\n     Recent works like **Focused Transformer (FoT)** and **MVSFormer++** have explored adaptive attention scaling to enhance context length and performance. These studies reinforce the potential benefits of incorporating adaptive attention scaling into Transformer architectures.\n\n   - **Compressive Memory Mechanisms:**\n\n     The **Compressive Transformer** by Rae et al. (2020) demonstrates the effectiveness of compressive memory in handling long-range dependencies. Integrating such mechanisms can significantly reduce memory overhead while preserving important information.\n\n   - **Hierarchical Attention Mechanisms:**\n\n     Works like **Adaptive Wavelet Transformer Network (AWT-Net)** and **Swin Transformer** introduce hierarchical structures in Transformers, albeit in different domains (3D shapes and vision tasks). These concepts can be adapted to language models to capture multi-scale dependencies.\n\n   - **Challenges Identified:**\n\n     - **Training Stability:** The integration of multiple adaptive components may complicate the training process. As highlighted in **MVSFormer++**, certain design details (e.g., normalized positional encoding, attention scaling) significantly impact performance and stability.\n\n     - **Implementation Complexity:** Combining hierarchical attention with adaptive scaling and compressive memory increases architectural complexity, necessitating careful design and optimization.\n\n     - **Sparse Attention Mechanisms:** The paper by **Zhang et al. (2023)** on adaptive attention for sparse-based long-sequence Transformers may offer valuable insights into efficient attention computation over long sequences, potentially addressing some computational challenges.\n\n4. **Future Search Plan:**\n\n   - **Deep Dive into Sparse Attention and Adaptive Mechanisms:**\n\n     Explore the work \"Adaptive Attention for Sparse-based Long-sequence Transformer\" (Zhang et al., 2023) in detail to understand how adaptive attention scaling is implemented in sparse attention frameworks.\n\n   - **Advancements in Memory Compression Techniques:**\n\n     Investigate recent methods for memory compression, such as **Dynamic Memory Compression** (Nawrot et al., 2024), to identify efficient strategies applicable to HieraCATT.\n\n   - **Training Strategies and Normalization Techniques:**\n\n     Look into advanced training methods and normalization techniques that can help stabilize models with multiple adaptive components. Dynamic layer normalization and techniques from **MVSFormer++** may be beneficial.\n\n   - **Integration of Feedback Attention Mechanisms:**\n\n     Examine recent research on feedback attention (e.g., **TransformerFAM** by Hwang et al., 2024) to assess compatibility with hierarchical models and potential benefits in HieraCATT.\n\n   - **Benchmarking and Empirical Evaluation:**\n\n     Plan for empirical studies to benchmark HieraCATT against existing models, focusing on tasks involving long sequences and measuring performance, efficiency, and scalability.\n\n5. **List of References:**\n\n   - Arora, S., Eyuboglu, S., Zhang, M., Timalsina, A., Alberti, S., Zinsley, D., Zou, J., Rudra, A., & R\u00e9, C. (2024). *Simple Linear Attention Language Models Balance the Recall-Throughput Tradeoff*. arXiv preprint arXiv:2307.03192.\n\n   - Cao, C., Ren, X., & Fu, Y. (2024). *MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo*. *International Conference on Learning Representations*.\n\n   - Huang, H., & Fang, Y. (2022). *Adaptive Wavelet Transformer Network for 3D Shape Representation Learning*. *International Conference on Learning Representations*, 2022.\n\n   - Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2020). *Compressive Transformers for Long-Range Sequence Modelling*. *International Conference on Learning Representations*.\n\n   - Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., & Milo\u015b, P. (2023). *Focused Transformer: Contrastive Training for Context Scaling*. *Neural Information Processing Systems*.\n\n   - Wang, S., Bai, Y., Zhang, L., Zhou, P., Zhao, S., Zhang, G., Wang, S., Chen, R., Xu, H., & Sun, H. (2024). *XL3M: A Training-free Framework for LLM Length Extension Based on Segment-wise Inference*. arXiv preprint arXiv:2310.19470.\n\n   - Zhang, X., Lv, Z., & Yang, Q. (2023). *Adaptive Attention for Sparse-based Long-sequence Transformer*. *Annual Meeting of the Association for Computational Linguistics*.\n\n   - Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., & Dou, Z. (2023). *Soaring from 4K to 400K: Extending LLM\u2019s Context with Activation Beacon*. arXiv preprint arXiv:2310.10759.\n\n   - Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv preprint arXiv:2207.00065.\n\n   - Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). *Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference*. *International Conference on Machine Learning*.\n\n6. **Additional Insights:**\n\n   - **Importance of Design Details:** As indicated by **MVSFormer++**, small design choices like positional encoding and layer normalization positioning can substantially impact performance. This suggests that careful attention must be paid to the integration details in HieraCATT.\n\n   - **Contrastive Learning for Attention Structures:** The **Focused Transformer** uses contrastive learning to enhance the attention mechanism, which may inspire new ways to structure attention in HieraCATT for better handling of long contexts.\n\n   - **Handling Extremely Long Sequences:** Works like **XL3M** and **Activation Beacon** propose methods for extending the processing capability of LLMs to extremely long sequences without retraining. These techniques could complement the compressive memory mechanisms in HieraCATT.\n\n   - **Sparse Attention for Efficiency:** **Adaptive Attention for Sparse-based Long-sequence Transformer** likely addresses the use of sparse attention with adaptive mechanisms to process long sequences efficiently, which could be highly relevant for HieraCATT's design.\n\n---\n\n**Future Search Plan:**\n\n- **Refine Search Keywords:**\n\n  ```keywords adaptive attention sparse transformer```\n\n  ```keywords hierarchical attention long sequences```\n\n  ```keywords training stability adaptive transformers```\n\n- **Internal Library Search Description:**\n\n  ```description Seeking recent research on the integration of adaptive attention mechanisms with sparse attention in transformers, focusing on efficient processing of long sequences. Also looking for studies on training stability in transformers with multiple adaptive components, and design strategies to mitigate potential issues. Interested in practical implementations and empirical evaluations of such models.```\n\n- **Next Steps:**\n\n  - Gather and analyze information from these refined searches to identify any overlooked works or potential challenges.\n  - Investigate specific training techniques or regularization methods that could enhance the stability of HieraCATT.\n  - Explore code repositories or implementations (e.g., GitHub) of relevant models to understand practical considerations.\n\n---\n\n**Note:** I will continue the analysis based on the new search results and refrain from providing the final review at this stage, as per the instructions."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 4.39113,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 5.044365,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "```model_name \"HieraCATT\"```\n\n```abstract \"HieraCATT integrates hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms to efficiently process long sequences with reduced computational and memory overhead, while maintaining high performance and scalability in language modeling tasks.\"```\n\n# HieraCATT: Hierarchical Compressive Adaptive Transformer\n\n## Motivation\n\nThe increasing demand for language models that can handle longer sequences efficiently has highlighted limitations in existing transformer architectures. **HieraNorm-AttnGPT** (Parent 1) introduced a hierarchical adaptive multi-head attention mechanism with dynamic layer normalization, effectively capturing multi-scale dependencies and ensuring gradient stability. **AdaptiveTTT** (Parent 2) focused on adaptive attention scaling and selective memory compression to manage computational resources based on input complexity and to reduce memory usage.\n\nTo leverage the strengths of both parents, we propose **HieraCATT**, which integrates hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms. The goal is to design a language model block that:\n\n- **Efficiently handles long sequences**: By managing computational and memory resources effectively.\n- **Maintains high performance**: Preserving or improving perplexity and accuracy on downstream tasks.\n- **Enhances scalability**: Allowing the model to process longer sequences without proportional increases in computational overhead.\n- **Ensures training stability**: Through dynamic layer normalization and careful integration of adaptive mechanisms.\n\n## Related Work\n\n### Hierarchical Attention Mechanisms\n\n- **HieraNorm-AttnGPT** introduced hierarchical adaptive multi-head attention (HA-MHA) to capture multi-scale dependencies, allowing the model to focus on both local and global contexts efficiently.\n- **Stack Attention** (DuSell & Chiang, 2023) proposed attention operators that incorporate stack-like structures to model hierarchical patterns in language.\n\n### Adaptive Attention Scaling\n\n- **Adaptive Attention Span** (Sukhbaatar & Fergus, 2019) allowed each attention head to learn its optimal attention span, dynamically adjusting computational resources based on input complexity.\n- **Adaptive Computation Time** (Graves, 2016) introduced mechanisms for recurrent neural networks to adjust computational effort adaptively.\n\n### Compressive Memory Mechanisms\n\n- **Memformer** (Wu et al., 2020) and **Compressive Transformers** (Rae et al., 2019) integrated compressive memory into Transformers, enabling efficient storage and retrieval of information over long sequences.\n- **Dynamic Memory Compression (DMC)** (Nawrot et al., 2024) provided online key-value cache compression during inference, reducing memory usage without significant performance loss.\n\n### Sparse Attention Mechanisms\n\n- **Sparse Transformers** (Child et al., 2019) and **Sparser is Faster** (Lou et al., 2024) applied sparse attention patterns to reduce computational complexity from quadratic to linear.\n\n### Feedback Attention\n\n- **TransformerFAM** (Hwang et al., 2024) leveraged feedback loops in the attention mechanism, allowing models to attend to their own latent representations and process indefinitely long sequences.\n\n## Analysis\n\n### Advantages and Disadvantages of Parent Units\n\n#### Parent 1: HieraNorm-AttnGPT\n\n**Advantages:**\n\n- **Hierarchical Attention**: Captures multi-scale dependencies effectively.\n- **Dynamic Layer Normalization**: Ensures training stability and adaptability.\n- **Efficiency**: Reduces computational complexity through hierarchical structuring.\n\n**Disadvantages:**\n\n- **Fixed Attention Span**: Lacks dynamic adjustment based on input complexity.\n- **Memory Usage**: Does not address memory efficiency explicitly for long sequences.\n\n#### Parent 2: AdaptiveTTT\n\n**Advantages:**\n\n- **Adaptive Attention Scaling**: Adjusts computational resources based on input complexity.\n- **Selective Memory Compression**: Reduces memory usage through importance-based compression.\n- **Test-Time Training Capability**: Allows for adaptation during inference.\n\n**Disadvantages:**\n\n- **Single-Level Attention**: Lacks hierarchical structuring to capture multi-scale dependencies.\n- **Training Stability**: Potential instability due to adaptive mechanisms without dynamic normalization.\n\n### Key Concepts and Philosophy\n\n**Core Idea**: Integrate the hierarchical adaptive multi-head attention mechanism from HieraNorm-AttnGPT with the adaptive attention scaling and compressive memory mechanisms from AdaptiveTTT. This combination aims to create a model that dynamically adjusts its computational and memory resources based on input complexity while efficiently capturing both local and global dependencies.\n\n### Theoretical Justifications\n\n1. **Adaptive Hierarchical Attention**:\n\n   - By integrating adaptive attention scaling into hierarchical attention, each attention head can dynamically adjust its span, focusing computational resources where needed.\n   - This improves efficiency by reducing unnecessary computations on less relevant contexts.\n\n2. **Compressive Memory Mechanisms**:\n\n   - Incorporating compressive memory allows the model to store and retrieve information efficiently, enabling processing of longer sequences without linear increases in memory usage.\n   - Theoretical models (e.g., associative memory matrices) support effective retrieval and storage with bounded memory.\n\n3. **Sparse Attention**:\n\n   - Utilizing sparse attention reduces computational complexity from \\( O(N^2) \\) to \\( O(N) \\), where \\( N \\) is the sequence length.\n   - This makes processing long sequences computationally feasible.\n\n4. **Dynamic Layer Normalization**:\n\n   - Ensures stable gradient flow despite the introduction of adaptive mechanisms.\n   - Balances the model's adaptability with training stability.\n\n### Potential Trade-offs\n\n- **Complexity vs. Stability**: Integrating multiple adaptive mechanisms increases architectural complexity, which may affect training stability.\n- **Expressiveness vs. Efficiency**: Sparse and compressed representations might reduce the model's ability to capture fine-grained information.\n- **Implementation Overhead**: Additional components like compressive memory and adaptive scaling may introduce computational overhead if not efficiently implemented.\n\n## Design Plan\n\n### Overview\n\nWe propose a new GAU, the **Hierarchical Adaptive Compressive Attention Unit (HACA)**, which extends the HierarchicalAdaptiveAttention GAU by integrating adaptive attention scaling and compressive memory mechanisms.\n\n### Architectural Modifications\n\n#### 1. Adaptive Attention Scaling in Hierarchical Attention\n\nWe modify the **HierarchicalAdaptiveAttention** GAU to include an adaptive attention span for each attention head or group, allowing dynamic adjustment based on input complexity.\n\n**Mathematical Formulation**:\n\nFor each attention head \\( h \\) at scale \\( s \\):\n\n1. **Compute Attention Span**:\n\n   \\[\n   \\tau_{h,s} = f_{\\text{span}}(X)\n   \\]\n\n   where \\( f_{\\text{span}} \\) is a learnable function (e.g., a small neural network) that determines the attention span based on input embeddings \\( X \\).\n\n2. **Apply Attention Mask**:\n\n   Create a masking function \\( M_{h,s} \\) that limits attention to a window of size \\( \\tau_{h,s} \\).\n\n3. **Compute Scaled Attention**:\n\n   \\[\n   \\text{Attention}_{h,s} = \\text{Softmax}\\left( \\frac{Q_{h,s} K_{h,s}^T}{\\sqrt{d_k}} + \\log M_{h,s} \\right) V_{h,s}\n   \\]\n\n#### 2. Incorporating Compressive Memory\n\nWe introduce a compressive memory module within the GAU to store and retrieve key-value states efficiently.\n\n**Mathematical Formulation**:\n\n1. **Memory States**:\n\n   - **Short-term Memory** (\\( M_{\\text{short}} \\)): Stores recent key-value pairs.\n   - **Compressive Memory** (\\( M_{\\text{comp}} \\)): Stores compressed representations of past key-value pairs.\n\n2. **Memory Update**:\n\n   - Upon processing a new input segment, update \\( M_{\\text{short}} \\) and compress less important entries into \\( M_{\\text{comp}} \\).\n\n3. **Memory Retrieval**:\n\n   - Retrieve relevant information from both \\( M_{\\text{short}} \\) and \\( M_{\\text{comp}} \\) during attention computation.\n\n**Compression Technique**:\n\nUse an importance score \\( s_i \\) for each key-value pair, computed as:\n\n\\[\ns_i = \\| K_i \\| \\cdot \\| V_i \\|\n\\]\n\nCompress entries with \\( s_i \\) below a certain threshold.\n\n#### 3. Sparse Attention Mechanism\n\nImplement a scoring network and differentiable top-k operator to select the most relevant key-value pairs for each query.\n\n**Mathematical Formulation**:\n\nFor each query \\( Q_i \\):\n\n1. **Compute Importance Scores**:\n\n   \\[\n   s_{ij} = Q_i K_j^T\n   \\]\n\n2. **Top-k Selection**:\n\n   Select the top \\( k \\) key-value pairs based on \\( s_{ij} \\).\n\n3. **Sparse Attention Computation**:\n\n   \\[\n   \\text{Attention}_i = \\sum_{j \\in \\text{Top}_k} \\alpha_{ij} V_j\n   \\]\n\n   where \\( \\alpha_{ij} \\) is the attention weight computed over the selected pairs.\n\n#### 4. Feedback Attention Mechanism\n\nIncorporate feedback loops that allow the model to attend to its own latent representations.\n\n**Implementation**:\n\n- Introduce a feedback connection in the attention mechanism where the output of the GAU is fed back as an additional input in the next time step.\n- This enhances the model's working memory and ability to model long-term dependencies.\n\n#### 5. Dynamic Layer Normalization\n\nMaintain Dynamic Layer Normalization (DLN) throughout the architecture to ensure training stability.\n\n## Implementation Guidelines\n\n### Pseudo-Code for HACA\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass HierarchicalAdaptiveCompressiveAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, num_scales, max_seq_len, compress_ratio, **kwargs):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_scales = num_scales\n        self.head_dim = embed_dim // (num_heads * num_scales)\n        self.max_seq_len = max_seq_len\n        self.compress_ratio = compress_ratio\n\n        # Adaptive attention span\n        self.span_proj = nn.Linear(embed_dim, num_heads * num_scales)\n\n        # Hierarchical attention projections\n        self.query_projs = nn.ModuleList([...])  # Similar to HierarchicalAdaptiveAttention\n        self.key_projs = nn.ModuleList([...])\n        self.value_projs = nn.ModuleList([...])\n\n        # Compressive memory\n        self.mem_short = []  # Short-term memory\n        self.mem_comp = []   # Compressive memory\n\n        # Sparse attention components\n        self.score_net = nn.Linear(embed_dim, 1)\n\n        # Normalization\n        self.dln = DynamicLayerNorm(embed_dim)\n\n    def forward(self, X):\n        # Adaptive attention span\n        spans = F.relu(self.span_proj(X))  # Compute attention spans\n\n        # Update memories\n        self.update_memory(X)\n\n        # Compute attention with adaptive spans and sparse selection\n        # Implement attention computations considering spans, sparse selection, and memory retrieval\n\n        # Apply dynamic layer normalization\n        Y = self.dln(output)\n\n        return Y\n\n    def update_memory(self, X):\n        # Update short-term memory and compress into compressive memory\n        pass\n```\n\n### Step-by-Step Instructions\n\n1. **Attention Span Computation**:\n\n   - For each input \\( X \\), compute adaptive attention spans \\( \\tau_{h,s} \\) using a linear projection followed by a ReLU activation.\n\n2. **Memory Update and Compression**:\n\n   - Store recent key-value pairs in short-term memory.\n   - Compute importance scores and compress less important pairs into compressive memory when the memory size exceeds a threshold.\n   - Use techniques like averaging or neural compression for compression.\n\n3. **Sparse Attention Computation**:\n\n   - For each query, compute importance scores using the score network.\n   - Apply a differentiable top-k operator to select the most relevant key-value pairs.\n   - Perform attention computation over the selected pairs.\n\n4. **Feedback Mechanism**:\n\n   - Feed the output of the GAU back into the next input, allowing the model to attend to its own latent representations.\n\n5. **Dynamic Layer Normalization**:\n\n   - Apply DLN after attention computations to ensure stable gradient flow.\n\n### Mathematical Formulas\n\n- **Adaptive Attention Weights**:\n\n  \\[\n  \\alpha_{ij} = \\frac{\\exp\\left( \\frac{ (Q_i K_j^T) }{ \\sqrt{d_k} } \\right)}{ \\sum_{k \\in \\text{Top}_k} \\exp\\left( \\frac{ (Q_i K_k^T) }{ \\sqrt{d_k} } \\right) }\n  \\]\n\n- **Compressive Memory Update**:\n\n  \\[\n  M_{\\text{comp}} = \\text{Compress}(M_{\\text{short}}, s_i, \\text{threshold})\n  \\]\n\n  where \\( \\text{Compress} \\) is a function that combines less important entries into compressed representations.\n\n- **Dynamic Layer Normalization**:\n\n  \\[\n  \\text{DLN}(Y) = \\gamma \\odot \\frac{Y - \\mu(Y)}{\\sqrt{\\sigma^2(Y) + \\epsilon}} + \\beta\n  \\]\n\n  with adaptive parameters \\( \\gamma \\) and \\( \\beta \\).\n\n## Conclusion\n\n**HieraCATT** proposes a new approach to handling long sequences efficiently by integrating hierarchical adaptive attention with adaptive attention scaling and compressive memory mechanisms. This design aims to:\n\n- **Improve Efficiency**: By reducing computational and memory overhead through adaptive attention spans, sparse selection, and memory compression.\n- **Enhance Performance**: By effectively capturing multi-scale dependencies and focusing computational resources where most needed.\n- **Ensure Scalability**: Allowing the model to handle longer sequences without linear increases in computational resources.\n- **Maintain Training Stability**: Through the use of dynamic layer normalization and careful integration of adaptive components.\n\nBy recombining the strengths of HieraNorm-AttnGPT and AdaptiveTTT, HieraCATT is expected to outperform current state-of-the-art models in terms of perplexity, accuracy on downstream tasks, robustness, and efficiency.\n\n## References\n\n1. **Sukhbaatar, S., & Fergus, R. (2019).** Adaptive Attention Span in Transformers. *Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics*, 331\u2013335.\n\n2. **Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020).** Memformer: A Memory-Augmented Transformer for Sequence Modeling. *AACL/IJCNLP*.\n\n3. **Rae, J. W., Potapenko, A., Jayakumar, S. M., Hillier, C., & Lillicrap, T. P. (2019).** Compressive Transformers for Long-Range Sequence Modelling. *arXiv preprint arXiv:1911.05507*.\n\n4. **Nawrot, P., La\u0144cucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024).** Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. *International Conference on Machine Learning*.\n\n5. **Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024).** Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. *arXiv preprint arXiv:2401.00001*.\n\n6. **Hwang, D., Wang, W., Huo, Z., Sim, K. C., & Mengibar, P. (2024).** TransformerFAM: Feedback Attention is Working Memory. *arXiv preprint arXiv:XXXX.XXXXX*.\n\n7. **Mohtashami, A., & Jaggi, M. (2023).** Landmark Attention: Random-Access Infinite Context Length for Transformers. *Neural Information Processing Systems*.\n\n8. **DuSell, B., & Chiang, D. (2023).** Stack Attention: Improving the Ability of Transformers to Model Hierarchical Patterns. *International Conference on Learning Representations*.\n\n9. **Graves, A. (2016).** Adaptive Computation Time for Recurrent Neural Networks. *arXiv preprint arXiv:1603.08983*.\n\n10. **Vaswani, A., et al. (2017).** Attention Is All You Need. *Advances in Neural Information Processing Systems*, 5998\u20136008.\n\n---",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "o1_preview",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "o1_preview",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "\"HieraCATT integrates hierarchical adaptive multi-head attention with adaptive attention scaling and compressive memory mechanisms to efficiently process long sequences with reduced computational and memory overhead, while maintaining high performance and scalability in language modeling tasks.\"",
    "ideation": null,
    "modelname": "hieracatt",
    "suggestions": null,
    "user_input": ""
}