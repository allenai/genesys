{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig = {\n    'd_model': 1536,\n    'n_block': 35\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 7\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig = {\n    'd_model': 2048,\n    'n_block': 35\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 17\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = AdaptiveAttention(embed_dim=embed_dim, block_loc=\n            block_loc, kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass AdaptiveAttention(GAUBase):\n    \"\"\"\n    AdaptiveAttention Module\n\n    This GAU implements an attention mechanism that integrates Gated Linear Attention (GLA)\n    with dynamic scaling based on input complexity, ensuring linear time and space complexity.\n\n    **Key Features:**\n    - **Gated Linear Attention**: Uses data-dependent gates to modulate queries and keys, enhancing expressiveness.\n    - **Dynamic Scaling**: Adjusts attention computation based on input complexity.\n    - **Linear Attention Computation**: Utilizes linear attention mechanisms for scalability.\n    - **Normalization**: Applies RMSNorm to stabilize computations.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the module.\n        dtype (torch.dtype, optional): Data type for parameters.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> attention = AdaptiveAttention(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = attention(X)\n\n    **References:**\n        - Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n        - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        assert embed_dim % num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_k = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.factory_kwargs)\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.complexity_estimator = nn.Sequential(nn.Linear(embed_dim, \n            embed_dim // 2, **self.factory_kwargs), nn.ReLU(), nn.Linear(\n            embed_dim // 2, 1, **self.factory_kwargs), nn.Sigmoid())\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        D_H = self.head_dim\n        complexity = self.complexity_estimator(X).mean(dim=1)\n        alpha = self.compute_scaling(complexity)\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = self.q_norm(Q)\n        K = self.k_norm(K)\n        G_Q = torch.sigmoid(self.gate_q(X))\n        G_K = torch.sigmoid(self.gate_k(X))\n        Q = Q * G_Q\n        K = K * G_K\n        Q = Q * alpha\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        KV = K_prime * V\n        KV_cumsum = torch.cumsum(KV, dim=2)\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        numerator = Q_prime * KV_cumsum\n        denominator = Q_prime * K_cumsum\n        epsilon = 1e-06\n        output = numerator / (denominator + epsilon)\n        output = output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.out_proj(output)\n        output = X + output\n        output, Z_ = self.norm(output, **Z)\n        return output, Z_\n\n    def compute_scaling(self, complexity):\n        alpha = 1.0 / (1.0 + complexity)\n        alpha = alpha.view(-1, 1, 1)\n        return alpha\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Key Features:**\n\n    - Efficient normalization without mean centering.\n    - Scales inputs to have unit variance along the last dimension.\n    - Supports different data types and devices.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of the same shape as **X**.\n\n    **Example:**\n\n        >>> rmsnorm = RMSNorm(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n        - Zhang, B., & Sennrich, R. (2019). \"Root Mean Square Layer Normalization\"\n          https://arxiv.org/abs/1910.07467\n\n    **Note:**\n        For more info on reStructuredText docstrings, see\n        [Sphinx Documentation](https://www.sphinx-doc.org/en/master/usage/restructuredtext/basics.html)\n        and [PEP 287](https://peps.python.org/pep-0287/).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(dim=-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y, Z\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-05}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 35\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}