{
    "variantname": null,
    "review": "The AdaptiveTTT proposal presents an innovative approach to enhancing TTTLinear by combining adaptive attention mechanisms with selective memory compression. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.5/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n- Well-defined objectives and motivation\n- Detailed architectural specifications\n- Clear implementation guidelines\n- Comprehensive theoretical analysis\n\nINNOVATION (4.2/5):\nThe proposal demonstrates significant innovation through:\n- Novel integration of adaptive attention scaling with test-time training\n- Unique approach to selective memory compression\n- Original hybrid update mechanism\nComparison with existing research shows this combination has not been previously explored in this context.\n\nFEASIBILITY (3.8/5):\nImplementation appears feasible but with some challenges:\n+ Clear implementation guidelines and pseudo-code provided\n+ Built on proven components from parent designs\n- Complex integration of adaptive mechanisms might require careful tuning\n- Stability guarantees need more theoretical foundation\n\nSCALABILITY (4.0/5):\nThe design shows promising scalability characteristics:\n+ Linear complexity maintained through selective compression\n+ Adaptive computation based on input complexity\n+ Efficient memory management through importance-based compression\n- Need for more detailed analysis of memory-computation trade-offs\n\nACCURACY AND ROBUSTNESS (4.2/5):\nExpected improvements in model performance:\n+ Enhanced ability to handle varying input complexities\n+ Improved adaptation through test-time training\n+ Better handling of long sequences through selective compression\n+ Adaptive attention patterns for diverse inputs\n\nEFFICIENCY (4.3/5):\nThe design offers several efficiency improvements:\n+ Reduced memory usage through selective compression\n+ Adaptive computation based on input complexity\n+ Hardware-efficient implementation considerations\n+ Efficient state updates through hybrid mechanism\n\nSTRENGTHS:\n1. Novel combination of adaptive attention and selective compression\n2. Well-thought-out memory management strategy\n3. Clear potential for improved efficiency and scalability\n4. Maintains test-time training capabilities while enhancing efficiency\n5. Strong theoretical foundation and justification\n\nCONCERNS:\n1. Stability guarantees for adaptive mechanisms need more development\n2. Memory-computation trade-offs require further analysis\n3. Integration complexity might affect practical implementation\n4. Impact on training stability needs more theoretical analysis",
    "search_stack": [
        {
            "ready": false,
            "query": "adaptive neural networks dynamic architecture test time training",
            "detail": "Analyze recent developments in adaptive neural networks and dynamic architectures, focusing on:\n1. Test-time adaptation mechanisms\n2. Dynamic parameter updates\n3. Efficient state management\n4. Integration with attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in adaptive neural networks and dynamic architectures, focusing on:\n1. Test-time adaptation mechanisms\n2. Dynamic parameter updates\n3. Efficient state management\n4. Integration with attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. ChordMixer: A Scalable Neural Attention Model for Sequences with Different Lengths (Avg. Score: 0.16)\n\n*Ruslan Khalitov, Tong Yu, Lei Cheng, Zhirong Yang*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** A simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths, and substantially outperforms other neural attention models.\n\n**Abstract:** Sequential data naturally have different lengths in many domains, with some very long sequences. As an important modeling tool, neural attention should capture long-range interaction in such sequences. However, most existing neural attention models admit only short sequences, or they have to employ chunking or padding to enforce a constant input length. Here we propose a simple neural network building block called ChordMixer which can model the attention for long sequences with variable lengths. Each ChordMixer block consists of a position-wise rotation layer without learnable parameters and an element-wise MLP layer. Repeatedly applying such blocks forms an effective network backbone that mixes the input signals towards the learning targets. We have tested ChordMixer on the synthetic adding problem, long document classification, and DNA sequence-based taxonomy classification. The experiment results show that our method substantially outperforms other neural attention models.\n\n##### *Relevant Chunk: No. 17/29 (Score: 0.16)*\n\n```\nIn Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations (ICLR), 2022. Jun He, Liqun Wang, Liu Liu, Jiao Feng, and Hao Wu. Long document classification from local word glimpses via recurrent attention learning. IEEE Access, 7:40707-40718, 2019. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Andrew Jaegle, Felix Axel Gimeno Gil, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning (ICML), 2021. Ruslan Khalitov, Tong Yu, Lei Cheng, and Zhirong Yang. Sparse factorization of square matrices with application to neural attention modeling. Neural Networks, 152:160-168, 2022. Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. arXiv, 2001.04451, 2020. Colin Lea, Rene Vidal, Austin Reiter, and Gregory D Hager. Temporal convolutional networks: A unified approach to action segmentation.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.07)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.07)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 3. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.05)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 26/46 (Score: 0.05)*\n\n```\nIn Aistats, volume 5, pages 246-252. Citeseer. Asier Mujika, Florian Meier, and Angelika Steger. 2017. Fast-slow recurrent neural networks. In $A d$ vances in Neural Information Processing Systems, pages 5915-5924. Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. 2012. Understanding the exploding gradient problem. CoRR, abs/1211.5063. Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettlemoyer. 2018. Deep contextualized word representations. arXiv preprint arXiv:1802.05365. Hieu Pham, Melody Y Guan, Barret Zoph, Quoc V Le, and Jeff Dean. 2018. Efficient neural architecture search via parameter sharing. arXiv preprint arXiv:1802.03268.\n```\n\n#### 4. Linear Transformers Are Secretly Fast Weight Programmers (Avg. Score: 0.03)\n\n*Imanol Schlag, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Machine Learning (2021)\t**Cited by** 151  (*Influential: 18*)\n\n**TL;DR:** This work infer a memory capacity limitation of recent linearised softmax attention variants, and replaces the purely additive outer products of self-invented activation patterns by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values.\n\n**Abstract:** We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early '90s, where a ``slow\"neural net learns by gradient descent to program the ``fast weights\"of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.\n\n##### *Relevant Chunk: No. 22/47 (Score: 0.03)*\n\n```\nHinton, G. E. and Plaut, D. C. Using fast weights to deblur old memories. In Proc. Conf. of Cognitive Science Society, pp. 177-186, Seatle, WA, USA, July 1987. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural computation, 9(8):1735-1780, 1997. Hopfield, J. J. Neural networks and physical systems with emergent collective computational abilities. Proc. of the national academy of sciences, 79(8):2554-2558, 1982. Irie, K., Zeyer, A., Schl\u00fcter, R., and Ney, H. Language modeling with deep Transformers. In Proc. Interspeech, pp. 3905-3909, Graz, Austria, September 2019a. Irie, K., Zeyer, A., Schl\u00fcter, R., and Ney, H. Training language models for long-span cross-sentence evaluation. In Proc. IEEE Automatic Speech Recog. and Understanding Workshop (ASRU), pp. 419-426, Sentosa, Singapore, December 2019b. Irie, K., Gerstenberger, A., Schl\u00fcter, R., and Ney, H. How much self-attention do we need? Trading attention for feed-forward layers. In Proc. IEEE Int. Conf. on Acoustics, Speech and Signal Processing (ICASSP), pp. 61546158, Virtual only, May 2020. Jia, X., De Brabandere, B., Tuytelaars, T., and Gool, L. V. Dynamic filter networks. In Proc. Advances in Neural Information Processing Systems (NIPS), pp. 667-675, Barcelona, Spain, 2016. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.02)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 28/47 (Score: 0.02)*\n\n```\nIn Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. In Proc. Association for Computational Linguistics (ACL), pp. 403-413, Virtual only, July 2020. Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work. Cognition, 35(2):183-204, 1990. Jerry A Fodor, Zenon W Pylyshyn, et al. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch\u00e4rli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. Preprint arXiv:2007.08970, 2020. Alex Graves. Adaptive computation time for recurrent neural networks. In Int. Conf. on Learning Representations (ICLR) Workshop Track, Vancouver, Canada, April 2016. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adri\u00e0 Puigdom\u00e8nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): $471-476,2016$. Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. Preprint arXiv:2012.05208, 2020. Stephen Jos\u00e9 Hanson. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena, 42 $(1-3): 265-272,1990$. Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proc. North American Chapter of the Association for Computational Linguistics on Human Language Technologies (NAACL-HLT), pp. 1118-1128, Minneapolis, USA, June 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, Las Vegas, NV, USA, June 2016. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, pp. $1735-1780,1997$. Dieuwke Hupkes, Anand Singh, Kris Korrel, German Kruszewski, and Elia Bruni. Learning compositionally through attentive guidance.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive neural networks dynamic architecture test time training\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 2. GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye*\n\n**TL;DR:** GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes, and can also mitigate the degree bias for either self-supervised or supervised GNNs.\n\n**Abstract:** Recent studies have shown that graph neural networks (GNNs) exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes. Existing works tackle this problem by deriving either designated GNN architectures or training strategies specifically for low-degree nodes. Though effective, these approaches unintentionally create an artificial out-of-distribution scenario, where models mainly or even only observe low-degree nodes during the training, leading to a downgraded performance for high-degree nodes that GNNs originally perform well at. In light of this, we propose a test-time augmentation framework, namely GraphPatcher, to enhance test-time generalization of any GNNs on low-degree nodes. Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs. Comprehensive experiments are conducted over seven benchmark datasets and GraphPatcher consistently enhances common GNNs' overall performance by up to 3.6% and low-degree performance by up to 6.5%, significantly outperforming state-of-the-art baselines. The source code is publicly available at https://github.com/jumxglhf/GraphPatcher.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 3. Are Defenses for Graph Neural Networks Robust?\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Felix Mujkanovic, Simon Geisler, Stephan Gunnemann, Aleksandar Bojchevski*\n\n**TL;DR:** A thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training, finds most defenses show no or only marginal improvement compared to an undefended baseline.\n\n**Abstract:** A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw - virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering - most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 41  (*Influential: 7*)\n\n#### 4. Adaptive Smoothing Gradient Learning for Spiking Neural Networks\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Ziming Wang, Runhao Jiang, S. Lian, Rui Yan, Huajin Tang*\n\n**TL;DR:** This work proposes a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise, in this way, the network learns adaptively the accurate gradients of loss landscape in SNNs.\n\n**Abstract:** Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is pro-hibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise. In this way, the network learns adaptively the accurate gradients of loss landscape in SNNs. The theoretical analysis further shows optimization on such a noisy network could be evolved into optimization on the embedded SNN with shared weights progressively. Moreover, The experiments on static images, dynamic event streams, speech, and instrumental sounds show the proposed method achieves state-of-the-art performance across all the datasets with remarkable robustness on different relaxation degrees.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 5. Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Hengguan Huang, Xiangming Gu, Hongya Wang, Chang Xiao, Hongfu Liu, Ye Wang*\n\n**TL;DR:** This paper proposes to formulate internal predictive modeling as a continuous-time Bayesian filtering problem within a stochastic dynamical system context and develops extrap-olative continuous-time Bayesian neural networks (ECBNN 2 ), which generalize existing Bayesian neural networks to represent temporal dynamics and allow us to extrapolate the distribution of model parameters before observing the incoming data, therefore effectively reducing the latency.\n\n**Abstract:** Human intelligence has shown remarkably lower latency and higher precision than most AI systems when processing non-stationary streaming data in real-time. Numerous neuroscience studies suggest that such abilities may be driven by internal predictive modeling. In this paper, we explore the possibility of introducing such a mechanism in unsupervised domain adaptation (UDA) for handling non-stationary streaming data for real-time streaming applications. We propose to formulate internal predictive modeling as a continuous-time Bayesian filtering problem within a stochastic dynamical system context. Such a dynamical system describes the dynamics of model parameters of a UDA model evolving with non-stationary streaming data. Building on such a dynamical system, we then develop extrap-olative continuous-time Bayesian neural networks (ECBNN 2 ), which generalize existing Bayesian neural networks to represent temporal dynamics and allow us to extrapolate the distribution of model parameters before observing the incoming data, therefore effectively reducing the latency. Remarkably, our empirical results show that ECBNN is capable of continuously generating better distributions of model parameters along the time axis given historical data only, thereby achieving (1) training-free test-time adaptation with low latency, (2) gradually improved alignment between the source and target features and (3) gradually improved model performance over time during the real-time testing stage.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. EvolveGCN: Evolving Graph Convolutional Networks for Dynamic Graphs\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Charles E. Leiserson, Tao B. Schardl, Jie Chen, Giacomo Domeniconi, Toyotaro Suzumura, Tim Kaler, Tengfei Ma, Hiroki Kanezashi, Aldo Pareja*\n\n**Abstract:** Graph representation learning resurges as a trending research subject owing to the widespread use of deep learning for Euclidean data, which inspire various creative designs of neural networks in the non-Euclidean domain, particularly graphs. With the success of these graph neural networks (GNN) in the static setting, we approach further practical scenarios where the graph dynamically evolves. Existing approaches typically resort to node embeddings and use a recurrent neural network (RNN, broadly speaking) to regulate the embeddings and learn the temporal dynamics. These methods require the knowledge of a node in the full time span (including both training and testing) and are less applicable to the frequent change of the node set. In some extreme scenarios, the node sets at different time steps may completely differ. To resolve this challenge, we propose EvolveGCN, which adapts the graph convolutional network (GCN) model along the temporal dimension without resorting to node embeddings. The proposed approach captures the dynamism of the graph sequence through using an RNN to evolve the GCN parameters. Two architectures are considered for the parameter evolution. We evaluate the proposed approach on tasks including link prediction, edge classification, and node classification. The experimental results indicate a generally higher performance of EvolveGCN compared with related approaches. The code is available at \\url{https://github.com/IBM/EvolveGCN}.\n\n**Published:** 2019-02-26\n\n\n\n#### 2. Communication Algorithms via Deep Learning\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Yihan Jiang, Sreeram Kannan, Sewoong Oh, Ranvir Rana, Pramod Viswanath, Hyeji Kim*\n\n**Abstract:** Coding theory is a central discipline underpinning wireline and wireless\nmodems that are the workhorses of the information age. Progress in coding\ntheory is largely driven by individual human ingenuity with sporadic\nbreakthroughs over the past century. In this paper we study whether it is\npossible to automate the discovery of decoding algorithms via deep learning. We\nstudy a family of sequential codes parameterized by recurrent neural network\n(RNN) architectures. We show that creatively designed and trained RNN\narchitectures can decode well known sequential codes such as the convolutional\nand turbo codes with close to optimal performance on the additive white\nGaussian noise (AWGN) channel, which itself is achieved by breakthrough\nalgorithms of our times (Viterbi and BCJR decoders, representing dynamic\nprograming and forward-backward algorithms). We show strong generalizations,\ni.e., we train at a specific signal to noise ratio and block length but test at\na wide range of these quantities, as well as robustness and adaptivity to\ndeviations from the AWGN setting.\n\n**Conference:** communication-algorithms-via-deep-learning-1\n\n**Published:** 2018-05-23\n\n\n\n#### 3. IG-RL: Inductive Graph Reinforcement Learning for Massive-Scale Traffic Signal Control\n\n*From Search Query: adaptive neural networks dynamic architecture test time training*\n\n*Denis Larocque, Fran\u00e7ois-Xavier Devailly, Laurent Charlin*\n\n**Abstract:** Scaling adaptive traffic-signal control involves dealing with combinatorial state and action spaces. Multi-agent reinforcement learning attempts to address this challenge by distributing control to specialized agents. However, specialization hinders generalization and transferability, and the computational graphs underlying neural-networks architectures -- dominating in the multi-agent setting -- do not offer the flexibility to handle an arbitrary number of entities which changes both between road networks, and over time as vehicles traverse the network. We introduce Inductive Graph Reinforcement Learning (IG-RL) based on graph-convolutional networks which adapts to the structure of any road network, to learn detailed representations of traffic-controllers and their surroundings. Our decentralized approach enables learning of a transferable-adaptive-traffic-signal-control policy. After being trained on an arbitrary set of road networks, our model can generalize to new road networks, traffic distributions, and traffic regimes, with no additional training and a constant number of parameters, enabling greater scalability compared to prior methods. Furthermore, our approach can exploit the granularity of available data by capturing the (dynamic) demand at both the lane and the vehicle levels. The proposed method is tested on both road networks and traffic settings never experienced during training. We compare IG-RL to multi-agent reinforcement learning and domain-specific baselines. In both synthetic road networks and in a larger experiment involving the control of the 3,971 traffic signals of Manhattan, we show that different instantiations of IG-RL outperform baselines.\n\n**Published:** 2020-03-06\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using adaptive neural networks and dynamic architectures, here are some key insights and recent developments that align with your research questions and goals:\n\n## Test-Time Adaptation Mechanisms\nTest-time adaptation is crucial for models to adjust to new data distributions without requiring labeled data. The AR-TTA method proposed in is particularly relevant. This method enhances the self-training framework by using a small memory buffer to maintain the knowledge of the source model and dynamically updates batch norm statistics based on the intensity of domain shift. This approach ensures stability and adaptability, which could be integrated into autoregressive language models to handle changing data distributions during test time.\n\n## Dynamic Parameter Updates\nDynamic parameter updates are essential for adaptive learning. The AR-TTA method mentioned in involves updating parameters based on the intensity of domain shift, which can be adapted for autoregressive models. Additionally, the concept of using memory buffers and mixup data augmentation can help in maintaining model stability while adapting to new data.\n\n## Efficient State Management\nBoth parent models (TTTLinear and FastTTTLinear) emphasize linear complexity through efficient state updates. The use of linear complexity in these models suggests that efficient state management can be achieved through mechanisms like the gated linear attention (GLA) in FastTTTLinear, which enhances expressiveness while maintaining linear complexity[Initial Analysis].\n\n## Integration with Attention Mechanisms\nThe integration of attention mechanisms is a key aspect. The FastTTTLinear model's use of Gated Linear Attention (GLA) provides a data-dependent gating mechanism that can be combined with the adaptive learning capabilities of TTTLinear. GLA enhances expressiveness and numerical stability, which are critical for efficient attention mechanisms in autoregressive models[Initial Analysis].\n\n### Additional Insights\n\n- **Spiking Neural Networks (SNNs):** While not directly applicable to traditional autoregressive models, SNNs like those described in use dynamic and adaptive mechanisms (e.g., integrate-and-fire neuron models, spike-train level backpropagation) that could inspire novel approaches to dynamic parameter updates and efficient state management in traditional neural networks.\n  \n- **Generalization and Out-of-Distribution Detection:** The study on generalization capacity and out-of-distribution detection in highlights the importance of designing models that can predict dynamics beyond the immediate training setting. This is relevant for ensuring that autoregressive models generalize well and maintain performance in unseen data distributions.\n\n### Combining Mechanisms\n\nTo effectively combine the strengths of TTT and GLA mechanisms:\n- **Adaptive Learning:** Incorporate TTT's adaptive learning capabilities to allow the model to adjust its parameters at test time, similar to the AR-TTA method.\n- **Efficient Attention:** Use GLA to enhance expressiveness and maintain linear complexity, ensuring efficient inference and numerical stability.\n- **Local and Global Context Processing:** Integrate local convolutional augmentation from FastTTTLinear with the long-context handling of TTTLinear to optimize the processing of both local and global contexts.\n\nBy integrating these mechanisms, the autoregressive language model can achieve better adaptability, efficiency, and performance across various data distributions.\n",
            "analysis": "Initial Analysis of Parent Designs and Research Direction\n\n1. Summary of Analysis:\n\nParent 1 (TTTLinear) key features:\n- Test-time training capability with expressive hidden states\n- Linear complexity through efficient state updates\n- Strong performance in long context\n- Matches/exceeds Transformer and Mamba baselines\n- Efficient inference with preliminary optimization\n\nParent 2 (FastTTTLinear) key features:\n- Integration of Gated Linear Attention (GLA)\n- Linear complexity with enhanced expressiveness\n- Data-dependent gating mechanisms\n- Local convolutional augmentation\n- Improved numerical stability\n\nInitial observations:\n- Both parents focus on linear complexity and long-context handling\n- They use different approaches for attention (TTT vs GLA)\n- There's potential to combine their strengths:\n  * TTT's adaptive learning\n  * GLA's efficient attention\n  * Local context processing\n  * Numerical stability improvements\n\n2. Key Research Questions:\n- How to effectively combine TTT and GLA mechanisms?\n- What are the latest advances in adaptive neural networks?\n- How to optimize the integration of local and global context?\n- What are recent developments in efficient attention mechanisms?\n\n3. Future Search Plan:\nRound 1: Investigate adaptive neural networks and dynamic architectures\nRound 2: Research efficient attention mechanisms and their combinations\nRound 3: Explore local-global context integration techniques\n\n4. References:\n1. Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n2. Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n3. Peng, B., et al. (2023). \"RWKV: Reinventing RNNs for the Transformer Era\""
        },
        {
            "ready": false,
            "query": "efficient memory mechanisms neural networks state representation",
            "detail": "Analyze recent developments in memory mechanisms and state representations in neural networks, focusing on:\n1. Efficient memory architectures\n2. State compression techniques\n3. Dynamic memory management\n4. Integration with attention mechanisms",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in memory mechanisms and state representations in neural networks, focusing on:\n1. Efficient memory architectures\n2. State compression techniques\n3. Dynamic memory management\n4. Integration with attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Memformer: A Memory-Augmented Transformer for Sequence Modeling (Avg. Score: 0.98)\n\n*Qingyang Wu, Zhenzhong Lan, Kun Qian, Jing Gu, A. Geramifard, Zhou Yu*\n\n**Published in:** AACL/IJCNLP (2020)\t**Cited by** 36  (*Influential: 2*)\n\n**TL;DR:** This work presents Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information, and proposes a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back- Propagation through time with a significantly reduced memory requirement.\n\n**Abstract:** Transformers have reached remarkable success in sequence modeling. However, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps.\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.99)*\n\n```\nHowever, these models have efficiency issues as they need to store all the history token-level representations as memory. We present Memformer, an efficient neural network for sequence modeling, that utilizes an external dynamic memory to encode and retrieve past information. Our model achieves linear time complexity and constant memory space complexity when processing long sequences. We also propose a new optimization scheme, memory replay back-propagation (MRBP), which promotes long-range back-propagation through time with a significantly reduced memory requirement. Experimental results show that Memformer has achieved comparable performance compared against the baselines by using 8.1 x less memory space and 3.2 x faster on inference. Analysis of the attention pattern shows that our external memory slots can encode and retain important information through timesteps. ## 1 Introduction\n\nMemory plays a fundamental role in human cognition. Humans perceive and encode sensory information into a compressed representation stored in neurons, and later we effectively retrieve the stored information to accomplish various tasks. The formation of memory involves complex cognitive processes. Modeling and studying the behavior of human memory is still a challenging research problem in many areas. Many researchers have attempted to incorporate memory systems in artificial neural networks. Early works like recurrent neural networks (RNN) (Rumelhart et al., 1988) including LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Chung et al., 2014) model temporal sequences with their internal compressed state vector as memory. However, they are limited in preserving the long-term information due to the memory bottleneck. To alleviate this limitation, more powerful memory network architectures such as Neural Turing Machine (NTM) (Graves et al., 2014), Differential Neural Computer (DNC) (Graves et al., 2016) have been proposed by leveraging a large external dynamic memory. Unfortunately, due to their complex memory interaction mechanism, they are not widely used for down-stream tasks at present. More recently, Vaswani et al. (2017) propose Transformer by discarding the use of recurrence and memory. Instead, it computes all the $\\mathcal{O}\\left(N^{2}\\right)$ paired dependencies in a sequence with selfattention (Bahdanau et al., 2015).\n```\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.98)*\n\n```\nTransformers have achieved great success in various natural language processing tasks. Nevertheless, the quadratic computation complexity can be costly. Some works try to address the limitations of selfattention, including Reformer, Sparse Transformer, Longformer, Linformer (Child et al., 2019; Kitaev et al., 2020; Wang et al., 2020), etc. They successfully reduce the complexity of self-attention and thus enable processing longer sequences. However, most of them still require linear memory space complexity. Transformer-XL (Dai et al., 2019) re-introduces the concept of memory and recurrence. It caches each layer's hidden states of self-attention into a fixed-size queue and re-uses them in the later attention computation. However, the memory as raw hidden states cannot effectively compress highlevel information. Thus, Transformer-XL in practice needs a massive memory size to perform well, and spends huge computation in using its memory. Compressive Transformer (Rae et al., 2020) improves upon Transformer-XL by further compressing its memories into fewer vectors via a compression network. However, as mentioned in the papers, both Transformer-XL and Compressive Transformer discard the information from the distant past, which causes a theoretical maximum tempo-\nral range given the fixed memory size. Inspired by the previous external memory networks, we propose Memformer, which incorporates a fixed-size external dynamic memory combined with the recent Transformer architecture. Memformer interacts with its external dynamic memory through the memory reading and writing modules. Also, we introduce a forgetting mechanism to improve the effectiveness of memorizing new information. By utilizing recurrence and a fixedsize memory, our model has a theoretically infinite temporal range of memorization and implies a linear computation complexity and constant memory space complexity. As the traditional backpropagation through time (BPTT) has an unaffordable memory cost in our model, we introduce a new optimization scheme, memory replay backpropagation (MRBP), to significantly reduce the memory cost in training recurrent neural networks with large size of memory representations. We evaluate Memformer on the autoregressive image generation and language modeling task. Experimental results show that Memformer performs on par with Transformer and Transformer XL with large memory size, while being much more efficient in terms of computation speed and memory space consumption. We also conduct an analysis showing that Memformer can retain information for an extended period. ## 2 Related Work\n\nThis section introduces some recent research directions that aim to alleviate the quadratic cost of selfattention. Moreover, we analyze their assumptions and limitations under the autoregressive setting to provide a broader view of these models. ### 2.1 Sparse Attention\n\nOne influential direction is to replace the full selfattention with sparse attention patterns to speed up the computation. Child et al. (2019) proposed Sparse Transformer, using a block sparse attention pattern to reduce the computation complexity to $\\mathcal{O}(N \\sqrt{N})$. Later, Longformer (Beltagy et al., 2020) and Big Bird (Zaheer et al., 2020) further explored this direction and proposed an even more sparse attention pattern to reduce the cost to $\\mathcal{O}(N)$. They introduced global tokens to encode the information from the entire sequence and kept the selfattention to the closest $k$ tokens and the global tokens to achieve linear complexity. Although linear sparse attention's theoretical soundness is proven for bidirectional encoders, it does not hold for the decoder. The main reason is that the global tokens cannot leak information to the future tokens in the autoregressive setting, where all the tokens can only see their previous tokens. Thus, linear sparse attention cannot guarantee a token to see its all past tokens. Only Sparse Transformer here with $\\mathcal{O}(N \\sqrt{N})$ complexity can theoretically cover all the past tokens for the sequence generation. ### 2.2 Linear Attention\n\nAnother direction is focusing on improving the softmax operation in the self-attention. Linformer (Wang et al., 2020) reduced the complexity to $\\mathcal{O}(N)$ by projecting the entire sequence to a constant size of keys and values, but this method has not been applied to autoregressive decoding. Performer (Choromanski et al., 2020) and Linear Transformer (Katharopoulos et al., 2020) used a linear dot-product of kernel feature maps to replace softmax. However, for Linear Transformer under the autoregressive setting, it needs to compute the cumulative summation to aggregate the history information. This assumption is too strong if the input sequence is long and the length is not fixed. After thousands of steps, the numerical values can become very large due to the summation, causing overflow and gradient instability. ### 2.3 Recurrence and Memory\n\nApplying recurrence and memory to Transformers is an orthogonal direction comparing to the efficient attention approaches. If the memory size is constant, recurrence enables the model to have constant memory complexity during inference. There are mainly two works exploring this direction. Transformer-XL (Dai et al., 2019) used relative positional encoding and consisted of a segmentlevel recurrence mechanism to encode beyond a fixed-length context. Compressive Transformer (Rae et al., 2020) extended from Transformer-XL by further compressing the previous cached hidden states to achieve a longer context. However, using past hidden states as memory would cause a theoretical maximum temporal range of context, meaning that a token is not guaranteed to see all the past tokens. Thus, in practice, Transformer-XL and Compressive Transformer need huge memory size to achieve good performance. ![](https://cdn.mathpix.com/cropped/2024_09_12_a5b0bfb29582fd0f4766g-03.jpg?height=915&width=717&top_left_y=225&top_left_x=224)\n\nFigure 1: Memformer overall architecture for the encoder (left) and decoder (right). Transformer encoder is responsible to interact with the memory. Sequence modeling is achieved by predicting the next segment conditioned to the current segment and memory. ### 2.3.1 Dynamic Memorization\n\nWithin the scope of memory networks, there are dynamic memorization techniques. Different from Transformer-XL which stores the token-level history representations as memory, dynamic memorization does not have a theoretical upper bound for the temporal range. Neural Turing Machine (NTM) (Graves et al., 2014) and Differential Neural Computer (DNC) (Graves et al., 2016) are two early models that can control external memory resources to achieve long-lasting memory.\n```\n\n#### 2. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.92)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.92)*\n\n```\narXiv preprint arXiv:1910.06611, 2019. Imanol Schlag, Tsendsuren Munkhdalai, and J\u00fcrgen Schmidhuber. Learning associative inference using fast weight memory. arXiv preprint arXiv:2011.07831, 2020. Imanol Schlag, Kazuki Irie, and J\u00fcrgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International Conference on Machine Learning, pp. 9355-9366. PMLR, 2021. J\u00fcrgen Schmidhuber. Learning to control fast-weight memories: An alternative to dynamic recurrent networks. Neural Computation, 4(1):131-139, 1992. Noam Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear memory cost. In International Conference on Machine Learning, pp. 4596-4604. PMLR, 2018. Zhuoran Shen, Mingyuan Zhang, Haiyu Zhao, Shuai Yi, and Hongsheng Li. Efficient attention: Attention with linear complexities. arXiv preprint arXiv:1812.01243, 2018. Paul Smolensky. Tensor product variable binding and the representation of symbolic structures in connectionist systems. Artificial intelligence, 46(1-2):159-216, 1990. Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems, 28, 2015. Sainbayar Sukhbaatar, Da Ju, Spencer Poff, Stephen Roller, Arthur Szlam, Jason Weston, and Angela Fan. Not all memories are created equal: Learning to forget by expiring. In International Conference on Machine Learning, pp. 9902-9912. PMLR, 2021. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, $30,2017$. Paul J Werbos. Generalization of backpropagation with application to a recurrent gas market model. Neural networks, 1(4):339-356, 1988. Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.\n```\n\n#### 3. Recurrent Memory Transformer (Avg. Score: 0.85)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.85)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.64)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 31/54 (Score: 0.64)*\n\n```\n## A. 2 Memory in Machine Learning\n\nMemory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention [70] and dilated convolutions [5], are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints [6, 15, 18, 42, 59, 60, 64, 74]. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models. Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients [56]), particular care must be paid to endow them with longer memory. The ubiquitous LSTM 34 and simplifications such as the GRU [17] control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies [31. Tallec and Ollivier [66] show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection [32] or non-saturating gates [9], which can suffer from instability issues.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient memory mechanisms neural networks state representation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. ED-Batch: Efficient Automatic Batching of Dynamic Neural Networks via Learned Finite State Machines\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Siyuan Chen, Pratik Fegade, Tianqi Chen, Phillip B. Gibbons, T. Mowry*\n\n**TL;DR:** This paper provides an approach for batching dynamic DNNs based on finite state machines, which enables the automatic discovery of batching policies specialized for each DNN via reinforcement learning and finds that memory planning that is aware of the batching policy can save significant data movement overheads.\n\n**Abstract:** Batching has a fundamental influence on the efficiency of deep neural network (DNN) execution. However, for dynamic DNNs, efficient batching is particularly challenging as the dataflow graph varies per input instance. As a result, state-of-the-art frameworks use heuristics that result in suboptimal batching decisions. Further, batching puts strict restrictions on memory adjacency and can lead to high data movement costs. In this paper, we provide an approach for batching dynamic DNNs based on finite state machines, which enables the automatic discovery of batching policies specialized for each DNN via reinforcement learning. Moreover, we find that memory planning that is aware of the batching policy can save significant data movement overheads, which is automated by a PQ tree-based algorithm we introduce. Experimental results show that our framework speeds up state-of-the-art frameworks by on average 1.15x, 1.39x, and 2.45x for chain-based, tree-based, and lattice-based DNNs across CPU and GPU.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Scalable and Efficient Training of Large Convolutional Neural Networks with Differential Privacy\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Zhiqi Bu, J. Mao, Shiyun Xu*\n\n**TL;DR:** An efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, is proposed that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy.\n\n**Abstract:** Large convolutional neural networks (CNN) can be difficult to train in the differentially private (DP) regime, since the optimization algorithms require a computationally expensive operation, known as the per-sample gradient clipping. We propose an efficient and scalable implementation of this clipping on convolutional layers, termed as the mixed ghost clipping, that significantly eases the private training in terms of both time and space complexities, without affecting the accuracy. The improvement in efficiency is rigorously studied through the first complexity analysis for the mixed ghost clipping and existing DP training algorithms. Extensive experiments on vision classification tasks, with large ResNet, VGG, and Vision Transformers, demonstrate that DP training with mixed ghost clipping adds $1\\sim 10\\%$ memory overhead and $<2\\times$ slowdown to the standard non-private training. Specifically, when training VGG19 on CIFAR10, the mixed ghost clipping is $3\\times$ faster than state-of-the-art Opacus library with $18\\times$ larger maximum batch size. To emphasize the significance of efficient DP training on convolutional layers, we achieve 96.7\\% accuracy on CIFAR10 and 83.0\\% on CIFAR100 at $\\epsilon=1$ using BEiT, while the previous best results are 94.8\\% and 67.4\\%, respectively. We open-source a privacy engine (\\url{https://github.com/woodyx218/private_vision}) that implements DP training of CNN with a few lines of code.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 36  (*Influential: 3*)\n\n#### 3. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 4. Geodesic Graph Neural Network for Efficient Graph Representation Learning\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Lecheng Kong, Yixin Chen, Muhan Zhang*\n\n**TL;DR:** An efficient GNN framework called Geodesic GNN (GDGNN) is proposed that requires only one GNN run and injects conditional relationships between nodes into the model without labeling and theoretically proves that GDGNN is more powerful than plain GNNs.\n\n**Abstract:** Graph Neural Networks (GNNs) have recently been applied to graph learning tasks and achieved state-of-the-art (SOTA) results. However, many competitive methods run GNNs multiple times with subgraph extraction and customized labeling to capture information that is hard for normal GNNs to learn. Such operations are time-consuming and do not scale to large graphs. In this paper, we propose an efficient GNN framework called Geodesic GNN (GDGNN) that requires only one GNN run and injects conditional relationships between nodes into the model without labeling. This strategy effectively reduces the runtime of subgraph methods. Specifically, we view the shortest paths between two nodes as the spatial graph context of the neighborhood around them. The GNN embeddings of nodes on the shortest paths are used to generate geodesic representations. Conditioned on the geodesic representations, GDGNN can generate node, link, and graph representations that carry much richer structural information than plain GNNs. We theoretically prove that GDGNN is more powerful than plain GNNs. We present experimental results to show that GDGNN achieves highly competitive performance with SOTA GNN models on various graph learning tasks while taking significantly less time.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 22  (*Influential: 1*)\n\n#### 5. Legendre Memory Units: Continuous-Time Representation in Recurrent Neural Networks\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Aaron R. Voelker, Ivana Kaji\u0107, C. Eliasmith*\n\n**TL;DR:** Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales and exceed state-of-the-art performance among RNNs on permuted sequential MNIST.\n\n**Abstract:** We propose a novel memory cell for recurrent neural networks that dynamically maintains information across long windows of time using relatively few resources. The Legendre Memory Unit~(LMU) is mathematically derived to orthogonalize its continuous-time history -- doing so by solving $d$ coupled ordinary differential equations~(ODEs), whose phase space linearly maps onto sliding windows of time via the Legendre polynomials up to degree $d - 1$. Backpropagation across LMUs outperforms equivalently-sized LSTMs on a chaotic time-series prediction task, improves memory capacity by two orders of magnitude, and significantly reduces training and inference times. LMUs can efficiently handle temporal dependencies spanning $100\\text{,}000$ time-steps, converge rapidly, and use few internal state-variables to learn complex functions spanning long windows of time -- exceeding state-of-the-art performance among RNNs on permuted sequential MNIST. These results are due to the network's disposition to learn scale-invariant features independently of step size. Backpropagation through the ODE solver allows each layer to adapt its internal time-step, enabling the network to learn task-relevant time-scales. We demonstrate that LMU memory cells can be implemented using $m$ recurrently-connected Poisson spiking neurons, $\\mathcal{O}( m )$ time and memory, with error scaling as $\\mathcal{O}( d / \\sqrt{m} )$. We discuss implementations of LMUs on analog and digital neuromorphic hardware.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 157  (*Influential: 25*)\n\n### 3 related papers from Papers with Code\n\n#### 1. GhostFaceNets: Lightweight Face Recognition Model From Cheap Operations\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Naoufel Werghi, Yahya Zweiri, Abdulhadi Shoufan, Sajid Javed, Oussama Abdul Hay, Mohamad Alansari*\n\n**Abstract:** The development of deep learning-based biometric models that can be deployed on devices with constrained memory and computational resources has proven to be a significant challenge. Previous approaches to this problem have not prioritized the reduction of feature map redundancy, but the introduction of Ghost modules represents a major innovation in this area. Ghost modules use a series of inexpensive linear transformations to extract additional feature maps from a set of intrinsic features, allowing for a more comprehensive representation of the underlying information. GhostNetV1 and GhostNetV2, both of which are based on Ghost modules, serve as the foundation for a group of lightweight face recognition models called GhostFaceNets. GhostNetV2 expands upon the original GhostNetV1 by adding an attention mechanism to capture long-range dependencies. Evaluation of GhostFaceNets using various benchmarks reveals that these models offer superior performance while requiring a computational complexity of approximately 60\u2013275 MFLOPs. This is significantly lower than that of State-Of-The-Art (SOTA) big convolutional neural network (CNN) models, which can require hundreds of millions of FLOPs. GhostFaceNets trained with the ArcFace loss on the refined MS-Celeb-1M dataset demonstrate SOTA performance on all benchmarks. In comparison to previous SOTA mobile CNNs, GhostFaceNets greatly improve efficiency for face verification tasks. The GhostFaceNets code is available at: https://github.com/HamadYA/GhostFaceNets .\n\n**Proceeding:** ieee-access-2023-4\n\n**Published:** 2023-04-10\n\n\n\n#### 2. Brain-inspired replay for continual learning with artificial neural networks\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Hava T. Siegelmann & Andreas S. Tolias, Gido M. van de Ven*\n\n**Abstract:** Artificial neural networks suffer from catastrophic forgetting. Unlike humans, when these networks are trained on something new, they rapidly forget what was learned before. In the brain, a mechanism thought to be important for protecting memories is the reactivation of neuronal activity patterns representing those memories. In artificial neural networks, such memory replay can be implemented as \u2018generative replay\u2019, which can successfully \u2013 and surprisingly efficiently \u2013 prevent catastrophic forgetting on toy examples even in a class-incremental learning scenario. However, scaling up generative replay to complicated problems with many tasks or complex inputs is challenging. We propose a new, brain-inspired variant of replay in which internal or hidden representations are replayed that are generated by the network\u2019s own, context-modulated feedback connections. Our method achieves state-of-the-art performance on challenging continual learning benchmarks (e.g., class-incremental learning on CIFAR-100) without storing data, and it provides a novel model for replay in the brain.\n\n**Published:** 2020-08-13\n\n\n\n#### 3. Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching\n\n*From Search Query: efficient memory mechanisms neural networks state representation*\n\n*Xiaopeng Hong, Haoyu Chen, Wei Peng, Guoying Zhao*\n\n**Abstract:** Human action recognition from skeleton data, fueled by the Graph Convolutional Network (GCN), has attracted lots of attention, due to its powerful capability of modeling non-Euclidean structure data. However, many existing GCN methods provide a pre-defined graph and fix it through the entire network, which can loss implicit joint correlations. Besides, the mainstream spectral GCN is approximated by one-order hop, thus higher-order connections are not well involved. Therefore, huge efforts are required to explore a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for skeleton-based action recognition. Specifically, we enrich the search space by providing multiple dynamic graph modules after fully exploring the spatial-temporal correlations between nodes. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a sampling- and memory-efficient evolution strategy is proposed to search an optimal architecture for this task. The resulted architecture proves the effectiveness of the higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, which is barely discussed before. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scaled datasets and the results show that our model gets the state-of-the-art results.\n\n**Published:** 2019-11-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on efficient memory mechanisms, state representation, and integration with attention mechanisms, here are some key insights and developments from the provided sources and related concepts:\n\n## Efficient Memory Architectures\n\n### Memory-Augmented Architectures\nMemory-augmented architectures, such as those using recurrent neural networks (RNNs) and transformers, are crucial for managing short-term dependencies and long-term context. For example, Long Short-Term Memory (LSTM) networks incorporate memory cells and gates to maintain long-term dependencies, which is essential for tasks requiring the retention of information over extended sequences.\n\n### Memristor-Based Accelerators\nThe use of memristor-based in-memory computing can significantly enhance the efficiency of matrix operations, which are fundamental in neural networks. This approach integrates computation into memory, reducing latency and energy consumption. For instance, a memristor-based accelerator for transformer self-attention can perform matrix operations in an analog manner, leading to improved efficiency and accuracy.\n\n## State Compression Techniques\n\n### Hebbian Learning and Unlearning\nThe concept of Hebbian learning and unlearning can be adapted to optimize state representation. By initializing synaptic connections through Hebbian learning and then refining them through anti-Hebbian unlearning, the network can memorize and retrieve patterns more efficiently. This approach can be particularly useful in recurrent neural networks for tasks like classification, memorization, and generation.\n\n### Hierarchical Learning\nDeep neural networks (DNNs) employ hierarchical learning, where lower layers capture basic features and higher layers interpret complex patterns. This hierarchical structure allows for efficient representation learning, mirroring the brain's organization. This can help in compressing and representing states in a more efficient and meaningful way.\n\n## Dynamic Memory Management\n\n### Elastic Weight Consolidation\nDynamic memory management can be improved using techniques like elastic weight consolidation (EWC). EWC penalizes large changes to parameters that were important for previous tasks, helping to mitigate catastrophic forgetting when training on sequential tasks. This method can be integrated into autoregressive models to ensure that the model retains knowledge from previous tasks while adapting to new ones.\n\n### Associative Memory Models\nAssociative memory models, which use recurrent neural networks evolving through deterministic dynamics, can optimize connections to store patterns in finite-sized basins of attraction. This approach can help in dynamically managing memory by ensuring that the network can retrieve and update memories efficiently.\n\n## Integration with Attention Mechanisms\n\n### Transformer Self-Attention\nThe integration of efficient memory mechanisms with attention mechanisms is crucial. Transformer models, which rely heavily on self-attention, can benefit from memristor-based accelerators to speed up the computation of attention weights. This integration can enhance both the efficiency and accuracy of the model.\n\n### Hybrid Attention Approaches\nCombining different attention mechanisms, such as those used in TTTLinear and FastTTTLinear, can improve the stability and adaptability of the model. For example, using a combination of local and global processing with adaptive learning can help in managing state representations more effectively and improving numerical stability[Analysis Note - Round 1].\n\nIn summary, to improve autoregressive language model design, researchers can explore:\n\n- **Memory-Augmented Architectures**: Utilize LSTMs, memory networks, and other architectures that efficiently manage short-term and long-term dependencies.\n- **Memristor-Based Accelerators**: Leverage in-memory computing to enhance the efficiency of matrix operations.\n- **Hebbian Learning and Unlearning**: Optimize state representation through Hebbian learning and unlearning.\n- **Elastic Weight Consolidation**: Implement EWC to mitigate catastrophic forgetting.\n- **Associative Memory Models**: Use deterministic dynamics in recurrent neural networks to optimize memory storage and retrieval.\n- **Hybrid Attention Mechanisms**: Combine different attention approaches to improve stability and adaptability.\n\nThese strategies can help in achieving efficient memory mechanisms, effective state representation, and robust integration with attention mechanisms, ultimately enhancing the performance of autoregressive language models.\n",
            "analysis": "Analysis Note - Round 1\n\n1. Summary of Analysis:\n\nThe initial search results reveal several promising directions for combining TTTLinear and FastTTTLinear's strengths while addressing their limitations:\n\na) Test-Time Adaptation:\n- Both parents use different approaches to test-time adaptation\n- Recent work shows promise in using memory buffers and dynamic updates\n- Need to explore how to combine TTT's adaptive learning with GLA's efficiency\n\nb) Dynamic Architecture:\n- Potential in combining local and global processing\n- Need to investigate efficient state management techniques\n- Opportunity to improve numerical stability\n\n2. Useful References with Excerpts:\n\nFrom \"Test-Time Training Can Close the Natural Distribution Shift Performance Gap\":\n\"...relies on self-supervision during training paired with test-time training at inference... essentially closes the distribution shift performance gap\"\n- Relevant for improving TTT's adaptation capabilities\n\nFrom \"Extrapolative Continuous-time Bayesian Neural Network\":\n\"...formulate internal predictive modeling as a continuous-time Bayesian filtering problem... allows us to extrapolate the distribution of model parameters before observing the incoming data\"\n- Potential approach for improving state management\n\nFrom \"Adaptive Smoothing Gradient Learning\":\n\"...fusing the learnable relaxation degree into the network... learns adaptively the accurate gradients\"\n- Insights for improving numerical stability\n\n3. Key Insights:\n\na) State Management:\n- Need for better memory mechanisms\n- Potential in combining discrete and continuous state representations\n- Opportunity to improve efficiency through predictive modeling\n\nb) Attention Mechanisms:\n- GLA shows promise but needs improvement in stability\n- Potential in combining with TTT's adaptive capabilities\n- Need to explore hybrid attention approaches\n\nc) Numerical Stability:\n- Current approaches have limitations\n- Opportunity to improve through adaptive smoothing\n- Need to investigate better initialization and normalization techniques\n\n4. Future Search Plan:\n\nRound 2: Focus on:\n- Memory mechanisms and state representations\n- Efficient attention mechanisms\n- Query: \"efficient memory mechanisms neural networks state representation\"\n\nRound 3: Investigate:\n- Numerical stability techniques\n- Hybrid attention mechanisms\n- Query: \"hybrid attention mechanisms stability optimization\"\n\n5. References:\n\n1. Darestani, M. Z., Liu, J., & Heckel, R. (2022). Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing. International Conference on Machine Learning.\n\n2. Huang, H., Gu, X., Wang, H., Xiao, C., Liu, H., & Wang, Y. (2022). Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation. Neural Information Processing Systems.\n\n3. Wang, Z., Jiang, R., Lian, S., Yan, R., & Tang, H. (2023). Adaptive Smoothing Gradient Learning for Spiking Neural Networks. International Conference on Machine Learning.\n\n4. Ju, M., Zhao, T., Yu, W., Shah, N., & Ye, Y. (2023). GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation. Neural Information Processing Systems.\n\n5. Mujkanovic, F., Geisler, S., Gunnemann, S., & Bojchevski, A. (2023). Are Defenses for Graph Neural Networks Robust? Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "hybrid attention mechanisms compression efficiency",
            "detail": "Analyze recent developments in hybrid attention mechanisms and memory compression techniques, focusing on:\n1. Efficient attention architectures\n2. Memory compression methods\n3. Integration of different attention types\n4. Performance-efficiency trade-offs",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in hybrid attention mechanisms and memory compression techniques, focusing on:\n1. Efficient attention architectures\n2. Memory compression methods\n3. Integration of different attention types\n4. Performance-efficiency trade-offs\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.63)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 2/38 (Score: 0.63)*\n\n```\nExisting methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence length. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9 \\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1 \\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9 \\%-36 \\%$ to within $5 \\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4 \\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance. ## 1 Introduction\n\nLarge Language Models (LLMs) exhibit remarkable versatility across numerous applications [6, 57, 63]. Central to LLM is the attention mechanism [62], which computes interactions among tokens within a certain span, thereby enabling context understanding. Scaling input length is crucial for enhancing LLM capabilities [7, 60], including fact retrieval, summarization, few-shot learning, question answering and so on [4, 70]. However, the ever-growing attention computation and KeyValue Cache (KV-Cache) pose significant efficiency challenges [54, 69, 26, 33]. Previous work proposes sparse attention methods to address the efficiency challenges of long contexts in generative LLMs. These methods typically employ a uniform, fixed-span sliding window mask across all heads and input lengths, limiting attention to local contexts only [69, 26]. This approach allows the LLM to take long inputs with a fixed attention span, keeping bounded attention computation and KV caching overhead. Following previous works [7, 60], we quantify the effective context length\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_55b306f08bc2c02b2c4bg-02.jpg?height=402&width=1397&top_left_y=208&top_left_x=359)\n\nFigure 1: Retrieval accuracy of the Vicuna-7B model using different attention methods across varying input lengths and retrieval positions on the LongEval dataset. This \"Needle In a Haystack\" experiment [19] takes massive key-value pairs as inputs and tests the accuracy to retrieve values based on given keys from diverse positions. (a) Original model with a full attention span; (b) StreamingLLM with half the attention span, showing reduced effectiveness beyond the span; (c) MoA with half the attention span, maintaining effectiveness beyond the span. as the maximum input length where content retrieval accuracy exceeds a $90 \\%$ threshold. In principle, fixed-span local attention can gradually aggregate global information through multiple model layers, yielding a longer effective context length than each attention span [18]. Nonetheless, we reveal that uniform masks, like StreamingLLM [69], hardly extend effective context length beyond the span, as shown in Figure 6. Figure 1(b) further illustrates such limitation: with a $50 \\%$ attention span mask, StreamingLLM fails to accurately retrieve content from the earlier half of the input and performs even worse at longer input lengths. Figure 2 reveals one possible explanation for the problem: while some attention heads focus on local contexts, others encompass the broad span of the entire input sequence. Consequently, the uniform approach fails to achieve a long effective context length as it limits the attention span of the global-context heads, while excessively allocates compute and memory budget for local-context heads. Additionally, as the input length increases, some attention heads need a faster increase in attention span than others to avoid serious performance degradation, as shown in Table 1 Unfortunately, the uniform approaches do not include heterogeneous rules to scale the attention spans differently for various heads. Besides, existing model compression methods [44, 40, 68, 36, 31, 37] use general language modeling corpora to decide the compression plan, which cannot accurately profile the influence of compression on long-context tasks. In this work, we propose Mixture of Attention (MoA), a training-free sparse attention method. As illustrated in Figure 3 . MoA constructs the search space of heterogeneous elastic rules of attention spans. For automatic LLM compression, MoA first utilizes gradient-based profiling to inspect the influences of each attention position on the prediction loss. Based on the profiling results, MoA tailors heterogeneous sparse attention configurations for each model layer and attention head. During profiling, MoA employs a calibration dataset with long-range dependencies and uses the original dense model's response instead of the human-written response as the reference to calculate the loss.\n```\n\n#### 2. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.17)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.17)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.14)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.14)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. The Devil in Linear Transformer  (Avg. Score: 0.10)\n\n*Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, Yiran Zhong*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2022)\t**Cited by** 39  (*Influential: 3*)\n\n**TL;DR:** This paper proposes a new linear attention that replaces the scaling of attention matrices with a normalization to stabilize gradients, and proposes a diagonal attention to confine attention to only neighbouring tokens in early layers.\n\n**Abstract:** Linear transformers aim to reduce the quadratic space-time complexity of vanilla transformers. However, they usually suffer from degraded performances on various tasks and corpus. In this paper, we examine existing kernel-based linear transformers and identify two key issues that lead to such performance gaps: 1) unbounded gradients in the attention computation adversely impact the convergence of linear transformer models; 2) attention dilution which trivially distributes attention scores over long sequences while neglecting neighbouring structures. To address these issues, we first identify that the scaling of attention matrices is the devil in unbounded gradients, which turns out unnecessary in linear attention as we show theoretically and empirically. To this end, we propose a new linear attention that replaces the scaling operation with a normalization to stabilize gradients. For the issue of attention dilution, we leverage a diagonal attention to confine attention to only neighbouring tokens in early layers. Benefiting from the stable gradients and improved attention, our new linear transformer model, transNormer, demonstrates superior performance on text classification and language modeling tasks, as well as on the challenging Long-Range Arena benchmark, surpassing vanilla transformer and existing linear variants by a clear margin while being significantly more space-time efficient. The code is available at https://github.com/OpenNLPLab/Transnormer .\n\n##### *Relevant Chunk: No. 4/25 (Score: 0.10)*\n\n```\nThe overall architecture is shown in Fig. 3. ### 4.1 The overall architecture\n\nVanilla attention suffers less in attention dilution while linear attention is more efficient and scalable on longer sequences. This motivate us to design a method that exploits the best of the both worlds by using these mechanisms in combined. Specifically, our network consists of two types of attention: DiagAttention for the early stage of the model and NORMATTENTION for the later stage. The former addresses the attention dilution issue and the later aims to stabilize training gradients. Note that by properly reshaping the inputs, the diagonal attention can be efficiently computed in linear space-time, thus preserving the overall linear complexity. ### 4.2 NormAtTEntion\n\nTable 1: Ablation of linear attention with scaling operation.\n```\n\n#### 5. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.10)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.10)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid attention mechanisms compression efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning\n\n*From Search Query: hybrid attention mechanisms compression efficiency*\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hybrid attention mechanisms compression efficiency*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Improving Natural Language Processing Tasks with Human Gaze-Guided Neural Attention\n\n*From Search Query: hybrid attention mechanisms compression efficiency*\n\n*Ekta Sood, Simon Tannert, Philipp Mueller, A. Bulling*\n\n**TL;DR:** This work proposes a novel hybrid text saliency model that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Abstract:** A lack of corpora has so far limited advances in integrating human gaze data as a supervisory signal in neural attention mechanisms for natural language processing(NLP). We propose a novel hybrid text saliency model(TSM) that, for the first time, combines a cognitive model of reading with explicit human gaze supervision in a single machine learning framework. On four different corpora we demonstrate that our hybrid TSM duration predictions are highly correlated with human gaze ground truth. We further propose a novel joint modeling approach to integrate TSM predictions into the attention layer of a network designed for a specific upstream NLP task without the need for any task-specific human gaze data. We demonstrate that our joint model outperforms the state of the art in paraphrase generation on the Quora Question Pairs corpus by more than 10% in BLEU-4 and achieves state of the art performance for sentence compression on the challenging Google Sentence Compression corpus. As such, our work introduces a practical approach for bridging between data-driven and cognitive models and demonstrates a new way to integrate human gaze-guided neural attention into NLP tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 65  (*Influential: 4*)\n\n#### 4. HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction\n\n*From Search Query: hybrid attention mechanisms compression efficiency*\n\n*Zalan Fabian, M. Soltanolkotabi*\n\n**TL;DR:** HUMUS-Net is proposed, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network that establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset.\n\n**Abstract:** In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a large number of patches to preserve fine detail information, both of which are typical in low-level vision problems such as MRI reconstruction, having a compounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network. HUMUS-Net extracts high-resolution features via convolutional blocks and refines low-resolution features via a novel Transformer-based multi-scale feature extractor. Features from both levels are then synthesized into a high-resolution output reconstruction. Our network establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two other popular MRI datasets and perform fine-grained ablation studies to validate our design.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 3*)\n\n#### 5. Efficient Modulation for Vision Networks\n\n*From Search Query: hybrid attention mechanisms compression efficiency*\n\n*Xu Ma, Xiyang Dai, Jianwei Yang, Bin Xiao, Yinpeng Chen, Yun Fu, Lu Yuan*\n\n**TL;DR:** This work revisits the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block, and proposes the efficient modulation (EfficientMod) block, which is considered the essential building block for networks.\n\n**Abstract:** In this work, we present efficient modulation, a novel design for efficient vision networks. We revisit the modulation mechanism, which operates input through convolutional context modeling and feature projection layers, and fuses features via element-wise multiplication and an MLP block. We demonstrate that the modulation mechanism is particularly well suited for efficient networks and further tailor the modulation design by proposing the efficient modulation (EfficientMod) block, which is considered the essential building block for our networks. Benefiting from the prominent representational ability of modulation mechanism and the proposed efficient design, our network can accomplish better trade-offs between accuracy and efficiency and set new state-of-the-art performance in the zoo of efficient networks. When integrating EfficientMod with the vanilla self-attention block, we obtain the hybrid architecture which further improves the performance without loss of efficiency. We carry out comprehensive experiments to verify EfficientMod's performance. With fewer parameters, our EfficientMod-s performs 0.6 top-1 accuracy better than EfficientFormerV2-s2 and is 25% faster on GPU, and 2.9 better than MobileViTv2-1.0 at the same GPU latency. Additionally, our method presents a notable improvement in downstream tasks, outperforming EfficientFormerV2-s by 3.6 mIoU on the ADE20K benchmark. Code and checkpoints are available at https://github.com/ma-xu/EfficientMod.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 1*)\n\n### 1 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: hybrid attention mechanisms compression efficiency*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on hybrid attention mechanisms, memory compression techniques, and the integration of different attention types, here are some key insights and recent developments:\n\n## Efficient Attention Architectures\n\n### Hybrid CTC-Attention Decoder\nThe MPSA-Conformer-CTC/Attention model integrates Connectionist Temporal Classification (CTC) with Attention mechanisms. This hybrid approach leverages the advantages of automatic alignment from CTC and attention weight extraction, enhancing accuracy and efficiency. It uses a Conformer encoder and jointly decodes speech features through CTC and Attention, which can be adapted for text generation tasks to improve robustness and efficiency.\n\n### Dynamic Sparse Attention\nThe CASAK-V approach introduces a dynamic sparse attention mechanism that adapts to different contexts. This method uses a meta-learning framework to fine-tune a compact pre-trained transformer for sparse pattern identification from per-layer attention scores. It dynamically generates and applies head-specific sparse attention patterns, which helps in reducing memory usage and maintaining performance.\n\n### Multi-Grained Self-Injection\nContext Window Extension with Multi-Grained Self-Injection involves using specialized attention patterns and context compression techniques. This approach includes methods like positional encoding rescaling, positional interpolation, and chunk-wise compression of past tokens to reduce memory usage and improve inference speed. However, it may require careful task design and pretraining to stabilize the fine-tuning process.\n\n## Memory Compression Methods\n\n### Adaptive KV-Cache Compression\nCASAK-V also implements adaptive chunk-wise KV-cache compression based on layer-wise sparse attention configurations. This dynamic compression adjusts during token generation, using an attention map reconstruction heuristic, which reduces memory usage by 40% while maintaining near-linear runtime complexity.\n\n### Memory Compression through Polynomial Projections\nThe HiPPO framework introduces online compression of continuous signals and discrete time series by projection onto polynomial bases. This method can be applied to compress states in autoregressive models, achieving efficient state compression with bounded memory requirements (referenced in the analysis note).\n\n### External Dynamic Memory\nModels like Memformer utilize external dynamic memory to encode and retrieve past information, achieving linear time complexity and constant memory space complexity when processing long sequences. This approach can be integrated into autoregressive models to enhance long-term retention and efficiency (referenced in the analysis note).\n\n## Integration of Different Attention Types\n\n### Probabilistic Sparse Attention\nThe MPSA-Conformer-CTC/Attention model incorporates Probabilistic Sparse Attention to address convergence challenges, particularly with long input feature sequences. This mechanism reduces computational complexity and memory usage while enhancing performance and stability.\n\n### Combination of Local and Global Information\nThe Recurrent Memory Transformer allows storing and processing both local and global information, passing information between segments of long sequences. This balanced approach can be integrated into hybrid attention mechanisms to enhance the model's ability to handle long-range dependencies (referenced in the analysis note).\n\n## Performance-Efficiency Trade-offs\n\n### Trade-offs in CASAK-V\nCASAK-V achieves minimal performance degradation on long-context benchmarks while significantly reducing memory usage (by 40%) and delivering near-linear runtime complexity. This approach balances performance and efficiency, making it suitable for resource-constrained environments.\n\n### Trade-offs in StreamingLLM and AutoCompressor\nStreamingLLM and AutoCompressor methods reduce memory usage and time costs but may compromise on long-range dependencies and model accuracy. These trade-offs need to be carefully considered when designing efficient attention architectures.\n\nBy integrating these recent developments, researchers can design autoregressive language models that are more efficient, scalable, and robust:\n\n- **Hybrid Attention Mechanisms**: Combine CTC and Attention mechanisms, or use dynamic sparse attention to leverage different strengths.\n- **Memory Compression**: Utilize adaptive KV-cache compression, external dynamic memory, and polynomial projections to reduce memory usage.\n- **Balanced Information Processing**: Integrate local and global information processing to handle long-range dependencies effectively.\n- **Performance-Efficiency Trade-offs**: Optimize models to achieve minimal performance degradation while reducing memory usage and computational complexity.\n",
            "analysis": "Analysis Note - Round 2\n\n1. Summary of Analysis:\n\nThe search results reveal several promising approaches for enhancing memory mechanisms and state representations in neural networks, particularly relevant to combining TTTLinear and FastTTTLinear:\n\na) Memory Architectures:\n- External dynamic memory shows promise for efficient sequence modeling\n- Constant memory attention mechanisms can reduce resource requirements\n- Memory replay and compression techniques can enhance long-term retention\n\nb) State Management:\n- Efficient state compression through polynomial projections\n- Dynamic memory management with bounded complexity\n- Integration of local and global information processing\n\n2. Useful References with Excerpts:\n\nFrom \"Memformer\":\n\"utilizes an external dynamic memory to encode and retrieve past information... achieves linear time complexity and constant memory space complexity when processing long sequences\"\n- Relevant for efficient memory management\n\nFrom \"Recurrent Memory Transformer\":\n\"Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence\"\n- Important for combining local and global processing\n\nFrom \"HiPPO\":\n\"introduces a general framework for the online compression of continuous signals and discrete time series by projection onto polynomial bases\"\n- Potential approach for efficient state compression\n\n3. Key Insights:\n\na) Memory Management:\n- External memory can be more efficient than storing all history\n- Memory compression through polynomial projections is promising\n- Dynamic memory allocation can improve efficiency\n\nb) Attention Integration:\n- Constant memory attention mechanisms are feasible\n- Local-global information processing can be balanced\n- Memory replay can enhance long-term dependencies\n\nc) State Representation:\n- Polynomial projections offer efficient compression\n- Bounded memory requirements are achievable\n- Dynamic state updates can maintain expressiveness\n\n4. Future Search Plan:\n\nRound 3: Focus on:\n- Hybrid attention mechanisms\n- Memory compression techniques\n- Query: \"hybrid attention mechanisms compression efficiency\"\n\n5. References:\n\n1. Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). Memformer: A Memory-Augmented Transformer for Sequence Modeling. AACL/IJCNLP.\n\n2. Bulatov, A., Kuratov, Y., & Burtsev, M. (2022). Recurrent Memory Transformer. Neural Information Processing Systems.\n\n3. Gu, A., Dao, T., Ermon, S., Rudra, A., & R\u00e9, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Neural Information Processing Systems.\n\n4. Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv.org.\n\n5. Feng, L., Tung, F., Hajimirsadeghi, H., Bengio, Y., & Ahmed, M. O. (2023). Memory Efficient Neural Processes via Constant Memory Attention Block. International Conference on Machine Learning."
        },
        {
            "ready": true,
            "query": "adaptive attention patterns memory optimization",
            "detail": "Analyze recent developments in adaptive attention patterns and memory optimization techniques, focusing on:\n1. Dynamic attention mechanisms\n2. Memory-efficient implementations\n3. Gradient stability\n4. Hardware efficiency",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in adaptive attention patterns and memory optimization techniques, focusing on:\n1. Dynamic attention mechanisms\n2. Memory-efficient implementations\n3. Gradient stability\n4. Hardware efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.05)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 13/71 (Score: 0.08)*\n\n```\nInternational Conference on Machine Learning, 2024. [8] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation 9, 1997. [9] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. International Conference on Machine Learning, 2023. [10] Tsendsuren Munkhdalai, Alessandro Sordoni, Tong Wang, and Adam Trischlern. Metalearned neural memory.\n```\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.03)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 2. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.03)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 9/72 (Score: 0.03)*\n\n```\neach token depends on every token before it in the sequence). Katharopoulos et al. [20, Choromanski et al. 21, Tsai et al. 32] show that we can select a feature map $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{\\tilde{d}}$ such that $\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right) \\approx \\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j} / \\sqrt{d}\\right)$. We can then rewrite the formula for softmax attention in Equation (1) as\n\n$$\n\\sum_{j=1}^{i} \\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right) \\boldsymbol{v}_{j}}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)}=\\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i}\\left(\\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top} \\boldsymbol{v}_{j}\\right)}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)}\n$$\n\nwhere every query attends to every past key in $\\mathcal{O}\\left(N d^{2}\\right)$ time and space complexity. Furthermore, Katharopoulos et al. 24] show that linear attention has a fixed size recurrent state during generation. Letting $\\boldsymbol{s}_{i}=\\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top} \\boldsymbol{v}_{j}$ and $\\boldsymbol{z}_{i}=\\sum_{j=1}^{i} \\phi\\left(\\boldsymbol{k}_{j}\\right)^{\\top}$ be a \"KV-state\" and \"K-state\" respectively, we can compute Equation (2) as\n\n$$\n\\begin{gathered}\n\\boldsymbol{s}_{i}=\\boldsymbol{s}_{i-1}+\\phi\\left(\\boldsymbol{k}_{i}\\right)^{\\top} \\boldsymbol{v}_{i}, \\quad \\boldsymbol{z}_{i}=\\boldsymbol{z}_{i-1}+\\phi\\left(\\boldsymbol{k}_{i}\\right)^{\\top} \\\\\n\\boldsymbol{y}_{i}=\\frac{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\boldsymbol{s}_{i}}{\\phi\\left(\\boldsymbol{q}_{i}\\right) \\boldsymbol{z}_{i}}\n\\end{gathered}\n$$\n\nwhere $\\boldsymbol{s}_{i} \\in \\mathbb{R}^{d \\times \\tilde{d}}$ and $\\boldsymbol{z}_{i} \\in \\mathbb{R}^{\\tilde{d}}$. Feature map. To approximate $\\exp \\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j} / \\sqrt{d}\\right)$, we use the $2^{\\text {nd }}$-order Taylor series feature map, picking $\\phi: \\mathbb{R}^{d} \\rightarrow \\mathbb{R}^{d^{2}}$ such that\n\n$$\n\\phi\\left(\\boldsymbol{q}_{i}\\right)^{\\top} \\phi\\left(\\boldsymbol{k}_{j}\\right)=1+\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j}+\\frac{\\left(\\boldsymbol{q}_{i}^{\\top} \\boldsymbol{k}_{j}\\right)^{2}}{2}\n$$\n\nWhile Zhang et al. 13 note that picking a feature map with $\\tilde{d}=d^{2}$ results in linear attention with $\\mathcal{O}\\left(N d^{3}\\right)$ time and space complexity and large recurrent state of size $O\\left(d^{3}\\right)$, we can tradeoff efficiency for recall capacity by projecting queries and keys to smaller dimensions i.e., $\\boldsymbol{W}_{q}, \\boldsymbol{W}_{k} \\in \\mathbb{R}^{d \\times d^{\\prime}}$ with $d^{\\prime}=16$. By changing $d^{\\prime}$ we modulate the size of the recurrent state. How does the choice of feature map affect the memory-recall tradeoff? Prior work demonstrates the strong performance of the Taylor feature map on associative recall [13]. Building on this analysis, we evaluate a broad set of feature maps $\\left(\\phi_{\\operatorname{ReLU}}(x)=\\max (x, 0)\\right.$, $\\phi_{\\text {PosELU }}(x)=\\operatorname{ELU}(x)+1, \\phi_{\\text {Square }}(x)=x^{2}, \\phi_{\\text {Identity }}(x)=x$, $\\phi_{\\text {CosFormer }}$ as defined in [23], and $\\phi_{\\text {Performer }}$ as defined in 21]) using the experimental setup described in Section 3.1. In Figure 3 (top), we plot the memory-recall tradeoff curves for these feature maps. The Taylor series feature map, along with the simple $\\phi_{\\text {PosELU }}$ and $\\phi_{\\text {ReLU }}$ feature maps, sits at the pareto frontier. One advantage of the Taylor feature map over these alternatives is that it expands the recurrent state size (improving recall capacity) without changing the number of parameters. As shown in Figure 3 (bottom), the Taylor series feature map requires fewer parameters than alternatives to achieve high recall capacity. This analysis and the ablations in Table 4 informed our decision to use the Taylor approximation, though other simple feature maps may be effective as well. ### 4.2 Local Exact Attention with Tensor Core Sliding Windows\n\nTo efficiently model fine-grained local interactions, BASED uses sliding window attention with window sizes set at small multiples of 16 (up to 64 tokens). Similar to past (causal) implementations 18 , 19], for window size $w$ each query $\\boldsymbol{q}_{i}$ only attends to past keys $\\left\\{\\boldsymbol{k}_{i-w+1}, \\ldots, \\boldsymbol{k}_{i}\\right\\}$. This enables $\\mathcal{O}(N w)$ time and space complexity for linear scaling in sequence length $N$, with a $w$-sized KV-cache for constant-memory generation. ![](https://cdn.mathpix.com/cropped/2024_09_12_8d23281e475274f360feg-07.jpg?height=909&width=472&top_left_y=630&top_left_x=1355)\n\nFigure 3: Linear attention feature maps on AR. $x$ : state size (bytes) during generation or param. count; $y$ : MQAR accuracy. This setting is harder than fig. 2 (256 key-value pairs). sizes 256 [17] to 4096 [10, BASED uses only $w=16,32$, or 64 to best exploit modern GPUs. In Section 5, we discuss how this \"Tensor core-aware\" window (TCWINDOW) achieves $1 \\mathrm{e}-5 \\times$ the latency than the $w=4096$ windows in modern LLMs (e.g., Mistral 7B [10]). While the small $w$ in TCWindOW enable fast local and exact attention, it presents a challenge for long range modeling. With just $w=64$, for every layer of $w=4096$ Mistral sliding window attention we would require 64 layers of BASED to achieve the same receptive field. Controlling for model depth and sequence length, Figure 2 indeed shows smaller $w$ linearly decreasing in associative recall accuracy. BASED's global linear attention described above overcomes the lack of long-range modeling presented with low $w$. Additional architectural details for BASED are discussed in Appendix C and the hybridization of layers used in experiments are provided in Table 7. We include ablations of architectural choices in Table 4 and evaluate the overall quality and efficiency of BASED in Section 6 . ## 5 Efficient Implementation\n\nIn this section we focus on the efficiency of BASED. A na\u00efve implementation is slower than the most efficient standard attention implementations (shown in Figure 4) as it requires large amounts of high latency memory movement. We first describe preliminaries of the GPU execution model and memory hierarchy. We next present the baseline and our hardware-aware algorithms for linear attention in Section 5.1 and for sliding window attention in Section 5.2 .\n```\n\n#### 3. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.01)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 6/24 (Score: 0.01)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [12] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. [14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [16] Facebook. Fully Sharded Data Parallel: faster AI training with fewer GPUs - engineering.fb.com. https://engineering.fb.com/2021/07/15/open-source/fsdp/.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.01)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 31/54 (Score: 0.01)*\n\n```\n## A. 2 Memory in Machine Learning\n\nMemory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention [70] and dilated convolutions [5], are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints [6, 15, 18, 42, 59, 60, 64, 74]. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models. Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients [56]), particular care must be paid to endow them with longer memory. The ubiquitous LSTM 34 and simplifications such as the GRU [17] control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies [31. Tallec and Ollivier [66] show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection [32] or non-saturating gates [9], which can suffer from instability issues.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive attention patterns memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Qi Xu, Yuyuan Gao, Jiangrong Shen, Yaxin Li, Xuming Ran, Huajin Tang, Gang Pan*\n\n**TL;DR:** This paper develops a recurrent spiking neural network model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns.\n\n**Abstract:** Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 2. Adaptive Robust Evidential Optimization For Open Set Detection from Imbalanced Data\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Hitesh Sapkota, Qi Yu*\n\n**TL;DR:** Adaptive Robust Evidential Optimization (AREO) is proposed that offers a principled way to quantify sample uncertainty through evidential learning while optimally balancing the model training over all classes in the closed set through adaptive distributively robust optimization (DRO).\n\n**Abstract:** Open set detection (OSD) aims at identifying data samples of an unknown class ( i.e., open set) from those of known classes ( i.e., closed set) based on a model trained from closed set samples. However, a closed set may involve a highly imbalanced class distribution. Accurately differentiating open set samples and those from a minority class in the closed set poses a fundamental challenge as the model may be equally uncertain when recognizing samples from the minority class. In this paper, we propose Adaptive Robust Evidential Optimization (AREO) that offers a principled way to quantify sample uncertainty through evidential learning while optimally balancing the model training over all classes in the closed set through adaptive distributively robust optimization (DRO). To avoid the model to primarily focus on the most difficult samples by following the standard DRO, adaptive DRO training is performed, which is governed by a novel multi-scheduler learning mechanism to ensure an optimal model training behavior that gives sufficient attention to the difficult samples and the minority class while capable of learning common patterns from the majority classes. Our experimental results on multiple real-world datasets demonstrate that the proposed model outputs uncertainty scores that can clearly separate samples from closed and open sets, respectively, and the detection results outperform the competitive baselines.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. Large Batch Optimization for Deep Learning: Training BERT in 76 minutes\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Yang You, Jing Li, Sashank J. Reddi, Jonathan Hseu, Sanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song, J. Demmel, K. Keutzer, Cho-Jui Hsieh*\n\n**TL;DR:** The empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning, and the optimizer enables use of very large batch sizes of 32868 without any degradation of performance.\n\n**Abstract:** Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at this https URL\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 890  (*Influential: 126*)\n\n#### 4. Stabilizing Transformer Training by Preventing Attention Entropy Collapse\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Shuangfei Zhai, T. Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, J. Susskind*\n\n**TL;DR:** This work investigates the training dynamics of Transformers by examining the evolution of the attention layers, and shows that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training a Vision Transformer without warmup, weight decay, layer normalization or adaptive optimizers.\n\n**Abstract:** Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\\textit{entropy collapse}$. As a remedy, we propose $\\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with $\\sigma$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer {to competitive performance} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at \\url{https://github.com/apple/ml-sigma-reparam}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 6*)\n\n#### 5. ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Ning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, Xin Jiang*\n\n**TL;DR:** A neural clustering method which can be seamlessly integrated into the Self-Attention Mechanism in Transformer, which groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency.\n\n**Abstract:** Recently, a lot of research has been carried out to improve the efficiency of Transformer. Among them, the sparse pattern-based method is an important branch of efficient Transformers. However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words. Other sparse methods use clustering patterns to select words, but the clustering process is separate from the training process of the target task, which causes a decrease in effectiveness. To address these limitations, we design a neural clustering method, which can be seamlessly integrated into the Self-Attention Mechanism in Transformer. The clustering task and the target task are jointly trained and optimized to benefit each other, leading to significant effectiveness improvement. In addition, our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency. We verified our method on machine translation, text classification, natural language inference, and text matching tasks. Experimental results show that our method outperforms two typical sparse attention methods, Reformer and Routing Transformer while having a comparable or even better time and memory efficiency.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 13  (*Influential: 0*)\n\n### 2 related papers from Papers with Code\n\n#### 1. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Yu Wang, Huazhong Yang, Guohao Dai, Shengen Yan, Shiyao Li, Zixiao Huang, Hongyi Wang, Tianqi Wu, Boju Chen, Genghan Zhang, Xuefei Ning, Haofeng Huang, Tianyu Fu*\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n**Published:** 2024-06-21\n\n\n\n#### 2. Neurodevelopmental disorders modeling using isogeometric analysis, dynamic domain expansion and local refinement\n\n*From Search Query: adaptive attention patterns memory optimization*\n\n*Yongjie Jessica Zhang, Victoria A. Webster-Wood, Ashlee S. Liao, Takahisa Kanekiyo, Toshihiko Nambara, Genesis Omana Suarez, Kuanren Qian*\n\n**Abstract:** Neurodevelopmental disorders (NDDs) have arisen as one of the most prevailing chronic diseases within the US. Often associated with severe adverse impacts on the formation of vital central and peripheral nervous systems during the neurodevelopmental process, NDDs are comprised of a broad spectrum of disorders, such as autism spectrum disorder, attention deficit hyperactivity disorder, and epilepsy, characterized by progressive and pervasive detriments to cognitive, speech, memory, motor, and other neurological functions in patients. However, the heterogeneous nature of NDDs poses a significant roadblock to identifying the exact pathogenesis, impeding accurate diagnosis and the development of targeted treatment planning. A computational NDDs model holds immense potential in enhancing our understanding of the multifaceted factors involved and could assist in identifying the root causes to expedite treatment development. To tackle this challenge, we introduce optimal neurotrophin concentration to the driving force and degradation of neurotrophin to the synaptogenesis process of a 2D phase field neuron growth model using isogeometric analysis to simulate neurite retraction and atrophy. The optimal neurotrophin concentration effectively captures the inverse relationship between neurotrophin levels and neurite survival, while its degradation regulates concentration levels. Leveraging dynamic domain expansion, the model efficiently expands the domain based on outgrowth patterns to minimize degrees of freedom. Based on truncated T-splines, our model simulates the evolving process of complex neurite structures by applying local refinement adaptively to the cell/neurite boundary. Furthermore, a thorough parameter investigation is conducted with detailed comparisons against neuron cell cultures in experiments, enhancing our fundamental understanding of the mechanisms underlying NDDs.\n\n**Published:** 2024-06-30\n\n\n\n\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\n\nThe search results reveal several promising approaches for combining the strengths of TTTLinear and FastTTTLinear while addressing their limitations:\n\na) Mixture of Attention (MoA):\n- Automatically tailors distinct sparse attention configurations\n- Adapts to varying input sizes\n- Shows significant improvements in efficiency and performance\n- Reduces memory usage while maintaining accuracy\n\nb) Memory Compression:\n- External dynamic memory shows promise for efficient sequence modeling\n- Polynomial projections for state compression\n- Adaptive KV-cache compression techniques\n\nc) Hybrid Attention Mechanisms:\n- Combination of local and global processing\n- Dynamic sparse attention patterns\n- Integration of different attention types for better efficiency\n\n2. Useful References with Excerpts:\n\nFrom \"MoA: Mixture of Sparse Attention\":\n\"MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence length... some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts\"\n- Relevant for optimizing attention patterns\n\nFrom \"Short-Long Convolutions Help Hardware-Efficient Linear Attention\":\n\"CHELA... replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner... enjoys global abstraction and data-dependent selection while maintaining real linear complexity\"\n- Important for efficient implementation\n\nFrom \"Samba: Simple Hybrid State Space Models\":\n\"Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism\"\n- Insights for combining compression with attention\n\n3. Key Insights:\n\na) Attention Optimization:\n- Need for dynamic, content-dependent attention patterns\n- Importance of balancing local and global context\n- Potential in combining sparse and dense attention\n\nb) Memory Management:\n- Selective compression of states\n- Dynamic allocation based on importance\n- Efficient recall mechanisms\n\nc) Implementation Strategy:\n- Hardware-efficient implementations are crucial\n- Need for stable gradient flow\n- Balance between compression and expressiveness\n\n4. Next Steps:\n\nReady to formulate proposal combining:\n- MoA's adaptive attention patterns\n- CHELA's efficient implementation\n- Samba's selective compression\n- Parent designs' strengths (TTT and GLA)\n\n5. References:\n\n1. Fu, T., et al. (2024). MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression. arXiv preprint.\n\n2. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n3. Chen, W., et al. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint.\n\n4. Qin, Z., et al. (2022). The Devil in Linear Transformer. Conference on Empirical Methods in Natural Language Processing.\n\n5. Rabe, M., & Staats, C. (2021). Self-attention Does Not Need O(n^2) Memory. arXiv preprint."
        },
        {
            "ready": false,
            "query": "stability analysis adaptive attention, convergence guarantees neural networks, hardware efficient implementation techniques",
            "detail": "Search for papers discussing:\n1. Theoretical stability analysis and convergence guarantees for adaptive attention mechanisms in neural networks\n2. Hardware-efficient implementation techniques for dynamic neural architectures\n3. Methods for combining multiple attention mechanisms with theoretical guarantees\n4. Memory optimization strategies for adaptive state updates\n5. Integration of compression with online adaptation in language models\nFocus on papers from 2022-2024 that provide theoretical analysis or empirical evaluations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Theoretical stability analysis and convergence guarantees for adaptive attention mechanisms in neural networks\n2. Hardware-efficient implementation techniques for dynamic neural architectures\n3. Methods for combining multiple attention mechanisms with theoretical guarantees\n4. Memory optimization strategies for adaptive state updates\n5. Integration of compression with online adaptation in language models\nFocus on papers from 2022-2024 that provide theoretical analysis or empirical evaluations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.96)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.96)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.56)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.56)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.53)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 7/30 (Score: 0.53)*\n\n```\nV. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 77-87, 2017. [6] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [8] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [9] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [11] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.48)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 45/86 (Score: 0.48)*\n\n```\nIn: arXiv preprint arXiv:2311.09431 (2023). [18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \"Language Models are Few-shot Learners\". In: Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877-1901. [19] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \"Rethinking Attention with Performers\". In: The International Conference on Learning Representations (ICLR). 2021. [20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \"PaLM: Scaling Language Modeling with Pathways\". In: Journal of Machine Learning Research 24.240 (2023), pp. 1-113. URL: http://jmlr.org/papers/v24/221144.html. [21] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". In: arXiv preprint arXiv:1412.3555 (2014). [22] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. \"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\". In: arXiv preprint arXiv:1803.05457 (2018). [23] Tri Dao. \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\". In: The International Conference on Learning Representations (ICLR). 2024. [24] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. \"Monarch: Expressive structured matrices for efficient and accurate training\".\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.43)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.43)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: stability analysis adaptive attention, convergence guarantees neural networks, hardware efficient implementation techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Polyhedron Attention Module: Learning Adaptive-order Interactions\n\n*From Search Query: stability analysis adaptive attention*\n\n*Tan Zhu, Fei Dou, Xinyu Wang, Jin Lu, Jinbo Bi*\n\n**TL;DR:** A Polyhedron Attention Module (PAM) is proposed to create piecewise polynomial models where the input space is split into poly-hedrons which define the different pieces and on each piece the hyperplanes that define the polyhedron boundary multiply to form the interactive terms, resulting in interactions of adaptive order to each piece.\n\n**Abstract:** Learning feature interactions can be the key for multivariate predictive modeling. ReLU-activated neural networks create piecewise linear prediction models. Other nonlinear activation functions lead to models with only high-order feature interactions, thus lacking of interpretability. Recent methods incorporate candidate polynomial terms of fixed orders into deep learning, which is subject to the issue of combinatorial explosion, or learn the orders that are difficult to adapt to different regions of the feature space. We propose a Polyhedron Attention Module (PAM) to create piecewise polynomial models where the input space is split into poly-hedrons which define the different pieces and on each piece the hyperplanes that define the polyhedron boundary multiply to form the interactive terms, resulting in interactions of adaptive order to each piece. PAM is interpretable to identify important interactions in predicting a target. Theoretic analysis shows that PAM has stronger expression capability than ReLU-activated networks. Extensive experimental results demonstrate the superior classification performance of PAM on massive datasets of the click-through rate prediction and PAM can learn meaningful interaction effects in a medical problem.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Learning When to Concentrate or Divert Attention: Self-Adaptive Attention Temperature for Neural Machine Translation\n\n*From Search Query: stability analysis adaptive attention*\n\n*Junyang Lin, Xu Sun, Xuancheng Ren, Muyu Li, Qi Su*\n\n**TL;DR:** This work proposes a new model with a mechanism called Self-Adaptive Control of Temperature (SACT) to control the softness of attention by means of an attention temperature and demonstrates that the model outperforms the baseline models.\n\n**Abstract:** Most of the Neural Machine Translation (NMT) models are based on the sequence-to-sequence (Seq2Seq) model with an encoder-decoder framework equipped with the attention mechanism. However, the conventional attention mechanism treats the decoding at each time step equally with the same matrix, which is problematic since the softness of the attention for different types of words (e.g. content words and function words) should differ. Therefore, we propose a new model with a mechanism called Self-Adaptive Control of Temperature (SACT) to control the softness of attention by means of an attention temperature. Experimental results on the Chinese-English translation and English-Vietnamese translation demonstrate that our model outperforms the baseline models, and the analysis and the case study show that our model can attend to the most relevant elements in the source-side contexts and generate the translation of high quality.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 33  (*Influential: 4*)\n\n#### 3. Stability-penalty-adaptive Follow-the-regularized-leader: Sparsity, Game-dependency, and Best-of-both-worlds\n\n*From Search Query: stability analysis adaptive attention*\n\n*Taira Tsuchiya, Shinji Ito, J. Honda*\n\n**TL;DR:** A generic adaptive learning rate, called stability-penalty-adaptive (SPA) learning rate for Follow-the-regularized-leader (FTRL) is developed, which yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed.\n\n**Abstract:** Adaptivity to the difficulties of a problem is a key property in sequential decision-making problems to broaden the applicability of algorithms. Follow-the-regularized-leader (FTRL) has recently emerged as one of the most promising approaches for obtaining various types of adaptivity in bandit problems. Aiming to further generalize this adaptivity, we develop a generic adaptive learning rate, called stability-penalty-adaptive (SPA) learning rate for FTRL. This learning rate yields a regret bound jointly depending on stability and penalty of the algorithm, into which the regret of FTRL is typically decomposed. With this result, we establish several algorithms with three types of adaptivity: sparsity, game-dependency, and best-of-both-worlds (BOBW). Despite the fact that sparsity appears frequently in real problems, existing sparse multi-armed bandit algorithms with $k$-arms assume that the sparsity level $s \\leq k$ is known in advance, which is often not the case in real-world scenarios. To address this issue, we first establish $s$-agnostic algorithms with regret bounds of $\\tilde{O}(\\sqrt{sT})$ in the adversarial regime for $T$ rounds, which matches the existing lower bound up to a logarithmic factor. Meanwhile, BOBW algorithms aim to achieve a near-optimal regret in both the stochastic and adversarial regimes. Leveraging the SPA learning rate and the technique for $s$-agnostic algorithms combined with a new analysis to bound the variation in FTRL output in response to changes in a regularizer, we establish the first BOBW algorithm with a sparsity-dependent bound. Additionally, we explore partial monitoring and demonstrate that the proposed SPA learning rate framework allows us to achieve a game-dependent bound and the BOBW simultaneously.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 4. On Feature Learning in Neural Networks with Global Convergence Guarantees\n\n*From Search Query: convergence guarantees neural networks*\n\n*Zhengdao Chen, E. Vanden-Eijnden, Joan Bruna*\n\n**TL;DR:** This work proves that when the input dimension is no less than the size of the training set, the training loss converges to zero at a linear rate under GF, and shows that, unlike in the Neural Tangent Kernel (NTK) regime, the multi-layer model exhibits feature learning and can achieve better generalization performance than its NTK counterpart.\n\n**Abstract:** We study the optimization of wide neural networks (NNs) via gradient flow (GF) in setups that allow feature learning while admitting non-asymptotic global convergence guarantees. First, for wide shallow NNs under the mean-field scaling and with a general class of activation functions, we prove that when the input dimension is no less than the size of the training set, the training loss converges to zero at a linear rate under GF. Building upon this analysis, we study a model of wide multi-layer NNs whose second-to-last layer is trained via GF, for which we also prove a linear-rate convergence of the training loss to zero, but regardless of the input dimension. We also show empirically that, unlike in the Neural Tangent Kernel (NTK) regime, our multi-layer model exhibits feature learning and can achieve better generalization performance than its NTK counterpart.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 10  (*Influential: 2*)\n\n#### 5. Robustness Guarantees for Adversarially Trained Neural Networks\n\n*From Search Query: convergence guarantees neural networks*\n\n*Poorya Mianjy, Raman Arora*\n\n**TL;DR:** This work proposes maximizing a lower bound on the 0 / 1 -loss by reflecting a surrogate loss about the origin by reflecting a surrogate loss about the origin for the inner loop that implements the adversarial attack during training using projected gradient descent (PGD).\n\n**Abstract:** We study robust adversarial training of two-layer neural networks as a bi-level optimization problem. In particular, for the inner loop that implements the adversarial attack during training using projected gradient descent (PGD), we propose maximizing a lower bound on the 0 / 1 -loss by reflecting a surrogate loss about the origin. This allows us to give a convergence guarantee for the inner-loop PGD attack. Furthermore, assuming the data is linearly separable, we provide precise iteration complexity results for end-to-end adversarial training, which holds for any width and initialization. We provide empirical evidence to support our theoretical results.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 6. On the Effective Number of Linear Regions in Shallow Univariate ReLU Networks: Convergence Guarantees and Implicit Bias\n\n*From Search Query: convergence guarantees neural networks*\n\n*Itay Safran, Gal Vardi, Jason D. Lee*\n\n**TL;DR:** GF converges in direction to a network achieving perfect training accuracy and having at most $\\mathcal{O}(r)$ linear regions, implying a generalization bound, unlike many other results in the literature.\n\n**Abstract:** We study the dynamics and implicit bias of gradient flow (GF) on univariate ReLU neural networks with a single hidden layer in a binary classification setting. We show that when the labels are determined by the sign of a target network with $r$ neurons, with high probability over the initialization of the network and the sampling of the dataset, GF converges in direction (suitably defined) to a network achieving perfect training accuracy and having at most $\\mathcal{O}(r)$ linear regions, implying a generalization bound. Unlike many other results in the literature, under an additional assumption on the distribution of the data, our result holds even for mild over-parameterization, where the width is $\\tilde{\\mathcal{O}}(r)$ and independent of the sample size.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 2*)\n\n#### 7. High Performance Natural Language Processing\n\n*From Search Query: hardware efficient implementation techniques*\n\n*Gabriel Ilharco, Cesar Ilharco, Iulia Turc, Tim Dettmers, F. Ferreira, Kenton Lee*\n\n**TL;DR:** This cutting-edge tutorial will recapitulate the state-of-the-art in natural language processing with scale in perspective, and cover a wide range of techniques for improving efficiency, including knowledge distillation, quantization, pruning, more efficient architectures, along with case studies and practical implementation tricks.\n\n**Abstract:** Scale has played a central role in the rapid progress natural language processing has enjoyed in recent years. While benchmarks are dominated by ever larger models, efficient hardware use is critical for their widespread adoption and further progress in the field. In this cutting-edge tutorial, we will recapitulate the state-of-the-art in natural language processing with scale in perspective. After establishing these foundations, we will cover a wide range of techniques for improving efficiency, including knowledge distillation, quantization, pruning, more efficient architectures, along with case studies and practical implementation tricks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 8. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient implementation techniques*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 9. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: hardware efficient implementation techniques*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 45  (*Influential: 4*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Stable Segment Anything Model\n\n*From Search Query: stability analysis adaptive attention*\n\n*Chi-Keung Tang, Yu-Wing Tai, Zhongyuan Wang, Pengfei Wan, Yuan Zhang, Mingqiao Ye, Lei Ke, Xin Tao, Qi Fan*\n\n**Abstract:** The Segment Anything Model (SAM) achieves remarkable promptable segmentation given high-quality prompts which, however, often require good skills to specify. To make SAM robust to casual prompts, this paper presents the first comprehensive analysis on SAM's segmentation stability across a diverse spectrum of prompt qualities, notably imprecise bounding boxes and insufficient points. Our key finding reveals that given such low-quality prompts, SAM's mask decoder tends to activate image features that are biased towards the background or confined to specific object parts. To mitigate this issue, our key idea consists of calibrating solely SAM's mask attention by adjusting the sampling locations and amplitudes of image features, while the original SAM model architecture and weights remain unchanged. Consequently, our deformable sampling plugin (DSP) enables SAM to adaptively shift attention to the prompted target regions in a data-driven manner, facilitated by our effective robust training strategy (RTS). During inference, dynamic routing plugin (DRP) is proposed that toggles SAM between the deformable and regular grid sampling modes, conditioned on the input prompt quality. Thus, our solution, termed Stable-SAM, offers several advantages: 1) improved SAM's segmentation stability across a wide range of prompt qualities, while 2) retaining SAM's powerful promptable segmentation efficiency and generality, with 3) minimal learnable parameters (0.08 M) and fast adaptation (by 1 training epoch). Extensive experiments across multiple datasets validate the effectiveness and advantages of our approach, underscoring Stable-SAM as a more robust solution for segmenting anything. Codes will be released upon acceptance. https://github.com/fanq15/Stable-SAM\n\n**Published:** 2023-11-27\n\n\n\n#### 2. Skyformer: Remodel Self-Attention with Gaussian Kernel and Nystr\\\"om Method\n\n*From Search Query: stability analysis adaptive attention*\n\n*Yun Yang, Heng Ji, Qi Zeng, Yifan Chen*\n\n**Abstract:** Transformers are expensive to train due to the quadratic time and space complexity in the self-attention mechanism. On the other hand, although kernel machines suffer from the same computation bottleneck in pairwise dot products, several approximation schemes have been successfully incorporated to considerably reduce their computational cost without sacrificing too much accuracy. In this work, we leverage the computation methods for kernel machines to alleviate the high computational cost and introduce Skyformer, which replaces the softmax structure with a Gaussian kernel to stabilize the model training and adapts the Nystr\u00f6m method to a non-positive semidefinite matrix to accelerate the computation. We further conduct theoretical analysis by showing that the matrix approximation error of our proposed method is small in the spectral norm. Experiments on Long Range Arena benchmark show that the proposed method is sufficient in getting comparable or even better performance than the full self-attention while requiring fewer computation resources.\n\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-05-21\n\n\n\n#### 3. On Model Compression for Neural Networks: Framework, Algorithm, and Convergence Guarantee\n\n*From Search Query: convergence guarantees neural networks*\n\n*Mengnan Du, Xianlian Zhou, Haimin Wang, Jihoon Chung, Bo Shen, Chenyang Li*\n\n**Abstract:** Model compression is a crucial part of deploying neural networks (NNs), especially when the memory and storage of computing devices are limited in many applications. This paper focuses on two model compression techniques: low-rank approximation and weight pruning in neural networks, which are very popular nowadays. However, training NN with low-rank approximation and weight pruning always suffers significant accuracy loss and convergence issues. In this paper, a holistic framework is proposed for model compression from a novel perspective of nonconvex optimization by designing an appropriate objective function. Then, we introduce NN-BCD, a block coordinate descent (BCD) algorithm to solve the nonconvex optimization. One advantage of our algorithm is that an efficient iteration scheme can be derived with closed-form, which is gradient-free. Therefore, our algorithm will not suffer from vanishing/exploding gradient problems. Furthermore, with the Kurdyka-{\\L}ojasiewicz (K{\\L}) property of our objective function, we show that our algorithm globally converges to a critical point at the rate of O(1/k), where k denotes the number of iterations. Lastly, extensive experiments with tensor train decomposition and weight pruning demonstrate the efficiency and superior performance of the proposed framework. Our code implementation is available at https://github.com/ChenyangLi-97/NN-BCD\n\n**Published:** 2023-03-13\n\n\n\n#### 4. Physics-informed Neural Networks for Functional Differential Equations: Cylindrical Approximation and Its Convergence Guarantees\n\n*From Search Query: convergence guarantees neural networks*\n\n*Takeru Yokota, Taiki Miyagawa*\n\n**Abstract:** We propose the first learning scheme for functional differential equations (FDEs). FDEs play a fundamental role in physics, mathematics, and optimal control. However, the numerical analysis of FDEs has faced challenges due to its unrealistic computational costs and has been a long standing problem over decades. Thus, numerical approximations of FDEs have been developed, but they often oversimplify the solutions. To tackle these two issues, we propose a hybrid approach combining physics-informed neural networks (PINNs) with the \\textit{cylindrical approximation}. The cylindrical approximation expands functions and functional derivatives with an orthonormal basis and transforms FDEs into high-dimensional PDEs. To validate the reliability of the cylindrical approximation for FDE applications, we prove the convergence theorems of approximated functional derivatives and solutions. Then, the derived high-dimensional PDEs are numerically solved with PINNs. Through the capabilities of PINNs, our approach can handle a broader class of functional derivatives more efficiently than conventional discretization-based methods, improving the scalability of the cylindrical approximation. As a proof of concept, we conduct experiments on two FDEs and demonstrate that our model can successfully achieve typical $L^1$ relative error orders of PINNs $\\sim 10^{-3}$. Overall, our work provides a strong backbone for physicists, mathematicians, and machine learning experts to analyze previously challenging FDEs, thereby democratizing their numerical analysis, which has received limited attention. Code is available at \\url{https://github.com/TaikiMiyagawa/FunctionalPINN}.\n\n**Published:** 2024-10-23\n\n\n\n#### 5. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n*From Search Query: hardware efficient implementation techniques*\n\n*Shen Li, Ajit Mathews, Yuchen Hao, Geeta Chauhan, Pritam Damania, Bernard Nguyen, Can Balioglu, Alban Desmaison, Sam Shleifer, Myle Ott, Hamid Shojanazeri, Less Wright, Min Xu, Chien-chin Huang, Liang Luo, Rohan Varma, Andrew Gu, Yanli Zhao*\n\n**Abstract:** It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.\n\n**Published:** 2023-04-21\n\n\n\n#### 6. Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters\n\n*From Search Query: hardware efficient implementation techniques*\n\n*dianhai yu, Yanjun Ma, Yu Sun, Ge Li, Yue Yu, Yaqian Han, Shaohuai Shi, Bin Wang, Long Li, Yongshuai Hou, Peng Liu, Shuohuan Wang, Yuang Liu, Xianjie Mo, Siyu Ding, Weibao Gong, Zhihua Wu, Yang Xiang*\n\n**Abstract:** The ever-growing model size and scale of compute have attracted increasing interests in training deep learning models over multiple nodes. However, when it comes to training on cloud clusters, especially across remote clusters, huge challenges are faced. In this work, we introduce a general framework, Nebula-I, for collaboratively training deep learning models over remote heterogeneous clusters, the connections between which are low-bandwidth wide area networks (WANs). We took natural language processing (NLP) as an example to show how Nebula-I works in different training phases that include: a) pre-training a multilingual language model using two remote clusters; and b) fine-tuning a machine translation model using knowledge distilled from pre-trained models, which run through the most popular paradigm of recent deep learning. To balance the accuracy and communication efficiency, in Nebula-I, parameter-efficient training strategies, hybrid parallel computing methods and adaptive communication acceleration techniques are jointly applied. Meanwhile, security strategies are employed to guarantee the safety, reliability and privacy in intra-cluster computation and inter-cluster communication. Nebula-I is implemented with the PaddlePaddle deep learning framework, which can support collaborative training over heterogeneous hardware, e.g. GPU and NPU. Experiments demonstrate that the proposed framework could substantially maximize the training efficiency while preserving satisfactory NLP performance. By using Nebula-I, users can run large-scale training tasks over cloud clusters with minimum developments, and the utility of existed large pre-trained models could be further promoted. We also introduced new state-of-the-art results on cross-lingual natural language inference tasks, which are generated based upon a novel learning framework and Nebula-I.\n\n**Published:** 2022-05-19\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on stability analysis, adaptive attention, hardware efficiency, and integration of multiple mechanisms, here are some key insights and methods from the recent literature:\n\n## Theoretical Stability Analysis and Convergence Guarantees\n\n- The paper on \"Simplifying, Stabilizing & Scaling Continuous-Time Consistency Models\" introduces several techniques to stabilize continuous-time consistency models, which can be relevant for understanding stability in adaptive mechanisms. It proposes techniques such as tangent normalization, adaptive weighting, and progressive annealing to improve the stability of continuous-time models. These methods can be adapted to analyze the stability of adaptive attention mechanisms in neural networks.\n\n## Hardware-Efficient Implementation Techniques\n\n- For hardware-efficient implementation, the concept of Deep Equilibrium Models (DEMs) is noteworthy. DEMs use implicit differentiation to estimate gradients without backpropagating through all layers, making them memory-efficient. This approach can be applied to dynamic neural architectures, including those with adaptive attention mechanisms, to improve hardware efficiency.\n\n## Combining Multiple Attention Mechanisms with Theoretical Guarantees\n\n- The \"Lightning Attention-2\" paper presents a method for handling unlimited sequence lengths in large language models efficiently. While it does not explicitly provide theoretical guarantees for combining multiple attention mechanisms, it shows consistent performance regardless of sequence length, which can be a starting point for integrating multiple attention mechanisms. Further research would be needed to provide theoretical guarantees for such combinations.\n\n## Memory Optimization Strategies for Adaptive State Updates\n\n- The \"Constant Memory Attention Block (CMAB)\" proposed by Feng et al. is a novel attention block that computes its output in constant memory, making it highly memory-efficient. This approach can be integrated into adaptive state updates to optimize memory usage.\n- Additionally, the \"REST\" method by Afzal et al. uses residual state updates to achieve significant speedups, which can be applied to optimize memory efficiency in adaptive mechanisms.\n\n## Integration of Compression with Online Adaptation in Language Models\n\n- The concept of adaptive weighting functions in the \"Simplifying, Stabilizing & Scaling Continuous-Time Consistency Models\" paper can be extended to integrate compression with online adaptation. By training an adaptive weighting function alongside the model, it is possible to minimize the variance of losses and incorporate prior weighting, which can be beneficial for compressed state updates and online adaptation.\n\n### Additional Considerations\n\n- **Hardware Performance and Efficiency**: Recent advancements in machine learning hardware, such as the use of tensor cores and optimized number formats (e.g., tensor-FP16, tensor-INT8), can significantly improve the performance and efficiency of training and inference for large language models. These improvements can be crucial for scaling up the models efficiently.\n\n### Future Directions\n\n- To further improve the stability and convergence of adaptive attention mechanisms, more research is needed on theoretical analysis and empirical evaluations. This includes studying the convergence properties of hybrid update rules and the impact of compression on model stability.\n- Integrating techniques like DEMs and CMAB into adaptive attention mechanisms could provide both theoretical guarantees and practical efficiency.\n- Exploring hardware-efficient implementations and memory optimization strategies will be essential for scaling these models to larger sizes and more complex tasks.\n\nBy combining these insights, researchers can develop more stable, efficient, and scalable autoregressive language models that leverage adaptive attention mechanisms effectively.\n",
            "analysis": "### Analysis Summary\n\nThe search results provide valuable insights into several key aspects of the AdaptiveTTT proposal, but some critical areas still require further investigation.\n\n### Key Findings from Current Search\n\n1. Test-Time Training (TTT):\n- Hardt & Sun (2023) demonstrate successful TTT with nearest neighbors in LLMs\n- Shows TTT can improve performance with as few as 20 neighbors and one gradient iteration\n- Highlights importance of index quality and size\n\n2. Memory Efficiency:\n- REST (Afzal et al., 2024) shows 9x speedup with residual state updates\n- CMANPs (Feng et al., 2023) achieves constant memory attention\n- Loki (Singhania et al., 2024) demonstrates efficient sparse attention through low-rank approximation\n\n3. Adaptive Mechanisms:\n- Lightning Attention-2 (Qin et al., 2024) shows consistent speed regardless of sequence length\n- Theoretical guarantees for adaptive mechanisms are still limited\n- Need more research on stability analysis\n\n### Useful References & Excerpts\n\n1. From Hardt & Sun (2023):\n\"Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks\"\n\n2. From Singhania et al. (2024):\n\"Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models\"\n\n3. From Feng et al. (2023):\n\"Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory\"\n\n### Areas Requiring Further Investigation\n\n1. Stability Analysis:\n- Need more research on theoretical guarantees for adaptive mechanisms\n- Convergence properties of hybrid update rules\n- Impact of compression on model stability\n\n2. Integration Techniques:\n- Detailed mechanisms for combining TTT with adaptive attention\n- Hardware-efficient implementations of hybrid approaches\n- Memory-computation trade-offs\n\n3. Scalability:\n- Performance at larger model sizes\n- Impact on different types of language tasks\n- Long-sequence behavior\n\n### Future Search Plan\n\nNeed additional searches focusing on:\n\n1. Stability and Convergence:\n- Theoretical analysis of adaptive attention mechanisms\n- Convergence guarantees for test-time training\n- Stability analysis of compressed state updates\n\n2. Hardware Efficiency:\n- Implementation techniques for adaptive mechanisms\n- Memory optimization strategies\n- Efficient state management\n\n3. Integration Methods:\n- Combining multiple attention mechanisms\n- Hybrid update rules\n- Compression with online adaptation\n\n### References\n\n1. Hardt, M., & Sun, Y. (2023). Test-Time Training on Nearest Neighbors for Large Language Models. International Conference on Learning Representations.\n\n2. Singhania, P., Singh, S., He, S., Feizi, S., & Bhatele, A. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n3. Feng, L., Tung, F., Hajimirsadeghi, H., Bengio, Y., & Ahmed, M. O. (2023). Memory Efficient Neural Processes via Constant Memory Attention Block. International Conference on Machine Learning.\n\n4. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n5. Afzal, A., Chrysos, G. G., Cevher, V., & Shoaran, M. (2024). REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates. International Conference on Machine Learning."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "adaptive attention scaling language models, memory compression neural networks, test-time training efficiency",
            "detail": "Find papers discussing:\n1. Adaptive attention mechanisms that dynamically adjust based on input complexity\n2. Memory compression techniques in language models, particularly those maintaining model expressiveness\n3. Efficient implementations of test-time training\n4. Methods for combining multiple attention mechanisms\n5. Techniques for dynamic scaling of attention patterns\nFocus on papers from 2022-2024 that specifically address efficiency and scalability in language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Adaptive attention mechanisms that dynamically adjust based on input complexity\n2. Memory compression techniques in language models, particularly those maintaining model expressiveness\n3. Efficient implementations of test-time training\n4. Methods for combining multiple attention mechanisms\n5. Techniques for dynamic scaling of attention patterns\nFocus on papers from 2022-2024 that specifically address efficiency and scalability in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.99)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.99)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Latent Attention for Linear Time Transformers (Avg. Score: 0.98)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 12/21 (Score: 0.98)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient Attention: Attention with Linear Complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3531-3539, 2021. Smith, J. T., Warrington, A., and Linderman, S. W. Simplified State Space Layers for Sequence Modeling. arXiv preprint arXiv:2208.04933, 2022. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006, 2020a. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient Transformers: A Survey. arXiv preprint arXiv:2009.06732, 2020 b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention Is All You Need. Advances In Neural Information Processing Systems, 30, 2017. Wang, N., Gan, G., Zhang, P., Zhang, S., Wei, J., Liu, Q., and Jiang, X. ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.97)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 45/86 (Score: 0.97)*\n\n```\nIn: arXiv preprint arXiv:2311.09431 (2023). [18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \"Language Models are Few-shot Learners\". In: Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877-1901. [19] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \"Rethinking Attention with Performers\". In: The International Conference on Learning Representations (ICLR). 2021. [20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \"PaLM: Scaling Language Modeling with Pathways\". In: Journal of Machine Learning Research 24.240 (2023), pp. 1-113. URL: http://jmlr.org/papers/v24/221144.html. [21] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". In: arXiv preprint arXiv:1412.3555 (2014). [22] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. \"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\". In: arXiv preprint arXiv:1803.05457 (2018). [23] Tri Dao. \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\". In: The International Conference on Learning Representations (ICLR). 2024. [24] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. \"Monarch: Expressive structured matrices for efficient and accurate training\".\n```\n\n#### 4. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.97)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 12/24 (Score: 0.97)*\n\n```\narXiv preprint arXiv:1511.07289, 2015. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128 k context. arXiv preprint arXiv:2402.10171, 2024. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model.\n```\n\n#### 5. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.97)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.97)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive attention scaling language models, memory compression neural networks, test-time training efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models\n\n*From Search Query: adaptive attention scaling language models*\n\n*Zhenyu (Allen) Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher R\u00e9, Clark W. Barrett, Zhangyang Wang, Beidi Chen*\n\n**TL;DR:** A novel approach for implementing the KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens is introduced and a theoretical guarantee for the novel eviction algorithm is proved.\n\n**Abstract:** Large Language Models (LLMs), despite their recent impressive accomplishments, are notably cost-prohibitive to deploy, particularly for applications involving long-content generation, such as dialogue systems and story writing. Often, a large amount of transient state information, referred to as the KV cache, is stored in GPU memory in addition to model parameters, scaling linearly with the sequence length and batch size. In this paper, we introduce a novel approach for implementing the KV cache which significantly reduces its memory footprint. Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores. We call these tokens Heavy Hitters (H$_2$). Through a comprehensive investigation, we find that (i) the emergence of H$_2$ is natural and strongly correlates with the frequent co-occurrence of tokens in the text, and (ii) removing them results in significant performance degradation. Based on these insights, we propose Heavy Hitter Oracle (H$_2$O), a KV cache eviction policy that dynamically retains a balance of recent and H$_2$ tokens. We formulate the KV cache eviction as a dynamic submodular problem and prove (under mild assumptions) a theoretical guarantee for our novel eviction algorithm which could help guide future work. We validate the accuracy of our algorithm with OPT, LLaMA, and GPT-NeoX across a wide range of tasks. Our implementation of H$_2$O with 20% heavy hitters improves the throughput over three leading inference systems DeepSpeed Zero-Inference, Hugging Face Accelerate, and FlexGen by up to 29$\\times$, 29$\\times$, and 3$\\times$ on OPT-6.7B and OPT-30B. With the same batch size, H2O can reduce the latency by up to 1.9$\\times$. The code is available at https://github.com/FMInference/H2O.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 106  (*Influential: 20*)\n\n#### 2. Adaptive Gating in Mixture-of-Experts based Language Models\n\n*From Search Query: adaptive attention scaling language models*\n\n*Jiamin Li, Qiang Su, Yitao Yang, Yimin Jiang, Cong Wang, Hong-Yu Xu*\n\n**TL;DR:** Adaptive gating in MoE is introduced, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution that preserves sparsity while improving training efficiency.\n\n**Abstract:** Large language models, such as OpenAI's ChatGPT, have demonstrated exceptional language understanding capabilities in various NLP tasks. Sparsely activated mixture-of-experts (MoE) has emerged as a promising solution for scaling models while maintaining a constant number of computational operations. Existing MoE model adopts a fixed gating network where each token is computed by the same number of experts. However, this approach contradicts our intuition that the tokens in each sequence vary in terms of their linguistic complexity and, consequently, require different computational costs. Little is discussed in prior research on the trade-off between computation per token and model performance. This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution. The proposed framework preserves sparsity while improving training efficiency. Additionally, curriculum learning is leveraged to further reduce training time. Extensive experiments on diverse NLP tasks show that adaptive gating reduces at most 22.5% training time while maintaining inference quality. Moreover, we conduct a comprehensive analysis of the routing decisions and present our insights when adaptive gating is used.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. Characterizing Mechanisms for Factual Recall in Language Models\n\n*From Search Query: adaptive attention scaling language models*\n\n*Qinan Yu, Jack Merullo, Ellie Pavlick*\n\n**TL;DR:** This work contributes to a body of evidence showing that the language model can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.\n\n**Abstract:** Language Models (LMs) often must integrate facts they memorized in pretraining with new information that appears in a given context. These two sources can disagree, causing competition within the model, and it is unclear how an LM will resolve the conflict. On a dataset that queries for knowledge of world capitals, we investigate both distributional and mechanistic determinants of LM behavior in such situations. Specifically, we measure the proportion of the time an LM will use a counterfactual prefix (e.g.,\"The capital of Poland is London\") to overwrite what it learned in pretraining (\"Warsaw\"). On Pythia and GPT2, the training frequency of both the query country (\"Poland\") and the in-context city (\"London\") highly affect the models' likelihood of using the counterfactual. We then use head attribution to identify individual attention heads that either promote the memorized answer or the in-context answer in the logits. By scaling up or down the value vector of these heads, we can control the likelihood of using the in-context answer on new data. This method can increase the rate of generating the in-context answer to 88\\% of the time simply by scaling a single head at runtime. Our work contributes to a body of evidence showing that we can often localize model behaviors to specific components and provides a proof of concept for how future methods might control model behavior dynamically at runtime.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 0*)\n\n#### 4. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: memory compression neural networks*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 5. Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding\n\n*From Search Query: memory compression neural networks*\n\n*Song Han, Huizi Mao, W. Dally*\n\n**TL;DR:** This work introduces \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy.\n\n**Abstract:** Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2015\n\n**Citations:** 8228  (*Influential: 820*)\n\n#### 6. A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks\n\n*From Search Query: memory compression neural networks*\n\n*Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Liang Chen, Zibin Zheng, Baokun Wang, Changhua Meng*\n\n**TL;DR:** This work proposes SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance, and provides theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts.\n\n**Abstract:** While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This work explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compression, SpikeGCL is either comparable to or outperforms many fancy state-of-the-art supervised and self-supervised methods across several graph benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 7. Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering\n\n*From Search Query: test-time training efficiency*\n\n*Yongyi Su, Xun Xu, K. Jia*\n\n**TL;DR:** This work revisits TTT assumptions and categorizes TTT protocols by two key factors, and develops a realistic sequential test-time training (sTTT) protocol, under which a test-Time anchored clustering (TTAC) approach is developed to enable stronger test- time feature learning.\n\n**Abstract:** Deploying models on target domain data subject to distribution shift requires adaptation. Test-time training (TTT) emerges as a solution to this adaptation under a realistic scenario where access to full source domain data is not available and instant inference on target domain is required. Despite many efforts into TTT, there is a confusion over the experimental settings, thus leading to unfair comparisons. In this work, we first revisit TTT assumptions and categorize TTT protocols by two key factors. Among the multiple protocols, we adopt a realistic sequential test-time training (sTTT) protocol, under which we further develop a test-time anchored clustering (TTAC) approach to enable stronger test-time feature learning. TTAC discovers clusters in both source and target domain and match the target clusters to the source ones to improve generalization. Pseudo label filtering and iterative updating are developed to improve the effectiveness and efficiency of anchored clustering. We demonstrate that under all TTT protocols TTAC consistently outperforms the state-of-the-art methods on six TTT datasets. We hope this work will provide a fair benchmarking of TTT methods and future research should be compared within respective protocols. A demo code is available at https://github.com/Gorilla-Lab-SCUT/TTAC.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 8*)\n\n#### 8. InfoNet: Neural Estimation of Mutual Information without Test-Time Optimization\n\n*From Search Query: test-time training efficiency*\n\n*Zhengyang Hu, Song Kang, Qunsong Zeng, Kaibin Huang, Yanchao Yang*\n\n**TL;DR:** This work introduces a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures and evaluates the effectiveness and generalization of the proposed mutual information estimation scheme on various families of distributions and applications.\n\n**Abstract:** Estimating mutual correlations between random variables or data streams is essential for intelligent behavior and decision-making. As a fundamental quantity for measuring statistical relationships, mutual information has been extensively studied and utilized for its generality and equitability. However, existing methods often lack the efficiency needed for real-time applications, such as test-time optimization of a neural network, or the differentiability required for end-to-end learning, like histograms. We introduce a neural network called InfoNet, which directly outputs mutual information estimations of data streams by leveraging the attention mechanism and the computational efficiency of deep learning infrastructures. By maximizing a dual formulation of mutual information through large-scale simulated training, our approach circumvents time-consuming test-time optimization and offers generalization ability. We evaluate the effectiveness and generalization of our proposed mutual information estimation scheme on various families of distributions and applications. Our results demonstrate that InfoNet and its training process provide a graceful efficiency-accuracy trade-off and order-preserving properties. We will make the code and models available as a comprehensive toolbox to facilitate studies in different fields requiring real-time mutual information estimation.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 1*)\n\n#### 9. Enhancing Neural Training via a Correlated Dynamics Model\n\n*From Search Query: test-time training efficiency*\n\n*Jonathan Brokman, Roy Betser, Rotem Turjeman, Tom Berkov, I. Cohen, Guy Gilboa*\n\n**TL;DR:** This work introduces Correlation Mode Decomposition (CMD), a algorithm that clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs that enables CMD to efficiently represent the training dynamics of complex networks, like ResNets and Transformers, using only a few modes.\n\n**Abstract:** As neural networks grow in scale, their training becomes both computationally demanding and rich in dynamics. Amidst the flourishing interest in these training dynamics, we present a novel observation: Parameters during training exhibit intrinsic correlations over time. Capitalizing on this, we introduce Correlation Mode Decomposition (CMD). This algorithm clusters the parameter space into groups, termed modes, that display synchronized behavior across epochs. This enables CMD to efficiently represent the training dynamics of complex networks, like ResNets and Transformers, using only a few modes. Moreover, test set generalization is enhanced. We introduce an efficient CMD variant, designed to run concurrently with training. Our experiments indicate that CMD surpasses the state-of-the-art method for compactly modeled dynamics on image classification. Our modeling can improve training efficiency and lower communication overhead, as shown by our preliminary experiments in the context of federated learning.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. LXMERT: Learning Cross-Modality Encoder Representations from Transformers\n\n*From Search Query: adaptive attention scaling language models*\n\n*Mohit Bansal, Hao Tan*\n\n**Abstract:** Vision-and-language reasoning requires an understanding of visual concepts, language semantics, and, most importantly, the alignment and relationships between these two modalities. We thus propose the LXMERT (Learning Cross-Modality Encoder Representations from Transformers) framework to learn these vision-and-language connections. In LXMERT, we build a large-scale Transformer model that consists of three encoders: an object relationship encoder, a language encoder, and a cross-modality encoder. Next, to endow our model with the capability of connecting vision and language semantics, we pre-train the model with large amounts of image-and-sentence pairs, via five diverse representative pre-training tasks: masked language modeling, masked object prediction (feature regression and label classification), cross-modality matching, and image question answering. These tasks help in learning both intra-modality and cross-modality relationships. After fine-tuning from our pre-trained parameters, our model achieves the state-of-the-art results on two visual question answering datasets (i.e., VQA and GQA). We also show the generalizability of our pre-trained cross-modality model by adapting it to a challenging visual-reasoning task, NLVR2, and improve the previous best result by 22% absolute (54% to 76%). Lastly, we demonstrate detailed ablation studies to prove that both our novel model components and pre-training strategies significantly contribute to our strong results; and also present several attention visualizations for the different encoders. Code and pre-trained models publicly available at: https://github.com/airsplay/lxmert\n\n**Conference:** lxmert-learning-cross-modality-encoder-1\n\n**Published:** 2019-08-20\n\n\n\n#### 2. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n\n*From Search Query: adaptive attention scaling language models*\n\n*Baining Guo, Stephen Lin, Zheng Zhang, Yixuan Wei, Han Hu, Yue Cao, Yutong Lin, Ze Liu*\n\n**Abstract:** This paper presents a new vision Transformer, called Swin Transformer, that capably serves as a general-purpose backbone for computer vision. Challenges in adapting Transformer from language to vision arise from differences between the two domains, such as large variations in the scale of visual entities and the high resolution of pixels in images compared to words in text. To address these differences, we propose a hierarchical Transformer whose representation is computed with \\textbf{S}hifted \\textbf{win}dows. The shifted windowing scheme brings greater efficiency by limiting self-attention computation to non-overlapping local windows while also allowing for cross-window connection. This hierarchical architecture has the flexibility to model at various scales and has linear computational complexity with respect to image size. These qualities of Swin Transformer make it compatible with a broad range of vision tasks, including image classification (87.3 top-1 accuracy on ImageNet-1K) and dense prediction tasks such as object detection (58.7 box AP and 51.1 mask AP on COCO test-dev) and semantic segmentation (53.5 mIoU on ADE20K val). Its performance surpasses the previous state-of-the-art by a large margin of +2.7 box AP and +2.6 mask AP on COCO, and +3.2 mIoU on ADE20K, demonstrating the potential of Transformer-based models as vision backbones. The hierarchical design and the shifted window approach also prove beneficial for all-MLP architectures. The code and models are publicly available at~\\url{https://github.com/microsoft/Swin-Transformer}.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-03-25\n\n\n\n#### 3. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5MB model size\n\n*From Search Query: memory compression neural networks*\n\n*Song Han, Forrest N. Iandola, Matthew W. Moskewicz, Khalid Ashraf, William J. Dally, Kurt Keutzer*\n\n**Abstract:** Recent research on deep neural networks has focused primarily on improving\naccuracy. For a given accuracy level, it is typically possible to identify\nmultiple DNN architectures that achieve that accuracy level. With equivalent\naccuracy, smaller DNN architectures offer at least three advantages: (1)\nSmaller DNNs require less communication across servers during distributed\ntraining. (2) Smaller DNNs require less bandwidth to export a new model from\nthe cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on\nFPGAs and other hardware with limited memory. To provide all of these\nadvantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet\nachieves AlexNet-level accuracy on ImageNet with 50x fewer parameters.\nAdditionally, with model compression techniques we are able to compress\nSqueezeNet to less than 0.5MB (510x smaller than AlexNet).\n  The SqueezeNet architecture is available for download here:\nhttps://github.com/DeepScale/SqueezeNet\n\n**Published:** 2016-02-24\n\n\n\n#### 4. Pareto-Optimal Quantized ResNet Is Mostly 4-bit\n\n*From Search Query: memory compression neural networks*\n\n*Lukasz Lew, Chas Leichner, Oleg Rybakov, Jonathan Malmaud, Shivani Agrawal, Lisa Wang, Amirali Abdolrashidi*\n\n**Abstract:** Quantization has become a popular technique to compress neural networks and reduce compute cost, but most prior work focuses on studying quantization without changing the network size. Many real-world applications of neural networks have compute cost and memory budgets, which can be traded off with model quality by changing the number of parameters. In this work, we use ResNet as a case study to systematically investigate the effects of quantization on inference compute cost-quality tradeoff curves. Our results suggest that for each bfloat16 ResNet model, there are quantized models with lower cost and higher accuracy; in other words, the bfloat16 compute cost-quality tradeoff curve is Pareto-dominated by the 4-bit and 8-bit curves, with models primarily quantized to 4-bit yielding the best Pareto curve. Furthermore, we achieve state-of-the-art results on ImageNet for 4-bit ResNet-50 with quantization-aware training, obtaining a top-1 eval accuracy of 77.09%. We demonstrate the regularizing effect of quantization by measuring the generalization gap. The quantization method we used is optimized for practicality: It requires little tuning and is designed with hardware capabilities in mind. Our work motivates further research into optimal numeric formats for quantization, as well as the development of machine learning accelerators supporting these formats. As part of this work, we contribute a quantization library written in JAX, which is open-sourced at https://github.com/google-research/google-research/tree/master/aqt.\n\n**Published:** 2021-05-07\n\n\n\n#### 5. Selecting Informative Contexts Improves Language Model Finetuning\n\n*From Search Query: test-time training efficiency*\n\n*Nicole Beckage, Richard Antonello, Javier Turek, Alexander Huth*\n\n**Abstract:** Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a test metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning -- we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.\n\n**Published:** 2020-05-01\n\n\n\n#### 6. Learning to Remember Rare Events\n\n*From Search Query: test-time training efficiency*\n\n*Ofir Nachum, \u0141ukasz Kaiser, Samy Bengio, Aurko Roy*\n\n**Abstract:** Despite recent advances, memory-augmented deep neural networks are still\nlimited when it comes to life-long and one-shot learning, especially in\nremembering rare events. We present a large-scale life-long memory module for\nuse in deep learning. The module exploits fast nearest-neighbor algorithms for\nefficiency and thus scales to large memory sizes. Except for the\nnearest-neighbor query, the module is fully differentiable and trained\nend-to-end with no extra supervision. It operates in a life-long manner, i.e.,\nwithout the need to reset it during training.\n  Our memory module can be easily added to any part of a supervised neural\nnetwork. To show its versatility we add it to a number of networks, from simple\nconvolutional ones tested on image classification to deep sequence-to-sequence\nand recurrent-convolutional models. In all cases, the enhanced network gains\nthe ability to remember and do life-long one-shot learning. Our module\nremembers training examples shown many thousands of steps in the past and it\ncan successfully generalize from them. We set new state-of-the-art for one-shot\nlearning on the Omniglot dataset and demonstrate, for the first time, life-long\none-shot learning in recurrent neural networks on a large-scale machine\ntranslation task.\n\n**Published:** 2017-03-09\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models, focusing on adaptive attention mechanisms, memory compression, efficient test-time training, and dynamic scaling, here are some relevant findings and directions:\n\n## Adaptive Attention Mechanisms\n- **Dynamic Attention Scaling**: Research on adaptive attention mechanisms that adjust based on input complexity is crucial. For instance, the concept of \"Gated Linear Attention Transformers\" could be relevant, as it involves hardware-efficient training methods that might be adaptable to dynamic attention scaling.\n- **Comparison with Existing Methods**: Studies like \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\" might provide insights into how adaptive attention compares to other existing methods, especially in the context of test-time learning and expressive hidden states.\n\n## Memory Compression Techniques\n- **Selective Memory Compression**: The AdaptiveTTT proposal's emphasis on selective memory compression for efficient state management is key. Research on \"RWKV: Reinventing RNNs for the Transformer Era\" could offer insights into memory-efficient designs, although it may not specifically focus on compression, it does discuss efficient state management in RNNs adapted for the transformer era.\n- **Trade-offs Between Compression and Expressiveness**: Investigating papers that discuss the balance between memory compression and model expressiveness is essential. For example, studies on multimodal large language models might provide insights into how compression techniques affect model performance and expressiveness.\n\n## Efficient Implementations of Test-Time Training\n- **Hybrid Update Mechanism**: The hybrid update mechanism combining test-time training (TTT) adaptation with efficient computation is a novel aspect. Research on \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\" and similar works can help in understanding how to efficiently implement test-time training while maintaining model performance.\n- **Efficiency Metrics and Benchmarks**: Papers that focus on the efficiency metrics and benchmarks of test-time training, such as those discussed in the context of machine translation and other tasks, can provide valuable insights into optimizing these mechanisms.\n\n## Combining Multiple Attention Mechanisms\n- **Hybrid Attention Mechanisms**: Studies on combining different attention mechanisms, such as the \"Gated Linear Attention Transformers,\" can help in designing more robust and efficient attention systems. These works often discuss how to integrate multiple attention types to achieve better performance and efficiency.\n\n## Dynamic Scaling of Attention Patterns\n- **Scalability and Efficiency**: The work on \"Simplifying, Stabilizing & Scaling Continuous-Time Consistency\" although focused on continuous-time models, discusses scaling techniques that could be analogous to dynamic scaling of attention patterns. This includes techniques like adaptive weighting and normalization, which could be adapted to attention mechanisms in language models.\n- **Adaptive Group Normalization**: The use of adaptive group normalization within network architectures, as discussed in the context of continuous-time models, could be a valuable technique for dynamically scaling attention patterns while maintaining stability and efficiency.\n\n### Additional Directions\n- **Multimodal Large Language Models**: Surveys on multimodal large language models, such as \"The (R)Evolution of Multimodal Large Language Models: A Survey,\" can provide insights into various techniques for efficient and scalable model design, including attention mechanisms and memory management.\n- **Efficient Multimodal Models**: Papers like \"Efficient Multimodal Large Language Models: A Survey\" can offer a comprehensive overview of techniques aimed at improving efficiency and scalability in large language models, which can be applied to autoregressive models.\n\nBy exploring these areas, the researchers can gather the necessary information to improve the design of autoregressive language models, focusing on adaptive attention, memory compression, efficient test-time training, and dynamic scaling.\n",
            "analysis": "Initial Analysis of AdaptiveTTT Proposal\n\n1. Summary of Analysis:\nThe AdaptiveTTT proposal presents an interesting fusion of TTTLinear and FastTTTLinear, introducing three key innovations:\n- Adaptive attention scaling based on input complexity\n- Selective memory compression for efficient state management\n- Hybrid update mechanism combining TTT's adaptation with efficient computation\n\nKey aspects requiring investigation:\na) Novelty and effectiveness of adaptive attention scaling\nb) Comparison with existing memory compression techniques\nc) Impact of hybrid update mechanism on model performance\nd) Relationship to existing adaptive attention mechanisms\ne) Efficiency gains from selective compression\n\n2. Comparison with Previous Designs:\n- Differs from HierTTT sibling by focusing on adaptive scaling rather than hierarchical processing\n- More emphasis on memory efficiency compared to CompressGPT\n- Different approach to hardware optimization than RWKV6_ScanKernel\n- Unique combination of adaptive attention and memory compression\n\n3. Key Research Questions:\na) How does adaptive attention scaling compare to existing methods?\nb) What are the trade-offs between compression and model expressiveness?\nc) How does the hybrid update mechanism affect training stability?\nd) What is the computational overhead of complexity estimation?\n\n4. Future Search Plan:\nRound 1:\n- Focus on adaptive attention mechanisms and dynamic scaling\n- Investigate memory compression in language models\n- Research test-time training with compressed states\n\nRound 2:\n- Deep dive into specific implementation techniques\n- Study efficiency metrics and benchmarks\n- Examine stability analysis methods\n\n5. References:\n[1] Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n[2] Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n[3] Peng, B., et al. (2023). \"RWKV: Reinventing RNNs for the Transformer Era\""
        },
        {
            "ready": false,
            "query": "adaptive attention stability neural networks, selective compression language models, hardware efficient dynamic attention",
            "detail": "Search for papers discussing:\n1. Theoretical analysis and stability guarantees for adaptive attention mechanisms\n2. Modern selective compression techniques for neural network states and activations\n3. Hardware-efficient implementations of dynamic attention mechanisms\n4. Integration of compression with test-time training\n5. Performance impact of adaptive mechanisms on downstream tasks\nFocus on papers from 2022-2024 that provide theoretical analysis or empirical evaluations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Theoretical analysis and stability guarantees for adaptive attention mechanisms\n2. Modern selective compression techniques for neural network states and activations\n3. Hardware-efficient implementations of dynamic attention mechanisms\n4. Integration of compression with test-time training\n5. Performance impact of adaptive mechanisms on downstream tasks\nFocus on papers from 2022-2024 that provide theoretical analysis or empirical evaluations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.82)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.82)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.18)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.18)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.12)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.12)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.08)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.08)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 5. Latent Attention for Linear Time Transformers (Avg. Score: 0.07)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.07)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectifier Neural Networks. In JMLR Workshop and Conference Proceedings, pp. 315-323, 2011. Gokaslan, A. and Cohen, V. OpenWebText Corpus, 2019. URL http://Skylion007.github.io/ OpenWebTextCorpus. Gu, A., Goel, K., and R\u00e9, C. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Hutter, M. The Human Knowledge Compression Prize, 2002. URL https:// www.kurzweilai.net/hutter-prizefor-lossless-compression-of-humanknowledge. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning, pp. 4651-4664. PMLR, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Khan, S., Naseer, M., Hayat, M., Zamir, S.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive attention stability neural networks, selective compression language models, hardware efficient dynamic attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Towards Deep Attention in Graph Neural Networks: Problems and Remedies\n\n*From Search Query: adaptive attention stability neural networks*\n\n*Soo Yong Lee, Fanchen Bu, Jaemin Yoo, Kijung Shin*\n\n**TL;DR:** AERO-GNN provably mitigates the proposed problems of deep graph attention, which is further empirically demonstrated with (a) its adaptive and less smooth attention functions and (b) higher performance at deep layers (up to 64).\n\n**Abstract:** Graph neural networks (GNNs) learn the representation of graph-structured data, and their expressiveness can be further enhanced by inferring node relations for propagation. Attention-based GNNs infer neighbor importance to manipulate the weight of its propagation. Despite their popularity, the discussion on deep graph attention and its unique challenges has been limited. In this work, we investigate some problematic phenomena related to deep graph attention, including vulnerability to over-smoothed features and smooth cumulative attention. Through theoretical and empirical analyses, we show that various attention-based GNNs suffer from these problems. Motivated by our findings, we propose AEROGNN, a novel GNN architecture designed for deep graph attention. AERO-GNN provably mitigates the proposed problems of deep graph attention, which is further empirically demonstrated with (a) its adaptive and less smooth attention functions and (b) higher performance at deep layers (up to 64). On 9 out of 12 node classification benchmarks, AERO-GNN outperforms the baseline GNNs, highlighting the advantages of deep graph attention. Our code is available at https://github.com/syleeheal/AERO-GNN.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 20  (*Influential: 1*)\n\n#### 2. Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks\n\n*From Search Query: adaptive attention stability neural networks*\n\n*Qi Xu, Yuyuan Gao, Jiangrong Shen, Yaxin Li, Xuming Ran, Huajin Tang, Gang Pan*\n\n**TL;DR:** This paper develops a recurrent spiking neural network model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns.\n\n**Abstract:** Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 3. Pruning Deep Neural Networks from a Sparsity Perspective\n\n*From Search Query: adaptive attention stability neural networks*\n\n*Enmao Diao, G. Wang, Jiawei Zhan, Yuhong Yang, Jie Ding, V. Tarokh*\n\n**TL;DR:** The proposed PQ Index (PQI) is proposed to measure the potential compressibility of deep neural networks and used to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm that is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.\n\n**Abstract:** In recent years, deep network pruning has attracted significant attention in order to enable the rapid deployment of AI into small devices with computation and memory constraints. Pruning is often achieved by dropping redundant weights, neurons, or layers of a deep network while attempting to retain a comparable test performance. Many deep pruning algorithms have been proposed with impressive empirical success. However, existing approaches lack a quantifiable measure to estimate the compressibility of a sub-network during each pruning iteration and thus may under-prune or over-prune the model. In this work, we propose PQ Index (PQI) to measure the potential compressibility of deep neural networks and use this to develop a Sparsity-informed Adaptive Pruning (SAP) algorithm. Our extensive experiments corroborate the hypothesis that for a generic pruning procedure, PQI decreases first when a large model is being effectively regularized and then increases when its compressibility reaches a limit that appears to correspond to the beginning of underfitting. Subsequently, PQI decreases again when the model collapse and significant deterioration in the performance of the model start to occur. Additionally, our experiments demonstrate that the proposed adaptive pruning algorithm with proper choice of hyper-parameters is superior to the iterative pruning algorithms such as the lottery ticket-based pruning methods, in terms of both compression efficiency and robustness.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 4. Concise and Precise Context Compression for Tool-Using Language Models\n\n*From Search Query: selective compression language models*\n\n*Yang Xu, ylfeng, Honglin Mu, Yutai Hou, Yitong Li, Xinghao Wang, Wanjun Zhong, Zhongyang Li, Dandan Tu, Qingfu Zhu, Min Zhang, Wanxiang Che*\n\n**TL;DR:** This work proposes two strategies for compressing tool documentation into concise and precise summary sequences for tool-using language models that reach a performance comparable to the upper-bound baseline under up to 16x compression ratio.\n\n**Abstract:** Through reading the documentation in the context, tool-using language models can dynamically extend their capability using external tools. The cost is that we have to input lengthy documentation every time the model needs to use the tool, occupying the input window as well as slowing down the decoding process. Given the progress in general-purpose compression, soft context compression is a suitable approach to alleviate the problem. However, when compressing tool documentation, existing methods suffer from the weaknesses of key information loss (specifically, tool/parameter name errors) and difficulty in adjusting the length of compressed sequences based on documentation lengths. To address these problems, we propose two strategies for compressing tool documentation into concise and precise summary sequences for tool-using language models. 1) Selective compression strategy mitigates key information loss by deliberately retaining key information as raw text tokens. 2) Block compression strategy involves dividing tool documentation into short chunks and then employing a fixed-length compression model to achieve variable-length compression. This strategy facilitates the flexible adjustment of the compression ratio. Results on API-Bank and APIBench show that our approach reaches a performance comparable to the upper-bound baseline under up to 16x compression ratio.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction\n\n*From Search Query: selective compression language models*\n\n*Pratyusha Sharma, Jordan T. Ash, Dipendra Misra*\n\n**TL;DR:** It is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices, and this simple intervention is called LAyer-SElective Rank reduction (LASER).\n\n**Abstract:** Transformer-based Large Language Models (LLMs) have become a fixture in modern machine learning. Correspondingly, significant resources are allocated towards research that aims to further advance this technology, typically resulting in models of increasing size that are trained on increasing amounts of data. This work, however, demonstrates the surprising result that it is often possible to significantly improve the performance of LLMs by selectively removing higher-order components of their weight matrices. This simple intervention, which we call LAyer-SElective Rank reduction (LASER), can be done on a model after training has completed, and requires no additional parameters or data. We show extensive experiments demonstrating the generality of this finding across language models and datasets, and provide in-depth analyses offering insights into both when LASER is effective and the mechanism by which it operates.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 58  (*Influential: 3*)\n\n#### 6. LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\n\n*From Search Query: selective compression language models*\n\n*Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo Zhao*\n\n**TL;DR:** LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix, which significantly outperforms existing compression methods.\n\n**Abstract:** Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 42  (*Influential: 4*)\n\n#### 7. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification\n\n*From Search Query: hardware efficient dynamic attention*\n\n*Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh*\n\n**TL;DR:** A dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input and an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens is proposed.\n\n**Abstract:** Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31%~37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 528  (*Influential: 103*)\n\n#### 8. ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs\n\n*From Search Query: hardware efficient dynamic attention*\n\n*Yang Bai, Wenqian Zhao, Shuo Yin, Zixiao Wang, Bei Yu*\n\n**TL;DR:** ATFormer is presented, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space and can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks.\n\n**Abstract:** The training and inference ef\ufb01ciency of ever-larger deep neural networks highly rely on the performance of tensor operators on speci\ufb01c hardware platforms. Therefore, a compilation-based optimization \ufb02ow with automatic tensor generation and parameter tuning is necessary for ef\ufb01cient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, AT-Former with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 9. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient dynamic attention*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n### 5 related papers from Papers with Code\n\n#### 1. Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism\n\n*From Search Query: adaptive attention stability neural networks*\n\n*Ulisses Braga-Neto, Levi McClenny*\n\n**Abstract:** Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of \"stiff\" PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. In addition, we show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.\n\n**Published:** 2020-09-07\n\n\n\n#### 2. Deep Transfer Learning Based Downlink Channel Prediction for FDD Massive MIMO Systems\n\n*From Search Query: adaptive attention stability neural networks*\n\n*Feifei Gao, Yuwen Yang, Bo Ai, Zhimeng Zhong, Ahmed Alkhateeb*\n\n**Abstract:** Artificial intelligence (AI) based downlink channel state information (CSI) prediction for frequency division duplexing (FDD) massive multiple-input multiple-output (MIMO) systems has attracted growing attention recently. However, existing works focus on the downlink CSI prediction for the users under a given environment and is hard to adapt to users in new environment especially when labeled data is limited. To address this issue, we formulate the downlink channel prediction as a deep transfer learning (DTL) problem, where each learning task aims to predict the downlink CSI from the uplink CSI for one single environment. Specifically, we develop the direct-transfer algorithm based on the fully-connected neural network architecture, where the network is trained on the data from all previous environments in the manner of classical deep learning and is then fine-tuned for new environments. To further improve the transfer efficiency, we propose the meta-learning algorithm that trains the network by alternating inner-task and across-task updates and then adapts to a new environment with a small number of labeled data. Simulation results show that the direct-transfer algorithm achieves better performance than the deep learning algorithm, which implies that the transfer learning benefits the downlink channel prediction in new environments. Moreover, the meta-learning algorithm significantly outperforms the direct-transfer algorithm in terms of both prediction accuracy and stability, which validates its effectiveness and superiority.\n\n**Published:** 2019-12-27\n\n\n\n#### 3. DynaBERT: Dynamic BERT with Adaptive Width and Depth\n\n*From Search Query: selective compression language models*\n\n*Xiao Chen, Lifeng Shang, Zhiqi Huang, Qun Liu, Lu Hou, Xin Jiang*\n\n**Abstract:** The pre-trained language models like BERT, though powerful in many natural language processing tasks, are both computation and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually compress the large BERT model to a fixed smaller size. They can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can flexibly adjust the size and latency by selecting adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allowing both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT-base (or RoBERTa-base), while at smaller widths and depths consistently outperforms existing BERT compression methods. Code is available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-04-08\n\n\n\n#### 4. ERNIE-Tiny : A Progressive Distillation Framework for Pretrained Transformer Compression\n\n*From Search Query: selective compression language models*\n\n*Haifeng Wang, Hua Wu, Hao Tian, Yu Sun, Weixin Liu, Jiaxiang Liu, Shikun Feng, Xuyi Chen, Weiyue Su*\n\n**Abstract:** Pretrained language models (PLMs) such as BERT adopt a training paradigm which first pretrain the model in general data and then finetune the model on task-specific data, and have recently achieved great success. However, PLMs are notorious for their enormous parameters and hard to be deployed on real-life applications. Knowledge distillation has been prevailing to address this problem by transferring knowledge from a large teacher to a much smaller student over a set of data. We argue that the selection of thee three key components, namely teacher, training data, and learning objective, is crucial to the effectiveness of distillation. We, therefore, propose a four-stage progressive distillation framework ERNIE-Tiny to compress PLM, which varies the three components gradually from general level to task-specific level. Specifically, the first stage, General Distillation, performs distillation with guidance from pretrained teacher, gerenal data and latent distillation loss. Then, General-Enhanced Distillation changes teacher model from pretrained teacher to finetuned teacher. After that, Task-Adaptive Distillation shifts training data from general data to task-specific data. In the end, Task-Specific Distillation, adds two additional losses, namely Soft-Label and Hard-Label loss onto the last stage. Empirical results demonstrate the effectiveness of our framework and generalization gain brought by ERNIE-Tiny.In particular, experiments show that a 4-layer ERNIE-Tiny maintains over 98.0%performance of its 12-layer teacher BERT base on GLUE benchmark, surpassing state-of-the-art (SOTA) by 1.0% GLUE score with the same amount of parameters. Moreover, ERNIE-Tiny achieves a new compression SOTA on five Chinese NLP tasks, outperforming BERT base by 0.4% accuracy with 7.5x fewer parameters and9.4x faster inference speed.\n\n**Published:** 2021-06-04\n\n\n\n#### 5. Slide-Transformer: Hierarchical Vision Transformer with Local Self-Attention\n\n*From Search Query: hardware efficient dynamic attention*\n\n*Gao Huang, Shiji Song, Zhuofan Xia, Tianzhu Ye, Xuran Pan*\n\n**Abstract:** Self-attention mechanism has been a key factor in the recent progress of Vision Transformer (ViT), which enables adaptive feature extraction from global contexts. However, existing self-attention methods either adopt sparse global attention or window attention to reduce the computation complexity, which may compromise the local feature learning or subject to some handcrafted designs. In contrast, local attention, which restricts the receptive field of each query to its own neighboring pixels, enjoys the benefits of both convolution and self-attention, namely local inductive bias and dynamic feature selection. Nevertheless, current local attention modules either use inefficient Im2Col function or rely on specific CUDA kernels that are hard to generalize to devices without CUDA support. In this paper, we propose a novel local attention module, Slide Attention, which leverages common convolution operations to achieve high efficiency, flexibility and generalizability. Specifically, we first re-interpret the column-based Im2Col function from a new row-based perspective and use Depthwise Convolution as an efficient substitution. On this basis, we propose a deformed shifting module based on the re-parameterization technique, which further relaxes the fixed key/value positions to deformed features in the local region. In this way, our module realizes the local attention paradigm in both efficient and flexible manner. Extensive experiments show that our slide attention module is applicable to a variety of advanced Vision Transformer models and compatible with various hardware devices, and achieves consistently improved performances on comprehensive benchmarks. Code is available at https://github.com/LeapLabTHU/Slide-Transformer.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-04-09\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models, particularly focusing on adaptive attention mechanisms, selective compression, hardware-efficient implementations, and integration with test-time training, here are some key points and references from the provided context and additional insights:\n\n## Theoretical Analysis and Stability Guarantees for Adaptive Attention Mechanisms\n\n- While the provided sources do not directly address the theoretical stability guarantees of adaptive attention mechanisms, research in this area often involves analyzing the convergence and stability of attention weights. For instance, studies on Graph Transformers (GTs) highlight the challenges of self-attention mechanisms introducing noise and the need for recalibrating attention distributions to reduce this noise. This is relevant as it suggests that modifying the self-attention module can improve stability and performance.\n\n## Modern Selective Compression Techniques for Neural Network States and Activations\n\n- The EXACT method mentioned in the analysis summary is a good example of modern approaches to activation compression. It reduces the memory footprint of activations by up to 32\u00d7, which is crucial for efficient training and inference.\n- Another approach could involve using techniques like those discussed in the context of tabular deep learning, where numerical feature embeddings and retrieval-based models demonstrate efficient use of resources. Although these are not directly about compression, they show how efficient representations can be achieved.\n\n## Hardware-Efficient Implementations of Dynamic Attention Mechanisms\n\n- The Graph Transformer (GT) variants discussed in the literature provide insights into making attention mechanisms more hardware-efficient. For example, NodeFormer and SGFormer reduce the computational complexity of self-attention from quadratic to linear, which is essential for large-scale graphs and potentially applicable to language models.\n- The use of single-layer, single-head global attention models, as in SGFormer, can significantly reduce computational complexity, making dynamic attention more feasible on hardware.\n\n## Integration of Compression with Test-Time Training\n\n- There is a need for more specific research on integrating compression techniques with test-time training. However, the concept of sequential test-time training (TTT) and adaptive gating in Mixture-of-Experts (MoE) models suggests that dynamic adjustments during test-time can be beneficial. Integrating compression techniques like EXACT into these frameworks could enhance efficiency without compromising performance.\n\n## Performance Impact of Adaptive Mechanisms on Downstream Tasks\n\n- The performance impact of adaptive mechanisms is often evaluated through empirical studies. For instance, adaptive gating in MoE models has shown promising results in terms of dynamic computation allocation, which can improve performance on downstream tasks by efficiently utilizing computational resources.\n- The use of numerical feature embeddings and retrieval-based models in tabular deep learning has demonstrated consistent performance improvements across various benchmarks, suggesting that similar adaptive mechanisms could be beneficial in language models.\n\n### Additional Considerations\n\n- **Stability and Convergence**: Research on Graph Transformers highlights the importance of recalibrating self-attention distributions to reduce noise and improve stability. Similar techniques could be applied to language models to ensure stable and convergent adaptive attention mechanisms.\n- **Memory-Computation Trade-offs**: Techniques like EXACT and the GT variants show how to balance memory and computational efficiency. These trade-offs are crucial for designing scalable and efficient language models.\n- **Real-World Deployment**: For real-world deployment, considerations such as the use of ensemble models, as discussed in the malware classification context, can provide robustness and stability. This can be particularly useful when integrating adaptive mechanisms and compression techniques into production environments.\n\nBy focusing on these areas and leveraging insights from recent research in related fields, the researchers can develop more efficient, scalable, and robust autoregressive language models.\n",
            "analysis": "### Analysis Summary\n\nThe initial search results provide valuable insights into the key components of the AdaptiveTTT proposal, particularly around adaptive attention mechanisms, memory compression, and test-time training. However, more specific information is needed in certain areas.\n\n### Key Findings\n\n1. Adaptive Attention & Scaling:\n- H2O's approach to dynamic token importance (Zhang et al., 2023) demonstrates the viability of adaptive mechanisms in LLMs\n- Adaptive gating in MoE (Li et al., 2023) shows promising results for dynamic computation allocation\n- Need more information on attention scaling stability and convergence\n\n2. Memory Compression:\n- Deep Compression (Han et al., 2015) provides foundational techniques but needs modern context\n- EXACT (Liu et al., 2022) shows promising results for activation compression\n- Need more research on dynamic/selective compression techniques\n\n3. Test-Time Training:\n- Sequential TTT (Su et al., 2022) provides important insights on realistic TTT protocols\n- InfoNet (Hu et al., 2024) demonstrates efficient neural estimation without test-time optimization\n- Need more information on hybrid update mechanisms\n\n### Useful References & Excerpts\n\n1. From H2O paper:\n\"Our approach is based on the noteworthy observation that a small portion of tokens contributes most of the value when computing attention scores... removing them results in significant performance degradation.\"\n\n2. From Adaptive Gating in MoE:\n\"This paper introduces adaptive gating in MoE, a flexible training strategy that allows tokens to be processed by a variable number of experts based on expert probability distribution.\"\n\n3. From EXACT:\n\"We provide an optimized GPU implementation which supports training with compressed activations... can reduce the memory footprint of activations by up to 32\u00d7\"\n\n### Future Search Needs\n\n1. Specific queries needed for:\n- Stability analysis of adaptive attention mechanisms\n- Modern approaches to selective state compression\n- Integration of TTT with compressed representations\n- Hardware efficiency of dynamic attention mechanisms\n\n2. Areas requiring deeper investigation:\n- Theoretical foundations of adaptive scaling\n- Impact on downstream task performance\n- Memory-computation trade-offs\n- Real-world deployment considerations\n\n### References\n\n1. Zhang, Z., et al. (2023). \"H2O: Heavy-Hitter Oracle for Efficient Generative Inference of Large Language Models.\" Neural Information Processing Systems.\n\n2. Li, J., et al. (2023). \"Adaptive Gating in Mixture-of-Experts based Language Models.\" Conference on Empirical Methods in Natural Language Processing.\n\n3. Liu, Z., et al. (2022). \"EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression.\" International Conference on Learning Representations.\n\n4. Han, S., et al. (2015). \"Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding.\" International Conference on Learning Representations.\n\n5. Su, Y., et al. (2022). \"Revisiting Realistic Test-Time Training: Sequential Inference and Adaptation by Anchored Clustering.\" Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "test-time training stability language models, theoretical guarantees adaptive attention, memory efficient state updates neural networks",
            "detail": "Find papers discussing:\n1. Stability analysis and theoretical guarantees for test-time training in language models\n2. Memory-efficient state update mechanisms for adaptive neural networks\n3. Convergence properties of adaptive attention mechanisms\n4. Hardware optimization techniques for dynamic neural architectures\n5. Integration of compression with online adaptation\nFocus on papers from 2022-2024 that provide theoretical analysis or empirical evaluations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Stability analysis and theoretical guarantees for test-time training in language models\n2. Memory-efficient state update mechanisms for adaptive neural networks\n3. Convergence properties of adaptive attention mechanisms\n4. Hardware optimization techniques for dynamic neural architectures\n5. Integration of compression with online adaptation\nFocus on papers from 2022-2024 that provide theoretical analysis or empirical evaluations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.84)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.84)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.47)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 33/51 (Score: 0.47)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. [22] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In In Uncertainty in Artificial Intelligence, pages 148-155. Morgan Kaufmann, 1998. [23] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [24] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [25] Xinyang Geng. EasyLM: A Simple And Scalable Training Framework for Large Language Models. https://github.com/young-geng/EasyLM, mar 2023. https://github.com/ young-geng/EasyLM. [26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\u00e0, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020. [29] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.\n```\n\n#### 3. Linearizing Large Language Models (Avg. Score: 0.30)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 10/22 (Score: 0.30)*\n\n```\narXiv preprint arXiv:2402.19427, 2024. emozilla. Dynamically scaled rope further increases strength of retaining walls, 2023. URL https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_ further_increases/. Reddit post. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Alexsandar Botev Griffin Team, Soham De, Samuel L Smith, Anushan Fernando, GeorgeChristian Muraru, Ruba Haroun, and Leonard Berrada et al. Recurrentgemma. arXiv preprint arXiv:2404.07839, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. Suchin Gururangan, Mitchell Wortsman, Samir Yitzhak Gadre, Achal Dave, Maciej Kilian, Weijia Shi, Jean Mercat, Georgios Smyrnis, Gabriel Ilharco, Matt Jordan, Reinhard Heckel, Alex Dimakis, Ali Farhadi, Vaishaal Shankar, and Ludwig Schmidt. OpenLM: a minimal but performative language modeling $(\\mathrm{lm})$ repository, 2023. URL https://github.com/mlfoundations/open_lm/. GitHub repository. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv preprint arXiv:2103.13076, 2021. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.22)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 45/86 (Score: 0.22)*\n\n```\nIn: arXiv preprint arXiv:2311.09431 (2023). [18] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. \"Language Models are Few-shot Learners\". In: Advances in Neural Information Processing Systems (NeurIPS) 33 (2020), pp. 1877-1901. [19] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. \"Rethinking Attention with Performers\". In: The International Conference on Learning Representations (ICLR). 2021. [20] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. \"PaLM: Scaling Language Modeling with Pathways\". In: Journal of Machine Learning Research 24.240 (2023), pp. 1-113. URL: http://jmlr.org/papers/v24/221144.html. [21] Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. \"Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling\". In: arXiv preprint arXiv:1412.3555 (2014). [22] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. \"Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge\". In: arXiv preprint arXiv:1803.05457 (2018). [23] Tri Dao. \"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\". In: The International Conference on Learning Representations (ICLR). 2024. [24] Tri Dao, Beidi Chen, Nimit S Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. \"Monarch: Expressive structured matrices for efficient and accurate training\".\n```\n\n#### 5. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.14)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.14)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: test-time training stability language models, theoretical guarantees adaptive attention, memory efficient state updates neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Test-Time Training on Nearest Neighbors for Large Language Models\n\n*From Search Query: test-time training stability language models*\n\n*Moritz Hardt, Yu Sun*\n\n**TL;DR:** This work establishes a first baseline of test-time training for language modeling by building a large-scale distributed index based on text embeddings of the Pile dataset and fine-tunes the model on retrieved data at test time.\n\n**Abstract:** Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 2. Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models\n\n*From Search Query: test-time training stability language models*\n\n*Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, T. Goldstein, Anima Anandkumar, Chaowei Xiao*\n\n**TL;DR:** Test-time prompt tuning (TPT) is proposed, a method that can learn adaptive prompts on the fly with a single test sample and performs on par with the state-of-the-art approaches that use additional training data.\n\n**Abstract:** Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. For image classification, TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPT performs on par with the state-of-the-art approaches that use additional training data. Project page: https://azshue.github.io/TPT.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 200  (*Influential: 43*)\n\n#### 3. Adaptive Batch Size for Safe Policy Gradients\n\n*From Search Query: theoretical guarantees adaptive attention*\n\n*M. Papini, Matteo Pirotta, Marcello Restelli*\n\n**TL;DR:** This paper proposes a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update.\n\n**Abstract:** Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 35  (*Influential: 1*)\n\n#### 4. Theoretical Guarantees for Variational Inference with Fixed-Variance Mixture of Gaussians\n\n*From Search Query: theoretical guarantees adaptive attention*\n\n*Tom Huix, Anna Korba, Alain Durmus, \u00c9ric Moulines*\n\n**TL;DR:** This work aims to contribute to the theoretical study of VI in the non-Gaussian case by investigating the setting of Mixture of Gaussians with fixed covariance and constant weights, and study two sources of error of variational inference in this context when optimizing the mollified relative entropy.\n\n**Abstract:** Variational inference (VI) is a popular approach in Bayesian inference, that looks for the best approximation of the posterior distribution within a parametric family, minimizing a loss that is typically the (reverse) Kullback-Leibler (KL) divergence. Despite its empirical success, the theoretical properties of VI have only received attention recently, and mostly when the parametric family is the one of Gaussians. This work aims to contribute to the theoretical study of VI in the non-Gaussian case by investigating the setting of Mixture of Gaussians with fixed covariance and constant weights. In this view, VI over this specific family can be casted as the minimization of a Mollified relative entropy, i.e. the KL between the convolution (with respect to a Gaussian kernel) of an atomic measure supported on Diracs, and the target distribution. The support of the atomic measure corresponds to the localization of the Gaussian components. Hence, solving variational inference becomes equivalent to optimizing the positions of the Diracs (the particles), which can be done through gradient descent and takes the form of an interacting particle system. We study two sources of error of variational inference in this context when optimizing the mollified relative entropy. The first one is an optimization result, that is a descent lemma establishing that the algorithm decreases the objective at each iteration. The second one is an approximation error, that upper bounds the objective between an optimal finite mixture and the target distribution.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 5. REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates\n\n*From Search Query: memory efficient state updates neural networks*\n\n*Arshia Afzal, Grigorios G. Chrysos, V. Cevher, Mahsa Shoaran*\n\n**TL;DR:** This paper introduces a novel graph-based residual state update mechanism (REST) for real-time EEG signal analysis in applications such as epileptic seizure detection, and achieves a remarkable 9-fold acceleration in inference speed compared to state-of-the-art models.\n\n**Abstract:** EEG-based seizure detection models face challenges in terms of inference speed and memory efficiency, limiting their real-time implementation in clinical devices. This paper introduces a novel graph-based residual state update mechanism (REST) for real-time EEG signal analysis in applications such as epileptic seizure detection. By leveraging a combination of graph neural networks and recurrent structures, REST efficiently captures both non-Euclidean geometry and temporal dependencies within EEG data. Our model demonstrates high accuracy in both seizure detection and classification tasks. Notably, REST achieves a remarkable 9-fold acceleration in inference speed compared to state-of-the-art models, while simultaneously demanding substantially less memory than the smallest model employed for this task. These attributes position REST as a promising candidate for real-time implementation in clinical devices, such as Responsive Neurostimulation or seizure alert systems.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations\n\n*From Search Query: memory efficient state updates neural networks*\n\n*Steffen Schotth\u00f6fer, Emanuele Zangrando, J. Kusch, Gianluca Ceruti, Francesco Tudisco*\n\n**TL;DR:** A novel algorithm to find efficient low-rank subnetworks that are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced.\n\n**Abstract:** Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to find efficient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efficiency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 32  (*Influential: 0*)\n\n#### 7. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: memory efficient state updates neural networks*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Global disease monitoring and forecasting with Wikipedia\n\n*From Search Query: test-time training stability language models*\n\n*Alina Deshpande, Nicholas Generous, Reid Priedhorsky, Sara Y. Del Valle, Geoffrey Fairchild*\n\n**Abstract:** Infectious disease is a leading threat to public health, economic stability,\nand other key social structures. Efforts to mitigate these impacts depend on\naccurate and timely monitoring to measure the risk and progress of disease.\nTraditional, biologically-focused monitoring techniques are accurate but costly\nand slow; in response, new techniques based on social internet data such as\nsocial media and search queries are emerging. These efforts are promising, but\nimportant challenges in the areas of scientific peer review, breadth of\ndiseases and countries, and forecasting hamper their operational usefulness.\n  We examine a freely available, open data source for this use: access logs\nfrom the online encyclopedia Wikipedia. Using linear models, language as a\nproxy for location, and a systematic yet simple article selection procedure, we\ntested 14 location-disease combinations and demonstrate that these data\nfeasibly support an approach that overcomes these challenges. Specifically, our\nproof-of-concept yields models with $r^2$ up to 0.92, forecasting value up to\nthe 28 days tested, and several pairs of models similar enough to suggest that\ntransferring models from one location to another without re-training is\nfeasible.\n  Based on these preliminary results, we close with a research agenda designed\nto overcome these challenges and produce a disease monitoring and forecasting\nsystem that is significantly more effective, robust, and globally comprehensive\nthan the current state of the art.\n\n**Published:** 2014-05-14\n\n\n\n#### 2. Data Feedback Loops: Model-driven Amplification of Dataset Biases\n\n*From Search Query: test-time training stability language models*\n\n*Tatsunori B. Hashimoto, Rohan Taori*\n\n**Abstract:** Datasets scraped from the internet have been critical to the successes of large-scale machine learning. Yet, this very success puts the utility of future internet-derived datasets at potential risk, as model outputs begin to replace human annotations as a source of supervision. In this work, we first formalize a system where interactions with one model are recorded as history and scraped as training data in the future. We then analyze its stability over time by tracking changes to a test-time bias statistic (e.g. gender bias of model predictions). We find that the degree of bias amplification is closely linked to whether the model's outputs behave like samples from the training distribution, a behavior which we characterize and define as consistent calibration. Experiments in three conditional prediction scenarios - image classification, visual role-labeling, and language generation - demonstrate that models that exhibit a sampling-like behavior are more calibrated and thus more stable. Based on this insight, we propose an intervention to help calibrate and stabilize unstable feedback systems. Code is available at https://github.com/rtaori/data_feedback.\n\n**Published:** 2022-09-08\n\n\n\n#### 3. Sequential Attention for Feature Selection\n\n*From Search Query: theoretical guarantees adaptive attention*\n\n*Vahab Mirrokni, Gang Fu, Matthew Fahrbach, Lin Chen, Mohammadhossein Bateni, Taisuke Yasuda*\n\n**Abstract:** Feature selection is the problem of selecting a subset of features for a machine learning model that maximizes model quality subject to a budget constraint. For neural networks, prior methods, including those based on $\\ell_1$ regularization, attention, and other techniques, typically select the entire feature subset in one evaluation round, ignoring the residual value of features during selection, i.e., the marginal contribution of a feature given that other features have already been selected. We propose a feature selection algorithm called Sequential Attention that achieves state-of-the-art empirical results for neural networks. This algorithm is based on an efficient one-pass implementation of greedy forward selection and uses attention weights at each step as a proxy for feature importance. We give theoretical insights into our algorithm for linear regression by showing that an adaptation to this setting is equivalent to the classical Orthogonal Matching Pursuit (OMP) algorithm, and thus inherits all of its provable guarantees. Our theoretical and empirical analyses offer new explanations towards the effectiveness of attention and its connections to overparameterization, which may be of independent interest.\n\n**Published:** 2022-09-29\n\n\n\n#### 4. Certified Adversarial Robustness with Additive Noise\n\n*From Search Query: theoretical guarantees adaptive attention*\n\n*Wenlin Wang, Lawrence Carin, Changyou Chen, Bai Li*\n\n**Abstract:** The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defensive models has been considered, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.\n\n**Conference:** certified-adversarial-robustness-with\n\n**Published:** 2018-09-10\n\n\n\n#### 5. Efficient Training of Retrieval Models using Negative Cache\n\n*From Search Query: memory efficient state updates neural networks*\n\n*Sanjiv Kumar, Ruiqi Guo, Sashank Reddi, Erik Lindgren*\n\n**Abstract:** Factorized models, such as two tower neural network models, are widely used for scoring (query, document) pairs in information retrieval tasks. These models are typically trained by optimizing the model parameters to score relevant positive\" pairs higher than the irrelevantnegative\" ones. While a large set of negatives typically improves the model performance, limited computation and memory budgets place constraints on the number of negatives used during training. In this paper, we develop a novel negative sampling technique for accelerating training with softmax cross-entropy loss. By using cached (possibly stale) item embeddings, our technique enables training with a large pool of negatives with reduced memory and computation. We also develop a streaming variant of our algorithm geared towards very large datasets. Furthermore, we establish a theoretical basis for our approach by showing that updating a very small fraction of the cache at each iteration can still ensure fast convergence. Finally, we experimentally validate our approach and show that it is efficient and compares favorably with more complex, state-of-the-art approaches.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-12-01\n\n\n\n#### 6. ROLAND: Graph Learning Framework for Dynamic Graphs\n\n*From Search Query: memory efficient state updates neural networks*\n\n*Jure Leskovec, Tianyu Du, Jiaxuan You*\n\n**Abstract:** Graph Neural Networks (GNNs) have been successfully applied to many real-world static graphs. However, the success of static graphs has not fully translated to dynamic graphs due to the limitations in model design, evaluation settings, and training strategies. Concretely, existing dynamic GNNs do not incorporate state-of-the-art designs from static GNNs, which limits their performance. Current evaluation settings for dynamic GNNs do not fully reflect the evolving nature of dynamic graphs. Finally, commonly used training methods for dynamic GNNs are not scalable. Here we propose ROLAND, an effective graph representation learning framework for real-world dynamic graphs. At its core, the ROLAND framework can help researchers easily repurpose any static GNN to dynamic graphs. Our insight is to view the node embeddings at different GNN layers as hierarchical node states and then recurrently update them over time. We then introduce a live-update evaluation setting for dynamic graphs that mimics real-world use cases, where GNNs are making predictions and being updated on a rolling basis. Finally, we propose a scalable and efficient training approach for dynamic GNNs via incremental training and meta-learning. We conduct experiments over eight different dynamic graph datasets on future link prediction tasks. Models built using the ROLAND framework achieve on average 62.7% relative mean reciprocal rank (MRR) improvement over state-of-the-art baselines under the standard evaluation settings on three datasets. We find state-of-the-art baselines experience out-of-memory errors for larger datasets, while ROLAND can easily scale to dynamic graphs with 56 million edges. After re-implementing these baselines using the ROLAND training strategy, ROLAND models still achieve on average 15.5% relative MRR improvement over the baselines.\n\n**Published:** 2022-08-15\n\n\n\n\n\n---\n## Web search results\n\n To address the researcher's goals and the detailed requirements, here are some key points and potential directions based on the provided analysis and the need for more specific information:\n\n## Stability Analysis and Theoretical Guarantees for Test-Time Training\n\n- While the provided sources do not directly address test-time training stability, research in continuous-time formulations and stabilization techniques could offer insights. For instance, the work on continuous-time Consistency Models (CMs) in discusses stabilization techniques, such as tangent normalization and adaptive group normalization, which could be adapted or inspire similar approaches for test-time training in language models. However, specific papers focusing on test-time training stability in language models are needed.\n\n## Memory-Efficient State Update Mechanisms\n\n- The concept of memory-efficient state updates can be informed by compression techniques like LoSparse, which combines low-rank and sparse approximation (though this reference is not directly cited, the idea is mentioned in the analysis). Papers like \"LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\" by Li et al. (2023) provide valuable insights into structured compression methods that could be adapted for state updates during inference.\n\n## Convergence Properties of Adaptive Attention Mechanisms\n\n- For adaptive attention mechanisms, the work on Gated Linear Attention (GLA) Transformers by Yang et al. (2023) is relevant. GLA shows promising results for efficient attention computation and length generalization, but more research is needed to establish theoretical guarantees and convergence properties. Investigating papers that analyze the convergence of adaptive attention mechanisms, such as those using stochastic gradient descent or other optimization methods, would be beneficial.\n\n## Hardware Optimization Techniques for Dynamic Neural Architectures\n\n- Hardware-aware optimizations are crucial for dynamic neural architectures. Papers like \"Gated Linear Attention Transformers with Hardware-Efficient Training\" by Yang et al. (2023) and \"ATFormer\" (though not directly cited, the concept is mentioned in the analysis) provide insights into hardware-aware tensor program optimization. These works can guide the implementation of efficient and optimized dynamic mechanisms on specific hardware.\n\n## Integration of Compression with Online Adaptation\n\n- Integrating compression with online adaptation is key. The \"Concise and Precise Context Compression for Tool-Using Language Models\" by Xu et al. (2024) discusses selective compression strategies that mitigate key information loss, which can be applied during online adaptation. This approach ensures that the model retains critical information while compressing less important data, which is essential for memory-efficient state updates during inference.\n\n### Future Search Directions\n\nTo further address the researcher's goals, here are some specific search directions:\n\n1. **Test-Time Training Stability**:\n   - Search for papers that specifically analyze the stability of test-time training in language models, focusing on theoretical guarantees and empirical evaluations from 2022-2024.\n\n2. **Memory-Efficient State Updates**:\n   - Look for papers that discuss adaptive state update mechanisms, particularly those that integrate compression techniques like low-rank and sparse approximations.\n\n3. **Convergence Properties of Adaptive Attention**:\n   - Investigate papers that provide theoretical analysis or empirical evaluations of the convergence properties of adaptive attention mechanisms in language models.\n\n4. **Hardware Optimization**:\n   - Search for papers that focus on hardware-aware optimizations for dynamic neural architectures, including tensor program optimization and memory access patterns.\n\n5. **Integration of Compression and Online Adaptation**:\n   - Find papers that discuss the integration of compression techniques with online adaptation in language models, ensuring that the compressed states do not compromise model performance.\n\n### Example Search Queries\n\n- \"test-time training stability language models 2022-2024\"\n- \"memory-efficient state updates neural networks 2022-2024\"\n- \"convergence properties adaptive attention mechanisms 2022-2024\"\n- \"hardware optimization dynamic neural architectures 2022-2024\"\n- \"integration compression online adaptation language models 2022-2024\"\n\nBy focusing on these specific areas and using these search queries, the researcher can gather more targeted information to improve the autoregressive language model design.\n",
            "analysis": "### Analysis Summary\n\nThe search results provide valuable insights into adaptive attention mechanisms, compression techniques, and hardware efficiency, but more specific information is needed about test-time training integration and theoretical stability guarantees.\n\n### Key Findings\n\n1. Adaptive Attention & Dynamic Mechanisms:\n- GLA (Gated Linear Attention) shows promising results for efficient attention computation with linear complexity\n- DynamicViT demonstrates successful token sparsification with minimal accuracy loss\n- Need more information on stability guarantees and convergence properties\n\n2. Compression & Memory Efficiency:\n- LoSparse combines low-rank and sparse approximation effectively\n- Concise context compression shows 16x compression while maintaining performance\n- LASER demonstrates improvement through selective rank reduction\n- Need more details on state compression during inference\n\n3. Hardware Efficiency:\n- FLASHLINEARATTENTION implementation shows speed improvements over FlashAttention-2\n- ATFormer provides insights into hardware-aware tensor program optimization\n- More investigation needed on hardware-specific optimizations for adaptive mechanisms\n\n### Useful References & Excerpts\n\n1. From Yang et al. (2023) on GLA:\n\"GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\n2. From Li et al. (2023) on LoSparse:\n\"Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons.\"\n\n3. From Xu et al. (2024) on context compression:\n\"Selective compression strategy mitigates key information loss by deliberately retaining key information as raw text tokens.\"\n\n### Key Insights for Design\n\n1. Adaptive Mechanisms:\n- Combine GLA's efficient attention with dynamic token selection\n- Consider hierarchical compression strategies\n- Need to investigate stability guarantees\n\n2. Memory Management:\n- Use selective compression based on token importance\n- Implement adaptive state updates\n- Consider hardware-aware memory access patterns\n\n3. Integration Strategy:\n- Need more information on test-time training with compressed states\n- Investigate stability of adaptive updates\n- Research efficient implementation techniques\n\n### Future Search Plan\n\nNeed additional searches focusing on:\n1. Test-time training mechanisms and stability\n2. Theoretical guarantees for adaptive attention\n3. Memory-efficient state updates in language models\n4. Hardware optimization techniques for dynamic mechanisms\n\n### References\n\n1. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. ICML 2023.\n\n2. Li, Y., Yu, Y., Zhang, Q., Liang, C., He, P., Chen, W., & Zhao, T. (2023). LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. ICML 2023.\n\n3. Xu, Y., et al. (2024). Concise and Precise Context Compression for Tool-Using Language Models. ACL 2024.\n\n4. Sharma, P., Ash, J.T., & Misra, D. (2023). The Truth is in There: Improving Reasoning in Language Models with Layer-Selective Rank Reduction. ICLR 2023.\n\n5. Rao, Y., et al. (2021). DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. NeurIPS 2021."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.6490800000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.718695,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "# AdaptiveTTT: Combining Test-Time Training with Adaptive Memory and Attention\n\n## Motivation\n\nCurrent language models face significant challenges in efficiently processing long sequences while maintaining expressiveness and adaptability. The parent designs, TTTLinear and FastTTTLinear, each address different aspects of this challenge:\n\n- TTTLinear excels in test-time adaptation and expressive hidden states\n- FastTTTLinear provides efficient attention mechanisms and improved numerical stability\n\nThe motivation behind AdaptiveTTT is to create a synergistic combination that preserves the strengths of both parents while introducing adaptive mechanisms for better efficiency and scalability.\n\n## Problem Analysis\n\n### Parent Design Analysis\n\nTTTLinear strengths:\n- Test-time training capability\n- Expressive hidden states\n- Strong performance on long sequences\n- Effective state updates\n\nTTTLinear limitations:\n- High memory usage\n- Computational overhead\n- Fixed attention patterns\n\nFastTTTLinear strengths:\n- Efficient gated linear attention\n- Improved numerical stability\n- Hardware-efficient implementation\n- Local convolutional augmentation\n\nFastTTTLinear limitations:\n- Less expressive than full TTT\n- Fixed memory allocation\n- Limited adaptation capability\n\n### Recombination Strategy\n\nThe key insight is that we can preserve TTTLinear's adaptability while incorporating FastTTTLinear's efficiency by:\n\n1. Introducing adaptive attention patterns that dynamically adjust based on input characteristics\n2. Implementing selective memory compression for efficient state management\n3. Maintaining test-time training capability through modified update rules\n4. Integrating hardware-efficient implementations from FastTTTLinear\n\n## Core Idea and Philosophy\n\nAdaptiveTTT introduces three key innovations:\n\n1. **Adaptive Attention Scaling**: Dynamically adjusts attention patterns based on input complexity and computational resources\n2. **Selective Memory Compression**: Efficiently manages state information through importance-based compression\n3. **Hybrid Update Mechanism**: Combines TTT's adaptive learning with FastTTTLinear's efficient computation\n\n## Design Plan\n\n### 1. Adaptive Attention Mechanism\n\nIntroduce a dynamic scaling factor \u03b1(x) for attention computation:\n\n```python\ndef compute_attention(Q, K, V, x):\n    # Compute input complexity\n    complexity = estimate_complexity(x)\n    \n    # Dynamic scaling factor\n    alpha = compute_scaling_factor(complexity)\n    \n    # Scaled attention computation\n    attention = scaled_dot_product(Q * alpha, K, V)\n    return attention\n```\n\n### 2. Selective Memory Compression\n\nImplement importance-based state compression:\n\n```python\ndef compress_state(state, importance_threshold):\n    # Compute state importance scores\n    scores = compute_importance_scores(state)\n    \n    # Select important components\n    mask = scores > importance_threshold\n    \n    # Compress state\n    compressed_state = apply_compression(state, mask)\n    return compressed_state\n```\n\n### 3. Hybrid Update Rules\n\nCombine TTT's adaptation with efficient computation:\n\n```python\ndef hybrid_update(x, state, learning_rate):\n    # Compute gradients efficiently\n    grads = compute_efficient_gradients(x, state)\n    \n    # Apply adaptive updates\n    new_state = apply_adaptive_update(state, grads, learning_rate)\n    \n    # Compress if needed\n    if needs_compression(new_state):\n        new_state = compress_state(new_state)\n    \n    return new_state\n```\n\n## Implementation Guidelines\n\n### Core Components\n\n1. **AdaptiveAttention Module**:\n```python\nclass AdaptiveAttention(nn.Module):\n    def __init__(self, dim, num_heads):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        self.qkv = nn.Linear(dim, dim * 3)\n        self.complexity_estimator = ComplexityEstimator()\n        \n    def forward(self, x):\n        qkv = self.qkv(x)\n        q, k, v = qkv.chunk(3, dim=-1)\n        \n        # Compute dynamic scaling\n        complexity = self.complexity_estimator(x)\n        alpha = self.compute_scaling(complexity)\n        \n        # Apply scaled attention\n        attn = (q * alpha) @ k.transpose(-2, -1)\n        out = attn @ v\n        return out\n```\n\n2. **SelectiveCompression Module**:\n```python\nclass SelectiveCompression(nn.Module):\n    def __init__(self, dim, compression_ratio):\n        super().__init__()\n        self.dim = dim\n        self.compression_ratio = compression_ratio\n        self.importance_net = nn.Linear(dim, 1)\n        \n    def forward(self, state):\n        # Compute importance scores\n        scores = self.importance_net(state)\n        \n        # Select top-k components\n        k = int(state.size(1) * self.compression_ratio)\n        mask = torch.topk(scores, k, dim=1)[1]\n        \n        # Apply compression\n        compressed = state.gather(1, mask.expand(-1, -1, state.size(-1)))\n        return compressed\n```\n\n3. **HybridUpdate Module**:\n```python\nclass HybridUpdate(nn.Module):\n    def __init__(self, dim, learning_rate):\n        super().__init__()\n        self.dim = dim\n        self.learning_rate = learning_rate\n        self.update_net = nn.Linear(dim * 2, dim)\n        \n    def forward(self, x, state):\n        # Compute efficient gradients\n        grads = self.compute_grads(x, state)\n        \n        # Apply adaptive update\n        combined = torch.cat([state, grads], dim=-1)\n        update = self.update_net(combined)\n        new_state = state + self.learning_rate * update\n        \n        return new_state\n```\n\n### Integration Steps\n\n1. Initialize the model with adaptive components:\n```python\nclass AdaptiveTTT(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.attention = AdaptiveAttention(config.dim, config.num_heads)\n        self.compression = SelectiveCompression(config.dim, config.compression_ratio)\n        self.update = HybridUpdate(config.dim, config.learning_rate)\n```\n\n2. Forward pass implementation:\n```python\ndef forward(self, x):\n    # Process input through attention\n    attn_out = self.attention(x)\n    \n    # Update state\n    new_state = self.update(x, self.state)\n    \n    # Apply compression if needed\n    if self.should_compress(new_state):\n        new_state = self.compression(new_state)\n    \n    self.state = new_state\n    return attn_out\n```\n\n## Theoretical Analysis\n\n### Complexity Analysis\n\n- Time Complexity: O(N * d) where N is sequence length and d is dimension\n- Space Complexity: O(d) for constant memory usage during generation\n- Adaptive Scaling: O(1) overhead for complexity estimation\n\n### Stability Analysis\n\nThe design ensures stability through:\n1. Bounded attention scaling\n2. Gradient-controlled updates\n3. Selective compression with importance thresholds\n\n## Expected Benefits\n\n1. **Improved Efficiency**:\n- Reduced memory usage through selective compression\n- Adaptive computation based on input complexity\n- Hardware-efficient implementation\n\n2. **Enhanced Adaptability**:\n- Preserved test-time training capability\n- Dynamic attention patterns\n- Efficient state updates\n\n3. **Better Scalability**:\n- Linear complexity in sequence length\n- Constant memory usage during generation\n- Efficient handling of long sequences\n\n## References\n\n1. Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n2. Fu, T., et al. (2024). \"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression\"\n3. Arora, S., et al. (2024). \"Simple linear attention language models balance the recall-throughput tradeoff\"\n4. Zhai, S., et al. (2023). \"Stabilizing Transformer Training by Preventing Attention Entropy Collapse\"\n5. Wang, N., et al. (2022). \"ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer\"",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel enhancement of TTTLinear that introduces adaptive attention patterns and selective memory optimization. The design combines the test-time training capabilities of TTTLinear with the efficient gated mechanisms of FastTTTLinear, while introducing dynamic memory management and adaptive attention scaling. This approach enables more efficient processing of long sequences while maintaining model expressiveness through selective compression and adaptive attention patterns.",
    "ideation": null,
    "modelname": "AdaptiveTTT",
    "suggestions": "1. Develop more rigorous theoretical analysis of stability guarantees for adaptive attention mechanisms.\n2. Provide detailed analysis of memory-computation trade-offs under different scenarios.\n3. Include more specific guidelines for tuning adaptive mechanisms.\n4. Add concrete metrics for measuring compression effectiveness.\n5. Elaborate on handling edge cases in adaptive scaling.\n6. Consider adding ablation study designs to validate each component's contribution.\n7. Include more detailed hardware optimization guidelines.\n8. Develop specific criteria for when to trigger compression.",
    "user_input": ""
}