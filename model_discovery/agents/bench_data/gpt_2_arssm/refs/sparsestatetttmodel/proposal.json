{
    "variantname": "SparseStateTTT",
    "review": "The SparseStateTTT proposal presents an innovative approach to enhancing FastTTTLinear by integrating State Space Models (SSMs) with sparse attention mechanisms. After comprehensive analysis of the proposal and comparison with existing research, here is a detailed review:\n\n1. CLARITY (4.2/5):\nThe proposal is well-structured and clearly articulated, with:\n+ Clear objectives and motivation for integrating SSMs with sparse attention\n+ Detailed mathematical formulations and implementation guidelines\n+ Well-defined architectural components and their interactions\nMinor weakness: Some implementation details regarding state management could be more specific.\n\n2. INNOVATION (4.0/5):\nThe proposal demonstrates significant innovation through:\n+ Novel integration of SSMs with sparse attention mechanisms\n+ Hierarchical processing structure for efficient sequence handling\n+ Adaptive computation through state space modeling\nHowever:\n- Some concepts overlap with existing work (e.g., Mamba, SPADE)\n- Sparse attention techniques share similarities with SPARSEK and CHELA\n\n3. FEASIBILITY (3.8/5):\nImplementation appears challenging but feasible:\n+ Clear mathematical formulations\n+ Detailed implementation guidelines\n+ Integration with existing frameworks\nConcerns:\n- Complex integration of multiple sophisticated components\n- Training stability with hybrid architecture\n- Memory management complexity\n\n4. SCALABILITY (4.3/5):\nThe design shows strong potential for scaling:\n+ Linear complexity through SSM integration\n+ Efficient memory usage via sparse attention\n+ Hierarchical processing for long sequences\nMinor concerns:\n- Memory bandwidth requirements for very large models\n- Communication overhead in distributed settings\n\n5. ACCURACY AND ROBUSTNESS (4.0/5):\nExpected improvements in performance:\n+ Better handling of long-range dependencies through SSMs\n+ Enhanced local processing via sparse attention\n+ Improved adaptability through hierarchical structure\nConcerns:\n- Potential impact on model expressiveness\n- Training stability with multiple mechanisms\n- Trade-offs between sparsity and accuracy\n\n6. EFFICIENCY (4.2/5):\nStrong focus on computational efficiency:\n+ Linear complexity through SSM integration\n+ Reduced memory footprint via sparse attention\n+ Efficient hierarchical processing\n+ Hardware-aware design considerations\n\nCOMPARATIVE ANALYSIS:\nThe proposal differs significantly from existing approaches:\n- More efficient than standard FastTTTLinear\n- Novel combination of SSMs and sparse attention\n- Unique hierarchical processing strategy\n- Different from sibling proposals (compressivehierarchicalttt, compressivememorygpt)\n\nTECHNICAL DEPTH:\nThe proposal demonstrates strong technical understanding:\n+ Comprehensive mathematical formulations\n+ Detailed implementation guidelines\n+ Thorough consideration of architectural components\n+ Clear analysis of trade-offs",
    "search_stack": [
        {
            "ready": false,
            "query": "sparse attention mechanisms",
            "detail": "state space models for long-sequence processing",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nstate space models for long-sequence processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Simplified State Space Layers for Sequence Modeling (Avg. Score: 1.00)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 1.00)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n#### 2. SMR: State Memory Replay for Long Sequence Modeling (Avg. Score: 1.00)\n\n*Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points.\n\n**Abstract:** Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.\n\n##### *Relevant Chunk: No. 1/24 (Score: 1.00)*\n\n```\n# SMR: State Memory Replay for Long Sequence Modeling \n\nBiqing $\\mathbf{Q i}^{1 \\mathbf{1 , 2 , 4 , *}}$, Junqi Gao ${ }^{\\mathbf{3}, *}$, Kaiyan Zhang ${ }^{\\mathbf{2}}$, Dong $\\mathbf{L i}^{\\mathbf{3}}$, Jianxing Liu ${ }^{\\mathbf{1}}$, Ligang $\\mathbf{W u}^{\\mathbf{1 , \\dagger}}$, Bowen $\\mathbf{Z h o u}{ }^{2, \\dagger}$<br>${ }^{1}$ Department of Control Science and Engineering, Harbin Institute of Technology,<br>${ }^{2}$ Department of Electronic Engineering, Tsinghua University,<br>${ }^{3}$ School of Mathematics, Harbin Institute of Technology,<br>${ }^{4}$ Frontis.AI, Beijing<br>\\{qibiqing7,gjunqi97, arvinlee826\\}@gmail.com,<br>zhang-ky22@mails.tsinghua.edu.cn, \\{jx.liu,ligangwu\\}@hit.edu.cn, zhoubowen@tsinghua.edu.cn\n\n\n#### Abstract\n\nDespite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 1.00)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 1.00)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 4. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 1.00)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 1.00)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 5. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 1.00)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 9 related papers from 3 external sources\n\n\n\nYour 1 raw search queries input to the search frame: sparse attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 3 related papers from Semantic Scholar\n\n#### 1. Prototype memory and attention mechanisms for few shot image generation\n\n*From Search Query: sparse attention mechanisms*\n\n*Tianqin Li, Zijie Li, Andrew Luo, Harold Rockwell, A. Farimani, T. Lee*\n\n**TL;DR:** The results demonstrate the feasibility of the idea that these super-sparse complex feature detectors of macaque monkeys can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Abstract:** Recent discoveries indicate that the neural codes in the super\ufb01cial layers of the primary visual cortex (V1) of macaque monkeys are complex, diverse and super-sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing during the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized through a memory-based attention operation. Integrating this mechanism, we propose Memory Concept Attention ( MoCA ) to improve few shot image generation quality. We show that having a prototype memory with attention mechanisms can improve image synthesis quality, learn interpretable visual concept clusters, and improve the robustness of the model. Our results demonstrate the feasibility of the idea that these super-sparse complex feature detectors can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 2. Inductive Biases and Variable Creation in Self-Attention Mechanisms\n\n*From Search Query: sparse attention mechanisms*\n\n*Benjamin L. Edelman, Surbhi Goel, S. Kakade, Cyril Zhang*\n\n**TL;DR:** The main result shows that bounded-norm Transformer networks\"create sparse variables\": a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length.\n\n**Abstract:** Self-attention, an architectural motif designed to model long-range interactions in sequential data, has driven numerous recent breakthroughs in natural language processing and beyond. This work provides a theoretical analysis of the inductive biases of self-attention modules. Our focus is to rigorously establish which functions and long-range dependencies self-attention blocks prefer to represent. Our main result shows that bounded-norm Transformer networks\"create sparse variables\": a single self-attention head can represent a sparse function of the input sequence, with sample complexity scaling only logarithmically with the context length. To support our analysis, we present synthetic experiments to probe the sample complexity of learning sparse Boolean functions with Transformers.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 100  (*Influential: 10*)\n\n#### 3. Exphormer: Sparse Transformers for Graphs\n\n*From Search Query: sparse attention mechanisms*\n\n*Hamed Shirzad, A. Velingker, B. Venkatachalam, Danica J. Sutherland, A. Sinop*\n\n**TL;DR:** It is shown that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets.\n\n**Abstract:** Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that Exphormer can scale to datasets on larger graphs than shown in previous graph transformer architectures. Code can be found at \\url{https://github.com/hamed1375/Exphormer}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 67  (*Influential: 15*)\n\n### 3 related papers from ArXiv\n\n#### 1. SPECTRA: Sparse Structured Text Rationalization\n\n*From Search Query: sparse attention mechanisms*\n\n*Nuno Miguel Guerreiro, Andr\u00e9 F. T. Martins*\n\n**Abstract:** Selective rationalization aims to produce decisions along with rationales\n(e.g., text highlights or word alignments between two sentences). Commonly,\nrationales are modeled as stochastic binary masks, requiring sampling-based\ngradient estimators, which complicates training and requires careful\nhyperparameter tuning. Sparse attention mechanisms are a deterministic\nalternative, but they lack a way to regularize the rationale extraction (e.g.,\nto control the sparsity of a text highlight or the number of alignments). In\nthis paper, we present a unified framework for deterministic extraction of\nstructured explanations via constrained inference on a factor graph, forming a\ndifferentiable layer. Our approach greatly eases training and rationale\nregularization, generally outperforming previous work on what comes to\nperformance and plausibility of the extracted rationales. We further provide a\ncomparative study of stochastic and deterministic methods for rationale\nextraction for classification and natural language inference tasks, jointly\nassessing their predictive power, quality of the explanations, and model\nvariability.\n\n**Published:** 2021-09-09T20:39:56Z  (*Updated: 2021-09-09T20:39:56Z*)\n\n\n\n#### 2. Sparse Attention with Linear Units\n\n*From Search Query: sparse attention mechanisms*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n\n**Published:** 2021-04-14T17:52:38Z  (*Updated: 2021-10-06T14:04:59Z*)\n\n\n\n#### 3. Sparser is Faster and Less is More: Efficient Sparse Attention for\n  Long-Range Transformers\n\n*From Search Query: sparse attention mechanisms*\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers,\nespecially within an extended context window, poses significant challenges due\nto the quadratic computational complexity and substantial KV memory\nrequirements inherent in self-attention mechanisms. In this work, we introduce\nSPARSEK Attention, a novel sparse attention mechanism designed to overcome\nthese computational and memory obstacles while maintaining performance. Our\napproach integrates a scoring network and a differentiable top-k mask operator,\nSPARSEK, to select a constant number of KV pairs for each query, thereby\nenabling gradient-based optimization. As a result, SPARSEK Attention offers\nlinear time complexity and constant memory footprint during generation.\nExperimental results reveal that SPARSEK Attention outperforms previous sparse\nattention methods and provides significant speed improvements during both\ntraining and inference, particularly in language modeling and downstream tasks.\nFurthermore, our method can be seamlessly integrated into pre-trained Large\nLanguage Models (LLMs) with minimal fine-tuning, offering a practical solution\nfor effectively managing long-range dependencies in diverse applications.\n\n**Published:** 2024-06-24T15:55:59Z  (*Updated: 2024-06-24T15:55:59Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. From Softmax to Sparsemax: A Sparse Model of Attention and Multi-Label Classification\n\n*From Search Query: sparse attention mechanisms*\n\n*Ram\u00f3n Fernandez Astudillo, Andr\u00e9 F. T. Martins*\n\n**Abstract:** We propose sparsemax, a new activation function similar to the traditional\nsoftmax, but able to output sparse probabilities. After deriving its\nproperties, we show how its Jacobian can be efficiently computed, enabling its\nuse in a network trained with backpropagation. Then, we propose a new smooth\nand convex loss function which is the sparsemax analogue of the logistic loss.\nWe reveal an unexpected connection between this new loss and the Huber\nclassification loss. We obtain promising empirical results in multi-label\nclassification problems and in attention-based neural networks for natural\nlanguage inference. For the latter, we achieve a similar performance as the\ntraditional softmax, but with a selective, more compact, attention focus.\n\n**Published:** 2016-02-05\n\n\n\n#### 2. A Regularized Framework for Sparse and Structured Neural Attention\n\n*From Search Query: sparse attention mechanisms*\n\n*Mathieu Blondel, Vlad Niculae*\n\n**Abstract:** Modern neural networks are often augmented with an attention mechanism, which\ntells the network where to focus within the input. We propose in this paper a\nnew framework for sparse and structured attention, building upon a smoothed max\noperator. We show that the gradient of this operator defines a mapping from\nreal values to probabilities, suitable as an attention mechanism. Our framework\nincludes softmax and a slight generalization of the recently-proposed sparsemax\nas special cases. However, we also show how our framework can incorporate\nmodern structured penalties, resulting in more interpretable attention\nmechanisms, that focus on entire segments or groups of an input. We derive\nefficient algorithms to compute the forward and backward passes of our\nattention mechanisms, enabling their use in a neural network trained with\nbackpropagation. To showcase their potential as a drop-in replacement for\nexisting ones, we evaluate our attention mechanisms on three large-scale tasks:\ntextual entailment, machine translation, and sentence summarization. Our\nattention mechanisms improve interpretability without sacrificing performance;\nnotably, on textual entailment and summarization, we outperform the standard\nattention mechanisms based on softmax and sparsemax.\n\n**Conference:** a-regularized-framework-for-sparse-and-1\n\n**Published:** 2017-05-22\n\n\n\n#### 3. Sparse and Continuous Attention Mechanisms\n\n*From Search Query: sparse attention mechanisms*\n\n*M\u00e1rio A. T. Figueiredo, Ant\u00f3nio Farinhas, Marcos Treviso, Andr\u00e9 F. T. Martins, Vlad Niculae, Pedro M. Q. Aguiar*\n\n**Abstract:** Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-12\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on sparse attention mechanisms and state space models for long-sequence processing, here are some key findings and suggestions based on the provided sources and analysis:\n\n## Sparse Attention Mechanisms\n\n### HSR-Enhanced Sparse Attention Acceleration\nThis approach introduces a method to accelerate attention computation in Large Language Models (LLMs) by leveraging the inherent sparsity within attention mechanisms. It uses a Half-Space Reporting (HSR) data structure to identify non-zero or \"massively activated\" entries in the attention matrix, significantly reducing the running time complexity. This method is particularly beneficial for long-context scenarios and maintains minimal error, making it a viable option for improving efficiency in handling long sequences.\n\n### SeerAttention\nSeerAttention is a new attention mechanism that learns intrinsic sparse attention rather than relying on predefined patterns or heuristics. It uses a learnable gate to adaptively select significant blocks in the attention map, effectively balancing accuracy and speedup. This mechanism excels in both post-training and long-context fine-tuning, achieving high sparsity ratios with minimal perplexity loss and significant speedup over traditional methods.\n\n### Differential Transformer\nThe Differential Transformer addresses the issue of attention noise in traditional Transformers by using a differential attention mechanism. This mechanism partitions the query and key vectors into two groups and computes two separate softmax attention maps, then uses the difference between these maps as the attention score. While not explicitly sparse, this method helps focus attention on relevant parts of the input, reducing the impact of irrelevant context and improving performance in long-context scenarios.\n\n## State Space Models and Hierarchical Processing\n\n### ConvSSM\nCombining tensor modeling ideas with state space methods, ConvSSM achieves subquadratic parallelization and fast autoregressive generation. This approach can inform hierarchical processing and memory management strategies, which are crucial for the proposed HierarchicalFastTTTLinear design. Integrating such state space models could enhance the model's ability to handle long sequences efficiently - though this source does not directly mention ConvSSM, the concept is aligned with the analysis note.\n\n### Integration with Hierarchical Processing\nThe hierarchical multi-scale architecture in HierarchicalFastTTTLinear can benefit from sparse attention mechanisms like those in SeerAttention or HSR-Enhanced Sparse Attention. These mechanisms can be integrated into the hierarchical framework to optimize memory usage and computational efficiency. For example, using SeerAttention's block-level sparsity could enhance the model's ability to handle long sequences while maintaining expressiveness and adaptability.\n\n## Key Insights and Recommendations\n\n- **Integrate Learnable Sparse Attention**: Incorporate mechanisms like SeerAttention or HSR-Enhanced Sparse Attention into the hierarchical processing framework to optimize memory and computational efficiency without sacrificing accuracy.\n- **Combine with State Space Models**: Utilize state space models such as ConvSSM to enhance hierarchical processing and memory management, allowing for more efficient handling of long sequences.\n- **Differential Attention for Noise Reduction**: Consider the Differential Transformer's approach to reduce attention noise and improve focus on relevant context, which can be complementary to sparse attention mechanisms.\n\nBy integrating these sparse attention mechanisms and state space models, the HierarchicalFastTTTLinear design can be significantly improved in terms of efficiency, scalability, and performance in handling long sequences.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe seed design, HierarchicalFastTTTLinear, aims to enhance FastTTTLinear by introducing a hierarchical multi-scale architecture, temporal latent bottlenecks, and adaptive state compression. The primary goal is to improve the model's efficiency and scalability in handling long sequences while maintaining or enhancing its expressiveness and adaptability during test-time training. The design is innovative but complex, requiring careful consideration of integration and training dynamics.\n\n#### 2. All Useful References with Excerpts\n- **ConvSSM**: Combines tensor modeling ideas of ConvLSTM with state space methods like S4 and S5, achieving subquadratic parallelization and fast autoregressive generation. This approach could inform hierarchical processing and memory management strategies.\n- **Linformer**: Demonstrates that self-attention can be approximated by a low-rank matrix, reducing complexity from O(n^2) to O(n). This concept could be applied to improve memory efficiency and computational complexity in the proposed design.\n- **SampleAttention**: Proposes an adaptive structured sparse attention mechanism that reduces latency and maintains accuracy. This could inspire efficient memory management and sparse processing strategies.\n\n#### 3. Key Insights and Detailed Analysis\n- **Hierarchical Processing**: ConvSSM's approach to combining convolutional and state space methods could provide insights into implementing hierarchical multi-scale processing in the proposed design.\n- **Memory Efficiency**: Linformer's low-rank approximation strategy could be adapted to enhance memory efficiency in HierarchicalFastTTTLinear, especially in handling long sequences.\n- **Sparse Attention**: SampleAttention's adaptive sparse attention mechanism could be integrated to optimize memory usage and computational efficiency without sacrificing accuracy.\n\n#### 4. Future Search Plan\n- **Explore Sparse Attention Mechanisms**: Investigate more on sparse attention techniques that can be integrated into the hierarchical processing framework.\n- **Investigate State Space Models**: Look into state space models that can be combined with hierarchical processing for improved long-sequence handling.\n\n#### 5. List of References\n1. Smith, J. T. H., et al. (2023). *Convolutional State Space Models for Long-Range Spatiotemporal Modeling*. Neural Information Processing Systems.\n2. Wang, S., et al. (2020). *Linformer: Self-Attention with Linear Complexity*. arXiv.org.\n3. Zhu, Q., et al. (2024). *Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention*. arXiv.org."
        },
        {
            "ready": false,
            "query": "integration of SSMs and sparse attention",
            "detail": "methods for combining state space models with sparse attention",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nmethods for combining state space models with sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.99)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 2/41 (Score: 0.99)*\n\n```\nIn this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. ## 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges $[1,20,19]$, owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods $[39,61]$ allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually $\\operatorname{cost} O(n \\log n)$ to maintain clusters. Ainslie et al. [1]\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_81cbe08ae077927ce965g-02.jpg?height=549&width=1261&top_left_y=254&top_left_x=432)\n\nFigure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query ( $\\tau(\\mathbf{u})$ ) such that the sum of normalized scores is $k$, which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency. incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SofTTOPK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows. ## Incremental KV Selection. The SPARSEK operator (\u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (\u00a73.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments\non language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. ## 2 Related Work\n\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence $[17,35,36,12]$ or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions $[15,27,42]$. Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns $[34,13]$, or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].\n```\n\n#### 2. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.99)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 2/35 (Score: 0.99)*\n\n```\nHowever, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State space $\\underline{A} u g m e n t e \\underline{D}$ TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. ## 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair\n\n[^0]of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model ( 250 M parameters) consumes over 80 G of GPU memory when the sequence length is 8 k . Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models' practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a $57.5 \\%$ accuracy, nearly $30 \\%$ less than stateof-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right)$ in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are pro-\nposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models' ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State space $\\underline{\\text { Augmente }} \\underline{\\mathbf{D}}$ TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines.\n```\n\n#### 3. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.99)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.99)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.92)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 0.92)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.91)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 1/38 (Score: 0.91)*\n\n```\n# MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression \n\nTianyu Fu ${ }^{1,2, *}$ Haofeng Huang ${ }^{1,2, *}$, Xuefei Ning ${ }^{1, *}$, Genghan Zhang ${ }^{3}$, Boju Chen ${ }^{1}$,<br>Tianqi Wu ${ }^{1,2}$, Hongyi Wang ${ }^{1,2}$, Zixiao Huang ${ }^{1,2}$, Shiyao $\\mathbf{L i}^{1,2}$,<br>Shengen Yan ${ }^{1,2}$, Guohao Dai ${ }^{2,4}$, Huazhong Yang ${ }^{1}$, Yu Wang ${ }^{1}$<br>${ }^{1}$ Tsinghua University ${ }^{2}$ Infinigence-AI ${ }^{3}$ Stanford University ${ }^{4}$ Shanghai Jiao Tong University\n\n\n#### Abstract\n\nSparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts.\n```\n\n\n\n---\n## Found 0 related papers from 0 external sources\n\n\n\nYour 1 raw search queries input to the search frame: integration of SSMs and sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by integrating state space models (SSMs) with sparse attention mechanisms, here are some key insights and methods from the provided sources and additional relevant information:\n\n## Integration of SSMs and Sparse Attention\n\n### State Space Models (SSMs)\n- SSMs, such as those described in the Mamba architecture, can efficiently handle long sequences by transforming input sequences into latent state representations. The Mamba SSMs architecture eliminates the need for attention layers or multilayer perceptron blocks, reducing computational complexity.\n- The Sparse-Mamba (S-Mamba) model reinforces controllability and observability in state-space equations, which improves performance, reduces parameters, and saves training time. This approach can be particularly beneficial for handling long sequences in language models.\n\n### Sparse Attention Mechanisms\n- Sparse attention mechanisms can significantly reduce computational complexity and memory usage. For example, the use of Half-Space Reporting (HSR) data structures to leverage the sparsity within attention mechanisms can reduce the running time complexity from \\(O(mn)\\) to \\(O(mn^{4/5})\\) for attention generation, as seen in the context of accelerating attention computation in LLMs.\n- Mechanisms like Rectified Linear Attention (ReLA) and SPARSEK Attention achieve sparsity by replacing softmax with ReLU or using a scoring network and differentiable top-k mask operator, respectively. These methods offer efficient training and decoding with high sparsity rates and linear time complexity.\n\n### Combining SSMs and Sparse Attention\n- To integrate SSMs with sparse attention, one could use the SSMs to handle the sequential processing efficiently and then apply sparse attention mechanisms to reduce the computational burden of attention calculations.\n  - For instance, the Mamba SSMs could be used to process the input sequence into a latent state representation, and then sparse attention mechanisms like HSR or SPARSEK could be applied to the latent states to compute attention scores efficiently.\n  - This combination can leverage the strengths of both approaches: the efficient sequence processing of SSMs and the reduced computational complexity of sparse attention.\n\n### Practical Implementations and Empirical Results\n- Empirical experiments on prominent LLMs have shown that leveraging sparsity in attention mechanisms introduces negligible approximation errors and significantly improves running time. Similar experiments could be conducted to validate the integration of SSMs with sparse attention.\n- The S-Mamba model has demonstrated improvements in perplexity and training time by reinforcing controllability and observability. Integrating this with sparse attention could further enhance these metrics.\n\n## Future Search Plan\n\n- **Explore Architectural Designs**: Investigate how different SSM architectures (e.g., Mamba, S5) can be integrated with various sparse attention mechanisms (e.g., HSR, SPARSEK, ReLA) to create a hierarchical and efficient autoregressive language model.\n- **Evaluate Performance Metrics**: Look for empirical results and benchmarks that compare the performance of integrated models against state-of-the-art models in terms of perplexity, accuracy in downstream tasks, robustness, efficiency, and scalability.\n- **Optimization Techniques**: Research optimization techniques that can further enhance the performance of the integrated model, such as fine-tuning the SSM parameters and the sparse attention thresholds to minimize approximation errors and maximize efficiency.\n\nBy combining the efficient sequence processing capabilities of SSMs with the computational efficiency of sparse attention mechanisms, researchers can develop autoregressive language models that are more scalable, efficient, and effective.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe search results provided insights into the use of state space models (SSMs) and sparse attention mechanisms, which are crucial for enhancing the HierarchicalFastTTTLinear design. Key findings include the potential of SSMs like S5 and SMR for efficient long-sequence processing and the benefits of sparse attention mechanisms such as SeerAttention and SPARSEK for improving computational efficiency and memory usage.\n\n#### 2. All Useful References with Excerpts\n- **Simplified State Space Layers for Sequence Modeling**: Introduces the S5 layer, which uses multi-input, multi-output SSMs for efficient parallel scans, achieving state-of-the-art performance on long-range sequence modeling tasks.\n- **SMR: State Memory Replay for Long Sequence Modeling**: Proposes a non-recursive sample processing strategy that stabilizes SSMs by adjusting current states with multi-step information, enhancing generalization at varying sampling points.\n- **Sparse Attention with Linear Units**: Introduces Rectified Linear Attention (ReLA), which achieves sparsity by replacing softmax with ReLU, offering efficient training and decoding with high sparsity rates.\n- **Sparser is Faster and Less is More**: Describes SPARSEK Attention, which uses a scoring network and differentiable top-k mask operator to achieve linear time complexity and constant memory footprint.\n\n#### 3. Key Insights and Detailed Analysis\n- **State Space Models**: The S5 layer's use of efficient parallel scans and multi-input, multi-output SSMs can inform the hierarchical processing strategy in HierarchicalFastTTTLinear, potentially improving its ability to handle long sequences.\n- **Sparse Attention**: Mechanisms like SPARSEK and ReLA offer promising approaches to reduce computational complexity and memory usage, which are critical for the proposed design's efficiency.\n- **Integration Potential**: Combining SSMs with sparse attention mechanisms could provide a robust framework for efficient long-sequence processing, enhancing both scalability and expressiveness.\n\n#### 4. Future Search Plan\n- **Explore Integration Techniques**: Investigate methods for integrating SSMs with sparse attention mechanisms in hierarchical architectures.\n- **Evaluate Practical Implementations**: Look for practical implementations and empirical results of combining these techniques in existing models.\n\n#### 5. List of References\n1. Smith, J. T. H., & Warrington, A., & Linderman, S. W. (2022). *Simplified State Space Layers for Sequence Modeling*. International Conference on Learning Representations.\n2. Qi, B., et al. (2024). *SMR: State Memory Replay for Long Sequence Modeling*. arXiv.org.\n3. Zhang, B., et al. (2021). *Sparse Attention with Linear Units*. arXiv.org.\n4. Lou, C., et al. (2024). *Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers*. arXiv.org."
        },
        {
            "ready": false,
            "query": "architectural designs for SSMs and sparse attention",
            "detail": "performance metrics for integrated models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nperformance metrics for integrated models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. \u221e-former: Infinite Memory Transformer (Avg. Score: 0.06)\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins*\n\n**Published in:** Volume 1 (2022)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 10/32 (Score: 0.06)*\n\n```\nFurther details and hyper-parameters are stated in App. C.3. To evaluate the models we use the metrics: perplexity, F1 score, Rouge-1 and Rouge-L (Lin, 2004), and Meteor (Banerjee and Lavie, 2005). Results. As shown in Table 2, by keeping the whole auxiliary document in memory, the $\\infty$-former and the compressive transformer are able to generate better utterances, according to all metrics. While the compressive and $\\infty$-former achieve essentially the same perplexity in this task, the $\\infty$-former achieves consistently better scores on all other metrics. Also, using sticky memories leads to slightly better results on those metrics, which suggests that attributing a larger space in the LTM to the most relevant tokens can be beneficial.\n```\n\n#### 2. Long-Context Language Modeling with Parallel Context Encoding (Avg. Score: 0.01)\n\n*Howard Yen, Tianyu Gao, Danqi Chen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work introduces Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window, and introduces a CEPE variant that can extend the context window of instruction-tuned models using only unlabeled data.\n\n**Abstract:** Extending large language models (LLMs) to process longer inputs is crucial for a wide range of applications. However, the substantial computational cost of transformers and limited generalization of positional encoding restrict the size of their context window. We introduce Context Expansion with Parallel Encoding (CEPE), a framework that can be applied to any existing decoder-only LLMs to extend their context window. CEPE employs a small encoder to process long inputs chunk by chunk, enabling the frozen decoder to utilize additional contexts via cross-attention. CEPE is efficient, generalizable, and versatile: trained with 8K-token documents, it extends the context window of LLAMA-2 to 128K tokens, offering 10x the throughput with only 1/6 of the memory. CEPE yields strong performance on language modeling and in-context learning. CEPE also excels in retrieval-augmented applications, while existing long-context models degenerate with retrieved contexts. We further introduce a CEPE variant that can extend the context window of instruction-tuned models using only unlabeled data, and showcase its effectiveness on LLAMA-2-CHAT, leading to a strong instruction-following model that can leverage very long contexts on downstream tasks.\n\n##### *Relevant Chunk: No. 36/45 (Score: 0.01)*\n\n```\n(2023) for all datasets. During evaluation, we first calculate the log-likelihood of each option and select the option with the highest likelihood. We sample the in-context learning demonstrations from the training set such that each label has an equal number of demonstrations (except for possible remainders). Furthermore, we first calculate the accuracy for each dataset using four different metrics: likelihood, likelihood normalized for length, calibrated likelihood, and calibrated likelihood normalized for length. We calibrate using Domain Conditional PMI (Holtzman et al., 2021), but use the empty\n\n|  | Stride | ArXiv | Book | PG19 | ProofPile | CodeParrot |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Total Tokens $=8192$ |  |  |  |  |  |  |\n| StReAmingLLM | 1 | 2.823 | 6.381 | 7.817 | 2.522 | 1.848 |\n|  | 2048 | 2.740 | 6.327 | 7.783 | 2.437 | 1.806 |\n\nTable 6: Performance of StreamingLLM with different stride lengths. |  | $k$ | NQ | TQA | PopQA |\n| :--- | :---: | :---: | :---: | :---: |\n|  | 1 | 28.37 | 56.44 | 27.17 |\n| LLAMA-2 | 5 | 31.91 | 61.08 | 33.83 |\n|  | 10 | 32.27 | 62.09 | 34.67 |\n|  | 15 | 31.19 | 61.35 | 33.67 |\n|  | 20 | 30.39 | 60.31 | 31.35 |\n|  | 10 | 30.64 | 56.00 | 32.38 |\n|  | 15 | 31.27 | 56.98 | 33.48 |\n|  | 20 | 31.97 | 57.28 | 33.75 |\n|  | 30 | 30.66 | 57.57 | 33.90 |\n|  | 60 | 30.58 | 57.03 | 34.61 |\n|  | 5 | 31.27 | 61.21 | 32.40 |\n|  | 10 | 31.52 | 61.35 | 32.31 |\n|  | 15 | 30.80 | 60.89 | 31.62 |\n|  | 20 | 30.30 | 60.41 | 31.11 |\n|  | 30 | 29.78 | 59.99 | 30.27 |\n|  | 10 | 32.27 | 62.09 | 34.67 |\n|  | 15 | 33.30 | 62.30 | 34.67 |\n|  | 20 | 33.85 | 62.26 | 34.83 |\n|  | 30 | 33.91 | 62.33 | 34.85 |\n|  | 60 | 34.07 | 62.26 | 34.98 |\n\nTable 7: Open-domain QA results. We report exact match scores for the Natural Questions(NQ) test set, TriviaQA(TQA) validation set, and PopQA test set. All models use two-shot in-context learning. $k$ is the number of retrieved passages, and CEPE uses the top 10 passages in the decoder and all passages in the encoder. string as the domain string for all datasets for simplicity. We then choose the metrics that yield the highest score for the LLAMA-2 model in the twoshot setting and apply the same metrics to all other models. The metrics used for each dataset are shown in Table 8. In this work, we did not investigate how to best calibrate CEPE in ICL settings.\n```\n\n#### 3. Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model (Avg. Score: 0.01)\n\n*Wenbing Li, Hang Zhou, Junqing Yu, Zikai Song, Wei Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Coupled SSM model is proposed, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes, and demonstrates that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n**Abstract:** The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n##### *Relevant Chunk: No. 5/22 (Score: 0.01)*\n\n```\nWe ran five times and reported the average value. We use bold text to show the best results. Traditionally, models that use aligned corpora tend to perform better [3]. In our experiments, we achieve significant improvements on all evaluation metrics compared to unaligned models. Our unaligned method is able to achieve better results even when compared with aligned models. Table 1: Results on CMU-MOSEI. All models are based on language features extracted by BERT. The one with $*$ indicates that the model reproduces under the same conditions. | Model | CMU-MOSEI |  |  |  |  |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n|  | $M A E \\downarrow$ | $C o r r \\uparrow$ | $A c c-2 \\uparrow$ | $F 1-S c o r e \\uparrow$ | Data Setting |\n| TFN [5] | 0.593 | 0.700 | 82.5 | 82.1 | Unaligned |\n| LMF [6] | 0.623 | 0.677 | 82.0 | 82.1 | Unaligned |\n| MFN [38] | - | - | 76.0 | 76.0 | Aligned |\n| MFM [46] | 0.568 | 0.717 | 84.4 | 84.3 | Aligned |\n| MulT [3] | 0.580 | 0.703 | 82.5 | 82.3 | Aligned |\n| MAG-BERT [47] | - | - | 84.7 | 84.5 | Aligned |\n| ICCN [48] | 0.565 | 0.713 | 84.2 | 84.2 | Aligned |\n| MISA [31] | 0.555 | 0.756 | 85.5 | 85.3 | Aligned |\n| TETFN [45] | 0.551 | 0.748 | 85.1 | 85.2 | Unaligned |\n| DMD [44] | - | - | 84.8 | 84.7 | Unaligned |\n| IMDer3 [1] | - | - | 85.1 | 85.1 | Unaligned |\n| MAG-BERT ${ }^{*}[47]$ | 0.549 | 0.753 | 85.2 | 85.1 | Aligned |\n| Coupled Mamba (Ours) | $\\mathbf{0 .\n```\n\n#### 4. Efficient Long-Text Understanding with Short-Text Models (Avg. Score: 0.01)\n\n*Maor Ivgi, Uri Shaham, Jonathan Berant*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2022)\t**Cited by** 49  (*Influential: 5*)\n\n**TL;DR:** This work proposes SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs and finds that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.\n\n**Abstract:** Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.\n\n##### *Relevant Chunk: No. 11/30 (Score: 0.01)*\n\n```\nAnalyzing test scores in Tab. 1, we see that increasing model size consistently improves performance (up to $46 \\%$ exact match), but increasing input length has a negligible effect. Since reported human accuracy on QuALITY is high\n\n[^5](93.5\\%), this hints that QuALITY might require commonsense reasoning and knowledge that are absent from models with a lower parameter count. Summary We have shown that taking offthe-shelf pretrained LMs and embedding them into SLED leads to competitive performance on SCROLLS. Importantly, any future pretrained LM can be easily plugged into SLED, without the need for an expensive pretraining step. ### 5.3 Datasets analysis\n\nSLED's simplicity and modularity allow it to be used as a useful tool for dataset analyses. Specifically, we can vary the chunk size, $c$, and the number of tokens, $n$, across datasets to analyze a) how local are individual pieces of relevant information, and b) how far into the document they are located. Locality of information SLED relies on an assumption that information can be contextualized locally at encoding time. To analyze locality, we vary the chunk size, $c$, which defines the attention window, and measure the effect on SCROLLS datasets with input length 16 K . Fig. 5 shows the results of this experiment, where the y-axis shows the relative improvement compared to $\\mathrm{BART}_{\\text {base }}$ on a target metric as a function of the chunk size $c$ for all datasets. We observe that in all datasets the best performing chunk size is relatively small (up to 256), and further increasing $c$ even hurts the performance in some cases. However, the summarization datasets show a much larger gain in performance when increasing $c$ up to that threshold. This coincides with a common hypothesis that QA and NLI require relatively local context, and thus increasing $c$ can add noise and hurt optimization, while summarization may require a more highlevel view of information. Distance from start of document We now analyze whether indeed the entire document is required for tasks in SCROLLS by varying the maximum document length, $n$. Fig. 6 shows the results of this experiment, where the y-axis shows relative improvement of $\\mathrm{BART}_{\\text {base }}$-SLED compared to $B A R T_{\\text {base }}$ as a function of the first $n$ tokens from the document (chunk size $c=256$ ). As expected, all datasets (except QuALITY) show a roughly monotonic improvement in performance with $n$. This shows that (a) SLED is able to effectively use all of the information in a long se-\n\n| Model | (Chunk/Input) | \\#Params | Avg | GovRep <br> ROUGE-1/2/L | SumScr <br> ROUGE-1/2/L | QMSum <br> ROUGE-1/2/L | Qspr <br> F1 | Nrtv <br> F1 | QALT <br> EM-T/H | CNLI <br> EM |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Development Scores |  |  |  |  |  |  |  |  |  |  |\n| $\\mathrm{LED}_{\\text {base }}$ | $(256 / 16 K)$ | 162 M | - | 57.3/27.9/30.0 | 30.7/6.3/17.9 | 32.5/9.0/21.1 | 30.4 | 20.2 | 30.9 | 82.3 |\n| $\\mathrm{T} 5_{\\text {base }}$ | $(1 K / 1 K)$ | 220 M | - | 32.8/11.7/20.2 | 22.2/3.7/15.3 | 26.1/6.6/19.8 | 13.2 | 14.9 | 35.1 | 76.8 |\n| T5 base -SLED | $(256 / 16 K)$ | 220 M | - | 47.0/20.2/25.2 | 25.3/5.0/16.6 | 29.9/8.7/21.4 | 38.2 | 18.2 | 34.6 | 82.4 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_17_400457f248ef0bd7e237g-08.jpg?height=45&width=114&top_left_y=472&top_left_x=246) | $(1 K / 1 K)$ | 139M | - | 47.7/18.5/22.3 | 30.1/7.0/18.3 | 32.2/9.3/21.1 | 23.3 | 15.9 | 33.8 | 78.4 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_17_400457f248ef0bd7e237g-08.jpg?height=45&width=185&top_left_y=508&top_left_x=246) | $(256 / 16 K)$ | 139M | - | 55.7/24.8/25.8 | 33.6/8.5/19.2 | 34.4/11.5/22.7 | 35.8 | 21.3 | 33.7 | 85.3 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_17_400457f248ef0bd7e237g-08.jpg?height=45&width=125&top_left_y=543&top_left_x=246) | $(1 K / 1 K)$ | 406M | - | 50.6/19.8/23.5 | 32.1/7.4/18.7 | 33.3/9.4/21.6 | 24.5 | 17.9 | 36.1 | 79.3 |\n| BART $_{\\text {large }}$-SLED | - $(256 / 16 K)$ | 406M | - | 57.4/26.3/27.5 | 35.3/8.8/19.5 | 36.3/12.2/23.3 | 42.5 | 23.6 | 37.2 | 85.3 |\n| Test Scores |  |  |  |  |  |  |  |  |  |  |\n| LED $_{\\text {base }}$ | $(256 / 16 K)$ | 162 M | 33.6 | 56.8/27.3/29.2 | 30.0/6.0/17.5 | 31.3/8.6/20.5 | 34.8 | 21.0 | 28.5/28.3 | 82.9 |\n| T5 base | $(1 K / 1 K)$ | 220 M | 26.3 | 33.2/12.1/20.4 | 21.4/3.6/15.0 | 24.2/5.9/18.6 | 16.3 | 15.0 | 31.9/28.6 | 76.3 |\n| T5 base -SLED | $(256 / 16 K)$ | 220 M | 33.3 | 46.6/20.1/25.1 | 24.5/4.6/16.5 | 28.4/8.7/20.5 | 43.0 | 18.9 | 31.2/29.4 | 81.4 |\n| $B A R T_{\\text {base }}$ | $(1 K / 1 K)$ | 139M | 30.6 | 48.0/19.1/22.7 | 30.1/6.6/18.1 | 31.2/9.1/20.3 | 27.6 | 16.0 | 32.5/31.6 | 77.1 |\n| $B A T_{\\text {base }}$-SLED | $(256 / 16 K)$ | 139M | 35.4 | 54.7/24.4/25.4 | 32.7/7.9/19.1 | 33.8/11.7/22.6 | 41.1 | 21.5 | 29.7/30.4 | 85.6 |\n| ![](https://cdn.mathpix.com/cropped/2024_09_17_400457f248ef0bd7e237g-08.jpg?height=45&width=125&top_left_y=862&top_left_x=246) | $(1 K / 1 K)$ | 406M | 32.1 | 50.7/20.1/23.5 | 31.6/6.8/18.5 | 32.0/9.1/20.8 | 29.2 | 18.3 | 34.8/33.9 | 79.7 |\n| BART $_{\\text {large }}$-SLED | $(256 / 16 K)$ | 406M | 38.0 | 57.5/26.3/27.4 | 35.2/8.7/19.4 | 34.2/11.0/22.0 | 46.9 | 24.1 | 34.8/34.8 | 87.3 |\n| - $\\overline{\\mathrm{LED}_{\\text {base }}^{\\text {ScroLlS }} \\text { - }}$ | $(1 K / 16 K)$ | 162M | 29.2 | 56.2/26.6/28.8 | 24.2/4.5/15.4 | 25.1/6.7/18.8 | 26.6 | 18.5 | 25.8/25.4 | 71.5 |\n| LongT5 base | $(255 / 16 K)$ | 220 M | 38.2 | 53.5/27.3/29.3 | 34.8/9.6/21.1 | 33.9/11.0/22.8 | 46.6 | 23.0 | 37.9/36.6 | 85.6 |\n| LongT5 ${ }_{\\text {large }}^{\\dagger}$ | (255/16K) | 770 M | 40.5 | 54.2/27.8/29.8 | 35.6/9.2/21.2 | 35.1/12.0/23.3 | 52.3 | 27.2 | 40.6/38.6 | 87.3 |\n| LongT5 $5_{\\mathrm{xL}}^{\\dagger}$ | (255/16K) | 3B | 41.9 | 54.7/28.2/30.2 | 35.8/9.6/21.1 | 34.9/11.8/23.5 | 53.1 | 29.3 | 46.0/42.1 | 88.2 |\n| UL2 ${ }^{\\dagger}$ | $(2 K / 2 K)$ | 20B | 37.9 | 53.6/26.1/28.8 | 32.9/7.8/19.4 | 31.1/8.5/20.4 | 37.6 | 24.2 | 45.8/40.7 | 88.7 |\n\nTable 1: Main results on the SCROLLS benchmark.\n```\n\n#### 5. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.00)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 36/40 (Score: 0.00)*\n\n```\n## B. 3 Architectures\n\nWe build architectures from a set of common channel- and sequence-mixing layer primitives. Each architecture is composed of 2 blocks with a total of 4 layers. In general, blocks combine a sequence mixing layer with a subsequent channel mixing layer, with the exception of Mamba layers, which combine sequence and channel mixing into a single layer [12]. All layers are set to a width of 128 for our main analysis (if not stated otherwise), with all other architecture settings given below. Common architecture primitives are composed of two identical blocks combining each sequence-mixing layer with each of the two channel-mixing layers. Striped hybrid architectures combine each unique block of the common architecture primitives with a second block composed of multi-headed attention and one of the two channel mixers. ## B.3.1 Channel-mixing Layers\n\n- SwiGLU MLP [31]: inner width: 512\n- Mixture of Experts MLP [41]: number of experts: 8, expert width: 16, number of active experts: 2\n\n\n## B.3.2 Sequence-mixing Layers\n\nWe normalize the (fixed) state dimension of all sequence mixers, before running the MAD pipeline. Whenever possible, we prioritize keeping the shape of the layer fixed, over the state dimension (e.g., reducing state dimension before expansion factors, or reducting state dimension before number of heads). - Hyena [10]: filter order: 2, short filter order: 3, filter featurization is implemented following [24]. - Mamba [12]: state dimension: 4, convolution dimension: 4, width expansion: 2, no bias for linear and convolution layers. - Multi-head Gated Linear Attention [13]: number of heads: 8, head dimension: 16\n- Multi-Head Attention [6]: number of heads: 16, head dimension: 8, no bias for linear layers\n- Multi-Head Hyena [24]: number of heads: 16, state dimension of heads: 2, filter order: 2, short filter order: 3 . - Hyena Experts: number of experts: 8, expert width: 16, number of active experts: 2. All other parameters are shared with standard Hyena. At these settings, all evaluated architectures that do not include attention layers are normalized to a total state dimension of 4,096 . ## B. 4 Training\n\nFor each MAD task, we train models according to the setting described in Table B.1, using a standard crossentropy loss objective. Note that we sweep all evaluated architectures over a $3 \\times 2$ grid of learning rate and weight decay values (see Table B.1) and only include the best runs in our final analysis (as determined by their evaluation accuracy). ## B. 5 Results\n\n## B.5.1 Task Performances\n\nTable B.1: MAD training setting. | Optimizer | AdamW |\n| :---: | :---: |\n| Optimizer momentum | $\\beta_{1}, \\beta_{2}=0.9,0.98$ |\n| Dropout | None |\n| Batch Size | 128 |\n| Training epochs | 200 |\n| Learning Rate sCHEDule | COSINE DECAY |\n| NUMBER Of LAYERS | 4 |\n| Number of evaluation Samples | 1,280 |\n| Base learning Rate | $[0.0001,0.0005,0.001]$ |\n| Weight decay | $[0.0,0.1]$ |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-20.jpg?height=1593&width=1614&top_left_y=931&top_left_x=219)\n\nFigure B.1: Architecture performances within and across the MAD synthetic tasks, when using evaluation accuracy as a performance metric (left) or evaluation loss (right). ## B.5.2 Performance on Individual Tasks\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-21.jpg?height=2142&width=1217&top_left_y=305&top_left_x=407)\n\nFigure B.2: In-context recall task model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multiheaded model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-22.jpg?height=2139&width=1215&top_left_y=207&top_left_x=407)\n\nFigure B.3: Fuzzy in-context recall task model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H, A, Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-23.jpg?height=2140&width=1615&top_left_y=205&top_left_x=220)\n\nFigure B.4: Noisy in-context recall task model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-24.jpg?height=2145&width=1616&top_left_y=205&top_left_x=220)\n\nFigure B.5: Selective Copying model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H, A, Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-25.jpg?height=2137&width=1197&top_left_y=202&top_left_x=424)\n\nFigure B.6: Compression model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-26.jpg?height=2346&width=426&top_left_y=209&top_left_x=809)\n\nFigure B.7: Memorization model performances. H: Hyena, Mb: Mamba, Alg: Gated Lin. Attention, A: Attention, He: Hyena Experts, Sg: SwiGLU, MoE: Mixture of Experts MLP, m\\{H,A,Alg\\}: multi-headed model variants. ![](https://cdn.mathpix.com/cropped/2024_09_12_46d2059d3f5ccf0ee670g-27.jpg?height=535&width=1592&top_left_y=497&top_left_x=237)\n\nFigure B.8: Improved performance on MAD synthetics correlates with better compute-optimal perplexity on The Pile across IsoFLOP groups. We highlight progressively improved versions of Hyena that were designed with the MAD pipeline.\n```\n\n\n\n---\n## Found 1 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: architectural designs for SSMs and sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 1 related papers from Papers with Code\n\n#### 1. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: architectural designs for SSMs and sparse attention*\n\n*ChengXiang Zhai, Chenguang Zhu, Yichong Xu, Shuohang Wang, Yang Liu, Liliang Ren*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.\n\n**Conference:** sparse-modular-activation-for-efficient\n\n**Published:** 2023-06-19\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by integrating State Space Models (SSMs) with sparse attention mechanisms, here are some key findings and insights from the provided sources and additional relevant information:\n\n## Architectural Designs for SSMs\n\n### Mamba and Sparse Mamba\nThe Mamba architecture, as described in, uses State Space Models (SSMs) to efficiently handle long sequences by transforming input sequences into latent state representations. This approach eliminates the need for attention layers or multilayer perceptron blocks, reducing computational complexity. The Sparse Mamba (S-Mamba) variant reinforces controllability and observability, leading to a reduction in parameters and training time, and an improvement in perplexity.\n\n### State Space Duality\nThe concept of State Space Duality, mentioned in the analysis note, connects structured SSMs with variants of attention, reframing them as matrix multiplication algorithms on structured matrices. This duality can be leveraged to integrate SSMs with sparse attention mechanisms efficiently.\n\n## Sparse Attention Mechanisms\n\n### Half-Space Reporting (HSR) Enhanced Attention\nThe HSR-enhanced attention framework, described in, leverages the inherent sparsity within attention mechanisms to reduce computational complexity. This method uses a data structure to rapidly identify non-zero or \"massively activated\" entries in the attention matrix, significantly reducing the running time complexity for attention computations.\n\n### SPARSEK and ReLA\nWhile not directly mentioned in the provided sources, the analysis note references SPARSEK and ReLA as examples of sparse attention mechanisms. These mechanisms select a constant number of KV pairs for each query, achieving linear time complexity and constant memory footprint, which is crucial for efficient long-sequence processing.\n\n## Performance Metrics for Integrated Models\n\n### Efficiency and Scalability\nThe integration of SSMs with sparse attention mechanisms can lead to significant improvements in efficiency and scalability. For instance, the Sparse Mamba architecture reduces the number of parameters and training time while improving perplexity.\n\n### Empirical Results\nStudies like those on LOCOST and MoA provide empirical results showing that integrated models can achieve state-of-the-art results on tasks such as long document summarization and large language model compression. These models demonstrate improved memory efficiency and throughput[Analysis Note].\n\n## Key Insights and Detailed Analysis\n\n### Efficient Sequence Processing\nSSMs like Mamba and LOCOST are designed to efficiently handle long sequences, providing a strong foundation for integrating with sparse attention mechanisms. This integration can enhance both the scalability and expressiveness of the model[Analysis Note].\n\n### Hierarchical and Efficient Autoregressive Models\nCombining SSMs with sparse attention can create a hierarchical and efficient autoregressive language model. This approach can leverage the strengths of both SSMs and sparse attention, potentially leading to significant improvements in model performance, including lower perplexity, higher accuracy in downstream tasks, and better robustness to variant inputs[Analysis Note].\n\n## Future Search Plan\n\n### Exploring Architectural Designs\nInvestigate how different SSM architectures, such as Mamba and its variants, can be integrated with various sparse attention mechanisms. This includes exploring the use of State Space Duality and other efficient local attention methods to complement long-range dependency issues[Analysis Note].\n\n### Evaluating Performance Metrics\nLook for empirical results and benchmarks that compare the performance of integrated models against state-of-the-art models. This includes evaluating metrics such as perplexity, training time, memory usage, and performance on downstream tasks[Analysis Note].\n\nBy focusing on these areas, researchers can develop novel autoregressive language models that combine the efficiency of SSMs with the computational benefits of sparse attention mechanisms, leading to improved overall performance and scalability.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe integration of state space models (SSMs) with sparse attention mechanisms offers a promising approach to enhancing the efficiency and scalability of autoregressive language models. SSMs, like those in the Mamba architecture, provide efficient handling of long sequences by transforming input sequences into latent state representations, while sparse attention mechanisms, such as SPARSEK and ReLA, reduce computational complexity and memory usage. Combining these approaches can leverage the strengths of both, potentially leading to significant improvements in model performance.\n\n#### 2. All Useful References with Excerpts\n- **SPARSEK Attention**: Introduces a novel sparse attention mechanism that uses a scoring network and differentiable top-k mask operator to select a constant number of KV pairs for each query, achieving linear time complexity and constant memory footprint.\n- **SPADE**: Augments a state space model into the bottom layer of a Transformer, using efficient local attention methods in other layers to complement long-range dependency issues.\n- **LOCOST**: Proposes an encoder-decoder architecture based on state-space models for handling long context inputs, achieving significant memory efficiency and state-of-the-art results on long document summarization tasks.\n- **State Space Duality**: Connects structured SSMs with variants of attention, showing that they can be reframed as matrix multiplication algorithms on structured matrices.\n- **MoA**: Proposes a mixture of sparse attention configurations tailored to different heads and layers, improving memory efficiency and throughput in large language models.\n\n#### 3. Key Insights and Detailed Analysis\n- **Efficient Sequence Processing**: SSMs like Mamba and LOCOST can efficiently handle long sequences, providing a strong foundation for integrating with sparse attention mechanisms.\n- **Sparse Attention Benefits**: Mechanisms like SPARSEK and ReLA offer significant reductions in computational complexity and memory usage, which are critical for handling long sequences efficiently.\n- **Integration Potential**: Combining SSMs with sparse attention can provide a robust framework for efficient long-sequence processing, enhancing both scalability and expressiveness.\n\n#### 4. Future Search Plan\n- **Explore Architectural Designs**: Investigate how different SSM architectures can be integrated with various sparse attention mechanisms to create a hierarchical and efficient autoregressive language model.\n- **Evaluate Performance Metrics**: Look for empirical results and benchmarks that compare the performance of integrated models against state-of-the-art models.\n\n#### 5. List of References\n1. Lou, C., et al. (2024). *Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers*. arXiv.org.\n2. Zuo, S., et al. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv.org.\n3. Le Bronnec, F., et al. (2024). *LOCOST: State-Space Models for Long Document Abstractive Summarization*. Conference of the European Chapter of the Association for Computational Linguistics.\n4. Dao, T., & Gu, A. (2024). *Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality*. arXiv.org.\n5. Fu, T., et al. (2024). *MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression*. arXiv.org."
        },
        {
            "ready": false,
            "query": "integration techniques for SSMs and sparse attention",
            "detail": "practical implementations of SSMs and sparse attention integration",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\npractical implementations of SSMs and sparse attention integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.98)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 2/35 (Score: 0.98)*\n\n```\nHowever, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State space $\\underline{A} u g m e n t e \\underline{D}$ TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. ## 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair\n\n[^0]of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model ( 250 M parameters) consumes over 80 G of GPU memory when the sequence length is 8 k . Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models' practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a $57.5 \\%$ accuracy, nearly $30 \\%$ less than stateof-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right)$ in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are pro-\nposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models' ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State space $\\underline{\\text { Augmente }} \\underline{\\mathbf{D}}$ TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.94)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.95)*\n\n```\n# Sparse Modular Activation for Efficient Sequence Modeling \n\nLiliang Ren ${ }^{1 *}$ Yang Liu ${ }^{2} \\quad$ Shuohang Wang ${ }^{2} \\quad$ Yichong Xu ${ }^{\\dagger}$<br>Chenguang Zhu ${ }^{2}$ Chengxiang Zhai ${ }^{1}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign $\\quad{ }^{2}$ Microsoft<br>\\{liliang3, czhai\\}@illinois.edu<br>\\{yaliu10, shuowa, chezhu\\}@microsoft.com<br>xuyc11@gmail.com\n\n\n#### Abstract\n\nRecent hybrid models combining Linear State Space Models (SSMs) with selfattention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to suboptimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat. ## 1 Introduction\n\nRecent advance on efficient sequence modeling with State Space Models (SSMs) [GGR21; GDE ${ }^{+}$20; GGGR22; GB22; SWL23] has shown impressive performance for a wide range of tasks across modalities, such as text classification, image recognition and speech recognition. SSMs, as firstorder linear models, defined by a set of input, output, and state variables connected by first-order differential equations, can efficiently capture the recurrent structure in sequential data with carefully designed state matrices and the application of convolutional parallelism [GGR21]. However, they still significantly underperform the self-attention [BCB14; $\\mathrm{VSP}^{+}$17] based model in both language modeling and machine translation [VPSP23] tasks. A recent work [FDS ${ }^{+}$23] reveals that this is due to its deficiency of modeling the second-order pairwise comparisons between the input tokens, and shows that the augmentation of an additional shifted SSM layer can improve SSM's associative recalling\n\n[^0]ability. Furthermore, better quality-efficiency trade-off can be achieved by directly introducing extra self-attention modules to form a hybrid model (e.g. MEGA [MZK ${ }^{+}$23] and Hybrid H3 [FDS ${ }^{+}$23]) that utilizes both the first and the second order inductive biases, i.e., SSM and self-attention. However, the current hybrid models apply the attention modules statically and uniformly to each of the input token regardless the property of the task itself. This can lead to sub-optimal quality-efficiency trade-offs since not all input tokens require second-order modeling and this computation need can vary substantially depending on both its context and the task difficulty. In this paper, we aim to answer the following research questions for efficiently combining attention with SSMs:\n\n- RQ1: Can neural networks learn to activate their attention modules on demand to achieve better quality-efficiency trade-off? - RQ2: How much extra attention is needed for the SSMs on a task-by-task basis? To answer these questions, we develop a new general mechanism, Sparse Modular Activation (SMA), that allows a neural network to sparsely and dynamically activate its sub-modules for each of the input token in a fully differentiable manner. Specifically, we assume a neural model can be composed of multiple heterogeneous sub-modules. For the input sequence, a latent configurator sparsely maps tokens to multiple compressed sequences corresponding to sub-modules. Each sub-module is then only applied on its mapped shorter sequence. Compared with activating all sub-modules on the whole input, Sparse Modular Activation can reduce computation and memory consumption for both the training and inference stages. Notably, SMA is proved to have a full coverage of the combinatorial search space of module activation, which is further explained in Section 3.2. Efficient learning of dynamic sparsity is notoriously difficult under the constraint of the current parallel hardware [LQC ${ }^{+} 22$; GZYE20; XM22]. To enable the practical efficiency gains from our module-level sparsity, we provide a simple yet efficient parallel implementation of SMA without any custom fused GPU kernels. Specifically, when compressing a batch of sequences in SMA, our implementation conducts both token selection and the sequence re-padding simultaneously using a single scatter operation that is widely optimized and present in modern deep learning frameworks. To address RQ1, we apply SMA to construct a novel neural architecture, SeqBoat, that sparsely activate a Gated Attention Unit (GAU) [HDLL22] based on the state representation learned from an SSM. Both the GAU and the SSM representations are then aggregated through simple addition and activation to form a layer-level representation. Multiple same-sized SeqBoat layers are stacked sequentially to form a full neural model. Inspired by the working memory mechanism [AS68] used in human cognition, we further restrict the GAU to only apply local attention on the compressed sequence, which allows our model to have linear sequence inference complexity but theoretically infinite attention span. We conduct comprehensive experiments to show that SeqBoat has significantly better qualityefficiency trade-off than state-of-the-art hybrid models on a wide range of tasks, including Long Range Arena (LRA) [TDA ${ }^{+}$20], speech classification [War18] and language modeling [Hut06]. On the competitive LRA benchmark, SeqBoat achieves 1.96 higher average accuracy than MEGA-chunk $\\left[\\mathrm{MZK}^{+} 23\\right]$, the previous best hybrid model, with a $10.4 \\times$ training speed up and a $95 \\%$ memory reduction compared to the Transformer [VSP ${ }^{+}$17] on the Text task with 4,096 input length. Thanks to the intrinsic modular sparsity brought by SMA, SeqBoat directly reveals the amount of attention needed for each data sample of each task through its sparse activation patterns of GAU, addressing RQ2. We demonstrate that our working memory mechanism provides substantially better computation-accuracy trade-off than chunking based models, and analyze the relationship between the working memory size and the effective attention span on various long sequence modeling tasks. ## 2 Background\n\nTo motivate and clarify our proposed techniques, we first present a mathematical formulation of our Sparse Modular Activation mechanism and show how it encompasses and generalizes previous attempts that aimed for module-level dynamic sparsity. A dedicated section for detailed comparisons between our approach with the related works is also included in Appendix F. We begin by reviewing how the standard sequence modeling is formalized to establish the common ground for our discussion. ### 2.1 Time-Invariant Sequence Modeling\n\nGiven a discrete sequence, $\\mathbf{x}=\\left\\{x_{1}, \\ldots, x_{n}\\right\\} \\in \\mathbb{R}^{n}$, consisting of $n$ tokens, a time-invariant sequence model $P_{\\theta}$ is optimized to maximize the likelihood of the observed sequences by factorizing them as follows:\n\n$$\n\\max _{\\theta} P_{\\theta}(\\mathbf{x})=\\prod_{t=1}^{n} P\\left(x_{t} \\mid \\mathbf{x}_{<t}, \\theta\\right)\n$$\n\nwhere $\\mathbf{x}_{<t}=\\left\\{x_{1}, \\ldots, x_{t-1}\\right\\}$ is the sequence history at time step $t$, and the parameter $\\theta$ is independent of the time step $t$. This formulation implies that the full model parameters $\\theta$ and the full history $\\mathbf{x}_{<t}$ are both essential for the conditional prediction of each token $x_{t}$. However, one potential issue is as the prediction difficulty of each token may differ depending on the context and the position, this static model $P_{\\theta}$ can lead to sub-optimal accuracy-efficiency trade-off by wasting computation on either unimportant context $\\left[\\mathrm{SJP}^{+} 21\\right]$ or easy-to-predict tokens [Gra16]. ## 3 Learning Sparse Modular Activation\n\nTo cover a larger search space that may contain more efficient sequence models, we propose to formulate sequence modeling as a problem of finding an optimal time-variant model that can dynamically activate a subset of modules from a pre-defined function space for each time step. ### 3.1 Time-Variant Sequence Modeling\n\nFormally, a time-variant sequence model is defined on a compact function space $\\mathcal{F}: \\mathcal{X}_{t}^{c} \\mapsto[0,1]^{n \\times V}$, where $V$ is the size of the vocabulary and $\\mathcal{X}_{t}^{c}=\\left\\{\\mathbf{x}_{t}^{c}: \\mathbf{x}_{t}^{c} \\subseteq \\mathbf{x}_{<t} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{n}\\right\\}$, contains all possible sub-sequences of the sequence history $\\mathbf{x}_{<t}$. Then for each of the token prediction at the time step $t$, the model learns to apply a function $f_{t} \\in \\mathcal{F}$ with the parameters $\\theta_{t}$ that maximizes the sequence probability, i.e.,\n\n$$\n\\max _{f_{t}, \\theta_{t}, \\mathbf{x}_{t}^{c}} P_{\\mathcal{F}}(\\mathbf{x})=\\prod_{t=1}^{n} P_{f_{t}}\\left(x_{t} \\mid \\mathbf{x}_{t}^{c}, \\theta_{t}\\right) \\quad \\text { s.t. } \\quad \\mathbf{x}_{t}^{c} \\subseteq \\mathbf{x}_{<t}\n$$\n\nThis formulation generalizes the previous works in pursuing a dynamic and sparse model for sequence modeling, where the connections are further explained in Appendix F. In this work, we assume the function space $\\mathcal{F}$ is chain-structured, i.e., $\\mathcal{F}=\\mathcal{H} \\circ \\mathcal{L}_{N} \\circ \\cdots \\circ \\mathcal{L}_{1} \\circ \\mathcal{E}$, where $\\mathcal{H}: \\mathbb{R}^{n \\times d_{m}} \\mapsto[0,1]^{n \\times V}$ is the classification function, $\\mathcal{E}: \\mathbb{R}^{n} \\mapsto \\mathbb{R}^{n \\times d_{m}}$ is the embedding function, $N$ is the number of intermediate layers, $d_{m}$ is the model size and $\\mathcal{L}: \\mathbb{R}^{n \\times d_{m}} \\mapsto \\mathbb{R}^{n \\times d_{m}}$ is the function space of the intermediate mappings. We further assume that $\\mathcal{L}$ is the spanning set of a finite number of the function $f_{i}^{l}$ with its parameters $\\theta_{i}^{l}$, i.e., $\\mathcal{L}=\\operatorname{span}\\left\\{f_{1}^{l}, \\ldots, f_{M}^{l}\\right\\}$, where $M$ is the number of pre-defined functions. These assumptions justify the design of our Sparse Modular Activation mechanism, which is further explained in the following section. ### 3.2 Sparse Modular Activation\n\nSparse Modular Activation (SMA) introduces a latent configurator at each time step $t$ and each layer of a neural sequence model.\n```\n\n##### *Relevant Chunk: No. 9/32 (Score: 0.94)*\n\n```\nWe also include comprehensive ablation studies of our model in Appendix E. How much attention is needed for different sequence modeling tasks? As shown in Figure 3 , we draw the activation time of the GAU module at different layers of our SeqBoat-full models for each of the task in the LRA benchmark. We measure the mean and the standard deviation (plotted as error bars) of the activation time on 100 sequences randomly sampled from the validation set of each task. The lines seem to be truncated due to the fact that different models have different number of layers. Generally, we can see that Image (in Blue and Circle) and Pathfinder (in Green and Triangle) need signif-\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_dc32d27cb1761fdbd8bcg-08.jpg?height=459&width=684&top_left_y=1755&top_left_x=1057)\n\nFigure 3: The activation time (with error bars) of the GAU module at different layers of the SeqBoatfull model for different tasks in the LRA benchmark. icantly more attentions than the text-based tasks (i.e., Text, Retrieval and ListOps). We can also see that our model trained on ListOps (in Orange and Square) has a much larger variance of GAU activation time than other tasks, which may reflect\nthat the task difficulty per data sample of ListOps has a larger variance than other tasks. Another trend we can see is that the bottom layers near the inputs need much less attention (and sometimes even drop the attention module entirely) than the high level layers. We conjecture that this is because the SSMs with first-order complexity is already capable to capture the low-level features while the attention modules are more demanded at higher levels for conducting more complex computations of second-order pairwise comparisons. ![](https://cdn.mathpix.com/cropped/2024_09_17_dc32d27cb1761fdbd8bcg-09.jpg?height=444&width=1366&top_left_y=533&top_left_x=378)\n\nFigure 4: The confidence probabilities of the GAU modular activation at each time step for the last two layers of the SeqBoat-full and the SeqBoat model. The results are measured on three input sequences randomly sampled from the validation set of the Pathfinder task. The sequences are reshaped back to $32 \\times 32$ squares for better visualization. Darker the blue color, higher the confidence. The white blocks indicate the time steps when GAUs are not activated. How is the learned sparse modular activation distributed over the input sequence? As shown in Figure 4, we draw the confidence probabilities of the sparse modular activation for both the SeqBoat-full and the SeqBoat models on the Pathfinder task from the LRA benchmark. Generally, we can see that different input sequences has different modular activation patterns, which indicates that our modular activation is dynamically sparse. We can also see that the SeqBoat activation pattern is learned to be more structured than the SeqBoat-full, which may explain the superior performance of SeqBoat over SeqBoat-full on the Pathfinder task. Another trend we can see is that different layers have their own patterns: e.g., the Layer 5 of the SeqBoat model seems trying to find the curly paths in the latent representation space, while the Layer 6 seems aggregating the results from the contiguous time steps. ![](https://cdn.mathpix.com/cropped/2024_09_17_dc32d27cb1761fdbd8bcg-09.jpg?height=549&width=1352&top_left_y=1638&top_left_x=392)\n\nFigure 5: Training Speed v.s. Validation Accuracy trade-off on Image and Pathfinder of the LRA benchmark for different models with varying memory/chunk sizes. SeqBoat keeps a working memory of the compressed sequence, while MEGA-chunk splits the input sequence into non-overlapping sub-sequences. The memory/chunk sizes are marked along the lines. The GPU-hours for Image are measured on NVIDIA RTX A5000 GPUs, and Pathfinder on V100 GPUs with 32GB memory. How does the working memory size affect the speed-quality trade-off? We compare the trade-off ability of our SeqBoat with MEGA-chunk on Image and Pathfinder tasks of the LRA benchmark, as\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_dc32d27cb1761fdbd8bcg-10.jpg?height=501&width=1370&top_left_y=246&top_left_x=376)\n\nFigure 6: Average Attention Span v.s. Layer Index on both the Pathfinder and the enwik8 tasks for different SeqBoat models with different working memory sizes. A smaller layer index indicates the layer is closer to the input. For reference, the average attention span of a sliding window based local attention is the half of the window size. shown in Figure 5. We draw the Pareto frontiers by respectively varying the working memory size for SeqBoat and the chunk size for MEGA-chunk from the set $\\{16,32,64,128,256,512\\}$. The results are averaged with three random seeds. We can see that our models have significantly better trade-off than MEGA-chunk models for both the Image and the Pathfinder tasks. Notably, the Mega-chunk models generally fail (achieving around $50 \\%$ accuracy as random guessing) on Pathfinder tasks with less than 128 chunk size, while our SeqBoat can still achieve around $90 \\%$ accuracy with only a memory size of 16 . This indicates the effectiveness of our working memory mechanism in capturing long term interactions. How does the size of working memory affect the effective attention length of the GAU module? We draw Figure 6 to investigate the relationship between the working memory size and the average effective attention length on both Pathfinder and the enwik8 datasets. We first measure the average attention distance from the query token to its key tokens for each of the time step, and the results are further averaged for all the time steps in a sequence. The average attention span on Pathfinder is measured on 100 sequences randomly sampled from the validation set. The average attention span on enwik8 is calculated based on a sequence of length 8,192 sampled from the validation set, and each token is equipped with a minimum of 7,000 tokens context window. From the figures, we can see a general trend that the models with a small working memory size (Size 16, 32 and 64 for Pathfinder, and Size 512 for enwik8) can have a far longer average attention span than its memory size. Notabaly, the layer 3 of the SeqBoat model with only 16 working memory slots surprisingly achieves an average attention span of 245 for the Pathfinder task. This partially explains the superior performance of our model over MEGA-chunk with small memory sizes and directly proves that our working memory mechanism can efficiently enable long-term interactions between the sequence elements. ## 6 Conclusion\n\nIn this paper, we present Sparse Modular Activation (SMA), a general mechanism for sparsely and dynamically activating sub-modules in neural networks. SMA not only has an efficient parallel implementation but also provides a theoretical guarantee for a full coverage of the function space of multiple modules. To address the challenges of combining attention with Linear State Space Models (SSMs), we apply SMA to develop a novel neural architecture, SeqBoat, that sparsely activates a Gated Attention Unit (GAU) based on the state representations from an SSM. The proposed model demonstrates a significantly better quality-efficiency trade-off compared to existing hybrid models across various sequence modeling tasks. To the best of our knowledge, SMA is the first mechanism that allows a neural network to obtain practical efficiency and complexity gains from sparsely activating a self-attention-like module. The strong empirical performance show that even with a preliminary application of SMA, we could already see substantial efficiency and interpretability benefits. For future work, we mainly want to explore the scaling behavior of SMA and also how to incorporate SMA with the pre-trained large language models. ## Acknowledgement\n\nWe would like to thank Canwen Xu, Ziyi Yang, Ruochen Xu and Azure Cognitive Services Research group members for their feedback. We also want to thank Qi Zeng and Zixuan Zhang for the early discussion of the project. ## References\n\n$\\left[\\mathrm{AOA}^{+} 20\\right]$ J.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.79)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.88)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n##### *Relevant Chunk: No. 22/86 (Score: 0.69)*\n\n```\nY. Fu, et al. 2023; De et al. 2024; Glorioso et al. 2024; Lieber et al. 2024) suggests that a hybrid architecture with both SSM layers and attention layers could improve the model quality over that of a Transformer, or a pure SSM (e.g., Mamba) model, especially for in-context learning. We explore the different ways that SSD layers can be combined with attention and MLP to understand the benefits of each. Empirically we find that having around $10 \\%$ of the total number of layers being attention performs best. Combining SSD layers, attention layers, and MLP also works better than either pure Transformer++ or Mamba-2. SSD and Attention We find that SSD and attention layers are complementary: by themselves (e.g. in the Mamba-2 architecture vs. Transformer++) their performance (measured by perplexity) is nearly the same, but a mixture of SSD and attention layers outperforms the pure Mamba-2 or Transformer++ architecture. We show some results (Table 2) for the 350 M model ( 48 layers) trained to 7B tokens on the Pile with the GPT- 2 tokenizer (same number of parameters, same hyperparameters, same training and validation set). Adding in just a few attention layers already yields notable improvement and strikes the best balance between quality and efficiency. We hypothesize that the SSM layers function well as a general sequence-to-sequence mapping, and attention layers act as a retrieval mechanism to quickly refer to previous tokens in the sequence instead of forcing the model to compress all the context to its memory (SSM states). Table 2: (Combining SSD and Attention Blocks.) Perplexity of a 350 M model with 48 layers, with different number of attention layers. Having around a $10 \\%$ ratio of attention layers performs best. | Num. AtTn Blocks | 0 (Mamba-2) | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 | 11 | 15 | 24 | Transformer++ |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Perplexity $\\downarrow$ | 8.60 | 8.38 | 8.32 | 8.29 | 8.29 | 8.28 | $\\mathbf{8 . 2 6}$ | 8.27 | 8.28 | 8.30 | 8.34 | 8.50 | 8.68 |\n\nHybrid Models with SSD, MLP, and Attention We compare different ways that SSD can be combined with the (gated) MLP and attention layers, and evaluate at the 2.7B scale (64 layers), trained to 300B tokens on the Pile (same number of parameters, same hyperparameters, same training and validation set, same data order):\n\n1. Transformer++: 32 attention layers and 32 gated MLP, interleaving. 2. Mamba-2: 64 SSD layers. 3. Mamba-2-MLP: 32 SSD and 32 gated MLP layers, interleaving. 4. Mamba-2-Attention: 58 SSD layers and 6 attention layers (at indices $9,18,27,36,45,56)^{6}$. 5. Mamba-2-MLP-Attention: 28 SSD layers and 4 attention layers, interleaving with 32 gated MLP layers. We report the validation perplexity on the Pile, as well as zero-shot evaluation, in Table 3. In general, the quality of Transformer++ and Mamba-2 models are around the same. We see that adding just 6 attention layers noticeably improves over the pure Mamba-2 model (and over Transformer++). Adding MLP layers reduces model quality, but can (i) speed up training and inference due to the simplicity and hardware-efficiency of the MLP layer (ii) be easier to up-cycle to MoE models by replacing MLP layers with mixture-of-experts. Table 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare different ways SSD, MLP, and attention layers can be combined, evaluated at 2.7 B scale trained to 300 B tokens on the Pile. | MODEl | Token. | PILE <br> PPL $\\downarrow$ | LAMBADA <br> PPL $\\downarrow$ | LAMBADA <br> ACC $\\uparrow$ | HellaSwag <br> ACC $\\uparrow$ | PIQA <br> ACC $\\uparrow$ | Arc-E <br> ACC $\\uparrow$ | Arc-C <br> ACC $\\uparrow$ | WinoGrande <br> ACC $\\uparrow$ | OpenbookQA <br> ACC $\\uparrow$ | Average <br> ACC $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer++ | NeoX | 6.13 | 3.99 | $\\underline{70.3}$ | 66.4 | 75.2 | 67.7 | $\\underline{37.8}$ | 63.9 | 40.4 | 60.2 |\n| Mamba-2 | NeoX | 6.09 | 4.10 | 69.7 | 66.6 | 76.4 | 69.6 | 36.4 | 64.0 | 38.8 | 60.2 |\n| Mamba-2-MLP | NeoX | 6.13 | 4.18 | 69.3 | 65.0 | 76.4 | 68.1 | 37.0 | 63.1 | 38.2 | 59.6 |\n| Mamba-2-Attention | NeoX | 5.95 | 3.85 | 71.1 | 67.8 | $\\underline{75.8}$ | 69.9 | $\\underline{37.8}$ | 65.3 | 39.0 | 61.0 |\n| Mamba-2-MLP-Attention | NeoX | 6.00 | 3.95 | 70.0 | 66.6 | 75.4 | 70.6 | 38.6 | 64.6 | 39.2 | 60.7 |\n\n### 9.3 Speed Benchmarks\n\nWe benchmark the speed of the SSD algorithm against Mamba's scan implementation and FlashAttention-2 (Figure 10). SSD, thanks to its reformulation to use matrix multiplication as a subroutine, can exploit specialized matrix multiplication (matmul) units on GPUs, also known as tensor cores. As a result, it is 2-8\u00d7 faster than Mamba's fused associative scan, which does not leverage matmul units. Due to its linear scaling in sequence length, SSD is faster than FlashAttention-2 starting at sequence length $2 K$. However, we note that the Mamba-2 model as a whole might not be as efficient to train as Transformer at short sequence length (e.g. at $2 K$ ), since a Transformer with $L$ layers would have $\\frac{L}{2}$ MLP layers and $\\frac{L}{2}$ attention layers, while a Mamba- 2 model would have $L$ SSD layers for the same number of parameters. Generally the MLP layers are very hardware efficient since they consist of simple matrix multiplication and pointwise linearity. As shown in Section 9.2.3, one can also combine $\\frac{L}{2}$ SSD layers and $\\frac{L}{2}$ MLP layers to speed up training at short sequence length. [^4]Table 4: (Ablations: Mamba-2 block.) We ablate the major differences between the Mamba-2 and Mamba-1 neural network blocks (Figure 6, Section 7.1). Note that these components are independent of the inner sequence mixing layer; in these ablations, we use SSD for the inner SSM layer (differing from the S6 layer of Mamba-1). | Block | ABCX Projections | Extra Normalization | Parameters | Perplexity |\n| :--- | :--- | :--- | :--- | :--- |\n| Mamba-1 | Sequential | $\\boldsymbol{X}$ | 129.3 M | 11.76 |\n|  | Sequential | $\\boldsymbol{\\checkmark}$ | 129.3 M | 11.54 |\n|  | Parallel | $\\boldsymbol{X}$ | 126.5 M | 11.66 |\n| Mamba-2 | Parallel | $\\boldsymbol{\\checkmark}$ | 126.5 M | 11.49 |\n\n### 9.4 Architecture Ablations\n\n### 9.4.1 Block Design\n\nSection 7.1 introduces the Mamba-2 block, which has small modifications to the Mamba-1 block which are partly motivated by the connection to attention and also to improve the scalability of Mamba-2. Table 4 ablates these architecture changes to the block, which occur outside of the core SSM layer. The ablations validate that parallel projections to create $(A, B, C, X)$ saves parameters and performs slightly better than Mamba's sequential projections. More importantly, this modification is amenable to tensor parallelism at larger model sizes (Section 8). Additionally, the extra normalization layer also slightly improves performance. More importantly, preliminary experiments at larger scales observed that it also helps with training stability. ### 9.4.2 Head Structure\n\nSection 7.2 describes how the dimensions of the $B, C, X$ projections can be viewed as a hyperparameter analogous to notions of multi-head attention and multi-query attention. We also showed how the original Mamba architecture is analogous to multi-value attention (Proposition 7.2), which was a choice that naturally developed from the state-space model point of view and was not previously ablated. Table 5 ablates choices of the multi-head structure for the Mamba-2 architecture. Strikingly, we find a large difference between multi-value and multi-query or multi-key head patterns, despite seeming very similar. Note that this is not explained by the total state size, which is the same for all of them (equal to HPN or the product of the number of heads, head dimension, and state dimension). We also compare to multi-head patterns where the number of $C, B, X$ (analogous to $Q, K, V$ ) heads is equal. We compare against the standard multi-head pattern, as well as one with aggressive sharing where they all have only 1 head. Note that in the latter case, the model still has H different sequence mixers $M$, because each head still has a different $A$. When parameter matched, these multi-head patterns perform similarly to each other, in between the MVA and MQA/MKA patterns. ### 9.4.3 Attention Kernel Approximations\n\nSection 7.3 noted how SSD can be combined with ideas from the linear attention literature, such as various forms of kernel approximations. We ablate several variants of these suggested by previous works in Table 6. These include the cosFormer (Qin, Weixuan Sun, et al. 2022), Random Feature Attention H. Peng et al. 2021, and Positive Random Features (Performer) (Choromanski et al. 2021). We also ablate adding a normalization term, akin to the denominator of the softmax function in standard attention. We found that this introduced instabilities to most variants, but slightly improved performance for the ReLU activation function $\\psi$. Table 7 also tests more recent proposals to improve linear attention that involve expanding the feature dimension (Based (Arora, Eyuboglu, Zhang, et al. 2024) and ReBased (Aksenov et al. 2024)). These linear attention extensions aim to appropriate the $\\exp$ kernel with a quadratic approximation. ReBased also proposes to replace the QK activation function with a layer normalization; from an SSM-centric view we apply a normalization on top of $(B, C)$ before applying the SSM function. Table 5: (Ablations: Multi-head structure.) All models have state expansion factor $N=64$ and head size $P=64$ and are trained to Chinchilla scaling law token counts. The number of $A$ heads is always equal to the total heads H , i.e. each head has a separate input-dependent $A$ decay factor. (Top) 125M models, 2.5B tokens (Bottom) 360 M models, 7 B tokens\n\n| SSM Head Pattern | Attn. Analog | $A$ heads | $B$ heads | $C$ heads | $X$ heads | Layers | Params | Ppl. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Multi-input (MIS) | Multi-value (MVA) | 24 | 1 | 1 | 24 | 24 | 126.5 M | $\\mathbf{1 1 . 6 6}$ |\n| Multi-contract (MCS) | Multi-query (MQA) | 24 | 1 | 24 | 1 | 24 | 126.5 M | 12.62 |\n| Multi-expand (MES) | Multi-key (MKA) | 24 | 24 | 1 | 1 | 24 | 126.5 M | 12.59 |\n| Multi-head (MHS) | Multi-head (MHA) | 24 | 24 | 24 | 24 | 15 | 127.6 M | 12.06 |\n| Multi-state (MSS) | - | 24 | 1 | 1 | 1 | 36 | 129.6 M | 12.00 |\n| Multi-input (MIS) | Multi-value (MVA) | 32 | 1 | 1 | 32 | 48 | 361.8 M | 8.73 |\n| Multi-contract (MCS) | Multi-query (MQA) | 32 | 1 | 32 | 1 | 48 | 361.8 M | 9.33 |\n| Multi-expand (MES) | Multi-key (MKA) | 32 | 32 | 1 | 1 | 48 | 361.8 M | 9.36 |\n| Multi-head (MHS) | Multi-head (MHA) | 32 | 1 | 1 | 1 | 70 | 361.3 M | 9.01 |\n| Multi-state (MSS) | - | 32 | 32 | 32 | 32 | 29 | 357.3 M | 9.04 |\n\nTable 6: (Ablations: Kernel approximations.) We test various proposals for the kernel activation function $\\psi$, including linear attention variants aiming to approximate the exp kernel from standard softmax attention. | Kernel activation $\\varphi$ | Perplexity |\n| :--- | :--- |\n| none | 11.58 |\n| Swish | 11.66 |\n| Exp | 11.62 |\n| ReLU | 11.73 |\n| ReLU + normalization | 11.64 |\n| cosFormer | 11.97 |\n| Random Feature Attention | 11.57 |\n| Positive Random Features (Performer) | 12.21 |\n\nTable 7: (Ablations: Kernel approximations.) We test the (Re)Based methods for linear attention approximations, which involve expanded feature maps. (Top) 130 M models. (Top) 380 M models with $N=256$. | Kernel activation $\\varphi$ | Perplexity |\n| :--- | :--- |\n| Swish | 11.67 |\n| Swish + Taylor (Based) | 12.19 |\n| LayerNorm | 11.50 |\n| LayerNorm + Square (ReBased) | 11.84 |\n| Swish | 8.58 |\n| Swish + Taylor (Based) | 8.71 |\n| LayerNorm | 8.61 |\n| LayerNorm + Square (ReBased) | 8.63 |\n\nWe note that this technique has been independently proposed as the \"QK-Norm\" for softmax attention (Team 2024) and an \"internal normalization\" for Mamba (Lieber et al. 2024). Overall, Table 6 and Table 7 found that the kernel approximation methods we tried did not seem to improve over simple pointwise non-linear activation functions for $\\psi$. Thus our default settings for Mamba-2 used $\\psi(x)=$ Swish $(x)$ to follow Mamba-1, but we suggest that removing this activation entirely may be a simpler choice that we did not extensively test. We emphasize however that SSD and vanilla linear attention differ in the inclusion of the 1-semiseparable mask $L$, while the various linear attention methods in the literature were derived to approximate softmax attention without this term; thus, our negative results may be not unexpected. ## 10 Related Work and Discussion\n\nThe state space duality framework bridges connections between SSMs, structured matrices, and attention. We discuss in more depth the relations between SSD and these concepts more broadly. Using ideas from each of the viewpoints, we also suggest some directions that the SSD framework can be extended in future work. ### 10.1 State Space Models\n\nStructured state space models can be characterized along the axes\n(i) whether it is time-invariant or time-varying.\n```\n\n\n\n---\n## Found 0 related papers from 0 external sources\n\n\n\nYour 1 raw search queries input to the search frame: integration techniques for SSMs and sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by integrating State Space Models (SSMs) with sparse attention mechanisms, here are some key insights and practical implementations from the provided sources and additional relevant information:\n\n## Integration Techniques for SSMs and Sparse Attention\n\n### State Space Models (SSMs)\n- **Mamba and Sparse Mamba**: The Mamba architecture, as described in, uses state-space models to efficiently handle long sequences. The Sparse Mamba (S-Mamba) variant reinforces controllability and observability, reducing parameters and improving training time and perplexity. This approach can be integrated with sparse attention to enhance efficiency and scalability.\n- **BiMamba Module**: The BiMamba module, introduced in, processes sequences in both forward and backward directions, enabling the capture of bidirectional contextual relationships. This can be beneficial when combined with sparse attention to handle complex sequence dependencies.\n\n### Sparse Attention Mechanisms\n- **Sparse Modular Activation**: Techniques like Sparse Modular Activation (SMA) allow neural networks to sparsely and dynamically activate sub-modules, reducing computation and memory consumption. This is crucial for efficient long-sequence processing and can be integrated with SSMs to create more efficient models.\n\n### Practical Implementations\n\n#### Coupled Mamba\n- The Coupled Mamba approach, as mentioned in the analysis note, enhances multi-modal fusion using a coupled state space model. This method achieves significant improvements in F1-score and GPU memory savings, demonstrating the potential of integrating SSMs with other efficient mechanisms[Analysis Note].\n\n#### Hierarchical and Efficient Models\n- Combining SSMs with sparse attention can create hierarchical and efficient autoregressive language models. For example, using a sliding-encoder and decoder approach, as in SLED, can leverage short-text pretrained LMs to achieve competitive performance with larger models while reducing computational complexity[Analysis Note].\n\n### Performance Metrics and Evaluation\n- Models integrating SSMs and sparse attention should be evaluated using metrics like perplexity, F1 score, Rouge, and Meteor. These metrics are crucial for assessing the effectiveness of the integrated models in terms of efficiency, scalability, and performance[Analysis Note].\n\n### Specific Architectural Designs\n\n#### Sparse Mamba Integration\n- The Sparse Mamba architecture, by reinforcing controllability and observability, can be integrated with sparse attention mechanisms. This integration can selectively filter information, ensuring only relevant inputs are integrated into the state, thereby efficiently compressing sequence context and discarding irrelevant information.\n\n#### BiMamba and Sparse Attention\n- The BiMamba module can be combined with sparse attention to process sequences in both directions, capturing bidirectional contextual relationships while reducing computational complexity. This approach ensures that the model can modulate its dynamics based on both content and context, enhancing overall performance and efficiency.\n\nBy focusing on these integration techniques, practical implementations, and performance metrics, researchers can design more efficient, scalable, and high-performing autoregressive language models that leverage the strengths of both SSMs and sparse attention mechanisms.\n",
            "analysis": "### Analysis Note\n\n#### 1. Summary of Analysis\nThe integration of State Space Models (SSMs) with sparse attention mechanisms is a promising approach to enhancing the efficiency and scalability of autoregressive language models. The search results provided insights into various architectural designs and performance metrics for integrated models, highlighting the potential benefits of combining these techniques.\n\n#### 2. All Useful References with Excerpts\n- **\u221e-former**: Uses metrics like perplexity, F1 score, Rouge, and Meteor to evaluate models, showing that keeping auxiliary documents in memory improves performance.\n- **Sparse Modular Activation for Efficient Sequence Modeling**: Introduces Sparse Modular Activation (SMA), which enables neural networks to sparsely and dynamically activate sub-modules, reducing computation and memory consumption.\n- **Coupled Mamba**: Demonstrates enhanced multi-modal fusion with a coupled state space model, achieving significant improvements in F1-score and GPU memory savings.\n- **SLED**: Proposes a sliding-encoder and decoder approach for long sequences, leveraging short-text pretrained LMs to achieve competitive performance with larger models.\n\n#### 3. Key Insights and Detailed Analysis\n- **Performance Metrics**: Models are evaluated using metrics like perplexity, F1 score, Rouge, and Meteor, which are crucial for assessing the effectiveness of integrated models.\n- **Sparse Activation**: Techniques like SMA and SPARSEK offer significant reductions in computational complexity and memory usage, which are critical for efficient long-sequence processing.\n- **State Space Models**: SSMs like Mamba and LOCOST efficiently handle long sequences, providing a strong foundation for integrating with sparse attention mechanisms.\n- **Hierarchical and Efficient Models**: Combining SSMs with sparse attention can create hierarchical and efficient autoregressive language models, leveraging the strengths of both approaches for improved performance.\n\n#### 4. Future Search Plan\n- **Explore Integration Techniques**: Investigate methods for integrating SSMs with sparse attention mechanisms in hierarchical architectures.\n- **Evaluate Practical Implementations**: Look for practical implementations and empirical results of combining these techniques in existing models.\n\n#### 5. List of References\n1. Martins, P. H., Marinho, Z., & Martins, A. F. T. (2022). *\u221e-former: Infinite Memory Transformer*.\n2. Zhai, C., Zhu, C., Xu, Y., Wang, S., Liu, Y., & Ren, L. (2023). *Sparse Modular Activation for Efficient Sequence Modeling*. sparse-modular-activation-for-efficient.\n3. Li, W., Zhou, H., Yu, J., Song, Z., & Yang, W. (2024). *Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model*. arXiv.org.\n4. Ivgi, M., Shaham, U., & Berant, J. (2022). *Efficient Long-Text Understanding with Short-Text Models*. Transactions of the Association for Computational Linguistics."
        },
        {
            "ready": false,
            "query": "\"hybrid model training strategies\", \"memory optimization techniques\", \"hardware implementation neural networks\"",
            "detail": "Search for papers discussing:\n\n1. Training strategies for hybrid architectures:\n   - Stability techniques\n   - Gradient flow optimization\n   - Loss function design\n   - Parameter sharing strategies\n\n2. Memory optimization techniques:\n   - Cache-friendly implementations\n   - Memory bandwidth optimization\n   - State compression methods\n   - Efficient data movement\n\n3. Hardware-aware implementation:\n   - Parallel processing strategies\n   - Memory hierarchy utilization\n   - Cache optimization techniques\n   - Hardware-specific optimizations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n\n1. Training strategies for hybrid architectures:\n   - Stability techniques\n   - Gradient flow optimization\n   - Loss function design\n   - Parameter sharing strategies\n\n2. Memory optimization techniques:\n   - Cache-friendly implementations\n   - Memory bandwidth optimization\n   - State compression methods\n   - Efficient data movement\n\n3. Hardware-aware implementation:\n   - Parallel processing strategies\n   - Memory hierarchy utilization\n   - Cache optimization techniques\n   - Hardware-specific optimizations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.07)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.07)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.03)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 6/40 (Score: 0.03)*\n\n```\non pp. 1, 2, 9, 16). [3] Colin White et al. \"Neural architecture search: Insights from 1000 papers\". In: arXiv preprint arXiv:2301.08727 (2023) (cit.\n```\n\n#### 3. Linear Attention Sequence Parallelism (Avg. Score: 0.01)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 21/24 (Score: 0.01)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open pre-trained transformer language models, 2022. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch FSDP: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, B., Liu, J., Sun, W., Chen, R., Tomlin, C. J., and Yuan, Y. pbSGD: Powered stochastic gradient descent methods for accelerated non-convex optimization. In IJCAI, pp. $3258-3266,2020$. ## A. Appendix\n\n## A.1. Hardware and Software\n\nHardware. Our experimental configuration involves a maximum of 16 DGX-A100 servers, each equipped with 8 A100 GPUs, these GPUs are interconnected through NVSwitch, ensuring an inter-GPU bandwidth of 600GBps. For inter-node communication, we employ RoCE (RDMA over Converged Ethernet) technology, utilizing 8 RoCE RDMA adapters in each server.\n```\n\n#### 4. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.01)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.01)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hybrid model training strategies\", \"memory optimization techniques\", \"hardware implementation neural networks\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Prathyusha Jwalapuram, Shafiq R. Joty, Youlin Shen*\n\n**TL;DR:** A class of conditional generative-discriminative hybrid losses is introduced that is used to fine-tune a trained machine translation model and improves the model performance of both a sentence-level and a contextual model without using any additional data.\n\n**Abstract:** Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En. Code available at .\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 2. Adaptive Textual Label Noise Learning based on Pre-trained Models\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Shaohuan Cheng, Wenyu Chen, Mingsheng Fu, Xuanting Xie, Hong Qu*\n\n**TL;DR:** An adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage followed by a hybrid training stage that incorporates several generalization strategies to gradually correct mislabeled instances, thereby making better use of noisy data.\n\n**Abstract:** The label noise in real-world scenarios is unpredictable and can even be a mixture of different types of noise. To meet this challenge, we develop an adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage followed by a hybrid training stage. Specifically, an early stopping method, relying solely on the training set, is designed to dynamically terminate the warm-up process based on the model\u2019s fit level to different noise scenarios. The hybrid training stage incorporates several generalization strategies to gradually correct mislabeled instances, thereby making better use of noisy data. Experiments on multiple datasets demonstrate that our approach performs on-par with or even better than the state-of-the-art methods in various noise scenarios, including scenarios with the mixture of multiple types of noise.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Sai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich\u00e9, B. Dhoedt, Aaron C. Courville, Alexandre Lacoste*\n\n**TL;DR:** This work studies the URLB and proposes a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks.\n\n**Abstract:** Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-scale empirical study, which we use to validate our design choices and analyze our models. We also show robust performance on the Real-Word RL benchmark, hinting at resiliency to environment perturbations during adaptation. Project website: https://masteringurlb.github.io/\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 3*)\n\n#### 4. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Xiangning Chen, Chen Liang, Da Huang, Esteban Real, Kaiyuan Wang, Yao Liu, Hieu Pham, Xuanyi Dong, Thang Luong, Cho-Jui Hsieh, Yifeng Lu, Quoc V. Le*\n\n**TL;DR:** Lion is a simple and effective optimization algorithm that requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function and is more memory-efficient than Adam as it only keeps track of the momentum.\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 240  (*Influential: 40*)\n\n#### 5. Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen*\n\n**TL;DR:** This study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance, and introduces novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity.\n\n**Abstract:** In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 6. Fast as CHITA: Neural Network Pruning with Combinatorial Optimization\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Riade Benbaki, Wenyu Chen, X. Meng, Hussein Hazimeh, N. Ponomareva, Zhe Zhao, R. Mazumder*\n\n**TL;DR:** This work proposes a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint, and extends the classical Optimal Brain Surgeon framework, CHITA, which leads to significantly better sparsity-accuracy tradeoffs than competing methods.\n\n**Abstract:** The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 7. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Haoran You, Baopu Li, Huihong Shi, Y. Fu, Yingyan Lin*\n\n**TL;DR:** This work proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs and integrates the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs.\n\n**Abstract:** Neural networks (NNs) with intensive multiplications (e.g., convolutions and transformers) are capable yet power hungry, impeding their more extensive deployment into resource-constrained devices. As such, multiplication-free networks, which follow a common practice in energy-efficient hardware implementation to parameterize NNs with more efficient operators (e.g., bitwise shifts and additions), have gained growing attention. However, multiplication-free networks usually under-perform their vanilla counterparts in terms of the achieved accuracy. To this end, this work advocates hybrid NNs that consist of both powerful yet costly multiplications and efficient yet less powerful operators for marrying the best of both worlds, and proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs. Our ShiftAddNAS highlights two enablers. Specifically, it integrates (1) the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs; and (2) a novel weight sharing strategy that enables effective weight sharing among different operators that follow heterogeneous distributions (e.g., Gaussian for convolutions vs. Laplacian for add operators) and simultaneously leads to a largely reduced supernet size and much better searched networks. Extensive experiments and ablation studies on various models, datasets, and tasks consistently validate the efficacy of ShiftAddNAS, e.g., achieving up to a +7.7% higher accuracy or a +4.9 better BLEU score compared to state-of-the-art NN, while leading to up to 93% or 69% energy and latency savings, respectively. Codes and pretrained models are available at https://github.com/RICE-EIC/ShiftAddNAS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 12  (*Influential: 2*)\n\n#### 8. Linear Symmetric Quantization of Neural Networks for Low-precision Integer Hardware\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Xiandong Zhao, Ying Wang, Xuyi Cai, Cheng Liu, Lei Zhang*\n\n**TL;DR:** A learned linear symmetric quantizer for integer neural network processors is proposed, which not only quantizes neural parameters and activations to low-bit integer but also accelerates hardware inference by using batch normalization fusion and low-precision accumulators and multipliers.\n\n**Abstract:** With the proliferation of specialized neural network processors that operate on low-precision integers, the performance of Deep Neural Network inference becomes increasingly dependent on the result of quantization. Despite plenty of prior work on the quantization of weights or activations for neural networks, there is still a wide gap between the software quantizers and the low-precision accelerator implementation, which degrades either the efficiency of networks or that of the hardware for the lack of software and hardware coordination at design-phase. In this paper, we propose a learned linear symmetric quantizer for integer neural network processors, which not only quantizes neural parameters and activations to low-bit integer but also accelerates hardware inference by using batch normalization fusion and low-precision accumulators (e.g., 16-bit) and multipliers (e.g., 4-bit). We use a unified way to quantize weights and activations, and the results outperform many previous approaches for various networks such as AlexNet, ResNet, and lightweight models like MobileNet while keeping friendly to the accelerator architecture. Additional, we also apply the method to object detection models and witness high performance and accuracy in YOLO-v2. Finally, we deploy the quantized models on our specialized integer-arithmetic-only DNN accelerator to show the effectiveness of the proposed quantizer. We show that even with linear symmetric quantization, the results can be better than asymmetric or non-linear methods in 4-bit networks. In evaluation, the proposed quantizer induces less than 0.4\\% accuracy drop in ResNet18, ResNet34, and AlexNet when quantizing the whole network as required by the integer processors.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 58  (*Influential: 6*)\n\n#### 9. Deep Molecular Programming: A Natural Implementation of Binary-Weight ReLU Neural Networks\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Marko Vasi\u0107, Cameron T. Chalk, S. Khurshid, D. Soloveichik*\n\n**TL;DR:** It is shown how a BinaryConnect neural network trained in silico using well-founded deep learning optimization techniques, can be compiled to an equivalent chemical reaction network, providing a novel molecular programming paradigm, setting the stage for rich knowledge transfer between neural network and molecular programming communities.\n\n**Abstract:** Embedding computation in molecular contexts incompatible with traditional electronics is expected to have wide ranging impact in synthetic biology, medicine, nanofabrication and other fields. A key remaining challenge lies in developing programming paradigms for molecular computation that are well-aligned with the underlying chemical hardware and do not attempt to shoehorn ill-fitting electronics paradigms. We discover a surprisingly tight connection between a popular class of neural networks (binary-weight ReLU aka BinaryConnect) and a class of coupled chemical reactions that are absolutely robust to reaction rates. The robustness of rate-independent chemical computation makes it a promising target for bioengineering implementation. We show how a BinaryConnect neural network trained in silico using well-founded deep learning optimization techniques, can be compiled to an equivalent chemical reaction network, providing a novel molecular programming paradigm. We illustrate such translation on the paradigmatic IRIS and MNIST datasets. Toward intended applications of chemical computation, we further use our method to generate a chemical reaction network that can discriminate between different virus types based on gene expression levels. Our work sets the stage for rich knowledge transfer between neural network and molecular programming communities.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 16  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza*\n\n**Abstract:** Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n\n**Published:** 2024-03-14T10:33:28Z  (*Updated: 2024-03-14T10:33:28Z*)\n\n\n\n#### 2. ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural\n  Network for Reliable Intelligence Identification on Vietnamese SNSs\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Hoang Viet Trinh, Tung Tien Bui, Tam Minh Nguyen, Huy Quang Dao, Quang Huu Pham, Ngoc N. Tran, Ta Minh Thanh*\n\n**Abstract:** The overwhelming abundance of data has created a misinformation crisis.\nUnverified sensationalism that is designed to grab the readers' short attention\nspan, when crafted with malice, has caused irreparable damage to our society's\nstructure. As a result, determining the reliability of an article has become a\ncrucial task. After various ablation studies, we propose a multi-input model\nthat can effectively leverage both tabular metadata and post content for the\ntask. Applying state-of-the-art finetuning techniques for the pretrained\ncomponent and training strategies for our complete model, we have achieved a\n0.9462 ROC-score on the VLSP private test set.\n\n**Published:** 2021-09-27T03:40:28Z  (*Updated: 2021-09-27T03:40:28Z*)\n\n\n\n#### 3. CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*\n\n**Abstract:** We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.\n\n**Published:** 2024-10-20T10:24:38Z  (*Updated: 2024-10-20T10:24:38Z*)\n\n\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 5. Language Modeling on a SpiNNaker 2 Neuromorphic Chip\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Khaleelulla Khan Nazeer, Mark Sch\u00f6ne, Rishav Mukherji, Bernhard Vogginger, Christian Mayr, David Kappel, Anand Subramoney*\n\n**Abstract:** As large language models continue to scale in size rapidly, so too does the\ncomputational power required to run them. Event-based networks on neuromorphic\ndevices offer a potential way to reduce energy consumption for inference\nsignificantly. However, to date, most event-based networks that can run on\nneuromorphic hardware, including spiking neural networks (SNNs), have not\nachieved task performance even on par with LSTM models for language modeling.\nAs a result, language modeling on neuromorphic devices has seemed a distant\nprospect. In this work, we demonstrate the first-ever implementation of a\nlanguage model on a neuromorphic device - specifically the SpiNNaker 2 chip -\nbased on a recently published event-based architecture called the EGRU.\nSpiNNaker 2 is a many-core neuromorphic chip designed for large-scale\nasynchronous processing, while the EGRU is architected to leverage such\nhardware efficiently while maintaining competitive task performance. This\nimplementation marks the first time a neuromorphic language model matches\nLSTMs, setting the stage for taking task performance to the level of large\nlanguage models. We also demonstrate results on a gesture recognition task\nbased on inputs from a DVS camera. Overall, our results showcase the\nfeasibility of this neuro-inspired neural network in hardware, highlighting\nsignificant gains versus conventional hardware in energy efficiency for the\ncommon use case of single batch inference.\n\n**Published:** 2023-12-14T16:16:35Z  (*Updated: 2024-01-24T10:56:24Z*)\n\n\n\n#### 6. Convolutional neural network compression for natural language processing\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Krzysztof Wr\u00f3bel, Marcin Pietro\u0144, Maciej Wielgosz, Micha\u0142 Karwatowski, Kazimierz Wiatr*\n\n**Abstract:** Convolutional neural networks are modern models that are very efficient in\nmany classification tasks. They were originally created for image processing\npurposes. Then some trials were performed to use them in different domains like\nnatural language processing. The artificial intelligence systems (like humanoid\nrobots) are very often based on embedded systems with constraints on memory,\npower consumption etc. Therefore convolutional neural network because of its\nmemory capacity should be reduced to be mapped to given hardware. In this\npaper, results are presented of compressing the efficient convolutional neural\nnetworks for sentiment analysis. The main steps are quantization and pruning\nprocesses. The method responsible for mapping compressed network to FPGA and\nresults of this implementation are presented. The described simulations showed\nthat 5-bit width is enough to have no drop in accuracy from floating point\nversion of the network. Additionally, significant memory footprint reduction\nwas achieved (from 85% up to 93%).\n\n**Published:** 2018-05-28T07:40:33Z  (*Updated: 2018-05-28T07:40:33Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*dianhai yu, Yanjun Ma, Yu Sun, Ge Li, Yue Yu, Yaqian Han, Shaohuai Shi, Bin Wang, Long Li, Yongshuai Hou, Peng Liu, Shuohuan Wang, Yuang Liu, Xianjie Mo, Siyu Ding, Weibao Gong, Zhihua Wu, Yang Xiang*\n\n**Abstract:** The ever-growing model size and scale of compute have attracted increasing interests in training deep learning models over multiple nodes. However, when it comes to training on cloud clusters, especially across remote clusters, huge challenges are faced. In this work, we introduce a general framework, Nebula-I, for collaboratively training deep learning models over remote heterogeneous clusters, the connections between which are low-bandwidth wide area networks (WANs). We took natural language processing (NLP) as an example to show how Nebula-I works in different training phases that include: a) pre-training a multilingual language model using two remote clusters; and b) fine-tuning a machine translation model using knowledge distilled from pre-trained models, which run through the most popular paradigm of recent deep learning. To balance the accuracy and communication efficiency, in Nebula-I, parameter-efficient training strategies, hybrid parallel computing methods and adaptive communication acceleration techniques are jointly applied. Meanwhile, security strategies are employed to guarantee the safety, reliability and privacy in intra-cluster computation and inter-cluster communication. Nebula-I is implemented with the PaddlePaddle deep learning framework, which can support collaborative training over heterogeneous hardware, e.g. GPU and NPU. Experiments demonstrate that the proposed framework could substantially maximize the training efficiency while preserving satisfactory NLP performance. By using Nebula-I, users can run large-scale training tasks over cloud clusters with minimum developments, and the utility of existed large pre-trained models could be further promoted. We also introduced new state-of-the-art results on cross-lingual natural language inference tasks, which are generated based upon a novel learning framework and Nebula-I.\n\n**Published:** 2022-05-19\n\n\n\n#### 2. Freeing Hybrid Distributed AI Training Configuration\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Haoran Wang*\n\n**Abstract:** Deep neural network (DNN) has become the leading technology to realize Artificial Intelligence (AI). As DNN models become larger and more complex, so do datasets. Being able to efficiently train DNNs in parallel has become a crucial need. Data Parallelism (DP) is the widest-used solution today to accelerate DNN training but could be inefficient when processing DNNs with large-size parameters. Hybrid Parallelism (HP), which applies different parallel strategies on different parts of DNNs, is more efficient but requires advanced configurations. Not all AI researchers are experts in parallel computing, thus automating the configuration of HP strategies is very desirable for all AI frameworks. We propose a parallel semantics analysis method, which can analyze the trade-offs among different kinds of parallelisms and systematically choose the HP strategies with good training time performance. We demonstrated experimentally 260% speedup when applying our method compared to using a conventional DP approach. With our proposal, AI researchers would be able to focus more on AI algorithm research without being disturbed by parallel analysis and engineering concerns.\n\n**Proceeding:** proceedings-of-the-29th-acm-joint-meeting-on\n\n**Published:** 2021-08-20\n\n\n\n#### 3. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Shen Li, Ajit Mathews, Yuchen Hao, Geeta Chauhan, Pritam Damania, Bernard Nguyen, Can Balioglu, Alban Desmaison, Sam Shleifer, Myle Ott, Hamid Shojanazeri, Less Wright, Min Xu, Chien-chin Huang, Liang Luo, Rohan Varma, Andrew Gu, Yanli Zhao*\n\n**Abstract:** It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.\n\n**Published:** 2023-04-21\n\n\n\n#### 4. Elixir: Train a Large Language Model on a Small GPU Cluster\n\n*From Search Query: \"memory optimization techniques\"*\n\n*Yang You, Shenggui Li, Hongxin Liu, Jiarui Fang, Haichen Huang*\n\n**Abstract:** In recent years, large language models have achieved great success due to their unprecedented size. However, training these models poses a challenge for most researchers as it requires a substantial number of GPUs. To reduce GPU memory usage, memory partitioning, and memory offloading have been proposed. These approaches eliminate memory redundancies and offload memory usage to the CPU and NVMe memory, respectively, enabling training on small GPU clusters. However, directly deploying these solutions often leads to suboptimal efficiency. Only experienced experts can unleash the full potential of hardware by carefully tuning the distributed configuration. Thus, we present a novel solution, Elixir, which automates efficient large-model training based on pre-runtime model profiling. Elixir aims to identify the optimal combination of partitioning and offloading techniques to maximize training throughput. In our experiments, Elixir significantly outperforms the current state-of-the-art baseline. Our optimal configuration achieves up to a 3.4$\\times$ speedup on GPT-2 models compared with SOTA solutions. We hope that our work will benefit individuals who lack computing resources and expertise, granting them access to large models. The beta version of Elixir is now available at https://github.com/hpcaitech/ColossalAI/tree/feature/elixir.\n\n**Published:** 2022-12-10\n\n\n\n#### 5. Recurrent Neural Networks Hardware Implementation on FPGA\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Eugenio Culurciello, Andre Xian Ming Chang, Berin Martini*\n\n**Abstract:** Recurrent Neural Networks (RNNs) have the ability to retain memory and learn\ndata sequences. Due to the recurrent nature of RNNs, it is sometimes hard to\nparallelize all its computations on conventional hardware. CPUs do not\ncurrently offer large parallelism, while GPUs offer limited parallelism due to\nsequential components of RNN models. In this paper we present a hardware\nimplementation of Long-Short Term Memory (LSTM) recurrent network on the\nprogrammable logic Zynq 7020 FPGA from Xilinx. We implemented a RNN with $2$\nlayers and $128$ hidden units in hardware and it has been tested using a\ncharacter level language model. The implementation is more than $21\\times$\nfaster than the ARM CPU embedded on the Zynq 7020 FPGA. This work can\npotentially evolve to a RNN co-processor for future mobile devices.\n\n**Published:** 2015-11-17\n\n\n\n#### 6. Stochastic Computing for Hardware Implementation of Binarized Neural Networks\n\n*From Search Query: \"hardware implementation neural networks\"*\n\n*Jean-Michel Portal, Jacques-Olivier Klein, Bogdan Penkovsky, Damien Querlioz, Tifenn Hirtzlin, Marc Bocquet*\n\n**Abstract:** Binarized Neural Networks, a recently discovered class of neural networks with minimal memory requirements and no reliance on multiplication, are a fantastic opportunity for the realization of compact and energy efficient inference hardware. However, such neural networks are generally not entirely binarized: their first layer remains with fixed point input. In this work, we propose a stochastic computing version of Binarized Neural Networks, where the input is also binarized. Simulations on the example of the Fashion-MNIST and CIFAR-10 datasets show that such networks can approach the performance of conventional Binarized Neural Networks. We evidence that the training procedure should be adapted for use with stochastic computing. Finally, the ASIC implementation of our scheme is investigated, in a system that closely associates logic and memory, implemented by Spin Torque Magnetoresistive Random Access Memory. This analysis shows that the stochastic computing approach can allow considerable savings with regards to conventional Binarized Neural networks in terms of area (62% area reduction on the Fashion-MNIST task). It can also allow important savings in terms of energy consumption, if we accept reasonable reduction of accuracy: for example a factor 2.1 can be saved, with the cost of 1.4% in Fashion-MNIST test accuracy. These results highlight the high potential of Binarized Neural Networks for hardware implementation, and that adapting them to hardware constrains can provide important benefits.\n\n**Published:** 2019-06-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hybrid architectures, memory optimization, and hardware-aware implementation, here are some key points and directions based on the provided analysis and the need for more specific information:\n\n### Training Strategies for Hybrid Architectures\n\n#### Stability Techniques\n- Ensuring stability in hybrid architectures involves careful integration of different components. For example, the Mamba paper suggests that letting SSM parameters be functions of the input addresses can improve the model's ability to perform content-based reasoning. This adaptive parameter approach can help stabilize the training process by reducing the impact of discrete modalities.\n- Gradient flow optimization is crucial for stability. Techniques such as gradient clipping, learning rate scheduling, and using gradient normalization can help stabilize the training process. The Gated Linear Attention paper discusses how optimizing gradient flow can lead to more stable and efficient training of hybrid models.\n\n#### Gradient Flow Optimization\n- Gradient flow optimization in hybrid models can be enhanced by using techniques like gradient checkpointing, which reduces memory usage during backpropagation. This is particularly useful in models that combine SSMs with attention mechanisms, as it helps manage the computational and memory requirements.\n- The use of adaptive learning rate schedulers, such as those based on cosine annealing or exponential decay, can also help in optimizing the gradient flow and stabilizing the training process.\n\n#### Loss Function Design\n- Designing an appropriate loss function is critical for the performance of hybrid models. For instance, using a combination of cross-entropy loss for the language modeling task and additional losses to regularize the SSM parameters can help in achieving better stability and performance. The DenseMamba paper highlights the importance of balancing different loss components to maintain training parallelizability and inference efficiency.\n\n#### Parameter Sharing Strategies\n- Parameter sharing between different components of the hybrid model can be optimized by using techniques such as weight tying and hierarchical parameter sharing. This approach can reduce the overall number of parameters, improve generalization, and enhance training stability. The CHELA paper discusses the use of hierarchical structures and tiling strategies, which can be adapted for parameter sharing in hybrid models.\n\n### Memory Optimization Techniques\n\n#### Cache-Friendly Implementations\n- To achieve cache-friendly implementations, tiling strategies can be employed to optimize memory access patterns. This involves dividing the data into smaller tiles that fit within the cache, reducing the number of cache misses and improving overall performance. The CHELA paper and the analysis on hierarchical processing highlight the effectiveness of tiling strategies for cache utilization.\n\n#### Memory Bandwidth Optimization\n- Optimizing memory bandwidth involves efficient data movement and minimizing unnecessary data transfers. Techniques such as data compression, using sparse data structures, and optimizing the memory hierarchy can help. For example, the DenseMamba paper discusses how dense connections can enhance information flow while managing memory bandwidth efficiently.\n\n#### State Compression Methods\n- State compression methods are essential for reducing memory usage in hybrid models. Techniques such as quantization, pruning, and knowledge distillation can be applied to compress the state of SSMs without significant loss in performance. The Mamba paper suggests that selective state spaces can help in reducing memory requirements while maintaining performance.\n\n#### Efficient Data Movement\n- Efficient data movement can be achieved by optimizing the data pipeline and using parallel processing strategies. This includes using batch processing, pipelining, and asynchronous data loading to minimize idle times and maximize throughput. The Gated Linear Attention paper discusses how parallel processing can be optimized for hardware-efficient training.\n\n### Hardware-Aware Implementation\n\n#### Parallel Processing Strategies\n- Parallel processing strategies are crucial for leveraging modern hardware efficiently. Techniques such as data parallelism, model parallelism, and pipeline parallelism can be used to distribute the computation across multiple GPUs or other accelerators. The DenseMamba paper and the analysis on hierarchical processing emphasize the importance of parallelizability in hybrid models.\n\n#### Memory Hierarchy Utilization\n- Utilizing the memory hierarchy effectively involves optimizing the use of different levels of memory (e.g., registers, cache, main memory). Tiling strategies and careful memory access patterns can help in maximizing the use of faster memory levels. The CHELA paper provides insights into how hierarchical structures can be used to optimize memory hierarchy utilization.\n\n#### Cache Optimization Techniques\n- Cache optimization techniques include prefetching, cache blocking, and minimizing cache thrashing. These techniques can significantly improve the performance of hybrid models by reducing the latency associated with memory accesses. The analysis on cache-friendly implementations highlights the importance of these techniques.\n\n#### Hardware-Specific Optimizations\n- Hardware-specific optimizations involve tailoring the model and its training process to the specific capabilities of the hardware. For example, using tensor cores for matrix multiplications, leveraging FP16 or INT8 precision for reduced memory and computational requirements, and optimizing for specific memory bandwidths can lead to significant performance improvements. The Gated Linear Attention paper discusses how hardware-efficient training can be achieved through such optimizations.\n\nBy focusing on these areas, researchers can develop more efficient, stable, and scalable autoregressive language models that leverage the strengths of hybrid architectures while optimizing for memory and hardware constraints.\n",
            "analysis": "Summary of Analysis:\n\nThe search results provide comprehensive insights into state space models (SSMs), hierarchical processing, and hardware-efficient implementations. Key findings reveal both promising directions and areas requiring further investigation.\n\nUseful References with Excerpts:\n\n1. From Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\n2. From DenseMamba:\n\"DenseSSM retains fine-grained information crucial for the final output [...] maintains the training parallelizability and inference efficiency\"\n\n3. From CHELA paper:\n\"leverage the thought of tiling and hierarchy [...] implements linear attention in a divide-and-conquer manner\"\n\n4. From Gated Linear Attention paper:\n\"FLASHLINEARATTENTION is faster than FLASHATTENTION-2 [...] as a standalone layer even on short sequence lengths\"\n\nKey Insights:\n\n1. SSM Integration:\n- SSMs show promise for efficient sequence processing\n- Need careful integration with attention mechanisms\n- Content-based reasoning is crucial for performance\n- Parameter adaptation based on input improves effectiveness\n\n2. Hierarchical Processing:\n- Multi-scale processing improves efficiency\n- Tiling and divide-and-conquer strategies are effective\n- Dense connections enhance information flow\n- Balance needed between local and global processing\n\n3. Hardware Efficiency:\n- Memory access patterns crucial for performance\n- Tiling strategies improve cache utilization\n- Linear complexity achievable with careful design\n- Trade-off between parallelism and memory efficiency\n\nNeed More Information On:\n\n1. Specific integration techniques:\n- How to combine SSMs with sparse attention effectively\n- Optimal parameter sharing strategies\n- Memory management between components\n\n2. Training dynamics:\n- Stability of hybrid architectures\n- Gradient flow optimization\n- Loss function design\n\n3. Hardware optimization:\n- Cache-friendly implementations\n- Memory bandwidth utilization\n- Parallel processing strategies\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Training strategies for hybrid architectures\n2. Memory optimization techniques\n3. Hardware-aware implementation details\n\nReferences:\n\n[1] Dao, T., & Gu, A. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv:2312.00752.\n\n[2] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\" arXiv:2401.04722.\n\n[3] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention.\" ICLR 2024.\n\n[4] Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\" arXiv:2312.06635."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"state space models language models\", \"sparse attention transformers\", \"hierarchical neural networks\"",
            "detail": "Search for papers discussing:\n1. Integration of state space models with attention mechanisms\n2. Efficient sparse attention patterns and their implementation\n3. Hierarchical processing in language models\n4. Memory optimization techniques for long sequence processing\n5. Hardware-efficient implementations of SSMs and sparse attention\n\nFocus on:\n- Architectural details of SSM integration\n- Sparse attention pattern effectiveness\n- Memory-computation trade-offs\n- Scalability solutions\n- Implementation challenges and solutions",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Integration of state space models with attention mechanisms\n2. Efficient sparse attention patterns and their implementation\n3. Hierarchical processing in language models\n4. Memory optimization techniques for long sequence processing\n5. Hardware-efficient implementations of SSMs and sparse attention\n\nFocus on:\n- Architectural details of SSM integration\n- Sparse attention pattern effectiveness\n- Memory-computation trade-offs\n- Scalability solutions\n- Implementation challenges and solutions\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 1.00)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 1.00)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 2/35 (Score: 1.00)*\n\n```\nHowever, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for State space $\\underline{A} u g m e n t e \\underline{D}$ TransformEr. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks. ## 1 Introduction\n\nTransformer models have achieved superior performance on various natural language processing tasks such as language modeling (Dai et al., 2019), natural language generation (Brown et al., 2020) and natural language understanding (Devlin et al., 2019; He et al., 2021). These models leverage the attention mechanism (Vaswani et al., 2017), which computes a dependency score for every pair\n\n[^0]of tokens in an input sequence. Therefore, full attention has a quadratic time and space complexity with respect to the sequence length. However, such a complexity is computationally prohibitive for tasks that involve long sequences, such as text summarization (Nallapati et al., 2016) and question answering (Kwiatkowski et al., 2019). For example, empirically we find that a Transformer model ( 250 M parameters) consumes over 80 G of GPU memory when the sequence length is 8 k . Additionally, Transformer models equipped with the full attention are easy to overfit because of the lack of structural biases (Lin et al., 2022). That is, the attention mechanism does not assume any structural prior over the inputs. For example, we even need order information (e.g., through sinusoidal encoding) to train a Transformer model. Therefore, the full attention is too flexible such that Transformer models may easily overfit to the noise. This significantly limits the models' practicality in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is often low. Empirically, we find that on a two-way classification task, Transformer with the full attention has a $57.5 \\%$ accuracy, nearly $30 \\%$ less than stateof-the-art methods with powerful structural biases (see Section 4.1 for details). Various approaches have been proposed to reduce the quadratic complexity and/or to introduce structural biases. In approximation methods, we approximate the full attention using fast algorithms with linear complexity. For example, we can approximate and speedup the computation of the attention score matrix (i.e., $\\operatorname{softmax}\\left(\\mathbf{Q K}^{\\top} / \\sqrt{d}\\right)$ in Eq. 1) using low-rank approximation (Wang et al., 2020b) or kernel methods (Peng et al., 2021). However, even though these methods reduce the complexity of full attention, they inherit the lack of structural bias issue. To incorporate structural biases to the Transformer model, partial attention methods are pro-\nposed. Such methods can be further categorized into sparse attention and clustering methods. In sparse attention (Beltagy et al., 2020), each token only attends to a subset of all the tokens according to pre-defined sparsity patterns. In clustering methods (Kitaev et al., 2020), tokens are divided into several clusters, and only intra-cluster attention is performed. However, the introduced structural biases restrict the models' ability to capture global information. For example, in local-window attention, we assume each token only depends on its neighbors, such that we inevitably lose long-range and global information. Contrary to partial attention, state space models (SSMs) introduce a different structural bias (Gu et al., 2021), which is tailored for computing global information. Specifically, SSMs design fixed global dependency patterns that facilitate effective and efficient computation. These models can be seen as linear recurrent neural networks with specifically designed fixed weights. Moreover, efficient algorithms are crafted for training such models. However, the integrated structural bias is restrictive in that SSMs are not refined enough to capture local information. This is because unlike attention, SSMs do not explicitly compute dependencies between input tokens. We propose SPADE, short for State space $\\underline{\\text { Augmente }} \\underline{\\mathbf{D}}$ TransformEr. The proposed model is a multi-layer Transformer model that can effectively and efficiently capture complicated dependencies. Specifically, we augment a SSM into the bottom layer of the model, such that after this layer, inputs are integrated with global information. Because the SSM only provides coarse global information, at the subsequent top layers of SPADE, we employ local attention variants to capture more complicated and refined local information. In other words, in SPADE, the SSM induces a strong structural bias that augments global information, and it complements the lack of long-range dependency issue in local attention methods. We demonstrate the efficiency and effectiveness of SPADE on various natural language processing tasks. First, we show that the proposed method outperforms existing approaches on the Long Range Arena (Tay et al., 2021b) benchmark, which is designed to test models' ability in modeling long sequences. Second, we show that in autoregressive language modeling, SPADE is not only significantly faster than the vanilla Transformer (Vaswani et al., 2017), but also yields better performance. Third, we demonstrate the scalability of SPADE by conducting language model pre-training and finetuning experiments. Specifically, we pre-train an encoder-decoder model similar to T5 (Raffel et al., 2020). And we fine-tune the model on various tasks, including natural language understanding and natural language generation benchmarks. In all the settings, SPADE outperforms the baselines.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.99)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.99)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 5. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"state space models language models\", \"sparse attention transformers\", \"hierarchical neural networks\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization\n\n*From Search Query: \"state space models language models\"*\n\n*Shida Wang, Qianxiao Li*\n\n**TL;DR:** This paper introduces a class of reparameterization techniques for SSMs that effectively lift its memory limitations and illustrates that a principled choice of reparameterization scheme can also enhance optimization stability.\n\n**Abstract:** In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this\"curse of memory\"as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets, language models and image classifications.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 2. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: \"state space models language models\"*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Exphormer: Sparse Transformers for Graphs\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Hamed Shirzad, A. Velingker, B. Venkatachalam, Danica J. Sutherland, A. Sinop*\n\n**TL;DR:** It is shown that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets.\n\n**Abstract:** Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that Exphormer can scale to datasets on larger graphs than shown in previous graph transformer architectures. Code can be found at \\url{https://github.com/hamed1375/Exphormer}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 67  (*Influential: 15*)\n\n#### 4. White-Box Transformers via Sparse Rate Reduction\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Yaodong Yu, Sam Buchanan, Druv Pai, Tianzhe Chu, Ziyang Wu, Shengbang Tong, B. Haeffele, Y. Ma*\n\n**TL;DR:** This paper contends that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces, and shows that the standard transformer block can be derived from alternating optimization on complementary parts of this objective.\n\n**Abstract:** In this paper, we contend that the objective of representation learning is to compress and transform the distribution of the data, say sets of tokens, towards a mixture of low-dimensional Gaussian distributions supported on incoherent subspaces. The quality of the final representation can be measured by a unified objective function called sparse rate reduction. From this perspective, popular deep networks such as transformers can be naturally viewed as realizing iterative schemes to optimize this objective incrementally. Particularly, we show that the standard transformer block can be derived from alternating optimization on complementary parts of this objective: the multi-head self-attention operator can be viewed as a gradient descent step to compress the token sets by minimizing their lossy coding rate, and the subsequent multi-layer perceptron can be viewed as attempting to sparsify the representation of the tokens. This leads to a family of white-box transformer-like deep network architectures which are mathematically fully interpretable. Despite their simplicity, experiments show that these networks indeed learn to optimize the designed objective: they compress and sparsify representations of large-scale real-world vision datasets such as ImageNet, and achieve performance very close to thoroughly engineered transformers such as ViT. Code is at \\url{https://github.com/Ma-Lab-Berkeley/CRATE}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 50  (*Influential: 4*)\n\n#### 5. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 6. Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*Di Jin, Peter Szolovits*\n\n**TL;DR:** This work presents a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence.\n\n**Abstract:** Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 71  (*Influential: 16*)\n\n#### 7. How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*Aditya Yedetore, Tal Linzen, R. Frank, R. Thomas McCoy*\n\n**TL;DR:** It is suggested that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.\n\n**Abstract:** When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children\u2019s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children\u2019s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 8. Discourse Parsing with Attention-based Hierarchical Neural Networks\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*Qi Li, Tianshi Li, Baobao Chang*\n\n**TL;DR:** Experimental results show that the proposed attention-based hierarchical neural network model for discourse parsing obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.\n\n**Abstract:** RST-style document-level discourse parsing remains a dif\ufb01cult task and ef\ufb01cient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2016\n\n**Citations:** 98  (*Influential: 14*)\n\n### 6 related papers from ArXiv\n\n#### 1. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: \"state space models language models\"*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 2. Bilingual-GAN: A Step Towards Parallel Text Generation\n\n*From Search Query: \"state space models language models\"*\n\n*Ahmad Rashid, Alan Do-Omri, Md. Akmal Haidar, Qun Liu, Mehdi Rezagholizadeh*\n\n**Abstract:** Latent space based GAN methods and attention based sequence to sequence\nmodels have achieved impressive results in text generation and unsupervised\nmachine translation respectively. Leveraging the two domains, we propose an\nadversarial latent space based model capable of generating parallel sentences\nin two languages concurrently and translating bidirectionally. The bilingual\ngeneration goal is achieved by sampling from the latent space that is shared\nbetween both languages. First two denoising autoencoders are trained, with\nshared encoders and back-translation to enforce a shared latent state between\nthe two languages. The decoder is shared for the two translation directions.\nNext, a GAN is trained to generate synthetic \"code\" mimicking the languages'\nshared latent space. This code is then fed into the decoder to generate text in\neither language. We perform our experiments on Europarl and Multi30k datasets,\non the English-French language pair, and document our performance using both\nsupervised and unsupervised machine translation.\n\n**Published:** 2019-04-09T15:42:08Z  (*Updated: 2019-05-14T19:57:24Z*)\n\n\n\n#### 3. Explicit Sparse Transformer: Concentrated Attention Through Explicit\n  Selection\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Guangxiang Zhao, Junyang Lin, Zhiyuan Zhang, Xuancheng Ren, Qi Su, Xu Sun*\n\n**Abstract:** Self-attention based Transformer has demonstrated the state-of-the-art\nperformances in a number of natural language processing tasks. Self-attention\nis able to model long-term dependencies, but it may suffer from the extraction\nof irrelevant information in the context. To tackle the problem, we propose a\nnovel model called \\textbf{Explicit Sparse Transformer}. Explicit Sparse\nTransformer is able to improve the concentration of attention on the global\ncontext through an explicit selection of the most relevant segments. Extensive\nexperimental results on a series of natural language processing and computer\nvision tasks, including neural machine translation, image captioning, and\nlanguage modeling, all demonstrate the advantages of Explicit Sparse\nTransformer in model performance. We also show that our proposed sparse\nattention method achieves comparable or better results than the previous sparse\nattention method, but significantly reduces training and testing time. For\nexample, the inference speed is twice that of sparsemax in Transformer model.\nCode will be available at\n\\url{https://github.com/lancopku/Explicit-Sparse-Transformer}\n\n**Published:** 2019-12-25T10:59:31Z  (*Updated: 2019-12-25T10:59:31Z*)\n\n\n\n#### 4. SEA: Sparse Linear Attention with Estimated Attention Mask\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Heejun Lee, Jina Kim, Jeffrey Willette, Sung Ju Hwang*\n\n**Abstract:** The transformer architecture has driven breakthroughs in recent years on\ntasks which require modeling pairwise relationships between sequential\nelements, as is the case in natural language understanding. However, long\nseqeuences pose a problem due to the quadratic complexity of the attention\noperation. Previous research has aimed to lower the complexity by sparsifying\nor linearly approximating the attention matrix. Yet, these approaches cannot\nstraightforwardly distill knowledge from a teacher's attention matrix and often\nrequire complete retraining from scratch. Furthermore, previous sparse and\nlinear approaches lose interpretability if they cannot produce full attention\nmatrices. To address these challenges, we propose SEA: Sparse linear attention\nwith an Estimated Attention mask. SEA estimates the attention matrix with\nlinear complexity via kernel-based linear attention, then subsequently creates\na sparse attention matrix with a top-k selection to perform a sparse attention\noperation. For language modeling tasks (Wikitext2), previous linear and sparse\nattention methods show roughly two-fold worse perplexity scores over the\nquadratic OPT-1.3B baseline, while SEA achieves better perplexity than\nOPT-1.3B, using roughly half the memory of OPT-1.3B, providing interpretable\nattention matrix. We believe that our work will have a large practical impact,\nas it opens the possibility of running large transformers on resource-limited\ndevices with less memory.\n\n**Published:** 2023-10-03T03:56:26Z  (*Updated: 2024-03-25T04:04:05Z*)\n\n\n\n#### 5. Hierarchical Neural Network Architecture In Keyword Spotting\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*Yixiao Qu, Sihao Xue, Zhenyi Ying, Hang Zhou, Jue Sun*\n\n**Abstract:** Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it\nis essential to ensure a high recall rate. However, its real-time property\nrequires low computation complexity. This contradiction inspires people to find\na suitable model which is small enough to perform well in multi environments.\nTo deal with this contradiction, we implement the Hierarchical Neural\nNetwork(HNN), which is proved to be effective in many speech recognition\nproblems. HNN outperforms traditional DNN and CNN even though its model size\nand computation complexity are slightly less. Also, its simple topology\nstructure makes easy to deploy on any device.\n\n**Published:** 2018-11-06T12:32:27Z  (*Updated: 2018-11-06T12:32:27Z*)\n\n\n\n#### 6. Self-organized Hierarchical Softmax\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*Yikang Shen, Shawn Tan, Chrisopher Pal, Aaron Courville*\n\n**Abstract:** We propose a new self-organizing hierarchical softmax formulation for\nneural-network-based language models over large vocabularies. Instead of using\na predefined hierarchical structure, our approach is capable of learning word\nclusters with clear syntactical and semantic meaning during the language model\ntraining process. We provide experiments on standard benchmarks for language\nmodeling and sentence compression tasks. We find that this approach is as fast\nas other efficient softmax approximations, while achieving comparable or even\nbetter performance relative to similar full softmax models.\n\n**Published:** 2017-07-26T18:01:32Z  (*Updated: 2017-07-26T18:01:32Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: \"state space models language models\"*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"state space models language models\"*\n\n*Christopher R\u00e9, Atri Rudra, Armin W. Thomas, Khaled K. Saab, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28\n\n\n\n#### 3. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 4. Efficient Content-Based Sparse Attention with Routing Transformers\n\n*From Search Query: \"sparse attention transformers\"*\n\n*Mohammad Saffar, David Grangier, Ashish Vaswani, Aurko Roy*\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic compute and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: it combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to $O\\left(n^{1.5}d\\right)$ from $O\\left(n^2d\\right)$ for sequence length $n$ and hidden dimension $d$. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity) as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192.\n\n**Published:** 2020-03-12\n\n\n\n#### 5. Learning Hierarchical Graph Neural Networks for Image Clustering\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*David Wipf, Stefano Soatto, Zheng Zhang, Wei Xia, Yuanjun Xiong, Yongxin Wang, Tianjun Xiao, Tong He, Yifan Xing*\n\n**Abstract:** We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an unknown number of identities using a training set of images annotated with labels belonging to a disjoint set of identities. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 54% improvement in F-score and 8% increase in Normalized Mutual Information (NMI) relative to current GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as intermediate steps of the clustering process. In contrast, our unified framework achieves a seven-fold decrease in computational cost. We release our training and inference code at https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-07-03\n\n\n\n#### 6. HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\n\n*From Search Query: \"hierarchical neural networks\"*\n\n*Shumin Han, Errui Ding, Dongliang He, Ruoyu Guo, Yuning Du, Cheng Cui, Shufei Lin, Pengcheng Yuan*\n\n**Abstract:** This paper addresses representational block named Hierarchical-Split Block, which can be taken as a plug-and-play block to upgrade existing convolutional neural networks, improves model performance significantly in a network. Hierarchical-Split Block contains many hierarchical split and concatenate connections within one single residual block. We find multi-scale features is of great importance for numerous vision tasks. Moreover, Hierarchical-Split block is very flexible and efficient, which provides a large space of potential network architectures for different applications. In this work, we present a common backbone based on Hierarchical-Split block for tasks: image classification, object detection, instance segmentation and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baseline. As shown in Figure1, for image classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1 accuracy with competitive latency on ImageNet-1k dataset. It also outperforms most state-of-the-art models. The source code and models will be available on: https://github.com/PaddlePaddle/PaddleClas\n\n**Published:** 2020-10-15\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by integrating state space models (SSMs) with sparse attention mechanisms and hierarchical processing, here are some key findings and suggestions based on the provided sources and the detailed analysis:\n\n## Integration of State Space Models with Attention Mechanisms\n\n- The paper \"Taipan: Efficient and Expressive State Space Language Models with Selective Attention\" introduces a hybrid architecture that combines Mamba-2 (a state space model) with Selective Attention Layers (SALs). This integration balances the efficiency of SSMs with the performance of Transformers in tasks requiring long-range interactions. Taipan identifies tokens needing long-range interactions, removes less important features, and augments their representations using attention, thus addressing the trade-offs between SSM and attention mechanisms.\n\n## Efficient Sparse Attention Patterns and Their Implementation\n\n- The Taipan model also highlights the use of selective attention layers, which efficiently manage the attention budget to extend accurate predictions to long context lengths while maintaining computational efficiency. This approach is crucial for reducing the computational complexity associated with traditional Transformer models.\n\n- The concept of \"Selective Dimension tuning (SDLoRA)\" in the paper \"Parameter-Efficient Fine-Tuning of State Space Models\" suggests that selectively updating certain channels and states in SSM modules while applying LoRA to linear projection matrices can be an effective way to maintain model expressiveness with reduced attention. This method optimizes the use of parameters and attention, leading to better performance with fewer resources.\n\n## Hierarchical Processing in Language Models\n\n- The Taipan architecture demonstrates a hierarchical structure by combining Mamba-2 with SALs. This hybrid approach allows for efficient long-context language modeling by managing information flow between different processing levels. The SALs identify and prioritize tokens that require long-range interactions, which is a key aspect of hierarchical processing.\n\n## Memory Optimization Techniques for Long Sequence Processing\n\n- State Space Models like Mamba and the proposed Taipan model offer constant memory usage, which is a significant advantage over Transformer models that suffer from quadratic computational complexity and linearly scaling memory costs for long sequences. The integration of SSMs with sparse attention mechanisms in Taipan ensures that memory usage remains efficient even for very long context lengths.\n\n## Hardware-Efficient Implementations of SSMs and Sparse Attention\n\n- The paper on \"Parameter-Efficient Fine-Tuning of State Space Models\" discusses the application of LoRA and SDLoRA, which are methods that can be hardware-efficient due to their parameter-efficient nature. These methods do not require significant changes to the underlying SSM modules, making them compatible with various hardware configurations.\n\n## Architectural Details, Sparse Attention Pattern Effectiveness, and Memory-Computation Trade-offs\n\n- The Taipan model provides a detailed architectural design that integrates SSMs with sparse attention. It shows that by constraining the attention budget and selectively augmenting token representations, the model can achieve superior performance across various scales and tasks while maintaining computational efficiency.\n\n- The comparison between SSMs and Transformer models in the context of time series forecasting and language modeling highlights the advantages of SSMs in terms of memory efficiency and the ability to handle long sequences. However, SSMs may underperform in tasks requiring extensive in-context retrieval, which is where the integration with sparse attention mechanisms becomes crucial.\n\n## Scalability Solutions and Implementation Challenges\n\n- The scalability of the Taipan model is demonstrated through its ability to handle context lengths of up to 1 million tokens while preserving computational efficiency. This scalability is achieved through the selective attention mechanism and the efficient integration with SSMs.\n\n- Implementation challenges such as training stability with multiple mechanisms and memory management across hierarchical levels are important considerations. The use of methods like LoRA and SDLoRA can help in stabilizing the training process and optimizing memory usage.\n\nIn summary, the Taipan model and the research on parameter-efficient fine-tuning of SSMs provide valuable insights into integrating SSMs with sparse attention mechanisms, optimizing memory and computational resources, and achieving scalable and efficient language modeling architectures. These findings align well with the researcher's goals and can help in improving the design of autoregressive language models.\n",
            "analysis": "Initial Analysis of SparseStateTTT Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) State Space Model Integration:\n- How effectively can SSMs be integrated with sparse attention?\n- What are the trade-offs between SSM and attention mechanisms?\n- How to balance global and local dependencies?\n\nb) Sparse Attention Implementation:\n- What are the most effective sparse attention patterns?\n- How to maintain model expressiveness with reduced attention?\n- What are the memory-computation trade-offs?\n\nc) Hierarchical Processing:\n- How to effectively combine SSM and sparse attention in a hierarchical structure?\n- What is the optimal balance between different processing levels?\n- How to manage information flow between levels?\n\n2. Investigation Plan:\n\nPhase 1: SSM and Sparse Attention Integration\n- Research existing SSM implementations in language models\n- Study sparse attention mechanisms and their effectiveness\n- Analyze hybrid architectures combining different mechanisms\n\nPhase 2: Efficiency and Scalability\n- Investigate computational complexity of similar approaches\n- Study memory optimization techniques\n- Research scalability solutions for long sequences\n\nPhase 3: Implementation Feasibility\n- Examine practical implementations of SSMs\n- Study sparse attention optimization techniques\n- Research hardware-aware implementation strategies\n\n3. Comparison with Previous Designs:\n\nThe proposal appears to be distinct from previous designs:\n- Different from compressivehierarchicalttt which focuses on hardware optimization\n- Distinct from compressivememorygpt which emphasizes sparse key selection\n- Novel integration of SSMs with sparse attention not seen in previous proposals\n\n4. Initial Findings:\n\nThe proposal shows promise in:\n- Efficient long sequence processing through SSM integration\n- Reduced computational complexity via sparse attention\n- Improved scalability through hierarchical processing\n- Potential for better expressiveness through combined mechanisms\n\nHowever, several challenges need investigation:\n- Integration complexity of SSMs and sparse attention\n- Memory management across hierarchical levels\n- Training stability with multiple mechanisms\n- Hardware optimization requirements\n\nReferences found so far:\n1. \"Convolutional State Space Models for Long-Range Spatiotemporal Modeling\"\n2. \"Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention\"\n3. \"Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers\"\n\nNext steps require deeper investigation into:\n1. SSM implementations and their effectiveness\n2. Sparse attention patterns and their impact\n3. Hierarchical processing strategies\n4. Memory optimization techniques"
        },
        {
            "ready": false,
            "query": "\"sparse attention implementation techniques\", \"hybrid model training strategies\", \"neural network memory optimization\"",
            "detail": "Search for papers discussing:\n1. Implementation details of sparse attention mechanisms, focusing on:\n   - Different sparsity patterns\n   - Efficiency optimizations\n   - Hardware considerations\n\n2. Training strategies for hybrid architectures:\n   - Stability techniques\n   - Loss functions\n   - Gradient flow optimization\n\n3. Memory management and compression:\n   - State compression techniques\n   - Memory-efficient attention\n   - Hierarchical memory structures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Implementation details of sparse attention mechanisms, focusing on:\n   - Different sparsity patterns\n   - Efficiency optimizations\n   - Hardware considerations\n\n2. Training strategies for hybrid architectures:\n   - Stability techniques\n   - Loss functions\n   - Gradient flow optimization\n\n3. Memory management and compression:\n   - State compression techniques\n   - Memory-efficient attention\n   - Hierarchical memory structures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.47)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 2/41 (Score: 0.85)*\n\n```\nIn this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. ## 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges $[1,20,19]$, owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods $[39,61]$ allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually $\\operatorname{cost} O(n \\log n)$ to maintain clusters. Ainslie et al. [1]\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_81cbe08ae077927ce965g-02.jpg?height=549&width=1261&top_left_y=254&top_left_x=432)\n\nFigure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query ( $\\tau(\\mathbf{u})$ ) such that the sum of normalized scores is $k$, which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency. incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SofTTOPK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows. ## Incremental KV Selection. The SPARSEK operator (\u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (\u00a73.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments\non language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. ## 2 Related Work\n\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence $[17,35,36,12]$ or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions $[15,27,42]$. Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns $[34,13]$, or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].\n```\n\n##### *Relevant Chunk: No. 40/41 (Score: 0.09)*\n\n```\nHowever, our method is not dependent on the input modality. Future research involving vision or speech could further substantiate the robustness of our method. ## E Impact Statement\n\nThis paper presents SPARSEK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We believe our innovative attention mechanism can benefit both NLP and machine learning communities in constructing long-range foundation models. Specifically, we highlight the potential impacts of SPARSEK as follows:\n\n- Efficient Long-Range Modeling. First and foremost, the SPARSEK attention mechanism significantly reduces computational requirements compared to traditional self-attention mechanisms. By prioritizing a subset of key-value pairs, SPARSEK attention effectively reduces the memory footprint without sacrificing model performance.\n```\n\n#### 2. Sparse Sinkhorn Attention (Avg. Score: 0.41)\n\n*Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 285  (*Influential: 36*)\n\n**TL;DR:** This work introduces a meta sorting network that learns to generate latent permutations over sequences and is able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module.\n\n**Abstract:** We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.\n\n##### *Relevant Chunk: No. 23/23 (Score: 0.41)*\n\n```\nIn International conference on machine learning, pp. 2048-2057, 2015. [^0]:    ${ }^{1}$ Google AI. Correspondence to: Yi Tay $<$ yitay@google.com $>$. [^1]:    ${ }^{1}$ As an illustration, when $\\ell=1024$ and $N_{B}=64$, this results in a memory saving factor of 240 times. [^2]:    ${ }^{2}$ That said, Sparse Attention requires highly specialized GPU kernels for efficient computation. This generally makes the approach less appealing, e.g., for portability purposes such as running on TPU pods. [^3]:    ${ }^{3}$ tensor2tensor/models/research/1m_ experiments.py\n\n[^4]:    ${ }^{4}$ To the best of our knowledge, (Shazeer et al., 2018) is the best performing model on per-word perplexity. (Baevski \\& Auli, 2018) and (Dai et al., 2019) report per-token perplexity\n\n\n```\n\n#### 3. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.29)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.29)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n#### 4. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.24)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.24)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"sparse attention implementation techniques\", \"hybrid model training strategies\", \"neural network memory optimization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Sparse Attention with Learning to Hash\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Zhiqing Sun, Yiming Yang, Shinjae Yoo*\n\n**TL;DR:** A new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively is proposed, which is applicable to the wide range of pre-trained Transformer models.\n\n**Abstract:** Transformer has become ubiquitous in sequence modeling tasks. As a key component of Transformer, self-attention does not scale to long sequences due to its quadratic time and space complexity with respect to the sequence length. To tackle this problem, recent work developed dynamic attention sparsification techniques based on Approximate Nearest Neighbor (ANN) methods, where similar queries and keys are allocated to the same hash bucket with high probability. However, the effectiveness of those ANN methods relies on the assumption that queries and keys should lie in the same space, which is not well justified. Besides, some of the ANN methods such as Locality-Sensitive Hashing (LSH) are randomized and cannot fully utilize the available real data distributions. To overcome these issues, this paper proposes a new strategy for sparse attention, namely LHA (Learningto-Hash Attention), which directly learns separate parameterized hash functions for queries and keys, respectively. Another advantage of LHA is that it does not impose extra constraints for queries and keys, which makes it applicable to the wide range of pre-trained Transformer models. Our experiments on evaluation of the WikiText-103 dataset for language modeling, the GLUE benchmark for natural language understanding, and the Lang-Range-Arena benchmark for multiple tasks (text/image classification, retrieval, etc.) show the superior performance of LHA over other strong Transformer variants.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 2. Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, F. Fleuret*\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention\u2014which is the only component scaling quadratically w.r.t. the sequence length\u2014becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementation concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attention often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by 2 . 0 \u00d7 and 3 . 3 \u00d7 for sequences of respectively 8 k and 16 k tokens.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. Sparse Attention with Linear Units\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**TL;DR:** This work introduces a novel, simple method for achieving sparsity in attention: it replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges from such a formulation.\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. \u2018switch off\u2019) for some queries, which is not possible with sparsified softmax alternatives.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 4*)\n\n#### 4. Pronoun-Targeted Fine-tuning for NMT with Hybrid Losses\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Prathyusha Jwalapuram, Shafiq R. Joty, Youlin Shen*\n\n**TL;DR:** A class of conditional generative-discriminative hybrid losses is introduced that is used to fine-tune a trained machine translation model and improves the model performance of both a sentence-level and a contextual model without using any additional data.\n\n**Abstract:** Popular Neural Machine Translation model training uses strategies like backtranslation to improve BLEU scores, requiring large amounts of additional data and training. We introduce a class of conditional generative-discriminative hybrid losses that we use to fine-tune a trained machine translation model. Through a combination of targeted fine-tuning objectives and intuitive re-use of the training data the model has failed to adequately learn from, we improve the model performance of both a sentence-level and a contextual model without using any additional data. We target the improvement of pronoun translations through our fine-tuning and evaluate our models on a pronoun benchmark testset. Our sentence-level model shows a 0.5 BLEU improvement on both the WMT14 and the IWSLT13 De-En testsets, while our contextual model achieves the best results, improving from 31.81 to 32 BLEU on WMT14 De-En testset, and from 32.10 to 33.13 on the IWSLT13 De-En testset, with corresponding improvements in pronoun translation. We further show the generalizability of our method by reproducing the improvements on two additional language pairs, Fr-En and Cs-En. Code available at .\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. Adaptive Textual Label Noise Learning based on Pre-trained Models\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Shaohuan Cheng, Wenyu Chen, Mingsheng Fu, Xuanting Xie, Hong Qu*\n\n**TL;DR:** An adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage followed by a hybrid training stage that incorporates several generalization strategies to gradually correct mislabeled instances, thereby making better use of noisy data.\n\n**Abstract:** The label noise in real-world scenarios is unpredictable and can even be a mixture of different types of noise. To meet this challenge, we develop an adaptive textual label noise learning framework based on pre-trained models, which consists of an adaptive warm-up stage followed by a hybrid training stage. Specifically, an early stopping method, relying solely on the training set, is designed to dynamically terminate the warm-up process based on the model\u2019s fit level to different noise scenarios. The hybrid training stage incorporates several generalization strategies to gradually correct mislabeled instances, thereby making better use of noisy data. Experiments on multiple datasets demonstrate that our approach performs on-par with or even better than the state-of-the-art methods in various noise scenarios, including scenarios with the mixture of multiple types of noise.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 6. Mastering the Unsupervised Reinforcement Learning Benchmark from Pixels\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Sai Rajeswar, Pietro Mazzaglia, Tim Verbelen, Alexandre Pich\u00e9, B. Dhoedt, Aaron C. Courville, Alexandre Lacoste*\n\n**TL;DR:** This work studies the URLB and proposes a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks.\n\n**Abstract:** Controlling artificial agents from visual sensory data is an arduous task. Reinforcement learning (RL) algorithms can succeed but require large amounts of interactions between the agent and the environment. To alleviate the issue, unsupervised RL proposes to employ self-supervised interaction and learning, for adapting faster to future tasks. Yet, as shown in the Unsupervised RL Benchmark (URLB; Laskin et al. 2021), whether current unsupervised strategies can improve generalization capabilities is still unclear, especially in visual control settings. In this work, we study the URLB and propose a new method to solve it, using unsupervised model-based RL, for pre-training the agent, and a task-aware fine-tuning strategy combined with a new proposed hybrid planner, Dyna-MPC, to adapt the agent for downstream tasks. On URLB, our method obtains 93.59% overall normalized performance, surpassing previous baselines by a staggering margin. The approach is empirically evaluated through a large-scale empirical study, which we use to validate our design choices and analyze our models. We also show robust performance on the Real-Word RL benchmark, hinting at resiliency to environment perturbations during adaptation. Project website: https://masteringurlb.github.io/\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 3*)\n\n#### 7. Fast as CHITA: Neural Network Pruning with Combinatorial Optimization\n\n*From Search Query: \"neural network memory optimization\"*\n\n*Riade Benbaki, Wenyu Chen, X. Meng, Hussein Hazimeh, N. Ponomareva, Zhe Zhao, R. Mazumder*\n\n**TL;DR:** This work proposes a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint, and extends the classical Optimal Brain Surgeon framework, CHITA, which leads to significantly better sparsity-accuracy tradeoffs than competing methods.\n\n**Abstract:** The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 8. Simultaneous embedding of multiple attractor manifolds in a recurrent neural network using constrained gradient optimization\n\n*From Search Query: \"neural network memory optimization\"*\n\n*Haggai Agmon, Y. Burak*\n\n**TL;DR:** It is shown that it is possible to dramatically attenuate these detrimental interference effects by adjusting the synaptic weights, derived from a loss function that quantifies the roughness of the energy landscape along each of the embedded attractor manifolds.\n\n**Abstract:** The storage of continuous variables in working memory is hypothesized to be sustained in the brain by the dynamics of recurrent neural networks (RNNs) whose steady states form continuous manifolds. In some cases, it is thought that the synaptic connectivity supports multiple attractor manifolds, each mapped to a different context or task. For example, in hippocampal area CA3, positions in distinct environments are represented by distinct sets of population activity patterns, each forming a continuum. It has been argued that the embedding of multiple continuous attractors in a single RNN inevitably causes detrimental interference: quenched noise in the synaptic connectivity disrupts the continuity of each attractor, replacing it by a discrete set of steady states that can be conceptualized as lying on local minima of an abstract energy landscape. Consequently, population activity patterns exhibit systematic drifts towards one of these discrete minima, thereby degrading the stored memory over time. Here we show that it is possible to dramatically attenuate these detrimental interference effects by adjusting the synaptic weights. Synaptic weight adjustments are derived from a loss function that quantifies the roughness of the energy landscape along each of the embedded attractor manifolds. By minimizing this loss function, the stability of states can be dramatically improved, without compromising the capacity.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 9. Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings\n\n*From Search Query: \"neural network memory optimization\"*\n\n*Edouard Yvinec, Arnaud Dapogny, K\u00e9vin Bailly*\n\n**TL;DR:** The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.\n\n**Abstract:** The massive interest in deep neural networks (DNNs) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity devices such as mobile phones. To address this limitation, quantization is a favored solution as it maps high precision tensors to a low precision, memory efficient format. In terms of memory footprint reduction, its most effective variants are based on codebooks. These methods, however, suffer from two limitations. First, they either define a single codebook for each tensor, or use a memory-expensive mapping to multiple codebooks. Second, gradient descent optimization of the mapping favors jumps toward extreme values, hence not defining a proximal search. In this work, we propose to address these two limitations. First, we initially group similarly distributed neurons and leverage the re-ordered structure to either apply different scale factors to the different groups, or map weights that fall in these groups to several codebooks, without any mapping overhead. Second, stemming from this initialization, we propose a joint learning of the codebook and weight mappings that bears similarities with recent gradient-based post-training quantization techniques. Third, drawing estimation from straight-through estimation techniques, we introduce a novel gradient update definition to enable a proximal search of the codebooks and their mappings. The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Accelerating Direct Preference Optimization with Prefix Sharing\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Franklin Wang, Sumanth Hegde*\n\n**Abstract:** Offline paired preference optimization algorithms have become a popular\napproach for fine-tuning on preference data, outperforming traditional\nsupervised fine-tuning in various tasks. However, traditional implementations\noften involve redundant computations, especially for tasks with long shared\nprompts. We introduce prefix sharing for preference tuning, a novel technique\nthat processes chosen and rejected responses as one sequence with a shared\nprefix. To prevent cross-response contamination, we use a custom block-sparse\nattention mask. Our method achieves $1.1$-$1.5\\times$ improvement in training\nthroughput on popular DPO datasets, without any effect on convergence. When\ncombined with sequence packing, we observe consistent $1.3$-$1.6\\times$\nspeedups, benefiting even datasets with smaller sequence lengths. While we\nfocus on Direct Preference Optimization (DPO), our approach is applicable to\nother paired preference tuning methods. By enhancing computational efficiency,\nour work contributes to making preference-based fine-tuning more accessible for\na wider range of applications and model sizes. We open-source our code at\nhttps://github.com/frankxwang/dpo-prefix-sharing.\n\n**Published:** 2024-10-27T02:06:17Z  (*Updated: 2024-10-30T18:02:43Z*)\n\n\n\n#### 2. Efficiently Dispatching Flash Attention For Partially Filled Attention\n  Masks\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Agniv Sharma, Jonas Geiping*\n\n**Abstract:** Transformers are widely used across various applications, many of which yield\nsparse or partially filled attention matrices. Examples include attention masks\ndesigned to reduce the quadratic complexity of attention, sequence packing\ntechniques, and recent innovations like tree masking for fast validation in\nMEDUSA. Despite the inherent sparsity in these matrices, the state-of-the-art\nalgorithm Flash Attention still processes them with quadratic complexity as\nthough they were dense. In this paper, we introduce Binary Block Masking, a\nhighly efficient modification that enhances Flash Attention by making it\nmask-aware. We further propose two optimizations: one tailored for masks with\ncontiguous non-zero patterns and another for extremely sparse masks. Our\nexperiments on attention masks derived from real-world scenarios demonstrate up\nto a 9x runtime improvement. The implementation will be publicly released to\nfoster further research and application.\n\n**Published:** 2024-09-23T15:11:07Z  (*Updated: 2024-09-24T12:56:13Z*)\n\n\n\n#### 3. To Label or Not to Label: Hybrid Active Learning for Neural Machine\n  Translation\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Abdul Hameed Azeemi, Ihsan Ayyub Qazi, Agha Ali Raza*\n\n**Abstract:** Active learning (AL) techniques reduce labeling costs for training neural\nmachine translation (NMT) models by selecting smaller representative subsets\nfrom unlabeled data for annotation. Diversity sampling techniques select\nheterogeneous instances, while uncertainty sampling methods select instances\nwith the highest model uncertainty. Both approaches have limitations -\ndiversity methods may extract varied but trivial examples, while uncertainty\nsampling can yield repetitive, uninformative instances. To bridge this gap, we\npropose HUDS, a hybrid AL strategy for domain adaptation in NMT that combines\nuncertainty and diversity for sentence selection. HUDS computes uncertainty\nscores for unlabeled sentences and subsequently stratifies them. It then\nclusters sentence embeddings within each stratum using k-MEANS and computes\ndiversity scores by distance to the centroid. A weighted hybrid score that\ncombines uncertainty and diversity is then used to select the top instances for\nannotation in each AL iteration. Experiments on multi-domain German-English\ndatasets demonstrate the better performance of HUDS over other strong AL\nbaselines. We analyze the sentence selection with HUDS and show that it\nprioritizes diverse instances having high model uncertainty for annotation in\nearly AL iterations.\n\n**Published:** 2024-03-14T10:33:28Z  (*Updated: 2024-03-14T10:33:28Z*)\n\n\n\n#### 4. ReINTEL Challenge 2020: A Comparative Study of Hybrid Deep Neural\n  Network for Reliable Intelligence Identification on Vietnamese SNSs\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Hoang Viet Trinh, Tung Tien Bui, Tam Minh Nguyen, Huy Quang Dao, Quang Huu Pham, Ngoc N. Tran, Ta Minh Thanh*\n\n**Abstract:** The overwhelming abundance of data has created a misinformation crisis.\nUnverified sensationalism that is designed to grab the readers' short attention\nspan, when crafted with malice, has caused irreparable damage to our society's\nstructure. As a result, determining the reliability of an article has become a\ncrucial task. After various ablation studies, we propose a multi-input model\nthat can effectively leverage both tabular metadata and post content for the\ntask. Applying state-of-the-art finetuning techniques for the pretrained\ncomponent and training strategies for our complete model, we have achieved a\n0.9462 ROC-score on the VLSP private test set.\n\n**Published:** 2021-09-27T03:40:28Z  (*Updated: 2021-09-27T03:40:28Z*)\n\n\n\n#### 5. ZeroQuant-HERO: Hardware-Enhanced Robust Optimized Post-Training\n  Quantization Framework for W8A8 Transformers\n\n*From Search Query: \"neural network memory optimization\"*\n\n*Zhewei Yao, Reza Yazdani Aminabadi, Stephen Youn, Xiaoxia Wu, Elton Zheng, Yuxiong He*\n\n**Abstract:** Quantization techniques are pivotal in reducing the memory and computational\ndemands of deep neural network inference. Existing solutions, such as\nZeroQuant, offer dynamic quantization for models like BERT and GPT but overlook\ncrucial memory-bounded operators and the complexities of per-token\nquantization. Addressing these gaps, we present a novel, fully\nhardware-enhanced robust optimized post-training W8A8 quantization framework,\nZeroQuant-HERO. This framework uniquely integrates both memory bandwidth and\ncompute-intensive operators, aiming for optimal hardware performance.\nAdditionally, it offers flexibility by allowing specific INT8 modules to switch\nto FP16/BF16 mode, enhancing accuracy.\n\n**Published:** 2023-10-26T18:34:41Z  (*Updated: 2023-10-26T18:34:41Z*)\n\n\n\n#### 6. Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of\n  Language Model\n\n*From Search Query: \"neural network memory optimization\"*\n\n*Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, Daochen Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, Vipin Chaudhary, Shuai Xu, Xia Hu*\n\n**Abstract:** With the rapid growth in model size, fine-tuning the large pre-trained\nlanguage model has become increasingly difficult due to its extensive memory\nusage. Previous works usually focus on reducing the number of trainable\nparameters in the network. While the model parameters do contribute to memory\nusage, the primary memory bottleneck during training arises from storing\nfeature maps, also known as activations, as they are crucial for gradient\ncalculation. Notably, neural networks are usually trained using stochastic\ngradient descent. We argue that in stochastic optimization, models can handle\nnoisy gradients as long as the gradient estimator is unbiased with reasonable\nvariance. Following this motivation, we propose a new family of unbiased\nestimators called WTA-CRS, for matrix production with reduced variance, which\nonly requires storing the sub-sampled activations for calculating the gradient.\nOur work provides both theoretical and experimental evidence that, in the\ncontext of tuning transformers, our proposed estimators exhibit lower variance\ncompared to existing ones. By replacing the linear operation with our\napproximated one in transformers, we can achieve up to 2.7$\\times$ peak memory\nreduction with almost no accuracy drop and enables up to $6.4\\times$ larger\nbatch size. Under the same hardware, WTA-CRS enables better down-streaming task\nperformance by applying larger models and/or faster training speed with larger\nbatch sizes.\n\n**Published:** 2023-05-24T15:52:08Z  (*Updated: 2024-10-30T06:12:05Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Powerpropagation: A sparsity inducing weight reparameterisation\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Yee Whye Teh, Peter E. Latham, Razvan Pascanu, Siddhant M. Jayakumar, Jonathan Schwarz*\n\n**Abstract:** The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a \"rich get richer\" dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its versatility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-10-01\n\n\n\n#### 2. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models\n\n*From Search Query: \"sparse attention implementation techniques\"*\n\n*Jiaya Jia, Song Han, Zhijian Liu, Xin Lai, Haotian Tang, Shengju Qian, Yukang Chen*\n\n**Abstract:** We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset.\n\n**Published:** 2023-09-21\n\n\n\n#### 3. Nebula-I: A General Framework for Collaboratively Training Deep Learning Models on Low-Bandwidth Cloud Clusters\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*dianhai yu, Yanjun Ma, Yu Sun, Ge Li, Yue Yu, Yaqian Han, Shaohuai Shi, Bin Wang, Long Li, Yongshuai Hou, Peng Liu, Shuohuan Wang, Yuang Liu, Xianjie Mo, Siyu Ding, Weibao Gong, Zhihua Wu, Yang Xiang*\n\n**Abstract:** The ever-growing model size and scale of compute have attracted increasing interests in training deep learning models over multiple nodes. However, when it comes to training on cloud clusters, especially across remote clusters, huge challenges are faced. In this work, we introduce a general framework, Nebula-I, for collaboratively training deep learning models over remote heterogeneous clusters, the connections between which are low-bandwidth wide area networks (WANs). We took natural language processing (NLP) as an example to show how Nebula-I works in different training phases that include: a) pre-training a multilingual language model using two remote clusters; and b) fine-tuning a machine translation model using knowledge distilled from pre-trained models, which run through the most popular paradigm of recent deep learning. To balance the accuracy and communication efficiency, in Nebula-I, parameter-efficient training strategies, hybrid parallel computing methods and adaptive communication acceleration techniques are jointly applied. Meanwhile, security strategies are employed to guarantee the safety, reliability and privacy in intra-cluster computation and inter-cluster communication. Nebula-I is implemented with the PaddlePaddle deep learning framework, which can support collaborative training over heterogeneous hardware, e.g. GPU and NPU. Experiments demonstrate that the proposed framework could substantially maximize the training efficiency while preserving satisfactory NLP performance. By using Nebula-I, users can run large-scale training tasks over cloud clusters with minimum developments, and the utility of existed large pre-trained models could be further promoted. We also introduced new state-of-the-art results on cross-lingual natural language inference tasks, which are generated based upon a novel learning framework and Nebula-I.\n\n**Published:** 2022-05-19\n\n\n\n#### 4. Freeing Hybrid Distributed AI Training Configuration\n\n*From Search Query: \"hybrid model training strategies\"*\n\n*Haoran Wang*\n\n**Abstract:** Deep neural network (DNN) has become the leading technology to realize Artificial Intelligence (AI). As DNN models become larger and more complex, so do datasets. Being able to efficiently train DNNs in parallel has become a crucial need. Data Parallelism (DP) is the widest-used solution today to accelerate DNN training but could be inefficient when processing DNNs with large-size parameters. Hybrid Parallelism (HP), which applies different parallel strategies on different parts of DNNs, is more efficient but requires advanced configurations. Not all AI researchers are experts in parallel computing, thus automating the configuration of HP strategies is very desirable for all AI frameworks. We propose a parallel semantics analysis method, which can analyze the trade-offs among different kinds of parallelisms and systematically choose the HP strategies with good training time performance. We demonstrated experimentally 260% speedup when applying our method compared to using a conventional DP approach. With our proposal, AI researchers would be able to focus more on AI algorithm research without being disturbed by parallel analysis and engineering concerns.\n\n**Proceeding:** proceedings-of-the-29th-acm-joint-meeting-on\n\n**Published:** 2021-08-20\n\n\n\n#### 5. OLLA: Optimizing the Lifetime and Location of Arrays to Reduce the Memory Usage of Neural Networks\n\n*From Search Query: \"neural network memory optimization\"*\n\n*James Hegarty, Jacob Kahn, Mostafa Elhoushi, Benoit Steiner*\n\n**Abstract:** The size of deep neural networks has grown exponentially in recent years. Unfortunately, hardware devices have not kept pace with the rapidly increasing memory requirements. To cope with this, researchers have turned to techniques such as spilling and recomputation, which increase training time, or reduced precision and model pruning, which can affect model accuracy. We present OLLA, an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks. Our method reduces the memory usage of existing neural networks, without needing any modification to the models or their training procedures. We formulate the problem as a joint integer linear program (ILP). We present several techniques to simplify the encoding of the problem, and enable our approach to scale to the size of state-of-the-art neural networks using an off-the-shelf ILP solver. We experimentally demonstrate that OLLA only takes minutes if not seconds to allow the training of neural networks using one-third less memory on average.\n\n**Published:** 2022-10-24\n\n\n\n#### 6. Optimizing Memory Efficiency of Graph Neural Networks on Edge Computing Platforms\n\n*From Search Query: \"neural network memory optimization\"*\n\n*Chunming Hu, Weisheng Zhao, Pengcheng Dai, Yunli Chen, Xiaoyi Wang, Yingjie Qi, Tong Qiao, Yeqi Gao, Jianlei Yang, Ao Zhou*\n\n**Abstract:** Graph neural networks (GNN) have achieved state-of-the-art performance on various industrial tasks. However, the poor efficiency of GNN inference and frequent Out-Of-Memory (OOM) problem limit the successful application of GNN on edge computing platforms. To tackle these problems, a feature decomposition approach is proposed for memory efficiency optimization of GNN inference. The proposed approach could achieve outstanding optimization on various GNN models, covering a wide range of datasets, which speeds up the inference by up to 3x. Furthermore, the proposed feature decomposition could significantly reduce the peak memory usage (up to 5x in memory efficiency improvement) and mitigate OOM problems during GNN inference.\n\n**Published:** 2021-04-07\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on sparse attention mechanisms, hybrid model training strategies, and neural network memory optimization, here are some key findings and insights from the provided sources:\n\n## Implementation Details of Sparse Attention Mechanisms\n\n### Different Sparsity Patterns\n- Sparse attention mechanisms can be categorized into several types, including sparse attention, linear attention, and kernel optimization. For instance, the Sparse Transformer reduces complexity from \\(O(n^2)\\) to \\(O(n\\sqrt{n})\\) by applying sparse factorizations to self-attention matrices.\n- Block Sparse Attention supports various sparse patterns, such as streaming attention with token or block granularity, which significantly reduces computational costs.\n- The S2-Attention method involves heterogeneous context sharding, where each attention head attends to a different subset of tokens, ensuring the full context is covered collectively.\n\n### Efficiency Optimizations\n- The S2-Attention kernel optimizes sparse attention by dynamically merging sparse query blocks that share common KVs, ensuring each tile is fully utilized and improving SRAM utilization.\n- SageAttention uses 8-bit quantization and combines ROPE with quantization, achieving faster and more accurate attention computation compared to existing methods like FlashAttention2 and xformers.\n\n### Hardware Considerations\n- The implementation of S2-Attention and SageAttention leverages hardware capabilities, such as using Triton libraries and Nvidia Tensor Core instructions, to enhance speed and efficiency.\n\n## Training Strategies for Hybrid Architectures\n\n### Stability Techniques\n- Hybrid architectures combining sparse and dense attention require careful design to ensure stability. For example, the Head-Split Sparse-Dense Attention (HSDA) method balances efficiency and performance by fusing sparse-dense and global attention, which helps in maintaining stability during training.\n\n### Loss Functions and Gradient Flow Optimization\n- While the sources do not explicitly discuss loss functions and gradient flow optimization for hybrid models, it is implied that these models need tailored training strategies. For instance, the integration of SSMs with attention mechanisms, as mentioned in the analysis, requires careful tuning of parameters to address content-based reasoning weaknesses and ensure stable training.\n\n## Memory Management and Compression\n\n### State Compression Techniques\n- State Space Models (SSMs) integrated into hierarchical processing structures can significantly reduce memory requirements. For example, SSMs can process entire books without truncation on a single GPU, highlighting their efficiency in state management[Analysis: LOCOST paper].\n\n### Memory-Efficient Attention\n- Sparse attention mechanisms inherently reduce memory usage by attending to only a subset of tokens. The Block Sparse Attention library and S2-Attention kernel are designed to minimize memory movement and optimize SRAM utilization, making them memory-efficient.\n\n### Hierarchical Memory Structures\n- Hierarchical processing structures, such as those proposed in CHELA, replace SSMs with short-long convolutions and implement linear attention in a divide-and-conquer manner. This approach helps in balancing memory usage with model expressiveness[Analysis: CHELA paper].\n\n## Additional Insights\n\n- **Hybrid Architectures**: Combining sparse and dense attention, as in the HSDA method, ensures that the model can capture both spatial and temporal features effectively, which is crucial for performance on downstream tasks.\n- **Hardware Optimization**: Techniques like SageAttention and S2-Attention emphasize the importance of hardware-aware design to achieve high parallelization and optimized memory IO, which is essential for efficient inference and training.\n\nBy integrating these insights, researchers can develop more efficient and scalable autoregressive language models that leverage sparse attention mechanisms, optimize memory usage, and ensure stable training in hybrid architectures.\n",
            "analysis": "Summary of Analysis:\nThe initial search results provide valuable insights into integrating State Space Models (SSMs) with sparse attention and hierarchical processing. Key findings reveal promising directions but also highlight areas requiring further investigation.\n\nUseful References with Excerpts:\n\n1. From LOCOST paper:\n\"SSMs can be competitive with transformer-based models while drastically reducing their memory requirements [...] able to process entire books, without truncation, and on a single GPU\"\n\n2. From Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] letting the SSM parameters be functions of the input addresses their weakness with discrete modalities\"\n\n3. From SPADE paper:\n\"We augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods\"\n\n4. From CHELA paper:\n\"leverage the thought of tiling and hierarchy to propose CHELA [...] which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner\"\n\nKey Insights:\n\n1. SSM Integration Approaches:\n- SSMs show promise for efficient long sequence processing\n- Need careful integration with attention mechanisms\n- Potential for hierarchical processing structures\n\n2. Memory Efficiency:\n- SSMs offer linear complexity vs quadratic for traditional attention\n- Opportunity for efficient state management\n- Need to balance memory usage with model expressiveness\n\n3. Architectural Considerations:\n- Hierarchical processing can improve efficiency\n- Sparse attention patterns need careful design\n- State management crucial for performance\n\n4. Implementation Challenges:\n- Training stability with multiple mechanisms\n- Memory management across hierarchical levels\n- Hardware optimization requirements\n\nFuture Search Plan:\n\n1. Need more information on:\n- Specific sparse attention patterns and their effectiveness\n- Training strategies for hybrid SSM-attention models\n- Hardware optimization techniques\n- State compression and management methods\n\n2. Proposed search queries for next round:\n- Detailed implementation of sparse attention mechanisms\n- Training stability in hybrid neural architectures\n- Memory compression techniques in language models\n\nReferences:\n\n[1] Le Bronnec, F., et al. (2024). \"LOCOST: State-Space Models for Long Document Abstractive Summarization.\" EACL 2024.\n\n[2] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv:2312.00752.\n\n[3] Zuo, S., et al. (2022). \"Efficient Long Sequence Modeling via State Space Augmented Transformer.\" arXiv:2212.00789.\n\n[4] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\" arXiv:2401.01325.\n\n[5] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models.\" arXiv:2401.04722."
        },
        {
            "ready": false,
            "query": "\"state space models architecture\", \"hierarchical neural networks implementation\", \"hardware efficient attention\"",
            "detail": "Search for papers discussing:\n1. State Space Model architectures and integration:\n   - Detailed implementation approaches\n   - Training methodologies\n   - Performance comparisons\n\n2. Hierarchical processing in neural networks:\n   - Information flow mechanisms\n   - Memory management strategies\n   - Efficiency optimization techniques\n\n3. Hardware-efficient attention implementations:\n   - Memory access patterns\n   - Cache optimization\n   - Parallel processing strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. State Space Model architectures and integration:\n   - Detailed implementation approaches\n   - Training methodologies\n   - Performance comparisons\n\n2. Hierarchical processing in neural networks:\n   - Information flow mechanisms\n   - Memory management strategies\n   - Efficiency optimization techniques\n\n3. Hardware-efficient attention implementations:\n   - Memory access patterns\n   - Cache optimization\n   - Parallel processing strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.76)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.76)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.19)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.19)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.18)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 9/20 (Score: 0.18)*\n\n```\nIn International Conference on Learning Representations, January 2021. [7] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971-35983, December 2022. [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [9] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, February 2023. [10] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [11] Eric Martin and Chris Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length. In International Conference on Learning Representations, February 2018. [12] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, June 2023. [13] Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 425-435. PMLR, May 2021. [14] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research, 23(42):1-85, 2022. ISSN 1533-7928. [15] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, March 1994.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.15)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.15)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 5. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.14)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 7/30 (Score: 0.14)*\n\n```\nV. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 77-87, 2017. [6] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [8] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [9] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [11] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"state space models architecture\", \"hierarchical neural networks implementation\", \"hardware efficient attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: \"state space models architecture\"*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 2. It's Raw! Audio Generation with State-Space Models\n\n*From Search Query: \"state space models architecture\"*\n\n*Karan Goel, Albert Gu, Chris Donahue, Christopher R'e*\n\n**TL;DR:** SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling, is proposed, identifying that S4 can be unstable during autoregressive generation, and providing a simple improvement to its parameterization by drawing connections to Hurwitz matrices.\n\n**Abstract:** Developing architectures suitable for modeling raw audio is a challenging problem due to the high sampling rates of audio waveforms. Standard sequence modeling approaches like RNNs and CNNs have previously been tailored to fit the demands of audio, but the resultant architectures make undesirable computational tradeoffs and struggle to model waveforms effectively. We propose SaShiMi, a new multi-scale architecture for waveform modeling built around the recently introduced S4 model for long sequence modeling. We identify that S4 can be unstable during autoregressive generation, and provide a simple improvement to its parameterization by drawing connections to Hurwitz matrices. SaShiMi yields state-of-the-art performance for unconditional waveform generation in the autoregressive setting. Additionally, SaShiMi improves non-autoregressive generation performance when used as the backbone architecture for a diffusion model. Compared to prior architectures in the autoregressive generation setting, SaShiMi generates piano and speech waveforms which humans find more musical and coherent respectively, e.g. 2x better mean opinion scores than WaveNet on an unconditional speech generation task. On a music generation task, SaShiMi outperforms WaveNet on density estimation and speed at both training and inference even when using 3x fewer parameters. Code can be found at https://github.com/HazyResearch/state-spaces and samples at https://hazyresearch.stanford.edu/sashimi-examples.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 154  (*Influential: 15*)\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: \"state space models architecture\"*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 160  (*Influential: 37*)\n\n#### 4. Hierarchical Attribute CNNs\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*J. Jacobsen, Edouard Oyallon, S. Mallat, A. Smeulders*\n\n**TL;DR:** This work introduces a subclass of deep neural network algorithms, called Hierarchical Attribute Convolutional Networks, where individual layers are indexed by progressively higherdimensional and increasingly invariant attributes, allowing to control the size of the layers while reducing their parameters.\n\n**Abstract:** Deep neural network algorithms are difficult to analyze because they lack structure allowing to understand the properties of underlying transforms and invariants. Multiscale Hierarchical Convolutional Networks are a theoretical class of structured deep convolutional networks that constitute a framework to understand neural network classification properties. However, a naive implementation of such networks is infeasible due to the exponential growth of parameters with depth, so their empirical properties remain to be studied. We introduce a subclass of them that overcomes this issue, called Hierarchical Attribute Convolutional Networks, where individual layers are indexed by progressively higherdimensional and increasingly invariant attributes. Each new layer is computed with multidimensional convolutions along spatial and attribute variables. The dimensionality is kept constant by averaging intermediate layers along attributes, allowing to control the size of the layers while reducing their parameters. This permits to train Hierarchical Attribute CNNs on CIFAR image databases where they obtain comparable accuracy to state of the art networks while having much fewer trainable parameters. We study some properties of the attributes learned from Cifar-10.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 5. How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*Aditya Yedetore, Tal Linzen, R. Frank, R. Thomas McCoy*\n\n**TL;DR:** It is suggested that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.\n\n**Abstract:** When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children\u2019s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children\u2019s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 6. Differentiable hierarchical and surrogate gradient search for spiking neural networks\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*Kaiwei Che, Luziwei Leng, Kaixuan Zhang, Jianguo Zhang, Qinghu Meng, Jie Cheng, Qinghai Guo, Jianxing Liao*\n\n**TL;DR:** A spike-based differentiable hierarchical search (SpikeDHS) framework, where spike- based computation is realized on both the cell and the layer level search space, which finds effective SNN architectures under limited computation cost and exceeds the accuracy of specially designed ANNs meanwhile.\n\n**Abstract:** Spiking neural network (SNN) has been viewed as a potential candidate for the next generation of artificial intelligence with appealing characteristics such as sparse computation and inherent temporal dynamics. By adopting architectures of deep artificial neural networks (ANNs), SNNs are achieving competitive performances in benchmark tasks such as image classification. However, successful architectures of ANNs are not necessary ideal for SNN and when tasks become more diverse effective architectural variations could be critical. To this end, we develop a spike-based differentiable hierarchical search (SpikeDHS) framework, where spike-based computation is realized on both the cell and the layer level search space. Based on this framework, we find effective SNN architectures under limited computation cost. During the training of SNN, a suboptimal surrogate gradient function could lead to poor approximations of true gradients, making the network enter certain local minima. To address this problem, we extend the differential approach to surrogate gradient search where the SG function is efficiently optimized locally. Our models achieve state-of-the-art performances on classification of CIFAR10/100 and ImageNet with accuracy of 95.50%, 76.25% and 68.64%. On event-based deep stereo, our method finds optimal layer variation and surpasses the accuracy of specially designed ANNs meanwhile with 26 \u00d7 lower energy cost ( 6 . 7mJ ), demonstrating the advantage of SNN in processing highly sparse and dynamic signals. Codes are available at https://github.com/Huawei-BIC/SpikeDHS .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 31  (*Influential: 3*)\n\n#### 7. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han*\n\n**TL;DR:** This work designs Hardware-Aware Transformers with neural architecture search, and trains a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing, and performs an evolutionary search with a hardware latency constraint.\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 243  (*Influential: 45*)\n\n### 6 related papers from ArXiv\n\n#### 1. Design Principle Transfer in Neural Architecture Search via Large\n  Language Models\n\n*From Search Query: \"state space models architecture\"*\n\n*Xun Zhou, Liang Feng, Xingyu Wu, Zhichao Lu, Kay Chen Tan*\n\n**Abstract:** Transferable neural architecture search (TNAS) has been introduced to design\nefficient neural architectures for multiple tasks, to enhance the practical\napplicability of NAS in real-world scenarios. In TNAS, architectural knowledge\naccumulated in previous search processes is reused to warm up the architecture\nsearch for new tasks. However, existing TNAS methods still search in an\nextensive search space, necessitating the evaluation of numerous architectures.\nTo overcome this challenge, this work proposes a novel transfer paradigm, i.e.,\ndesign principle transfer. In this work, the linguistic description of various\nstructural components' effects on architectural performance is termed design\nprinciples. They are learned from established architectures and then can be\nreused to reduce the search space by discarding unpromising architectures.\nSearching in the refined search space can boost both the search performance and\nefficiency for new NAS tasks. To this end, a large language model\n(LLM)-assisted design principle transfer (LAPT) framework is devised. In LAPT,\nLLM is applied to automatically reason the design principles from a set of\ngiven architectures, and then a principle adaptation method is applied to\nrefine these principles progressively based on the new search results.\nExperimental results show that LAPT can beat the state-of-the-art TNAS methods\non most tasks and achieve comparable performance on others.\n\n**Published:** 2024-08-21T04:27:44Z  (*Updated: 2024-08-21T04:27:44Z*)\n\n\n\n#### 2. FlexiBERT: Are Current Transformer Architectures too Homogeneous and\n  Rigid?\n\n*From Search Query: \"state space models architecture\"*\n\n*Shikhar Tuli, Bhishma Dedhia, Shreshth Tuli, Niraj K. Jha*\n\n**Abstract:** The existence of a plethora of language models makes the problem of selecting\nthe best one for a custom task challenging. Most state-of-the-art methods\nleverage transformer-based models (e.g., BERT) or their variants. Training such\nmodels and exploring their hyperparameter space, however, is computationally\nexpensive. Prior work proposes several neural architecture search (NAS) methods\nthat employ performance predictors (e.g., surrogate models) to address this\nissue; however, analysis has been limited to homogeneous models that use fixed\ndimensionality throughout the network. This leads to sub-optimal architectures.\nTo address this limitation, we propose a suite of heterogeneous and flexible\nmodels, namely FlexiBERT, that have varied encoder layers with a diverse set of\npossible operations and different hidden dimensions. For better-posed surrogate\nmodeling in this expanded design space, we propose a new graph-similarity-based\nembedding scheme. We also propose a novel NAS policy, called BOSHNAS, that\nleverages this new scheme, Bayesian modeling, and second-order optimization, to\nquickly train and use a neural surrogate model to converge to the optimal\narchitecture. A comprehensive set of experiments shows that the proposed\npolicy, when applied to the FlexiBERT design space, pushes the performance\nfrontier upwards compared to traditional models. FlexiBERT-Mini, one of our\nproposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9%\nhigher GLUE score. A FlexiBERT model with equivalent performance as the best\nhomogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed\nmodel, achieves state-of-the-art results, outperforming the baseline models by\nat least 5.7% on the GLUE benchmark.\n\n**Published:** 2022-05-23T22:44:34Z  (*Updated: 2022-05-23T22:44:34Z*)\n\n\n\n#### 3. Hierarchical Neural Network Architecture In Keyword Spotting\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*Yixiao Qu, Sihao Xue, Zhenyi Ying, Hang Zhou, Jue Sun*\n\n**Abstract:** Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it\nis essential to ensure a high recall rate. However, its real-time property\nrequires low computation complexity. This contradiction inspires people to find\na suitable model which is small enough to perform well in multi environments.\nTo deal with this contradiction, we implement the Hierarchical Neural\nNetwork(HNN), which is proved to be effective in many speech recognition\nproblems. HNN outperforms traditional DNN and CNN even though its model size\nand computation complexity are slightly less. Also, its simple topology\nstructure makes easy to deploy on any device.\n\n**Published:** 2018-11-06T12:32:27Z  (*Updated: 2018-11-06T12:32:27Z*)\n\n\n\n#### 4. A Conceptual Framework For Trie-Augmented Neural Networks (TANNS)\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*Temitayo Adefemi*\n\n**Abstract:** Trie-Augmented Neural Networks (TANNs) combine trie structures with neural\nnetworks, forming a hierarchical design that enhances decision-making\ntransparency and efficiency in machine learning. This paper investigates the\nuse of TANNs for text and document classification, applying Recurrent Neural\nNetworks (RNNs) and Feed forward Neural Networks (FNNs). We evaluated TANNs on\nthe 20 NewsGroup and SMS Spam Collection datasets, comparing their performance\nwith traditional RNN and FFN Networks with and without dropout regularization.\nThe results show that TANNs achieve similar or slightly better performance in\ntext classification. The primary advantage of TANNs is their structured\ndecision-making process, which improves interpretability. We discuss\nimplementation challenges and practical limitations. Future work will aim to\nrefine the TANNs architecture for more complex classification tasks.\n\n**Published:** 2024-06-11T17:08:16Z  (*Updated: 2024-06-11T17:08:16Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, Christopher R\u00e9*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence\nmodeling performance in some modalities, but underperform attention in language\nmodeling. Moreover, despite scaling nearly linearly in sequence length instead\nof quadratically, SSMs are still slower than Transformers due to poor hardware\nutilization. In this paper, we make progress on understanding the expressivity\ngap between SSMs and attention in language modeling, and on reducing the\nhardware barrier between SSMs and attention. First, we use synthetic language\nmodeling tasks to understand the gap between SSMs and attention. We find that\nexisting SSMs struggle with two capabilities: recalling earlier tokens in the\nsequence and comparing tokens across the sequence. To understand the impact on\nlanguage modeling, we propose a new SSM layer, H3, that is explicitly designed\nfor these abilities. H3 matches attention on the synthetic languages and comes\nwithin 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid\n125M-parameter H3-attention model that retains two attention layers\nsurprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to\nimprove the efficiency of training SSMs on modern hardware, we propose\nFlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on\nsequences up to 8K, and introduces a novel state passing algorithm that\nexploits the recurrent properties of SSMs to scale to longer sequences.\nFlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows\nhybrid language models to generate text 2.4$\\times$ faster than Transformers.\nUsing FlashConv, we scale hybrid H3-attention language models up to 2.7B\nparameters on the Pile and find promising initial results, achieving lower\nperplexity than Transformers and outperforming Transformers in zero- and\nfew-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28T17:56:03Z  (*Updated: 2023-04-29T03:18:40Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: \"state space models architecture\"*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. PixMamba: Leveraging State Space Models in a Dual-Level Architecture for Underwater Image Enhancement\n\n*From Search Query: \"state space models architecture\"*\n\n*Kai-Lung Hua, Jyun-Wei Chen, Yong-Xiang Lin, Wei-Tung Lin*\n\n**Abstract:** Underwater Image Enhancement (UIE) is critical for marine research and exploration but hindered by complex color distortions and severe blurring. Recent deep learning-based methods have achieved remarkable results, yet these methods struggle with high computational costs and insufficient global modeling, resulting in locally under- or over- adjusted regions. We present PixMamba, a novel architecture, designed to overcome these challenges by leveraging State Space Models (SSMs) for efficient global dependency modeling. Unlike convolutional neural networks (CNNs) with limited receptive fields and transformer networks with high computational costs, PixMamba efficiently captures global contextual information while maintaining computational efficiency. Our dual-level strategy features the patch-level Efficient Mamba Net (EMNet) for reconstructing enhanced image feature and the pixel-level PixMamba Net (PixNet) to ensure fine-grained feature capturing and global consistency of enhanced image that were previously difficult to obtain. PixMamba achieves state-of-the-art performance across various underwater image datasets and delivers visually superior results. Code is available at: https://github.com/weitunglin/pixmamba.\n\n**Published:** 2024-06-12\n\n\n\n#### 3. Mutual Information and Diverse Decoding Improve Neural Machine Translation\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*Dan Jurafsky, Jiwei Li*\n\n**Abstract:** Sequence-to-sequence neural translation models learn semantic and syntactic\nrelations between sentence pairs by optimizing the likelihood of the target\ngiven the source, i.e., $p(y|x)$, an objective that ignores other potentially\nuseful sources of information. We introduce an alternative objective function\nfor neural MT that maximizes the mutual information between the source and\ntarget sentences, modeling the bi-directional dependency of sources and\ntargets. We implement the model with a simple re-ranking method, and also\nintroduce a decoding algorithm that increases diversity in the N-best list\nproduced by the first pass. Applied to the WMT German/English and\nFrench/English tasks, the proposed models offers a consistent performance boost\non both standard LSTM and attention-based neural MT architectures.\n\n**Published:** 2016-01-04\n\n\n\n#### 4. Building End-To-End Dialogue Systems Using Generative Hierarchical Neural Network Models\n\n*From Search Query: \"hierarchical neural networks implementation\"*\n\n*Joelle Pineau, Iulian V. Serban, Alessandro Sordoni, Yoshua Bengio, Aaron Courville*\n\n**Abstract:** We investigate the task of building open domain, conversational dialogue\nsystems based on large dialogue corpora using generative models. Generative\nmodels produce system responses that are autonomously generated word-by-word,\nopening up the possibility for realistic, flexible interactions. In support of\nthis goal, we extend the recently proposed hierarchical recurrent\nencoder-decoder neural network to the dialogue domain, and demonstrate that\nthis model is competitive with state-of-the-art neural language models and\nback-off n-gram models. We investigate the limitations of this and similar\napproaches, and show how its performance can be improved by bootstrapping the\nlearning from a larger question-answer pair corpus and from pretrained word\nembeddings.\n\n**Published:** 2015-07-17\n\n\n\n#### 5. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: \"hardware efficient attention\"*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space models, hierarchical neural networks, and hardware-efficient attention mechanisms, here are some key findings and insights from the provided sources:\n\n## State Space Model Architectures and Integration\n\n### Detailed Implementation Approaches\n- State Space Models (SSMs) are being integrated into various neural network architectures to leverage their efficiency in handling sequential data. For example, the Mamba Neural Operator (MNO) combines SSMs with neural operators to solve partial differential equations (PDEs) and handle continuous dynamics more effectively than traditional Transformers.\n- The SC-MAMBA2 model integrates SSMs into a foundation model for single-cell transcriptomic data, enabling efficient and scalable modeling of ultra-long gene sequences. This integration involves a bidirectional modeling approach and specific design modifications to handle large gene sequences.\n\n### Training Methodologies\n- Training SSMs involves discretizing continuous state space models using techniques like the zero-order hold (ZOH) to make them suitable for modern computational frameworks. This process transforms continuous matrices into discrete counterparts, which is crucial for implementation.\n- The MNO framework provides a theoretical connection between SSMs and neural operators, which can be generalized to various architectures, including Transformer-based models. This theoretical foundation helps in training and adapting SSMs to diverse scenarios.\n\n### Performance Comparisons\n- SSMs, such as those in MNO and SC-MAMBA2, have shown superior performance compared to traditional Transformer-based models, especially in handling long-range dependencies and continuous data. SC-MAMBA2 outperformed state-of-the-art models in single-cell transcriptomics, demonstrating improved accuracy and computational efficiency.\n\n## Hierarchical Processing in Neural Networks\n\n### Information Flow Mechanisms\n- Hierarchical processing can be achieved through the use of structure-aware state fusion equations, as seen in the Spatial-Mamba model. This approach integrates local spatial dependencies into the state space, enhancing the model's ability to capture complex image structures.\n- In SC-MAMBA2, hierarchical processing is facilitated by a bidirectional modeling approach within the state-space modules, allowing the model to capture intricate biological variations and regulatory elements in gene sequences.\n\n### Memory Management Strategies\n- SSMs maintain linear complexity, which makes them scalable for long sequences. This is achieved through efficient state transition equations and the use of hidden attention mechanisms that focus on relevant parts of the input data.\n- The Spatial-Mamba model employs a local perception unit (LPU) to extract local information inside image patches, which helps in managing memory and computational resources efficiently.\n\n### Efficiency Optimization Techniques\n- The use of dilated convolutions in Spatial-Mamba to capture spatial dependencies in the latent state space improves the model's adaptability and efficiency. This approach retains the benefits of original Mamba while incorporating local structural information.\n- SC-MAMBA2's design adaptations, such as using the [CLS] token to represent each cell and an adversarial domain adaptation approach, optimize the model's performance in handling large-scale biological data.\n\n## Hardware-Efficient Attention Implementations\n\n### Memory Access Patterns\n- Efficient attention mechanisms, such as those discussed in the context of sparse attention, emphasize the importance of optimal memory access patterns. For example, selecting a constant number of KV pairs for each query can reduce memory footprint and improve generation efficiency (though not directly from the provided sources, this is a general insight from sparse attention mechanisms).\n- SSMs, particularly those integrated into models like SC-MAMBA2, benefit from linear complexity and efficient state transition equations, which optimize memory access patterns and reduce computational overhead.\n\n### Cache Optimization\n- The use of structure-aware state fusion equations in Spatial-Mamba and similar approaches can be optimized for cache utilization by ensuring that the state variables are processed in a manner that minimizes cache misses. This is achieved by leveraging local neighborhood connectivity in the state space.\n\n### Parallel Processing Strategies\n- SSMs can be parallelized efficiently due to their linear complexity and the nature of their state transition equations. This makes them suitable for implementation on parallel processing architectures, enhancing overall computational efficiency.\n- The Spatial-Mamba model, by incorporating dilated convolutions and structure-aware state fusion, can be implemented in a way that leverages parallel processing strategies to handle large-scale visual data efficiently.\n\nIn summary, integrating SSMs into autoregressive language models can offer significant advantages in terms of handling long-range dependencies, reducing computational complexity, and optimizing memory access patterns. Hierarchical processing within these models can be enhanced through structure-aware mechanisms and efficient memory management strategies. Hardware-efficient attention implementations can further optimize these models by leveraging sparse attention mechanisms, optimizing cache utilization, and employing parallel processing strategies.\n",
            "analysis": "Summary of Analysis:\n\nThe search results provide valuable insights into sparse attention mechanisms, state space models, and memory optimization techniques. Key findings reveal both promising directions and areas requiring further investigation.\n\nUseful References with Excerpts:\n\n1. From SPARSEK paper:\n\"SPARSEK Attention offers linear time complexity and constant memory footprint during generation [...] integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query\"\n\n2. From Fast Attention Over Long Sequences paper:\n\"extends FlashAttention to accommodate a large class of attention sparsity patterns [...] leads to implementations with no computational complexity overhead and a multi-fold runtime speedup\"\n\n3. From Sparse Attention with Linear Units:\n\"replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges [...] achieves translation performance comparable to several strong baselines\"\n\nKey Insights:\n\n1. Sparse Attention Mechanisms:\n- Multiple approaches to achieve sparsity (scoring networks, ReLU activation, LSH)\n- Trade-offs between complexity and performance\n- Implementation considerations for hardware efficiency\n\n2. Memory Optimization:\n- Importance of efficient KV pair selection\n- Role of caching and memory access patterns\n- Hardware-aware implementation strategies\n\n3. Integration Challenges:\n- Balance between sparsity and model expressiveness\n- Training stability considerations\n- Hardware optimization requirements\n\nFuture Search Plan:\n\nNeed more information on:\n1. State Space Model integration specifics:\n   - Detailed architectures\n   - Training strategies\n   - Performance metrics\n\n2. Hierarchical processing:\n   - Information flow between levels\n   - Memory management strategies\n   - Computational efficiency\n\n3. Hardware optimization:\n   - Implementation details\n   - Memory access patterns\n   - Cache utilization\n\nReferences:\n\n[1] Lou, C., et al. (2024). \"Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers.\" arXiv:2402.04248.\n\n[2] Pagliardini, M., et al. (2023). \"Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention.\" Neural Information Processing Systems.\n\n[3] Zhang, B., et al. (2021). \"Sparse Attention with Linear Units.\" Conference on Empirical Methods in Natural Language Processing.\n\n[4] Rabe, M., & Staats, C. (2021). \"Self-attention Does Not Need O(n^2) Memory.\" arXiv:2112.05682.\n\n[5] Roy, A., et al. (2020). \"Efficient Content-Based Sparse Attention with Routing Transformers.\" Transactions of the Association for Computational Linguistics."
        }
    ],
    "rating": 4.1,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.58857,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.945453,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "# Proposal: SparseStateTTT\n\n## Title: SparseStateTTT: Integrating State Space Models with Sparse Attention for Efficient Long-Sequence Processing\n\n### Motivation\nThe challenge of efficiently processing long sequences while maintaining model expressiveness and scalability is a significant hurdle in modern language models. Traditional attention mechanisms, despite their effectiveness, suffer from quadratic complexity, making them impractical for long sequences. State Space Models (SSMs) offer a promising alternative by efficiently handling long sequences, but they lack the flexibility to capture complex local dependencies. Sparse attention mechanisms, on the other hand, provide a means to reduce computational complexity and memory usage. This proposal aims to integrate SSMs with sparse attention to create a hierarchical and efficient autoregressive language model.\n\n### Problem Analysis\n- **Efficiency**: Current models struggle with the quadratic complexity of attention mechanisms, leading to high computational costs and memory usage.\n- **Scalability**: Handling long sequences efficiently is crucial for scalability, but existing models often fall short in this regard.\n- **Expressiveness**: Maintaining high model expressiveness while reducing complexity is a key challenge.\n\n### Core Idea and Philosophy\nThe core idea is to integrate SSMs with sparse attention mechanisms to leverage the strengths of both approaches. SSMs provide efficient handling of long sequences, while sparse attention reduces computational complexity and memory usage. By combining these techniques, the model can achieve efficient long-sequence processing, maintain high expressiveness, and scale effectively.\n\n### Design Plan\n#### Selected GAU to Modify\n```selection FastTTTLinear```\n\n#### Modification Overview\n1. **State Space Model Integration**:\n   - Incorporate SSMs into the FastTTTLinear GAU to handle long sequences efficiently.\n   - Use SSMs to capture global dependencies and provide a strong structural bias.\n\n2. **Sparse Attention Mechanism**:\n   - Integrate sparse attention mechanisms to reduce computational complexity and memory usage.\n   - Employ techniques like Sparse Modular Activation (SMA) to dynamically activate sub-modules, reducing computation and memory consumption.\n\n3. **Hierarchical Processing**:\n   - Implement a hierarchical processing structure that combines SSMs and sparse attention to capture both global and local dependencies efficiently.\n\n### Detailed Modifications\n#### 1. State Space Model Integration\n- **SSM Layer**: Add an SSM layer to the FastTTTLinear GAU to handle long sequences efficiently.\n- **Global Dependency Capture**: Use the SSM to capture global dependencies and provide a structural bias.\n\n#### 2. Sparse Attention Mechanism\n- **Sparse Modular Activation**: Integrate SMA to dynamically activate sub-modules, reducing computation and memory consumption.\n- **Local Attention**: Use sparse attention to capture local dependencies efficiently.\n\n#### 3. Hierarchical Processing\n- **Multi-Scale Processing**: Implement a hierarchical processing structure that combines SSMs and sparse attention to capture both global and local dependencies efficiently.\n\n### Implementation Guidelines\n#### Pseudo-Code for SparseStateTTT\n```python\nclass SparseStateTTT(GAUBase):\n    def __init__(self, embed_dim, block_loc, kwarg_all, **kwargs):\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ssm_layer = SSM(embed_dim)\n        self.sparse_attention = SparseAttention(embed_dim)\n\n    def _forward(self, X, **Z):\n        # State Space Model processing\n        X_ssm = self.ssm_layer(X)\n\n        # Sparse Attention processing\n        X_sparse = self.sparse_attention(X_ssm)\n\n        # Combine SSM and Sparse Attention outputs\n        Y = X_ssm + X_sparse\n        return Y, Z\n```\n\n### Integration Steps\n1. **Integrate SSM Layer**: Add an SSM layer to the FastTTTLinear GAU to handle long sequences efficiently.\n2. **Integrate Sparse Attention**: Add sparse attention mechanisms to reduce computational complexity and memory usage.\n3. **Hierarchical Processing**: Implement a hierarchical processing structure that combines SSMs and sparse attention to capture both global and local dependencies efficiently.\n\n### Conclusion\nSparseStateTTT integrates State Space Models with sparse attention mechanisms to enhance the efficiency and scalability of autoregressive language models. By leveraging the strengths of both SSMs and sparse attention, the design aims to improve long-sequence processing, reduce computational complexity, and maintain high model expressiveness.\n\n### References\n1. Zuo, S., et al. (2022). *Efficient Long Sequence Modeling via State Space Augmented Transformer*. arXiv.org.\n2. Ren, L., et al. (2023). *Sparse Modular Activation for Efficient Sequence Modeling*. Neural Information Processing Systems.\n3. Dao, T., & Gu, A. (2024). *Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality*. arXiv.org.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "gpt4o_0806",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "The proposal introduces a novel integration of State Space Models (SSMs) with sparse attention mechanisms to enhance the efficiency and scalability of autoregressive language models. By leveraging the strengths of both SSMs and sparse attention, the design aims to improve long-sequence processing, reduce computational complexity, and maintain high model expressiveness.",
    "ideation": null,
    "modelname": "sparsestatetttmodel",
    "suggestions": "1. Implementation Details:\n- Provide more specific guidelines for SSM-attention integration\n- Include detailed state management strategies\n- Add memory optimization techniques\n- Outline training stability measures\n\n2. Architectural Refinements:\n- Consider adaptive sparsity patterns\n- Implement hierarchical state compression\n- Add residual connections for stability\n- Include gradient checkpointing mechanisms\n\n3. Training Strategy:\n- Develop specialized initialization schemes\n- Design progressive training approach\n- Include stability monitoring metrics\n- Add regularization techniques\n\n4. Memory Optimization:\n- Implement efficient state caching\n- Add adaptive compression mechanisms\n- Consider quantization strategies\n- Optimize memory access patterns\n\n5. Hardware Considerations:\n- Provide hardware-specific optimizations\n- Include parallel processing strategies\n- Add cache utilization guidelines\n- Consider bandwidth optimization techniques\n\n6. Evaluation Framework:\n- Define specific performance metrics\n- Include ablation study guidelines\n- Add comparison benchmarks\n- Provide stability monitoring protocols",
    "user_input": ""
}