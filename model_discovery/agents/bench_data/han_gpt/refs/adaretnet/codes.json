{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = AdaptiveLayer(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveLayer(GAUBase):\n    \"\"\"\n    AdaptiveLayer implements an adaptive computation layer that adjusts its computation based on input complexity.\n\n    The layer consists of:\n    - A complexity estimator which predicts a complexity score for each token in the input sequence.\n    - A gating mechanism that decides whether to bypass computation or perform standard MLP computation based on the complexity score.\n    - Standard MLP computations applied selectively based on the gating decision.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for flexible configuration.\n        device (torch.device, optional): Device to instantiate the parameters on.\n        dtype (torch.dtype, optional): Data type of the parameters.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, complexity_threshold=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = embed_dim\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.intermediate_size = intermediate_size\n        self.complexity_estimator = nn.Linear(hidden_size, 1, **self.\n            factory_kwargs)\n        self.gate_proj = nn.Linear(hidden_size, intermediate_size * 2, bias\n            =False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(intermediate_size, hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.act_fn = nn.SiLU()\n        self.kwargs = kwargs\n        self.complexity_threshold = complexity_threshold\n\n    def _forward(self, X, **Z):\n        complexity_scores = torch.sigmoid(self.complexity_estimator(X))\n        complexity_scores = complexity_scores.to(dtype=X.dtype)\n        threshold = Z.get('complexity_threshold', self.complexity_threshold)\n        k = 10.0\n        compute_mask = torch.sigmoid((complexity_scores - threshold) * k)\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, dim=-1)\n        z = self.act_fn(gate) * y\n        mlp_output = self.down_proj(z)\n        Y = X * (1 - compute_mask) + mlp_output * compute_mask\n        Z_ = {}\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'complexity_threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}