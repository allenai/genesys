{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = EventVQ(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass EventVQ(GAUBase):\n    \"\"\"\n    EventVQ Block: Event-Driven Vector Quantized Language Model Block\n\n    This block orchestrates the main components of the EventVQ design,\n    integrating event detection, vector quantization, and attention mechanisms\n    to create an efficient and adaptive language model block.\n\n    **Core Components:**\n    - **Event Detection and Quantization Module**: Prepares inputs for attention based on detected events.\n    - **Hierarchical State Manager**: Manages state compression and updates.\n    - **Selective Attention Computer**: Computes attention using quantized keys and values.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z**: Dictionary of intermediate variables.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device to place the model on.\n        dtype (torch.dtype, optional): Data type of the model parameters.\n\n    **Example Usage:**\n\n        >>> eventvq_block = EventVQ(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = eventvq_block(X)\n\n    **Note:**\n    - This block is designed to operate within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = EDVQAttention(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        residual = hidden_states\n        hidden_states, _ = self.seq_norm(hidden_states, **Z)\n        hidden_states, Z = self.attention(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states, _ = self.ffn_norm(hidden_states, **Z)\n        hidden_states, _ = self.mlp(hidden_states, **Z)\n        hidden_states = residual + hidden_states\n        return hidden_states, Z\n\n\nimport torch.nn.functional as F\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    **Args:**\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    **Attributes:**\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Example Usage:**\n\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output, _ = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    **References:**\n\n    - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n      https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \"\"\"Initialize RMSNorm module.\"\"\"\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        Y = self.weight * X.to(input_dtype)\n        return Y\n\n\nimport torch.nn.functional as F\n\n\nclass EDVQAttention(GAUBase):\n    \"\"\"\n    EDVQAttention: Event-Driven Vector Quantized Attention Unit\n\n    This unit integrates event detection, vector quantization, and attention computation\n    to create an efficient and adaptive attention mechanism.\n\n    **Core Components:**\n    - **Event Detection**: Identifies important events in the input sequence.\n    - **Vector Quantization**: Compresses inputs based on importance.\n    - **Attention Mechanism**: Computes attention using quantized and original inputs with causal masking.\n\n    **Mathematical Formulation:**\n    1. Event Detection:\n       \\\\[ e(x) = \\\\sigma(W_e x + b_e) \\\\]\n       \\\\[ \text{importance} = e(x) \\\\]\n\n    2. Vector Quantization:\n       \\\\[ x_{q} = \text{VQ}(x) \\\\]\n\n    3. Attention Computation:\n       \\\\[ y = \text{Attention}(Q, K', V') \\\\]\n       where \\\\( K' = \text{importance} \\\\cdot K + (1 - \text{importance}) \\\\cdot x_{q} \\\\)\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location within the network.\n        kwarg_all (dict): Additional keyword arguments.\n        num_heads (int, optional): Number of attention heads. Default is 8.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n        - **X**: Input tensor of shape (batch_size, seq_len, embed_dim).\n\n    **Outputs:**\n        - **Y**: Output tensor of shape (batch_size, seq_len, embed_dim).\n        - **Z'**: Dictionary containing intermediate variables, e.g., 'importance'.\n\n    **Example Usage:**\n\n        >>> edvq_attn = EDVQAttention(embed_dim=512, block_loc=(0, 1), kwarg_all={})\n        >>> X = torch.randn(2, 128, 512)\n        >>> Y, Z = edvq_attn(X)\n\n    **Note:**\n        - This unit is designed to be used within a stack of blocks in an autoregressive language model.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.event_linear = nn.Linear(embed_dim, 1, **self.factory_kwargs)\n        self.codebook = nn.Parameter(torch.randn(256, self.head_dim, **self\n            .factory_kwargs) / self.head_dim ** 0.5)\n        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n\n    def _quantize(self, x):\n        BNH, L, D = x.shape\n        x_flat = x.view(-1, D)\n        distances = torch.cdist(x_flat, self.codebook)\n        indices = distances.argmin(dim=1)\n        x_q = self.codebook[indices]\n        x_q = x_q + (x_flat - x_q).detach()\n        x_q = x_q.view(BNH, L, D)\n        return x_q\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        importance = torch.sigmoid(self.event_linear(X))\n        Z_ = {'importance': importance.squeeze(-1)}\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        K = K.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        V = V.view(B, L, self.num_heads, self.head_dim).transpose(1, 2)\n        B_heads = B * self.num_heads\n        K_reshaped = K.contiguous().view(B_heads, L, self.head_dim)\n        V_reshaped = V.contiguous().view(B_heads, L, self.head_dim)\n        K_quantized = self._quantize(K_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        V_quantized = self._quantize(V_reshaped).view(B, self.num_heads, L,\n            self.head_dim)\n        importance_expanded = importance.unsqueeze(1)\n        importance_expanded = importance_expanded.expand(-1, self.num_heads,\n            -1, self.head_dim)\n        K_q = importance_expanded * K + (1 - importance_expanded) * K_quantized\n        V_q = importance_expanded * V + (1 - importance_expanded) * V_quantized\n        attn_scores = torch.matmul(Q, K_q.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        seq_len = L\n        causal_mask = torch.triu(torch.ones(seq_len, seq_len, device=X.\n            device, dtype=torch.bool), diagonal=1)\n        causal_mask = causal_mask.unsqueeze(0).unsqueeze(0)\n        attn_scores = attn_scores.masked_fill(causal_mask, float('-inf'))\n        attn_weights = F.softmax(attn_scores, dim=-1)\n        attn_output = torch.matmul(attn_weights, V_q)\n        attn_output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        Y = self.out_proj(attn_output)\n        assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n        return Y, Z_\n\n\nimport torch.nn.functional as F\nfrom typing import Optional\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n    \"\"\"\n    SwiGluMLP: Feed-Forward Network with SwiGLU activation function.\n\n    This unit implements a feed-forward neural network using the SwiGLU activation function,\n    as described in the paper \"GLU Variants Improve Transformer\" by Shazeer (2020).\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = \text{DownProj}(\text{SwiGLU}(\text{GateProj}(X)) \\\\odot \text{UpProj}(X))\n\n    where:\n\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n    - \\\\( \text{GateProj} \\\\), \\\\( \text{UpProj} \\\\), and \\\\( \text{DownProj} \\\\) are linear projections.\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication.\n    - \\\\( \text{SwiGLU}(x) = \text{SiLU}(x) \\\\).\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        intermediate_size (int, optional): Dimension of the intermediate projection.\n            If None, defaults to int(embed_dim * 2.5).\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq\\\\_len, embed\\\\_dim).\n\n    **Outputs:**\n\n        - **Y**: Output tensor of the same shape as input X.\n\n    **Example:**\n\n        >>> swiglu_mlp = SwiGluMLP(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 1024, 512)\n        >>> Y, Z = swiglu_mlp(X)\n\n    **References:**\n\n    - Shazeer, N. (2020). \"GLU Variants Improve Transformer\". arXiv preprint arXiv:2002.05202.\n\n    **Note:**\n\n    - The activation function used is 'silu', which is also known as the SiLU or Swish function.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size: Optional[int]=None, **\n        kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        gate_output = self.act_fn(self.gate_proj(X))\n        up_output = self.up_proj(X)\n        hidden = gate_output * up_output\n        Y = self.down_proj(hidden)\n        return Y, {}\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'num_heads': 8}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}