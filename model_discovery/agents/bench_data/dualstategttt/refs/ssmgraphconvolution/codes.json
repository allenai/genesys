{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = TTT(embed_dim=embed_dim, block_loc=block_loc, kwarg_all\n            =kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\n\n\nclass TTT(GAUBase):\n    \"\"\"\n    Problem Statement\nThis paper addresses the challenge of long context in recurrent neural networks (RNNs). While RNNs offer linear computational complexity, their performance suffers in long sequences due to the limited expressive power of their fixed-size hidden states. This limitation contrasts with Transformers, which excel in long-context scenarios but have quadratic complexity.\n\nMain Claims\nThe paper proposes a new class of sequence modeling layers called Test-Time Training (TTT) layers that offer both linear complexity and expressive hidden states.\nThe key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\nThe paper introduces two instantiations of TTT layers: TTT-Linear, with a linear model as the hidden state, and TTT-MLP, with a two-layer multi-layer perceptron (MLP) as the hidden state.\nBoth TTT-Linear and TTT-MLP demonstrate competitive performance compared to strong Transformer and Mamba (a modern RNN) baselines across various model sizes.\nUnlike Mamba, both TTT layers show a continuous decrease in perplexity as they condition on more tokens in long sequences.\nTTT-Linear, with preliminary systems optimization, is faster than Transformers at 8k context and matches Mamba in wall-clock time.\nMethodology\nThe paper introduces TTT layers, which use a self-supervised learning approach to update the hidden state. The update rule is effectively a gradient step on a self-supervised loss function, allowing for \"training\" of the hidden state at test time. Two implementations are explored: TTT-Linear, where the hidden state is a linear model, and TTT-MLP, where the hidden state is a two-layer MLP. The paper also proposes mini-batch TTT and a dual form to improve hardware efficiency and speed up computations.\n\nKey Results\nIn short-context (2k and 8k tokens) experiments on the Pile dataset, both TTT-Linear and TTT-MLP demonstrate performance comparable to or exceeding Mamba and Transformer baselines.\nIn long-context (1k to 32k tokens) experiments on the Books3 subset of the Pile, both TTT-Linear and TTT-MLP outperform Mamba, especially at longer context lengths.\nTTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\nWith preliminary systems optimization, TTT-Linear is already faster than Transformers at 8k context and matches Mamba in wall-clock time.\nTTT-MLP shows potential for even better performance in long-context scenarios but currently faces challenges in memory I/O.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        kwarg_all['num_attention_heads'] = max(4, embed_dim // 64)\n        self.seq_modeling_block = GatedTTTLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        kwarg_all['intermediate_size'] = int(embed_dim * 2.5)\n        self.mlp = SwiGluMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = Conv(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.seq_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = X\n        position_ids = torch.arange(0, X.shape[1], dtype=torch.long, device\n            =X.device).unsqueeze(0)\n        residual = hidden_states\n        hidden_states = self.conv(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.seq_norm(hidden_states, **Z)[0]\n        Z['position_ids'] = position_ids\n        hidden_states = self.seq_modeling_block(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        residual = hidden_states\n        hidden_states = self.ffn_norm(hidden_states, **Z)[0]\n        hidden_states = self.mlp(hidden_states, **Z)[0]\n        hidden_states = residual + hidden_states\n        return hidden_states\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\nfrom typing import Tuple, Dict\n\n\nclass GatedTTTLinear(GAUBase):\n    \"\"\"\n    GatedTTTLinear GAU.\n\n    This GAU enhances the existing TTTLinear GAU by integrating gating mechanisms and \n    graph-inspired convolutional operations. It allows the model to dynamically adapt \n    during test-time training by capturing both local and global dependencies efficiently.\n\n    **Code Example:**\n\n    ```python\n        from gatedtttlinear import GatedTTTLinear\n\n    embed_dim = 128\n    block_loc = (0, 0)\n    gau = GatedTTTLinear(embed_dim=embed_dim, block_loc=block_loc, kwarg_all={})\n\n    X = torch.randn(2, 50, embed_dim)  # Batch size 2, sequence length 50\n    Z = {}\n    Y, Z = gau(X, **Z)\n    print(Y.shape)  # Should output: torch.Size([2, 50, 128])\n    ```\n\n    **Todo:**\n        * Implement dynamic adjacency matrix computation based on input embeddings.\n        * Ensure computational efficiency for scalability.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n\n    Returns:\n        Output embeddings Y and updated intermediate variables Z.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.gate_proj = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.activation = nn.Sigmoid()\n        self.linear_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.graph_conv = GraphConvolution(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _forward(self, X: torch.Tensor, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GatedTTTLinear GAU.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        G = self.activation(self.gate_proj(X))\n        Y_gated = G * self.linear_proj(X)\n        Y_graph, Z = self.graph_conv(Y_gated, **Z)\n        return Y_graph, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple, Dict\n\n\nclass GraphConvolution(GAUBase):\n    \"\"\"\n    GraphConvolution GAU with integrated State Space Models (SSMs).\n\n    This GAU replaces the standard multi-head attention mechanism with a State Space Model (SSM) to efficiently capture long-range dependencies in the input sequence. By leveraging the linear complexity of SSMs, it enhances scalability and computational efficiency, especially for long sequences.\n\n    **Mathematical Framework:**\n\n    The state-space equations are defined as:\n\n    \\\\[\n    \\\\mathbf{h}_{t+1} = \\\\mathbf{A} \\\\mathbf{h}_t + \\\\mathbf{B} \\\\mathbf{x}_t + \\\\sum_{k \\\\in \\\\mathcal{N}(t)} a_{t,k} \\\\mathbf{E}_{t,k} \\\\mathbf{h}_k\n    \\\\]\n\n    \\\\[\n    \\\\mathbf{y}_t = \\\\mathbf{C} \\\\mathbf{h}_t + \\\\mathbf{D} \\\\mathbf{x}_t\n    \\\\]\n\n    - \\\\( \\\\mathbf{A} \\\\in \\\\mathbb{R}^{N \times N} \\\\): State transition matrix.\n    - \\\\( \\\\mathbf{B} \\\\in \\\\mathbb{R}^{N \times D} \\\\): Input-to-state matrix.\n    - \\\\( \\\\mathbf{C} \\\\in \\\\mathbb{R}^{D \times N} \\\\): State-to-output matrix.\n    - \\\\( \\\\mathbf{D} \\\\in \\\\mathbb{R}^{D \times D} \\\\): Direct input-to-output matrix.\n    - \\\\( \\\\mathbf{E} \\\\in \\\\mathbb{R}^{N \times N} \\\\): Edge influence matrices.\n    - \\\\( \\\\mathcal{N}(t) \\\\): Set of neighboring nodes of node \\\\( t \\\\) based on the adjacency matrix.\n\n    **Args:**\n        embed_dim (int): The size of the input and output feature dimensions.\n        block_loc (tuple): The location of this GAU within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments for initializing child GAUs.\n        device (torch.device, optional): Device to allocate the GAU's parameters.\n        dtype (torch.dtype, optional): Data type of the GAU's parameters.\n        state_dim (int, optional): Dimension of the hidden state. Defaults to embed_dim.\n        dropout (float, optional): Dropout probability for state outputs. Defaults to 0.1.\n\n    Returns:\n        Output embeddings Y of shape (B, L, D) and updated intermediate variables Z.\n\n    Example:\n        >>>         >>> graph_conv = GraphConvolution(embed_dim=128, block_loc=(0, 0), kwarg_all={})\n        >>> X = torch.randn(2, 50, 128)\n        >>> Y, Z = graph_conv(X)\n        >>> print(Y.shape)\n        torch.Size([2, 50, 128])\n\n    Todo:\n        * Optimize state updates for computational efficiency.\n        * Implement sparse adjacency matrix handling if necessary.\n        * Ensure compatibility with larger models and longer sequences.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: Optional[int]=None, dropout:\n        float=0.1, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.B = nn.Parameter(torch.randn(self.state_dim, embed_dim, **self\n            .factory_kwargs))\n        self.C = nn.Parameter(torch.randn(embed_dim, self.state_dim, **self\n            .factory_kwargs))\n        self.D = nn.Parameter(torch.randn(embed_dim, embed_dim, **self.\n            factory_kwargs))\n        self.E = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs))\n        self.layer_norm = nn.LayerNorm(embed_dim, eps=1e-05, **self.\n            factory_kwargs)\n        self.dropout = nn.Dropout(dropout)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        nn.init.xavier_uniform_(self.A)\n        nn.init.xavier_uniform_(self.B)\n        nn.init.xavier_uniform_(self.C)\n        nn.init.xavier_uniform_(self.D)\n        nn.init.xavier_uniform_(self.E)\n\n    def _forward(self, X: torch.Tensor, adjacency_matrix: Optional[torch.\n        Tensor]=None, **Z) ->Tuple[torch.Tensor, Dict]:\n        \"\"\"\n        Forward pass of GraphConvolution GAU with integrated State Space Model.\n\n        Args:\n            X (torch.Tensor): Input embeddings of shape (B, L, D).\n            adjacency_matrix (torch.Tensor, optional): Adjacency matrix of shape (L, L). Defaults to causal adjacency matrix if None.\n            **Z: Intermediate variables.\n\n        Returns:\n            Tuple containing output embeddings Y and updated intermediate variables Z.\n        \"\"\"\n        B, L, D = X.shape\n        N = self.state_dim\n        X_norm = self.layer_norm(X)\n        h = Z.get('h', torch.zeros(B, L, N, device=X.device, dtype=X.dtype))\n        if adjacency_matrix is None:\n            adjacency_matrix = torch.tril(torch.ones(L, L, device=X.device,\n                dtype=X.dtype))\n        else:\n            assert adjacency_matrix.shape == (L, L\n                ), f'Adjacency matrix must be of shape ({L}, {L}), got {adjacency_matrix.shape}.'\n        A_h = torch.matmul(h, self.A.T)\n        B_x = torch.matmul(X_norm, self.B.T)\n        E_h = torch.matmul(h, self.E.T)\n        adjacency_expanded = adjacency_matrix.unsqueeze(0).expand(B, -1, -1)\n        sum_aE_hk = torch.bmm(adjacency_expanded, E_h)\n        h_new = A_h + B_x + sum_aE_hk\n        C_h = torch.matmul(h_new, self.C.T)\n        D_x = torch.matmul(X_norm, self.D.T)\n        Y = C_h + D_x\n        Y = self.dropout(Y)\n        Y = Y + X\n        Z['h'] = h_new\n        return Y, Z\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\n\n\nclass SwiGluMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, intermediate_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.intermediate_size = (intermediate_size if intermediate_size is not\n            None else int(embed_dim * 2.5))\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['silu']\n\n    def _forward(self, X, **Z):\n        down_proj = self.down_proj(self.act_fn(self.gate_proj(X)) * self.\n            up_proj(X))\n        return down_proj\n\n\nimport torch.nn.functional as F\nfrom typing import Any, Dict, Optional, Tuple, Union\nimport torch.nn.functional as F\nimport torch.utils.checkpoint\nfrom torch.utils._pytree import tree_map\nfrom transformers.utils import logging\nfrom transformers.activations import ACT2FN\ntry:\n    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\nexcept:\n    causal_conv1d_update, causal_conv1d_fn = None, None\n\n\nclass Conv(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, conv_kernel=4, rms_norm_eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        kwarg_all['eps'] = rms_norm_eps\n        self.norm = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.conv = nn.Conv1d(embed_dim, embed_dim, bias=True, kernel_size=\n            conv_kernel, groups=embed_dim, padding=conv_kernel - 1, **self.\n            factory_kwargs)\n\n    def __call__(self, X, **Z):\n        hidden_states = X\n        seq_len = hidden_states.shape[1]\n        hidden_states = self.norm(hidden_states, **Z)[0]\n        hidden_states = hidden_states.transpose(1, 2)\n        if causal_conv1d_fn is None:\n            hidden_states = self.conv(hidden_states)[..., :seq_len]\n        else:\n            conv_weights = self.conv.weight.view(self.conv.weight.size(0),\n                self.conv.weight.size(2))\n            hidden_states = causal_conv1d_fn(hidden_states, conv_weights,\n                self.conv.bias, activation=None)\n        hidden_states = hidden_states.transpose(1, 2)\n        return hidden_states\n\n\ngab_config = {'eps': 1e-05, 'intermediate_size': None, 'conv_kernel': 4,\n    'rms_norm_eps': 1e-06, 'state_dim': None, 'dropout': 0.1}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}