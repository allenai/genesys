{
    "variantname": "DualStateGatedTTTLinear",
    "review": "The DualStateGatedTTTLinear proposal presents an innovative approach to enhancing the GatedTTTLinear GAU by integrating state space duality and structured matrix optimizations. Here's a comprehensive analysis:\n\n1. CLARITY (4.5/5):\nThe proposal is exceptionally well-structured and articulated:\n+ Clear objectives focusing on computational efficiency and memory optimization\n+ Detailed mathematical formulations and implementation guidelines\n+ Well-defined components and their interactions\n+ Comprehensive documentation of design decisions\n- Could benefit from more detailed analysis of edge cases in dual-form switching\n\n2. INNOVATION (4.3/5):\nThe proposal demonstrates significant innovation through:\n+ Novel integration of state space duality with TTTLinear\n+ Unique approach to structured matrix optimization\n+ Hardware-aware design considerations\n+ Clear differentiation from existing approaches\n\nComparison with existing research shows:\n- Builds upon recent advances in state space models\n- Extends beyond traditional structured matrices\n- Introduces new concepts for memory optimization\n- Differentiates from previous GAU modifications\n\n3. FEASIBILITY (4.2/5):\nImplementation feasibility shows strong promise:\n+ Well-defined mathematical foundation\n+ Clear implementation guidelines with practical considerations\n+ Hardware-aware design choices\n+ Efficient state management strategies\n- Some complexity in dual-form computation implementation\n\n4. SCALABILITY (4.4/5):\nThe design shows excellent potential for scaling:\n+ Linear complexity option for long sequences\n+ Memory-efficient state management\n+ Hardware-optimized implementation strategy\n+ Clear path for handling larger models\n- Needs careful consideration of form switching thresholds\n\n5. ACCURACY AND ROBUSTNESS (4.3/5):\nExpected improvements in:\n+ Better handling of long-range dependencies through dual-form computation\n+ Improved ability to adapt to different sequence lengths\n+ Enhanced robustness through structured computations\n+ Maintained model expressivity despite optimization\n- Potential impact of compression needs careful validation\n\n6. EFFICIENCY (4.5/5):\nSignificant improvements in:\n+ Reduced computational complexity through dual-form computation\n+ Optimized memory usage through structured matrices\n+ Better hardware utilization\n+ Reduced memory bandwidth requirements\n+ Clear potential for parallel processing\n\nSTRENGTHS:\n1. Strong theoretical foundation combining state space duality and structured matrices\n2. Hardware-efficient design with clear optimization strategies\n3. Well-thought-out memory management approach\n4. Maintained model expressivity while improving efficiency\n5. Clear differentiation from existing approaches\n6. Comprehensive consideration of implementation details\n\nCONCERNS:\n1. Complex interaction between dual forms needs careful handling\n2. Memory access patterns during form switching need optimization\n3. Trade-offs between compression and accuracy\n4. Implementation complexity may require significant engineering effort\n5. Need for careful initialization and training procedures",
    "search_stack": [
        {
            "ready": false,
            "query": "long context length, efficient attention, state space models",
            "detail": "Analyze recent developments in efficient attention mechanisms and state space models for handling long sequences, focusing on techniques that could improve the GatedTTTLinear GAU's ability to process longer contexts while maintaining computational efficiency.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in efficient attention mechanisms and state space models for handling long sequences, focusing on techniques that could improve the GatedTTTLinear GAU's ability to process longer contexts while maintaining computational efficiency.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 1.00)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 2. Long Range Language Modeling via Gated State Spaces (Avg. Score: 1.00)\n\n*Harsh Mehta, Ankit Gupta, Ashok Cutkosky, Behnam Neyshabur*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 134  (*Influential: 17*)\n\n**TL;DR:** This work proposes a new layer named Gated State Space (GSS) and shows that it trains significantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.\n\n**Abstract:** State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.\n\n##### *Relevant Chunk: No. 1/28 (Score: 1.00)*\n\n```\n# Long Range Language Modeling via Gated State Spaces \n\nHarsh Mehta ${ }^{1 *} \\quad$ Ankit Gupta $^{2} \\quad$ Ashok Cutkosky ${ }^{3} \\quad$ Behnam Neyshabur ${ }^{1}$\n\n\n#### Abstract\n\nState space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further. ## 1 Introduction\n\nModeling long range dependencies on sequential data is a crucial step towards closing the gap with human-level performance on many tasks. Attention based models like Transformer [Vaswani et al., 2017] have proven to be a strong choice of backbone architecture for a considerable number of tasks across modalities and scale [Devlin et al., 2019, Brown et al., 2020, Dosovitskiy et al., 2021]. Vanilla Multi-Head-Attention famously incurs $\\Omega\\left(L^{2}\\right)$ penalty in modeling a sequence of length $L$. This is prohibitive at best for tasks where the model is required to capture long range dependencies from various parts of the input. Over the years, a variety of improvements have been proposed to alleviate this quadratic complexity (cf. [Tay et al., 2020]). On a somewhat orthogonal direction, attention-free models based on state spaces, such as $\\mathrm{S} 4[\\mathrm{Gu}$ et al., 2022a] and DSS [Gupta et al., 2022], have shown remarkable improvements on Long Range Arena (LRA) [Tay et al., 2021], a benchmark designed with long range modeling as its focus and consists of diverse tasks with $1 \\mathrm{k}-16 \\mathrm{k}$ sequence length across modalities. These models require careful initialization, originally borrowing ideas from the theory of HiPPO matrices [Voelker et al., 2019, Gu et al., 2020], to achieve good results on LRA. In this work, we explore and extend the use of state space models by focusing solely on the task of autoregressive sequence modeling [Brown et al., 2020, Rae et al., 2021, Chowdhery et al., 2022, Zhang et al., 2022, Hoffmann et al., 2022, Srivastava et al., 2022]. Several key properties endowed by the state space model family makes it particularly attractive, to at least fully explore it, in the context of language modeling. First, it reduces the $\\Omega\\left(L^{2}\\right)$ complexity on input sequence length to $O(L \\log L)$. This complexity results from the use of Fast Fourier Transform (FFT) [Cooley and Tukey, 1965] for performing convolutions. We will describe this in detail in later sections. Second, the state space model is fully parallelizable in the length dimension. This is an arguably subtle but an important property at training time. Note that transformers are also fully parallelizable, a worthy advantage over traditional RNNs for modeling sequences, which otherwise incurs only an $O(L)$ penalty. While this parallelism is useful at training time, it may also be a curse at inference time\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_12_5ab9ed67021bd72a4442g-02.jpg?height=470&width=563&top_left_y=242&top_left_x=369)\n\n```\ndef gss(x, F=4096, L=4096, \\(E=1024, \\mathrm{H}=256\\) ):\n    shortcut, \\(\\mathrm{x}=\\mathrm{x}, \\operatorname{norm}(\\mathrm{x})\\)\n    \\(\\mathrm{v}=\\) dense(x, F, activation='gelu')\n    \\(\\mathrm{u}=\\) dense(x, H, activation='gelu')\n    \\(\\mathrm{y}=\\operatorname{dss}(u, \\mathrm{H}, \\mathrm{L})\\)\n    \\# yh1,..,yhL are linear in uh1,..,uhL\n    \\(\\mathrm{uc}=\\) dense(y, F)\n    \\(0=\\) dense(uc \\(*\\) v, E)\n    return o + shortcut\n```\n\nFigure 1: (a) Our proposed Gated State Space (GSS) layer, (b) Pseudocode for GSS (full implementation in \u00a7A.2).\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Efficient Long-Text Understanding with Short-Text Models (Avg. Score: 1.00)\n\n*Maor Ivgi, Uri Shaham, Jonathan Berant*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2022)\t**Cited by** 49  (*Influential: 5*)\n\n**TL;DR:** This work proposes SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs and finds that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.\n\n**Abstract:** Transformer-based pretrained language models (LMs) are ubiquitous across natural language understanding, but cannot be applied to long sequences such as stories, scientific articles, and long documents due to their quadratic complexity. While a myriad of efficient transformer variants have been proposed, they are typically based on custom implementations that require expensive pretraining from scratch. In this work, we propose SLED: SLiding-Encoder and Decoder, a simple approach for processing long sequences that re-uses and leverages battle-tested short-text pretrained LMs. Specifically, we partition the input into overlapping chunks, encode each with a short-text LM encoder and use the pretrained decoder to fuse information across chunks (fusion-in-decoder). We illustrate through controlled experiments that SLED offers a viable strategy for long text understanding and evaluate our approach on SCROLLS, a benchmark with seven datasets across a wide range of language understanding tasks. We find that SLED is competitive with specialized models that are up to 50x larger and require a dedicated and expensive pretraining step.\n\n##### *Relevant Chunk: No. 22/30 (Score: 1.00)*\n\n```\nAlbert Gu, Karan Goel, and Christopher R'e. 2021. Efficiently modeling long sequences with structured state spaces. ArXiv, abs/2111.00396. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-totext transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724-736, Seattle, United States. Association for Computational Linguistics. Ankit Gupta. 2022. Diagonal state spaces are as effective as structured state spaces. ArXiv, abs/2203.14343. Ankit Gupta and Jonathan Berant. 2020. GMAT: Global memory augmentation for transformers. ArXiv preprint, abs/2006.03274. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419-1436, Online. Association for Computational Linguistics. Gautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874880, Online. Association for Computational Linguistics. Yichen Jiang and Mohit Bansal. 2019. Avoiding reasoning shortcuts: Adversarial evaluation, training, and model development for multi-hop QA.\n```\n\n#### 5. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 1.00)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 19/35 (Score: 1.00)*\n\n```\nAlbert Gu, Karan Goel, and Christopher R\u00e9. 2021. Efficiently modeling long sequences with structured state spaces. ArXiv preprint, abs/2111.00396. Mandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724-736, Seattle, United States. Association for Computational Linguistics. Pengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2021. Deberta: decoding-enhanced bert with disentangled attention. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. 2022. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR. Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. 2021. Efficient attentions for long document summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1419-1436, Online. Association for Computational Linguistics. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 1318 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 51565165.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: long context length, efficient attention, state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\n\n*From Search Query: long context length*\n\n*Yushi Bai, Xin Lv, Jiajie Zhang, Hong Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li*\n\n**TL;DR:** This paper introduces LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understandings of large language models.\n\n**Abstract:** Although large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability. The code and datasets are available at https://github.com/THUDM/LongBench.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 270  (*Influential: 78*)\n\n#### 2. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models\n\n*From Search Query: long context length*\n\n*Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, Jiaya Jia*\n\n**TL;DR:** LongLoRA is presented, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost, and is compatible with most existing techniques, like Flash-Attention2.\n\n**Abstract:** We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost. Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 107  (*Influential: 11*)\n\n#### 3. A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis\n\n*From Search Query: long context length*\n\n*Izzeddin Gur, Hiroki Furuta, Austin Huang, Mustafa Safdari, Yutaka Matsuo, D. Eck, Aleksandra Faust*\n\n**TL;DR:** WebAgent is introduced, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.\n\n**Abstract:** Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation. However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML. We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions. WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those. We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization. We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 127  (*Influential: 3*)\n\n#### 4. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: efficient attention*\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 351  (*Influential: 61*)\n\n#### 5. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 6. State-space Models with Layer-wise Nonlinearity are Universal Approximators with Exponential Decaying Memory\n\n*From Search Query: state space models*\n\n*Shida Wang, Beichen Xue*\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n#### 7. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: state space models*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n### 6 related papers from ArXiv\n\n#### 1. Landmark Attention: Random-Access Infinite Context Length for\n  Transformers\n\n*From Search Query: long context length*\n\n*Amirkeivan Mohtashami, Martin Jaggi*\n\n**Abstract:** While Transformers have shown remarkable success in natural language\nprocessing, their attention mechanism's large memory requirements have limited\ntheir ability to handle longer contexts. Prior approaches, such as recurrent\nmemory or retrieval-based augmentation, have either compromised the\nrandom-access flexibility of attention (i.e., the capability to select any\ntoken in the entire context) or relied on separate mechanisms for relevant\ncontext retrieval, which may not be compatible with the model's attention. In\nthis paper, we present a novel approach that allows access to the complete\ncontext while retaining random-access flexibility, closely resembling running\nattention on the entire context. Our method uses a landmark token to represent\neach block of the input and trains the attention to use it for selecting\nrelevant blocks, enabling retrieval of blocks directly through the attention\nmechanism instead of by relying on a separate mechanism. Our approach\nseamlessly integrates with specialized data structures and the system's memory\nhierarchy, enabling processing of arbitrarily long context lengths. We\ndemonstrate that our method can obtain comparable performance with\nTransformer-XL while significantly reducing the number of retrieved tokens in\neach step. Finally, we show that fine-tuning LLaMA 7B with our method\nsuccessfully extends its context length capacity to over 32k tokens, allowing\nfor inference at the context lengths of GPT-4. We release the implementation of\nlandmark attention and the code to reproduce our experiments at\nhttps://github.com/epfml/landmark-attention/.\n\n**Published:** 2023-05-25T17:53:42Z  (*Updated: 2023-11-20T01:16:17Z*)\n\n\n\n#### 2. Eigen Attention: Attention in Low-Rank Space for KV Cache Compression\n\n*From Search Query: long context length*\n\n*Utkarsh Saxena, Gobinda Saha, Sakshi Choudhary, Kaushik Roy*\n\n**Abstract:** Large language models (LLMs) represent a groundbreaking advancement in the\ndomain of natural language processing due to their impressive reasoning\nabilities. Recently, there has been considerable interest in increasing the\ncontext lengths for these models to enhance their applicability to complex\ntasks. However, at long context lengths and large batch sizes, the key-value\n(KV) cache, which stores the attention keys and values, emerges as the new\nbottleneck in memory usage during inference. To address this, we propose Eigen\nAttention, which performs the attention operation in a low-rank space, thereby\nreducing the KV cache memory overhead. Our proposed approach is orthogonal to\nexisting KV cache compression techniques and can be used synergistically with\nthem. Through extensive experiments over OPT, MPT, and Llama model families, we\ndemonstrate that Eigen Attention results in up to 40% reduction in KV cache\nsizes and up to 60% reduction in attention operation latency with minimal drop\nin performance.\n\n**Published:** 2024-08-10T22:47:12Z  (*Updated: 2024-08-10T22:47:12Z*)\n\n\n\n#### 3. Efficient Transformer Knowledge Distillation: A Performance Review\n\n*From Search Query: efficient attention*\n\n*Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence*\n\n**Abstract:** As pretrained transformer language models continue to achieve\nstate-of-the-art performance, the Natural Language Processing community has\npushed for advances in model compression and efficient attention mechanisms to\naddress high computational requirements and limited input sequence length.\nDespite these separate efforts, no investigation has been done into the\nintersection of these two fields. In this work, we provide an evaluation of\nmodel compression via knowledge distillation on efficient attention\ntransformers. We provide cost-performance trade-offs for the compression of\nstate-of-the-art efficient attention architectures and the gains made in\nperformance in comparison to their full attention counterparts. Furthermore, we\nintroduce a new long-context Named Entity Recognition dataset, GONERD, to train\nand test the performance of NER models on long sequences. We find that\ndistilled efficient attention transformers can preserve a significant amount of\noriginal model performance, preserving up to 98.6% across short-context tasks\n(GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context\nQuestion-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on\nlong-context Named Entity Recognition (GONERD), while decreasing inference\ntimes by up to 57.8%. We find that, for most models on most tasks, performing\nknowledge distillation is an effective method to yield high-performing\nefficient attention models with low costs.\n\n**Published:** 2023-11-22T19:19:37Z  (*Updated: 2023-11-22T19:19:37Z*)\n\n\n\n#### 4. Treeformer: Dense Gradient Trees for Efficient Attention Computation\n\n*From Search Query: efficient attention*\n\n*Lovish Madaan, Srinadh Bhojanapalli, Himanshu Jain, Prateek Jain*\n\n**Abstract:** Standard inference and training with transformer based architectures scale\nquadratically with input sequence length. This is prohibitively large for a\nvariety of applications especially in web-page translation, query-answering\netc. Consequently, several approaches have been developed recently to speedup\nattention computation by enforcing different attention structures such as\nsparsity, low-rank, approximating attention using kernels. In this work, we\nview attention computation as that of nearest neighbor retrieval, and use\ndecision tree based hierarchical navigation to reduce the retrieval cost per\nquery token from linear in sequence length to nearly logarithmic. Based on such\nhierarchical navigation, we design Treeformer which can use one of two\nefficient attention layers -- TF-Attention and TC-Attention. TF-Attention\ncomputes the attention in a fine-grained style, while TC-Attention is a coarse\nattention layer which also ensures that the gradients are \"dense\". To optimize\nsuch challenging discrete layers, we propose a two-level bootstrapped training\nmethod. Using extensive experiments on standard NLP benchmarks, especially for\nlong-sequences, we demonstrate that our Treeformer architecture can be almost\nas accurate as baseline Transformer while using 30x lesser FLOPs in the\nattention layer. Compared to Linformer, the accuracy can be as much as 12%\nhigher while using similar FLOPs in the attention layer.\n\n**Published:** 2022-08-18T18:31:40Z  (*Updated: 2023-03-17T14:25:43Z*)\n\n\n\n#### 5. LOCOST: State-Space Models for Long Document Abstractive Summarization\n\n*From Search Query: state space models*\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for\nencoding long sequences and capturing long-term dependencies. We propose\nLOCOST: an encoder-decoder architecture based on state-space models for\nconditional text generation with long context inputs. With a computational\ncomplexity of $O(L \\log L)$, this architecture can handle significantly longer\nsequences than state-of-the-art models that are based on sparse attention\npatterns. We evaluate our model on a series of long document abstractive\nsummarization tasks. The model reaches a performance level that is 93-96%\ncomparable to the top-performing sparse transformers of the same size while\nsaving up to 50% memory during training and up to 87% during inference.\nAdditionally, LOCOST effectively handles input texts exceeding 600K tokens at\ninference time, setting new state-of-the-art results on full-book summarization\nand opening new perspectives for long input processing.\n\n**Published:** 2024-01-31T15:33:37Z  (*Updated: 2024-03-25T12:52:42Z*)\n\n\n\n#### 6. Mimetic Initialization Helps State Space Models Learn to Recall\n\n*From Search Query: state space models*\n\n*Asher Trockman, Hrayr Harutyunyan, J. Zico Kolter, Sanjiv Kumar, Srinadh Bhojanapalli*\n\n**Abstract:** Recent work has shown that state space models such as Mamba are significantly\nworse than Transformers on recall-based tasks due to the fact that their state\nsize is constant with respect to their input sequence length. But in practice,\nstate space models have fairly large state sizes, and we conjecture that they\nshould be able to perform much better at these tasks than previously reported.\nWe investigate whether their poor copying and recall performance could be due\nin part to training difficulties rather than fundamental capacity constraints.\nBased on observations of their \"attention\" maps, we propose a structured\ninitialization technique that allows state space layers to more readily mimic\nattention. Across a variety of architecture settings, our initialization makes\nit substantially easier for Mamba to learn to copy and do associative recall\nfrom scratch.\n\n**Published:** 2024-10-14T23:17:46Z  (*Updated: 2024-10-14T23:17:46Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K\n\n*From Search Query: long context length*\n\n*Yu Wang, Shengen Yan, Guohao Dai, Boxun Li, Dahua Lin, Zhuyu Yao, Zheyue Tan, Minghui Zhuang, Shiyao Li, Zhijie Yang, Dong Zhou, Xuefei Ning, Tao Yuan*\n\n**Abstract:** State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion, keyword and phrase replacement, and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across different context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 15 LLMs on LV-Eval and conduct ablation studies on the benchmarking techniques. The results reveal that: (i) Moonshot-v1 and recent large-scale open-source models, such as Qwen-2.5-72B and Llama-3.1-70B, achieve the highest performance on LV-Eval, particularly at lengths below 64k. (ii) Models exhibit distinct score trends. For example, GLM-4-9B-128k, Yi-6B-200k, and Llama3-8B-1M exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of \"needle in a haystack\". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval. All datasets and evaluation codes are released at: https://github.com/infinigence/LVEval.\n\n**Published:** 2024-02-06\n\n\n\n#### 2. Ada-LEval: Evaluating long-context LLMs with length-adaptable benchmarks\n\n*From Search Query: long context length*\n\n*Kai Chen, Dahua Lin, Songyang Zhang, Haodong Duan, Chonghua Wang*\n\n**Abstract:** Recently, the large language model (LLM) community has shown increasing interest in enhancing LLMs' capability to handle extremely long documents. As various long-text techniques and model architectures emerge, the precise and detailed evaluation of models' long-text capabilities has become increasingly important. Existing long-text evaluation benchmarks, such as L-Eval and LongBench, construct long-text test sets based on open-source datasets, focusing mainly on QA and summarization tasks. These datasets include test samples of varying lengths (from 2k to 32k+) entangled together, making it challenging to assess model capabilities across different length ranges. Moreover, they do not cover the ultralong settings (100k+ tokens) that the latest LLMs claim to achieve. In this paper, we introduce Ada-LEval, a length-adaptable benchmark for evaluating the long-context understanding of LLMs. Ada-LEval includes two challenging subsets, TSort and BestAnswer, which enable a more reliable evaluation of LLMs' long context capabilities. These benchmarks support intricate manipulation of the length of test cases, and can easily produce text samples up to 128k tokens. We evaluate 4 state-of-the-art closed-source API models and 6 open-source models with Ada-LEval. The evaluation results demonstrate the limitations of current LLMs, especially in ultra-long-context settings. Our code is available at https://github.com/open-compass/Ada-LEval.\n\n**Published:** 2024-04-09\n\n\n\n#### 3. Training data-efficient image transformers & distillation through attention\n\n*From Search Query: efficient attention*\n\n*Herv\u00e9 J\u00e9gou, Alexandre Sablayrolles, Francisco Massa, Matthijs Douze, Matthieu Cord, Hugo Touvron*\n\n**Abstract:** Recently, neural networks purely based on attention were shown to address image understanding tasks such as image classification. However, these visual transformers are pre-trained with hundreds of millions of images using an expensive infrastructure, thereby limiting their adoption. In this work, we produce a competitive convolution-free transformer by training on Imagenet only. We train them on a single computer in less than 3 days. Our reference vision transformer (86M parameters) achieves top-1 accuracy of 83.1% (single-crop evaluation) on ImageNet with no external data. More importantly, we introduce a teacher-student strategy specific to transformers. It relies on a distillation token ensuring that the student learns from the teacher through attention. We show the interest of this token-based distillation, especially when using a convnet as a teacher. This leads us to report results competitive with convnets for both Imagenet (where we obtain up to 85.2% accuracy) and when transferring to other tasks. We share our code and models.\n\n**Published:** 2020-12-23\n\n\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n*From Search Query: efficient attention*\n\n*Christopher R\u00e9, Atri Rudra, Stefano Ermon, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n**Published:** 2022-05-27\n\n\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 6. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space models*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly in handling long context lengths efficiently, several recent developments and techniques involving state space models (SSMs) and efficient attention mechanisms are noteworthy.\n\n## State Space Models (SSMs)\n\nSSMs have emerged as a promising approach for handling long sequences due to their linear complexity and ability to model complex dependencies.\n\n### Linear Complexity and Scalability\nSSMs maintain linear complexity with respect to sequence length, making them highly scalable for long sequences. This is a significant advantage over traditional attention mechanisms, which often have quadratic complexity.\n\n### Selective State Updates\nModels like DeciMamba and Mamba-2 use selective state update mechanisms to extend context lengths efficiently. These mechanisms allow the model to focus computational resources on important tokens, thereby processing longer sequences without additional computational resources. For example, DeciMamba's approach can extrapolate to context lengths 25 times longer than those seen during training without using extra resources.\n\n### Integration with Attention Mechanisms\nRecent research, such as the work by Peng et al. (2023), suggests integrating SSMs with attention mechanisms to enhance model performance. This integration can help in capturing long-range dependencies more effectively while maintaining computational efficiency.\n\n## Efficient Attention Mechanisms\n\n### Selective Attention\nThe Taipan architecture, which combines the efficiency of Mamba with strategically placed Selective Attention Layers, demonstrates how to optimize resource allocation. By applying attention only to tokens that benefit from it, Taipan achieves high performance while improving computational efficiency. This selective attention mechanism reduces the computational demands and allows for longer sliding windows to capture longer-range dependencies.\n\n### Sparse Attention Weight Maps\nModels like Taipan use a combination of Selective Attention and Sliding Window Attention, resulting in a significantly sparser attention weight map compared to full attention or standard windowed attention. This sparsity enhances the computational efficiency of processing long sequences.\n\n## Parallelization and Hierarchical Processing\n\n### Parallel Encoding\nThe Fast-R2D2 architecture shows how recursive structures can be parallelized effectively using a top-down unsupervised parser as a model-guided pruning method. This approach enables parallel encoding during inference, significantly improving efficiency. Such strategies could be adapted to improve the hierarchical processing in autoregressive models[Analysis Note].\n\n## Theoretical Foundations\n\n### Mathematical Interpretation\nThe HiPPO framework provides a mathematical interpretation of state space models, explaining their ability to capture long dependencies through exponentially-warped Legendre polynomials. This theoretical foundation can inspire new approaches to sequence modeling and improve the ability of autoregressive models to handle long contexts[Analysis Note].\n\n## Future Directions\n\n### Combining SSMs with Graph-Based Architectures\nExploring methods to combine state space models with graph-based architectures could offer new avenues for efficient and expressive modeling. This integration might leverage the strengths of both paradigms to handle complex dependencies and long-range interactions more effectively[Analysis Note].\n\n### Detailed Implementation Techniques\nFocusing on detailed implementation techniques for selective state updates and efficient parallelization strategies will be crucial. These techniques can help in adapting the benefits of SSMs and efficient attention mechanisms to the GatedTTTLinear GAU architecture[Analysis Note].\n\nBy incorporating these advancements, researchers can significantly improve the ability of autoregressive language models to handle long context lengths efficiently while maintaining or enhancing their performance.\n",
            "analysis": "Analysis Note: Long Context Processing in Language Models\n\n1. Summary of Analysis:\nThe initial search reveals several key trends and innovations in handling long context sequences in language models:\n\na) State Space Models (SSMs) are emerging as a powerful alternative to traditional attention mechanisms, offering linear complexity scaling with sequence length while maintaining model expressiveness.\n\nb) Recent work like DeciMamba demonstrates the potential for extending context length through selective state space mechanisms without requiring additional computational resources.\n\nc) The Fast-R2D2 architecture shows how recursive structures can be parallelized effectively, suggesting possibilities for improving the efficiency of hierarchical processing.\n\n2. Useful References with Excerpts:\n\nFrom DeciMamba paper:\n\"DeciMamba introduces a context-extension method specifically designed for Mamba that can extrapolate to context lengths that are 25x times longer than the ones seen during training, and does so without utilizing additional computational resources.\"\n\nKey implementation details:\n```python\ndef forward(self, hidden_states, inference_params=None, resp_len=None):\n    # Selective state update mechanism\n    not_decimated = get_non_decimated_indices(dt, self.dt_proj.bias.float().detach().clone(), \n                                            resp_len, layer_num=self.layer_num, \n                                            decimation_config=self.decimation_config)\n```\n\nFrom Fast-R2D2:\n\"Uses a top-down unsupervised parser as a model-guided pruning method, which enables parallel encoding during inference and significantly improves efficiency.\"\n\nFrom HiPPO paper:\n\"Provides a mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\"\n\n3. Key Insights and Analysis:\n\na) Context Length Extension:\n- DeciMamba's approach of selective state updates could be adapted for the GatedTTTLinear architecture\n- The decimation mechanism allows efficient processing of longer sequences by focusing computational resources on important tokens\n\nb) Efficient Processing:\n- Fast-R2D2's parallel encoding strategy suggests ways to improve the efficiency of hierarchical processing\n- The combination of top-down parsing with pruning offers a balance between accuracy and speed\n\nc) Theoretical Foundations:\n- HiPPO's mathematical framework provides insights into how state space models can effectively capture long-range dependencies\n- The use of exponentially-warped basis functions could inspire new approaches to sequence modeling\n\n4. Future Search Plan:\n\nNext search should focus on:\n1. Detailed implementation techniques for selective state updates\n2. Methods for combining state space models with graph-based architectures\n3. Efficient parallelization strategies for hierarchical models\n\n5. References:\n\n[1] Assaf Ben-Kish, et al. (2024). \"DeciMamba: Exploring the Length Extrapolation Potential of Mamba.\" arXiv.org.\n\n[2] Xiang Hu, et al. (2022). \"Fast-R2D2: A Pretrained Recursive Neural Network based on Pruned CKY for Grammar Induction and Text Representation.\" Conference on Empirical Methods in Natural Language Processing.\n\n[3] Albert Gu, et al. (2022). \"How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections.\" International Conference on Learning Representations.\n\n[4] bloc97. (2023). \"NTK-Aware Scaled RoPE.\" Technical Report."
        },
        {
            "ready": false,
            "query": "selective state space models, hardware efficient implementation, memory optimization",
            "detail": "Analyze implementation techniques and optimizations for selective state space models, focusing on hardware efficiency and memory management strategies for processing long sequences.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementation techniques and optimizations for selective state space models, focusing on hardware efficiency and memory management strategies for processing long sequences.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 1.00)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 2. Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors (Avg. Score: 1.00)\n\n*Ido Amos, Jonathan Berant, Ankit Gupta*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 10  (*Influential: 0*)\n\n**TL;DR:** This work finds vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and improves the best reported results of SSMs on the PathX-256 task by 20 absolute points.\n\n**Abstract:** Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using $\\textit{only the downstream task data}$, leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.\n\n##### *Relevant Chunk: No. 16/31 (Score: 1.00)*\n\n```\narXiv.org, abs/2112.10740, 12 2021. ISSN 2331-8422. URL/https://arxiv.org/abs/2112.10740\n\nDaniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling. In Andreas Krause 0001, Emma Brunskill, KyungHyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan Scarlett (eds.), International Conference on Machine Learning, ICML 2023, 2329 July 2023, Honolulu, Hawaii, USA, volume abs/2302.06646, pp. 10373-10391. PMLR, 2 2023. doi: 10.48550/arxiv.2302.06646. URLhttps://proceedings.mlr.press/v202/ fu23a.html. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling. volume abs/2101.00027, 12 2020. URL https://arxiv.org/abs/2101.00027. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, volume abs/2008.07669, 8 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/102f0bb6efb3a6128a3c750dd16729be-Abstract.html. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022a. URL https: / / openreview. net/ forum?id=uYLFoz1vlAC\n\nAlbert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 1.00)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 1.00)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 4. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 1.00)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 1.00)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 5. Softmax Attention with Constant Cost per Token (Avg. Score: 0.99)\n\n*Franz A. Heinsen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work proposes a simple modification to the conventional attention mechanism applied by Transformers, which quantifies pairwise query-key similarity with scaled dot-products with the logarithms of scaled dot-products of exponentials, and linearizes attention with exponential kernel feature maps.\n\n**Abstract:** We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.\n\n##### *Relevant Chunk: No. 5/8 (Score: 0.99)*\n\n```\nAlbert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher R\u00e9. 2021. Efficiently modeling long sequences with structured state spaces. CoRR abs/2111.00396. Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. 2020. Transformers are rnns: Fast autoregressive transformers with linear attention. CoRR abs/2006.16236. Tobias Katsch. 2023. Gateloop: Fully data-controlled linear recurrence for sequence modeling.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: selective state space models, hardware efficient implementation, memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: selective state space models*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 166  (*Influential: 39*)\n\n#### 2. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space models*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: selective state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient implementation*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 5. Simple Hardware-Efficient PCFGs with Independent Left and Right Productions\n\n*From Search Query: hardware efficient implementation*\n\n*Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu*\n\n**TL;DR:** This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions, and finds that this formalism scales more effectively both as a language model and as an unsupervised parser.\n\n**Abstract:** Scaling dense PCFGs to thousands of nonterminals via a low-rank parameterization of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism scales more effectively both as a language model and as an unsupervised parser. As an unsupervised parser, our simple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language model, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank PCFGs. We further introduce \\emph{FlashInside}, a hardware IO-aware implementation of the inside algorithm for efficiently scaling simple PCFGs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: memory optimization*\n\n*Yuzhong Wang, Xu Han, Weilin Zhao, Guoyang Zeng, Zhiyuan Liu, Maosong Sun*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Memory Optimization for Deep Networks\n\n*From Search Query: memory optimization*\n\n*Aashaka Shah, Chaoxia Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Krahenbuhl*\n\n**TL;DR:** MONeT is an automatic framework that minimizes both the memory footprint and computational overhead of deep networks, and is able to outperform all prior hand-tuned operations as well as automated checkpointing.\n\n**Abstract:** Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by 32x over the last five years, the total available memory only grew by 2.5x. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by 3x for various PyTorch models, with a 9-16% overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x less memory than current state-of-the-art automated checkpointing frameworks. Our code is available at this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 21  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: selective state space models*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 2. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: selective state space models*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n#### 3. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: memory optimization*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 4. L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models\n\n*From Search Query: memory optimization*\n\n*Hyesung Jeon, Yulhwa Kim, Jae-joon Kim*\n\n**Abstract:** Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically follow a two-step approach: first,\napplying post-training quantization (PTQ) to model weights, followed by PEFT on\nthe quantized model. However, recovering from the quantization error introduced\nby PTQ through fine-tuning has proven challenging. Additionally, most PTQ-based\nPEFT methods result in a mixture of low-precision quantized weights and\nhigh-precision adapter weights, limiting the efficiency of full quantization\nduring inference. While a previous method attempted to address these issues, it\nstill suffers from limited adaptability due to the constrained LoRA parameter\nstructure required to produce fully-quantized models. To overcome these\nchallenges, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA to effectively reduce quantization error. By employing\na memory-optimized layer design, L4Q significantly reduces QAT's memory\noverhead while producing fully-quantized weights, enabling effective adaptation\nto downstream tasks. Our experiments demonstrate that this combined approach to\nquantization and fine-tuning achieves superior accuracy compared to decoupled\nfine-tuning schemes, particularly in sub-4-bit quantization, positioning L4Q as\nan efficient QAT solution. Using the LLaMA model families and instructional\ndatasets, we showcase L4Q's capabilities in language tasks and few-shot\nlearning.\n\n**Published:** 2024-02-07T14:35:05Z  (*Updated: 2024-10-28T04:41:02Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: selective state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 2. PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery\n\n*From Search Query: selective state space models*\n\n*Danfeng Hong, Xiaokang Zhang, Xiaoliang Meng, Sijun Dong, Dongxu Li, Libo Wang*\n\n**Abstract:** Semantic segmentation, as a basic tool for intelligent interpretation of remote sensing images, plays a vital role in many Earth Observation (EO) applications. Nowadays, accurate semantic segmentation of remote sensing images remains a challenge due to the complex spatial-temporal scenes and multi-scale geo-objects. Driven by the wave of deep learning (DL), CNN- and Transformer-based semantic segmentation methods have been explored widely, and these two architectures both revealed the importance of multi-scale feature representation for strengthening semantic information of geo-objects. However, the actual multi-scale feature fusion often comes with the semantic redundancy issue due to homogeneous semantic contents in pyramid features. To handle this issue, we propose a novel Mamba-based segmentation network, namely PyramidMamba. Specifically, we design a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy in multi-scale feature fusion. Comprehensive ablation experiments illustrate the effectiveness and superiority of the proposed method in enhancing multi-scale feature representation as well as the great potential for real-time semantic segmentation. Moreover, our PyramidMamba yields state-of-the-art performance on three publicly available datasets, i.e. the OpenEarthMap (70.8% mIoU), ISPRS Vaihingen (84.8% mIoU) and Potsdam (88.0% mIoU) datasets. The code will be available at https://github.com/WangLibo1995/GeoSeg.\n\n**Published:** 2024-06-16\n\n\n\n#### 3. Hardware-efficient entangled measurements for variational quantum algorithms\n\n*From Search Query: hardware efficient implementation*\n\n*Anonymous*\n\n**Abstract:** Variational algorithms have received significant attention in recent years due to their potential to solve practical problems using noisy intermediate-scale quantum (NISQ) devices. A fundamental step of these algorithms is the evaluation of the expected value of Hamiltonians, and hence efficient schemes to perform this task are required. The standard approach employs local measurements of Pauli operators and requires a large number of circuits. An alternative is to make use of entangled measurements, which might introduce additional gates between physically disconnected qubits that harm the performance. As a solution to this problem, we propose hardware-efficient entangled measurements (HEEM), that is, measurements that permit only entanglement between physically connected qubits. We show that this strategy enhances the evaluation of molecular Hamiltonians in NISQ devices by reducing the number of circuits required without increasing their depth. We provide quantitative metrics of how this approach offers better results than local measurements and arbitrarily entangled measurements. We estimate the ground-state energy of the H$_2$O molecule with classical simulators and quantum hardware using the variational quantum eigensolver with HEEM.\n\n**Published:** 2022-02-14\n\n\n\n#### 4. HELP: Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning\n\n*From Search Query: hardware efficient implementation*\n\n*Sung Ju Hwang, Song Chong, Sewoong Lee, Hayeon Lee*\n\n**Abstract:** For deployment, neural architecture search should be hardware-aware, in order to satisfy the device-specific constraints (e.g., memory usage, latency and energy consumption) and enhance the model efficiency. Existing methods on hardware-aware NAS collect a large number of samples (e.g., accuracy and latency) from a target device, either builds a lookup table or a latency estimator. However, such approach is impractical in real-world scenarios as there exist numerous devices with different hardware specifications, and collecting samples from such a large number of devices will require prohibitive computational and monetary cost. To overcome such limitations, we propose Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the device-specific latency estimation problem as a meta-learning problem, such that we can estimate the latency of a model's performance for a given task on an unseen device with a few samples. To this end, we introduce novel hardware embeddings to embed any devices considering them as black-box functions that output latencies, and meta-learn the hardware-adaptive latency predictor in a device-dependent manner, using the hardware embeddings. We validate the proposed HELP for its latency estimation performance on unseen platforms, on which it achieves high estimation performance with as few as 10 measurement samples, outperforming all relevant baselines. We also validate end-to-end NAS frameworks using HELP against ones without it, and show that it largely reduces the total time cost of the base NAS method, in latency-constrained settings. Code is available at https://github.com/HayeonLee/HELP.\n\n**Published:** 2021-06-16\n\n\n\n#### 5. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n\n*From Search Query: memory optimization*\n\n*Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He*\n\n**Abstract:** Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.\n\n**Published:** 2019-10-04\n\n\n\n#### 6. Memory-Efficient Adaptive Optimization\n\n*From Search Query: memory optimization*\n\n*Yoram Singer, Tomer Koren, Vineet Gupta, Rohan Anil*\n\n**Abstract:** Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.\n\n**Published:** 2019-01-30\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on selective state space models (SSMs), hardware efficiency, and memory management for processing long sequences, here are some key insights and techniques from the available sources:\n\n## Selective State Space Models (SSMs)\n\n### Efficiency and Scalability\nSSMs, such as the Mamba model, offer significant advantages in terms of efficiency and scalability. Mamba enjoys fast inference and linear scaling in sequence length, which is crucial for processing long sequences. It incorporates selective SSM parameters and a hardware-aware parallel algorithm, making it efficient for both inference and training.\n\n### Selective State Updates\nThe Mamba model's selective state updates allow it to focus computational resources on important tokens, enhancing efficiency. This approach simplifies the end-to-end neural network architecture without the need for attention or MLP blocks, which can be computationally intensive.\n\n## Hybrid Approaches with Attention Mechanisms\n\n### Combining SSMs and Attention\nHybrid architectures that combine SSMs with attention mechanisms show promising results. For example, the CHELA model replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach maintains global abstraction and data-dependent selection while keeping real linear complexity, which is beneficial for long sequence processing[Analysis Note].\n\n### Efficient Attention Integration\nUsing short-long convolutions as an alternative to traditional attention can be efficient. The LongLoRA model demonstrates that fine-tuning with sparse local attention can effectively extend context while maintaining efficiency. This hybrid approach combines local and global processing, which is essential for handling long sequences[Analysis Note].\n\n## Hardware-Efficient Implementations\n\n### Parallel Processing and Tiling\nHardware-aware implementations are crucial for practical efficiency. Techniques such as parallel processing and tiling strategies can significantly improve performance. For instance, the S4 model, which is a variant of SSMs, uses a convolutional form under linear time invariance to allow parallel training, enhancing efficiency.\n\n### Memory Optimization\nMemory access patterns and optimization are critical for real-world performance. The use of sparse storage formats, as seen in sparse sequence-to-sequence models, can reduce memory footprint and speed up computations by optimizing operations on sparse tensors. This is particularly beneficial when dealing with high-dimensional data or large-scale neural networks.\n\n## Memory Management Strategies\n\n### Reduced Memory Footprint\nSparse models, such as the Sparse Transformer, reduce memory consumption by storing only non-zero elements. This optimization leads to a minimized memory footprint and faster computation times, which is essential for processing long sequences efficiently.\n\n### Optimized Memory Access\nOptimizing memory reads/writes between high-bandwidth memory (HBM) and on-chip SRAM can significantly speed up attention mechanisms. The FlashLinearAttention mechanism, for example, optimizes softmax attention by reducing the number of memory accesses, which can be applied to other attention mechanisms to improve efficiency.\n\n## Conclusion\n\nTo achieve the goal of improving autoregressive language model design, the following strategies are recommended:\n\n- **Adopt Selective SSMs**: Use models like Mamba that incorporate selective state updates to focus on important tokens and scale linearly with sequence length.\n- **Hybrid Architectures**: Combine SSMs with efficient attention mechanisms, such as short-long convolutions, to maintain performance while reducing complexity.\n- **Hardware-Aware Implementations**: Utilize parallel processing and tiling strategies to enhance hardware efficiency.\n- **Memory Optimization**: Employ sparse storage formats and optimize memory access patterns to reduce memory footprint and speed up computations.\n\nThese approaches can help in designing more efficient, scalable, and hardware-friendly autoregressive language models.\n",
            "analysis": "Analysis Note: Efficient Long Context Processing in Language Models\n\n1. Summary of Analysis:\nThe initial search reveals several promising directions for improving the GatedTTTLinear architecture:\n\na) State Space Models (SSMs) offer linear complexity scaling and efficient long sequence processing, with recent innovations like selective state updates showing strong potential.\n\nb) Hybrid approaches combining SSMs with attention mechanisms demonstrate superior performance while maintaining computational efficiency.\n\nc) Hardware-aware implementations and parallel processing strategies are crucial for practical deployment.\n\n2. Useful References with Excerpts:\n\nFrom Mamba paper (Dao & Gu, 2023):\n\"Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length [...] selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks.\"\n\nFrom CHELA paper (Liu et al., 2024):\n\"CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner [...] enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nFrom LongLoRA paper (Chen et al., 2023):\n\"Although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention [...] shifted sparse attention effectively enables context extension.\"\n\n3. Key Insights and Analysis:\n\na) Selective State Processing:\n- SSMs can be enhanced with selective mechanisms to focus computational resources on important tokens\n- Combining local and global processing through hybrid architectures shows promise\n- Hardware-aware implementation is crucial for practical efficiency\n\nb) Efficient Attention Integration:\n- Short-long convolutions offer an efficient alternative to traditional attention\n- Divide-and-conquer approaches can maintain performance while reducing complexity\n- Sparse attention patterns during training can enable efficient context extension\n\nc) Implementation Considerations:\n- Parallel processing and tiling strategies are essential for hardware efficiency\n- Memory access patterns significantly impact real-world performance\n- Trade-offs between model complexity and computational efficiency need careful consideration\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of selective state mechanisms in SSMs\n2. Hardware-efficient implementations of hybrid attention-SSM architectures\n3. Memory optimization techniques for long sequence processing\n4. Techniques for combining local and global context processing\n\n5. References:\n\n[1] Dao, T., & Gu, A. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv preprint arXiv:2312.00752.\n\n[2] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\" arXiv.org.\n\n[3] Chen, Y., et al. (2023). \"LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models.\" International Conference on Learning Representations.\n\n[4] Xiao, G., et al. (2023). \"Efficient Streaming Language Models with Attention Sinks.\" International Conference on Learning Representations.\n\n[5] Gu, A., et al. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "state space duality, hybrid architectures, parallel implementation",
            "detail": "Analyze theoretical foundations of state space duality and its implications for combining SSMs with gating mechanisms, focusing on efficient parallel implementations and memory optimization techniques.",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze theoretical foundations of state space duality and its implications for combining SSMs with gating mechanisms, focusing on efficient parallel implementations and memory optimization techniques.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.98)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 1.00)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n##### *Relevant Chunk: No. 29/86 (Score: 0.99)*\n\n```\nChung et al. 2014) as well as more modern variants such as the QRNN (Bradbury et al. 2016), SRU (Lei 2021; Lei et al. 2017), RWKV (B. Peng, Alcaide, et al. 2023), HGRN (Qin, Yang, and Zhong 2023), and Griffin (Botev et al. 2024; De et al. 2024). These RNNs differ in their parameterizations in various ways, most importantly in the lack of a state expansion. Dimensionality and State Expansion. An important characteristic of SSD, shared by previous SSMs in its lineage (S4, H3, Mamba), is that it is a single-input single-output (SISO) system where input channels are processed independently. This leads to a much larger effective state size of ND where $N$ is the SSM state size (also called state expansion factor) and $D$ is the standard model dimension. Traditional RNNs either have $N=1$ or are multi-input multi-output (MIMO) with dense $B, C$ matrices, either of which leads to a smaller state. While MIMO SSMs have been shown to work well in some domains (Lu et al. 2023; Orvieto et al. 2023; J. T. Smith, Warrington, and Linderman 2023), Mamba showed that state expansion is crucial for information-dense domains such as language. One of the main advantages of SSD is allowing for even larger state expansion factors without slowing down the model. Many subsequent works have since adopted state expansion (Section 10.4). Structure. Compared to previous structured SSMs, the main restriction of SSD is on the expressivity of the state transitions $A_{t}$. We note that more general SSMs, such as the case of diagonal $A_{t}$, have the same theoretical efficiency as SSD, but are less hardware-friendly. This is because the dual quadratic form loses its attention-like interpretation and becomes more difficult to compute. Thus compared to Mamba, SSD differs only in a slightly more restrictive form of diagonal $A_{t}$, and trades off this expressivity for improved hardware efficiency (and ease of implementation). We hypothesize that it may be possible to refine our structured matrix algorithms to improve to the general diagonal SSM case as well. ### 10.2 Structured Matrices\n\nThe first viewpoint of the state space duality adopts the viewpoint of these models as matrix sequence transformations or \"matrix mixers\": sequence transformations (Definition 2.1) that can be represented as matrix multiplication (by a $\\mathrm{T} \\times \\mathrm{T}$ matrix) along the sequence dimension $T$. Several such matrix mixers have been proposed before, where the primary axis of variation is the representation of the matrix. These include MLP-Mixer (Tolstikhin et al. 2021) (unstructured matrix), FNet (Lee-Thorp et al. 2021) (Fourier Transform matrix), M2 (Dao, B. Chen, et al. 2022; Dao, Gu, et al. 2019; Dao, Sohoni, et al. 2020; D. Fu et al. 2024) (butterfly/monarch matrix), Toeplitz matrices (Poli et al. 2023; Qin, Han, Weixuan Sun, B. He, et al. 2023), and even more exotic structures (De Sa et al. 2018; Thomas et al. 2018). An important characterization is that efficient (sub-quadratic) matrix sequence transformations are exactly those which have structured matrix mixers. A core result of the SSD framework is viewing SSMs as matrix mixers with a particular structure - semiseparable matrices (Section 3). The linear vs. quadratic duality then takes the form of structured matrix multiplication vs. naive matrix multiplication. The structure matrix representation led to our efficient SSD algorithm through block decompositions of particular semiseparable matrices (Section 6). We note that semiseparable matrices are well-studied in the scientific computing literature, and incorporating those ideas may be a promising avenue for more improvements to state space models. We also suggest that focusing on the matrix mixer viewpoint can lead to more fruitful directions for sequence models, such as designing principled non-causal variants of Mamba, or finding ways to characterize and bridge the gap between softmax attention and sub-quadratic models through analyzing their matrix transformation structure. ## 10.3 (Linear) Attention\n\nCompared to standard (causal) attention, SSD has only two main differences. First, SSD does not use the softmax activation of standard attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017), which is what gives attention its quadratic complexity. When the softmax is dropped, the sequence can be computed with linear scaling through the linear attention framework (Katharopoulos et al. 2020). Second, SSD multiplies the logits matrix by an input-dependent 1-semiseparable mask. Thus this mask can be viewed as replacing the softmax in standard attention. This semiseparable mask can also be viewed as providing positional information. The elements $a_{t}$ act as \"gates\" in the RNN sense, or a \"selection\" mechanism (see discussion in Mamba paper), and their cumulative products $a_{j: i}$ control how much interaction is allowed between positions $i$ and $j$.\n```\n\n##### *Relevant Chunk: No. 7/86 (Score: 0.97)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n##### *Relevant Chunk: No. 30/86 (Score: 0.97)*\n\n```\nPositional embeddings (e.g. sinusoidal (Vaswani et al. 2017), AliBi (Press, N. Smith, and Lewis 2022), and RoPE (Su et al. 2021)) are an important component of Transformers that are often viewed as heuristics, and the 1-SS mask of SSD can be seen as a more principled form of relative positional embeddings. We note that this view was also posited concurrently by GateLoop (Katsch 2023). The second viewpoint of state space duality is a special case of our more general structured masked attention (SMA) framework, where the duality is revealed as different contraction orderings on a simple 4 -way tensor contraction. SMA is a strong generalization of linear attention that is much more general than SSD as well; other forms of structured masks may lead to more variants of efficient attention with different properties than SSD. Beside leading to new models, these connections to attention can lead to other directions for understanding SSMs. For example, we are curious whether the phenomenon of attention sinks (Darcet et al. 2024; Xiao et al. 2024) exist for Mamba models, and more broadly whether interpretability techniques can be transferred to SSMs (Ali, Zimerman, and Wolf 2024). Finally, many other variants of linear attention have been proposed (Arora, Eyuboglu, Timalsina, et al.\n```\n\n#### 2. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.95)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.95)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: state space duality, hybrid architectures, parallel implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Deep Reinforcement Learning for Cost-Effective Medical Diagnosis\n\n*From Search Query: state space duality*\n\n*Zheng Yu, Yikuan Li, Joseph Kim, Kai Huang, Yuan Luo, Mengdi Wang*\n\n**TL;DR:** This work uses reinforcement learning (RL) to find a dynamic policy that selects lab test panels sequentially based on previous observations, ensuring accurate testing at a low cost, and proposes a Semi-Model-based Deep Diagnosis Policy Optimization framework that is compatible with end-to-end training and online learning.\n\n**Abstract:** Dynamic diagnosis is desirable when medical tests are costly or time-consuming. In this work, we use reinforcement learning (RL) to find a dynamic policy that selects lab test panels sequentially based on previous observations, ensuring accurate testing at a low cost. Clinical diagnostic data are often highly imbalanced; therefore, we aim to maximize the $F_1$ score instead of the error rate. However, optimizing the non-concave $F_1$ score is not a classic RL problem, thus invalidates standard RL methods. To remedy this issue, we develop a reward shaping approach, leveraging properties of the $F_1$ score and duality of policy optimization, to provably find the set of all Pareto-optimal policies for budget-constrained $F_1$ score maximization. To handle the combinatorially complex state space, we propose a Semi-Model-based Deep Diagnosis Policy Optimization (SM-DDPO) framework that is compatible with end-to-end training and online learning. SM-DDPO is tested on diverse clinical tasks: ferritin abnormality detection, sepsis mortality prediction, and acute kidney injury diagnosis. Experiments with real-world data validate that SM-DDPO trains efficiently and identifies all Pareto-front solutions. Across all tasks, SM-DDPO is able to achieve state-of-the-art diagnosis accuracy (in some cases higher than conventional methods) with up to $85\\%$ reduction in testing cost. The code is available at [https://github.com/Zheng321/Deep-Reinforcement-Learning-for-Cost-Effective-Medical-Diagnosis].\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 2. Enhancing Variational Autoencoders with Mutual Information Neural Estimation for Text Generation\n\n*From Search Query: state space duality*\n\n*Dong Qian, W. K. Cheung*\n\n**TL;DR:** This paper proposes to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE and employs neural networks for the estimation of the MI and provides a training algorithm based on the convex duality approach.\n\n**Abstract:** While broadly applicable to many natural language processing (NLP) tasks, variational autoencoders (VAEs) are hard to train due to the posterior collapse issue where the latent variable fails to encode the input data effectively. Various approaches have been proposed to alleviate this problem to improve the capability of the VAE. In this paper, we propose to introduce a mutual information (MI) term between the input and its latent variable to regularize the objective of the VAE. Since estimating the MI in the high-dimensional space is intractable, we employ neural networks for the estimation of the MI and provide a training algorithm based on the convex duality approach. Our experimental results on three benchmark datasets demonstrate that the proposed model, compared to the state-of-the-art baselines, exhibits less posterior collapse and has comparable or better performance in language modeling and text generation. We also qualitatively evaluate the inferred latent space and show that the proposed model can generate more reasonable and diverse sentences via linear interpolation in the latent space.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 17  (*Influential: 0*)\n\n#### 3. HUMUS-Net: Hybrid unrolled multi-scale network architecture for accelerated MRI reconstruction\n\n*From Search Query: hybrid architectures*\n\n*Zalan Fabian, M. Soltanolkotabi*\n\n**TL;DR:** HUMUS-Net is proposed, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network that establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset.\n\n**Abstract:** In accelerated MRI reconstruction, the anatomy of a patient is recovered from a set of under-sampled and noisy measurements. Deep learning approaches have been proven to be successful in solving this ill-posed inverse problem and are capable of producing very high quality reconstructions. However, current architectures heavily rely on convolutions, that are content-independent and have difficulties modeling long-range dependencies in images. Recently, Transformers, the workhorse of contemporary natural language processing, have emerged as powerful building blocks for a multitude of vision tasks. These models split input images into non-overlapping patches, embed the patches into lower-dimensional tokens and utilize a self-attention mechanism that does not suffer from the aforementioned weaknesses of convolutional architectures. However, Transformers incur extremely high compute and memory cost when 1) the input image resolution is high and 2) when the image needs to be split into a large number of patches to preserve fine detail information, both of which are typical in low-level vision problems such as MRI reconstruction, having a compounding effect. To tackle these challenges, we propose HUMUS-Net, a hybrid architecture that combines the beneficial implicit bias and efficiency of convolutions with the power of Transformer blocks in an unrolled and multi-scale network. HUMUS-Net extracts high-resolution features via convolutional blocks and refines low-resolution features via a novel Transformer-based multi-scale feature extractor. Features from both levels are then synthesized into a high-resolution output reconstruction. Our network establishes new state of the art on the largest publicly available MRI dataset, the fastMRI dataset. We further demonstrate the performance of HUMUS-Net on two other popular MRI datasets and perform fine-grained ablation studies to validate our design.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 3*)\n\n#### 4. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid architectures*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Low-Complexity Deep Convolutional Neural Networks on Fully Homomorphic Encryption Using Multiplexed Parallel Convolutions\n\n*From Search Query: parallel implementation*\n\n*Eunsang Lee, Joon-Woo Lee, Junghyun Lee, Young-Sik Kim, Yongjune Kim, Jong-Seon, W. Choi*\n\n**TL;DR:** This work proposes the imaginary-removing bootstrapping to prevent the deep neural networks from catastrophic divergence during approximate ReLU operations and successfully implements ResNet-110 with high accuracy on the RNS-CKKS scheme for the first time.\n\n**Abstract:** Recently, the standard ResNet-20 network was successfully implemented on the fully homomorphic encryption scheme, residue number system variant Cheon-Kim-Kim-Song (RNS-CKKS) scheme using bootstrapping, but the implementation lacks practicality due to high latency and low security level. To improve the performance, we first minimize total bootstrapping runtime using multiplexed parallel convolution that collects sparse output data for multiple channels com-pactly. We also propose the imaginary-removing bootstrapping to prevent the deep neural networks from catastrophic divergence during approximate ReLU operations. In addition, we optimize level consumptions and use lighter and tighter parameters. Simulation results show that we have 4.67 \u00d7 lower inference latency and 134 \u00d7 less amortized runtime (runtime per image) for ResNet-20 compared to the state-of-the-art previous work, and we achieve standard 128-bit security. Furthermore, we successfully implement ResNet-110 with high accuracy on the RNS-CKKS scheme for the first time.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 71  (*Influential: 22*)\n\n#### 6. A Fast and Accurate Splitting Method for Optimal Transport: Analysis and Implementation\n\n*From Search Query: parallel implementation*\n\n*Vien V. Mai, Jacob Lindb\u00e4ck, M. Johansson*\n\n**Abstract:** We develop a fast and reliable method for solving large-scale optimal transport (OT) problems at an unprecedented combination of speed and accuracy. Built on the celebrated Douglas-Rachford splitting technique, our method tackles the original OT problem directly instead of solving an approximate regularized problem, as many state-of-the-art techniques do. This allows us to provide sparse transport plans and avoid numerical issues of methods that use entropic regularization. The algorithm has the same cost per iteration as the popular Sinkhorn method, and each iteration can be executed efficiently, in parallel. The proposed method enjoys an iteration complexity $O(1/\\epsilon)$ compared to the best-known $O(1/\\epsilon^2)$ of the Sinkhorn method. In addition, we establish a linear convergence rate for our formulation of the OT problem. We detail an efficient GPU implementation of the proposed method that maintains a primal-dual stopping criterion at no extra cost. Substantial experiments demonstrate the effectiveness of our method, both in terms of computation times and robustness.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 7. Parallel and Distributed Thompson Sampling for Large-scale Accelerated Exploration of Chemical Space\n\n*From Search Query: parallel implementation*\n\n*Jos\u00e9 Miguel Hern\u00e1ndez-Lobato, James Requeima, Edward O. Pyzer-Knapp, Al\u00e1n Aspuru-Guzik*\n\n**TL;DR:** This work proposes a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS), and shows that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic.\n\n**Abstract:** Chemical space is so large that brute force searches for new interesting molecules are infeasible. High-throughput virtual screening via computer cluster simulations can speed up the discovery process by collecting very large amounts of data in parallel, e.g., up to hundreds or thousands of parallel measurements. Bayesian optimization (BO) can produce additional acceleration by sequentially identifying the most useful simulations or experiments to be performed next. However, current BO methods cannot scale to the large numbers of parallel measurements and the massive libraries of molecules currently used in high-throughput screening. Here, we propose a scalable solution based on a parallel and distributed implementation of Thompson sampling (PDTS). We show that, in small scale problems, PDTS performs similarly as parallel expected improvement (EI), a batch version of the most widely used BO heuristic. Additionally, in settings where parallel EI does not scale, PDTS outperforms other scalable baselines such as a greedy search, $\\epsilon$-greedy approaches and a random search method. These results show that PDTS is a successful solution for large-scale parallel BO.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 167  (*Influential: 11*)\n\n### 4 related papers from ArXiv\n\n#### 1. BERT-Based Combination of Convolutional and Recurrent Neural Network for\n  Indonesian Sentiment Analysis\n\n*From Search Query: hybrid architectures*\n\n*Hendri Murfi, Syamsyuriani, Theresia Gowandi, Gianinna Ardaneswari, Siti Nurrohmah*\n\n**Abstract:** Sentiment analysis is the computational study of opinions and emotions\nex-pressed in text. Deep learning is a model that is currently producing\nstate-of-the-art in various application domains, including sentiment analysis.\nMany researchers are using a hybrid approach that combines different deep\nlearning models and has been shown to improve model performance. In sentiment\nanalysis, input in text data is first converted into a numerical\nrepresentation. The standard method used to obtain a text representation is the\nfine-tuned embedding method. However, this method does not pay attention to\neach word's context in the sentence. Therefore, the Bidirectional Encoder\nRepresentation from Transformer (BERT) model is used to obtain text\nrepresentations based on the context and position of words in sentences. This\nresearch extends the previous hybrid deep learning using BERT representation\nfor Indonesian sentiment analysis. Our simulation shows that the BERT\nrepresentation improves the accuracies of all hybrid architectures. The\nBERT-based LSTM-CNN also reaches slightly better accuracies than other\nBERT-based hybrid architectures.\n\n**Published:** 2022-11-10T00:32:40Z  (*Updated: 2022-11-10T00:32:40Z*)\n\n\n\n#### 2. Empirical Evaluation of Multi-task Learning in Deep Neural Networks for\n  Natural Language Processing\n\n*From Search Query: hybrid architectures*\n\n*Jianquan Li, Xiaokang Liu, Wenpeng Yin, Min Yang, Liqun Ma, Yaohong Jin*\n\n**Abstract:** Multi-Task Learning (MTL) aims at boosting the overall performance of each\nindividual task by leveraging useful information contained in multiple related\ntasks. It has shown great success in natural language processing (NLP).\nCurrently, a number of MLT architectures and learning mechanisms have been\nproposed for various NLP tasks. However, there is no systematic exploration and\ncomparison of different MLT architectures and learning mechanisms for their\nstrong performance in-depth. In this paper, we conduct a thorough examination\nof typical MTL methods on a broad range of representative NLP tasks. Our\nprimary goal is to understand the merits and demerits of existing MTL methods\nin NLP tasks, thus devising new hybrid architectures intended to combine their\nstrengths.\n\n**Published:** 2019-08-16T03:16:40Z  (*Updated: 2020-08-07T08:06:18Z*)\n\n\n\n#### 3. Transfer Topic Modeling with Ease and Scalability\n\n*From Search Query: parallel implementation*\n\n*Jeon-Hyung Kang, Jun Ma, Yan Liu*\n\n**Abstract:** The increasing volume of short texts generated on social media sites, such as\nTwitter or Facebook, creates a great demand for effective and efficient topic\nmodeling approaches. While latent Dirichlet allocation (LDA) can be applied, it\nis not optimal due to its weakness in handling short texts with fast-changing\ntopics and scalability concerns. In this paper, we propose a transfer learning\napproach that utilizes abundant labeled documents from other domains (such as\nYahoo! News or Wikipedia) to improve topic modeling, with better model fitting\nand result interpretation. Specifically, we develop Transfer Hierarchical LDA\n(thLDA) model, which incorporates the label information from other domains via\ninformative priors. In addition, we develop a parallel implementation of our\nmodel for large-scale applications. We demonstrate the effectiveness of our\nthLDA model on both a microblogging dataset and standard text collections\nincluding AP and RCV1 datasets.\n\n**Published:** 2013-01-24T02:02:13Z  (*Updated: 2013-01-26T18:00:19Z*)\n\n\n\n#### 4. Nested Named Entity Recognition with Partially-Observed TreeCRFs\n\n*From Search Query: parallel implementation*\n\n*Yao Fu, Chuanqi Tan, Mosha Chen, Songfang Huang, Fei Huang*\n\n**Abstract:** Named entity recognition (NER) is a well-studied task in natural language\nprocessing. However, the widely-used sequence labeling framework is difficult\nto detect entities with nested structures. In this work, we view nested NER as\nconstituency parsing with partially-observed trees and model it with\npartially-observed TreeCRFs. Specifically, we view all labeled entity spans as\nobserved nodes in a constituency tree, and other spans as latent nodes. With\nthe TreeCRF we achieve a uniform way to jointly model the observed and the\nlatent nodes. To compute the probability of partial trees with partial\nmarginalization, we propose a variant of the Inside algorithm, the\n\\textsc{Masked Inside} algorithm, that supports different inference operations\nfor different nodes (evaluation for the observed, marginalization for the\nlatent, and rejection for nodes incompatible with the observed) with efficient\nparallelized implementation, thus significantly speeding up training and\ninference. Experiments show that our approach achieves the state-of-the-art\n(SOTA) F1 scores on the ACE2004, ACE2005 dataset, and shows comparable\nperformance to SOTA models on the GENIA dataset. Our approach is implemented\nat: \\url{https://github.com/FranxYao/Partially-Observed-TreeCRFs}.\n\n**Published:** 2020-12-15T18:20:36Z  (*Updated: 2020-12-15T18:20:36Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space duality*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n#### 2. VSSD: Vision Mamba with Non-Causal State Space Duality\n\n*From Search Query: state space duality*\n\n*Chang Xu, Mingjia Li, Minjing Dong, Yuheng Shi*\n\n**Abstract:** Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous tokens. Together with the involvement of multi-scan strategies, we show that the scanning results can be integrated to achieve non-causality, which not only improves the performance of SSD in vision tasks but also enhances its efficiency. We conduct extensive experiments on various benchmarks including image classification, detection, and segmentation, where VSSD surpasses existing state-of-the-art SSM-based models. Code and weights are available at \\url{https://github.com/YuHengsss/VSSD}.\n\n**Published:** 2024-07-26\n\n\n\n#### 3. UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation\n\n*From Search Query: hybrid architectures*\n\n*Dimitris Metaxas, Mu Zhou, Yunhe Gao*\n\n**Abstract:** Transformer architecture has emerged to be successful in a number of natural language processing tasks. However, its applications to medical vision remain largely unexplored. In this study, we present UTNet, a simple yet powerful hybrid Transformer architecture that integrates self-attention into a convolutional neural network for enhancing medical image segmentation. UTNet applies self-attention modules in both encoder and decoder for capturing long-range dependency at different scales with minimal overhead. To this end, we propose an efficient self-attention mechanism along with relative position encoding that reduces the complexity of self-attention operation significantly from $O(n^2)$ to approximate $O(n)$. A new self-attention decoder is also proposed to recover fine-grained details from the skipped connections in the encoder. Our approach addresses the dilemma that Transformer requires huge amounts of data to learn vision inductive bias. Our hybrid layer design allows the initialization of Transformer into convolutional networks without a need of pre-training. We have evaluated UTNet on the multi-label, multi-vendor cardiac magnetic resonance imaging cohort. UTNet demonstrates superior segmentation performance and robustness against the state-of-the-art approaches, holding the promise to generalize well on other medical image segmentations.\n\n**Published:** 2021-07-02\n\n\n\n#### 4. Mechanistic Design and Scaling of Hybrid Architectures\n\n*From Search Query: hybrid architectures*\n\n*Stefano Massaroli, Ce Zhang, Christopher R\u00e9, Stefano Ermon, Brian Hie, Taiji Suzuki, Kristian Kersting, Bj\u00f6rn Deiseroth, Pragaash Ponnusamy, Eric Nguyen, Armin W Thomas, Michael Poli*\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Published:** 2024-03-26\n\n\n\n#### 5. PyChain: A Fully Parallelized PyTorch Implementation of LF-MMI for End-to-End ASR\n\n*From Search Query: parallel implementation*\n\n*Yiwen Shao, Sanjeev Khudanpur, Yiming Wang, Daniel Povey*\n\n**Abstract:** We present PyChain, a fully parallelized PyTorch implementation of end-to-end lattice-free maximum mutual information (LF-MMI) training for the so-called \\emph{chain models} in the Kaldi automatic speech recognition (ASR) toolkit. Unlike other PyTorch and Kaldi based ASR toolkits, PyChain is designed to be as flexible and light-weight as possible so that it can be easily plugged into new ASR projects, or other existing PyTorch-based ASR tools, as exemplified respectively by a new project PyChain-example, and Espresso, an existing end-to-end ASR toolkit. PyChain's efficiency and flexibility is demonstrated through such novel features as full GPU training on numerator/denominator graphs, and support for unequal length sequences. Experiments on the WSJ dataset show that with simple neural networks and commonly used machine learning techniques, PyChain can achieve competitive results that are comparable to Kaldi and better than other end-to-end ASR systems.\n\n**Published:** 2020-05-20\n\n\n\n#### 6. The Case for Co-Designing Model Architectures with Hardware\n\n*From Search Query: parallel implementation*\n\n*Dhabaleswar Panda, Hari Subramoni, Aamir Shafi, Junqi Yin, Stas Bekman, Stella Biderman, Deepak Narayanan, Jacob Hatef, Quentin Anthony*\n\n**Abstract:** While GPUs are responsible for training the vast majority of state-of-the-art deep learning models, the implications of their architecture are often overlooked when designing new deep learning (DL) models. As a consequence, modifying a DL model to be more amenable to the target hardware can significantly improve the runtime performance of DL training and inference. In this paper, we provide a set of guidelines for users to maximize the runtime performance of their transformer models. These guidelines have been created by carefully considering the impact of various model hyperparameters controlling model shape on the efficiency of the underlying computation kernels executed on the GPU. We find the throughput of models with efficient model shapes is up to 39\\% higher while preserving accuracy compared to models with a similar number of parameters but with unoptimized shapes.\n\n**Published:** 2024-01-25\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space duality, hybrid architectures, and efficient parallel implementations, here are some key insights and techniques derived from the provided sources and analysis:\n\n## State Space Duality (SSD)\n\n- State Space Duality is a concept introduced in the context of State Space Models (SSMs) that simplifies the matrix \\( A \\) into a scalar, enhancing the efficiency and scalability of SSMs. This framework allows for designing architectures that are 2-8X faster than traditional SSMs.\n- SSD enables the model to selectively propagate or forget information along the sequence length dimension by making SSM parameters input-dependent. This addresses the weakness of traditional SSMs in performing content-based reasoning.\n\n## Hybrid Architectures\n\n- Combining SSMs with efficient attention mechanisms shows significant promise. For example, the Taipan architecture integrates Mamba (a variant of SSMs) with Selective Attention Layers (SALs) to handle long-range dependencies more effectively. This hybrid approach outperforms Mamba-2 in terms of latency and performance, especially for long sequences.\n- Another example is SC-MAMBA2, which integrates SSMs with a bidirectional modeling approach tailored for gene sequences. This model demonstrates superior performance in handling ultra-long sequences and capturing intricate biological variations.\n\n## Efficient Parallel Implementations\n\n- Mamba and its variants utilize hardware-aware parallel computing algorithms to ensure efficient training. These algorithms, such as those based on associative scans, maintain linear computational complexity despite the increased complexity of input-dependent SSM parameters.\n- The use of shared linear projection layers, as in BiMamba-S, can make the model deeper with almost the same parameter count, enhancing efficiency and parallelizability.\n\n## Memory Optimization Techniques\n\n- To handle long sequences efficiently, memory optimization is crucial. Techniques such as sparse storage formats, optimized memory access patterns, and divide-and-conquer approaches can reduce memory overhead. For instance, the Taipan architecture demonstrates constant memory usage during inference, a significant advantage over Transformer-based models.\n- Efficient parallel processing strategies, such as those implemented in SC-MAMBA2, allow for the modeling of extensive sequences without significant memory constraints.\n\n## Combining SSMs with Gating Mechanisms\n\n- The BiMamba-S model illustrates how to combine SSMs with gating mechanisms. It uses a novel hardware-aware parallel computing algorithm and shared linear projection layers to ensure efficient training. The model applies a residual connection on a linear projection of the candidate output and input sequence, which helps in maintaining efficiency and scalability.\n- The Taipan architecture also combines SSMs with selective attention layers, which act as a gating mechanism to focus on important tokens and handle long-range dependencies effectively.\n\n## Theoretical Foundations and Future Directions\n\n- The theoretical foundations of state space duality provide a framework for understanding how SSMs can be refined and combined with other mechanisms. This framework allows for the design of new architectures that leverage the strengths of both SSMs and attention mechanisms.\n- Future research should focus on specific techniques for combining SSMs with gating mechanisms, methods for efficient parallel implementation of selective state updates, and approaches for optimizing memory access in hybrid architectures. Investigating the impact of state space duality on these aspects will be crucial for improving autoregressive language model design[Analysis Note].\n\nBy integrating these insights, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage the strengths of state space models, hybrid architectures, and optimized parallel implementations.\n",
            "analysis": "Analysis Note: Efficient Long-Range Dependency Modeling\n\n1. Summary of Analysis:\nThe search results reveal several promising directions for enhancing the GatedTTTLinear architecture:\n\na) Selective State Space Models (SSMs) offer significant advantages for efficient long sequence processing:\n- Linear time complexity with sequence length\n- Hardware-aware implementations for parallel processing\n- Ability to selectively focus on important tokens\n\nb) Memory optimization techniques are crucial for handling long sequences:\n- Sparse storage formats\n- Optimized memory access patterns\n- Efficient parallel processing strategies\n\n2. Useful References with Excerpts:\n\nFrom Mamba paper (Dao & Gu, 2024):\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension.\"\n\nFrom CHELA paper (Liu et al., 2024):\n\"Implements linear attention in a divide-and-conquer manner [...] enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nFrom Transformers are SSMs (Dao & Gu, 2024):\n\"The state space duality (SSD) framework allows us to design a new architecture whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster.\"\n\nFrom Gated Linear Attention paper (Yang et al., 2023):\n\"FLASHLINEARATTENTION is faster than FLASHATTENTION-2 as a standalone layer even on short sequence lengths [...] especially effective at length generalization.\"\n\n3. Key Insights and Analysis:\n\na) Selective State Processing:\n- Making SSM parameters input-dependent enables content-based reasoning\n- Hardware-aware parallel algorithms can maintain efficiency despite increased complexity\n- Selective propagation allows focusing computational resources on important tokens\n\nb) Memory Efficiency:\n- Divide-and-conquer approaches can reduce memory overhead\n- Sparse storage formats optimize memory usage\n- Careful memory access patterns crucial for performance\n\nc) Hybrid Architectures:\n- Combining SSMs with efficient attention mechanisms shows promise\n- Short-long convolutions offer efficient alternative to traditional attention\n- State space duality framework enables theoretical understanding of relationships\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for combining SSMs with gating mechanisms\n2. Methods for efficient parallel implementation of selective state updates\n3. Approaches for optimizing memory access in hybrid architectures\n4. Theoretical foundations of state space duality\n\n5. References:\n\n[1] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" International Conference on Machine Learning.\n\n[2] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\" International Conference on Machine Learning.\n\n[3] Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\" International Conference on Machine Learning.\n\n[4] Wang, J., et al. (2024). \"MambaByte: Token-free Selective State Space Model.\" arXiv preprint.\n\n[5] Ren, L., et al. (2024). \"Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "structured matrices, efficient parallel processing, memory optimization",
            "detail": "Analyze implementation techniques for structured matrices and efficient parallel processing in state space models, focusing on memory optimization strategies and hardware-efficient implementations.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementation techniques for structured matrices and efficient parallel processing in state space models, focusing on memory optimization strategies and hardware-efficient implementations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 7/86 (Score: 1.00)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n##### *Relevant Chunk: No. 29/86 (Score: 1.00)*\n\n```\nChung et al. 2014) as well as more modern variants such as the QRNN (Bradbury et al. 2016), SRU (Lei 2021; Lei et al. 2017), RWKV (B. Peng, Alcaide, et al. 2023), HGRN (Qin, Yang, and Zhong 2023), and Griffin (Botev et al. 2024; De et al. 2024). These RNNs differ in their parameterizations in various ways, most importantly in the lack of a state expansion. Dimensionality and State Expansion. An important characteristic of SSD, shared by previous SSMs in its lineage (S4, H3, Mamba), is that it is a single-input single-output (SISO) system where input channels are processed independently. This leads to a much larger effective state size of ND where $N$ is the SSM state size (also called state expansion factor) and $D$ is the standard model dimension. Traditional RNNs either have $N=1$ or are multi-input multi-output (MIMO) with dense $B, C$ matrices, either of which leads to a smaller state. While MIMO SSMs have been shown to work well in some domains (Lu et al. 2023; Orvieto et al. 2023; J. T. Smith, Warrington, and Linderman 2023), Mamba showed that state expansion is crucial for information-dense domains such as language. One of the main advantages of SSD is allowing for even larger state expansion factors without slowing down the model. Many subsequent works have since adopted state expansion (Section 10.4). Structure. Compared to previous structured SSMs, the main restriction of SSD is on the expressivity of the state transitions $A_{t}$. We note that more general SSMs, such as the case of diagonal $A_{t}$, have the same theoretical efficiency as SSD, but are less hardware-friendly. This is because the dual quadratic form loses its attention-like interpretation and becomes more difficult to compute. Thus compared to Mamba, SSD differs only in a slightly more restrictive form of diagonal $A_{t}$, and trades off this expressivity for improved hardware efficiency (and ease of implementation). We hypothesize that it may be possible to refine our structured matrix algorithms to improve to the general diagonal SSM case as well. ### 10.2 Structured Matrices\n\nThe first viewpoint of the state space duality adopts the viewpoint of these models as matrix sequence transformations or \"matrix mixers\": sequence transformations (Definition 2.1) that can be represented as matrix multiplication (by a $\\mathrm{T} \\times \\mathrm{T}$ matrix) along the sequence dimension $T$. Several such matrix mixers have been proposed before, where the primary axis of variation is the representation of the matrix. These include MLP-Mixer (Tolstikhin et al. 2021) (unstructured matrix), FNet (Lee-Thorp et al. 2021) (Fourier Transform matrix), M2 (Dao, B. Chen, et al. 2022; Dao, Gu, et al. 2019; Dao, Sohoni, et al. 2020; D. Fu et al. 2024) (butterfly/monarch matrix), Toeplitz matrices (Poli et al. 2023; Qin, Han, Weixuan Sun, B. He, et al. 2023), and even more exotic structures (De Sa et al. 2018; Thomas et al. 2018). An important characterization is that efficient (sub-quadratic) matrix sequence transformations are exactly those which have structured matrix mixers. A core result of the SSD framework is viewing SSMs as matrix mixers with a particular structure - semiseparable matrices (Section 3). The linear vs. quadratic duality then takes the form of structured matrix multiplication vs. naive matrix multiplication. The structure matrix representation led to our efficient SSD algorithm through block decompositions of particular semiseparable matrices (Section 6). We note that semiseparable matrices are well-studied in the scientific computing literature, and incorporating those ideas may be a promising avenue for more improvements to state space models. We also suggest that focusing on the matrix mixer viewpoint can lead to more fruitful directions for sequence models, such as designing principled non-causal variants of Mamba, or finding ways to characterize and bridge the gap between softmax attention and sub-quadratic models through analyzing their matrix transformation structure. ## 10.3 (Linear) Attention\n\nCompared to standard (causal) attention, SSD has only two main differences. First, SSD does not use the softmax activation of standard attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017), which is what gives attention its quadratic complexity. When the softmax is dropped, the sequence can be computed with linear scaling through the linear attention framework (Katharopoulos et al. 2020). Second, SSD multiplies the logits matrix by an input-dependent 1-semiseparable mask. Thus this mask can be viewed as replacing the softmax in standard attention. This semiseparable mask can also be viewed as providing positional information. The elements $a_{t}$ act as \"gates\" in the RNN sense, or a \"selection\" mechanism (see discussion in Mamba paper), and their cumulative products $a_{j: i}$ control how much interaction is allowed between positions $i$ and $j$.\n```\n\n##### *Relevant Chunk: No. 9/86 (Score: 0.96)*\n\n```\nIndexing. We use Python-style indexing, e.g. $i: j$ refers to the range $(i, i+1, \\ldots, j-1)$ when $i<j$ and $(i, i-1, \\ldots, j+1)$ when $i>j$. For example, for any symbol $v$ we let $v_{j: i}$ for $j \\geq i$ denote the sequence $\\left(v_{j}, \\ldots, v_{i+1}\\right)$. [i] is equivalent to $0: i=(0, \\ldots, i-1)$. For shorthand, we also let $v_{j: i}^{\\times}$denote the product $v_{j} \\times \\cdots \\times v_{i+1} .{ }^{3}$\n\nDimensions. To distinguish from matrices and tensors, we often use capital letters in typewriter fonts (e.g. $D, N, T$ ) to denote dimensions and tensor shapes. Instead of the traditional notation $M \\in \\mathbb{R}^{T \\times T}$ we frequently use $M \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{T})}$ to reflect tensor shapes in code. Tensor Contractions. We will heavily rely on tensor contraction or einsum notation both for clarity and as a central tool in stating and proving our results. We assume the reader to be familiar with this notation, which is commonly used\n\n[^1]in modern tensor libraries such as numpy. For example, we can use contract(MN, NK $\\rightarrow \\mathrm{MK}$ ) to denote the matrix-matrix multiplication operator, and in our notation contract $(\\mathrm{MN}, \\mathrm{NK} \\rightarrow \\mathrm{MK})(X, Y)$ (which is equivalent to $X \\cdot Y$ ) can be translated to code as numpy.einsum('mn, $\\mathrm{nk} \\rightarrow \\mathrm{mk}^{\\prime}, \\mathrm{X}, \\mathrm{Y}$ ). A large glossary of notation is included in Appendix A. ## 3 State Space Models are Structured Matrices\n\nThis section explores different perspectives of the state space model as a sequence transformation, and outlines properties and algorithms of such maps. The main results of this section are about the equivalence between state space models and a family of structured matrices called semiseparable matrices, which imply new efficiency results (Theorems 3.5 and 3.7). ### 3.1 The Matrix Transformation Form of State Space Models\n\nRecall that our definition of an SSM is defined as a parameterized map defined through (2). Our theoretical framework starts by simply writing this transformation as a matrix multiplication mapping the vectors $x \\in \\mathbb{R}^{\\top} \\mapsto y \\in \\mathbb{R}^{\\top}$. By definition, $h_{0}=B_{0} x_{0}$. By induction,\n\n$$\n\\begin{aligned}\nh_{t} & =A_{t} \\ldots A_{1} B_{0} x_{0}+A_{t} \\ldots A_{2} B_{1} x_{1}+\\cdots+A_{t} A_{t-1} B_{t-2} x_{t-2}+A_{t} B_{t-1} x_{t-1}+B_{t} x_{t} \\\\\n& =\\sum_{s=0}^{t} A_{t: s}^{\\times} B_{s} x_{s} . \\end{aligned}\n$$\n\nMultiplying by $C_{t}$ to produce $y_{t}$ and vectorizing the equation over $t \\in[\\mathrm{T}]$, we derive the matrix transformation form of SSMs. $$\n\\begin{aligned}\ny_{t} & =\\sum_{s=0}^{t} C_{t}^{\\top} A_{t: s}^{\\times} B_{s} x_{s} \\\\\ny & =\\operatorname{SSM}(A, B, C)(x)=M x \\\\\nM_{j i} & :=C_{j}^{\\top} A_{j} \\cdots A_{i+1} B_{i}\n\\end{aligned}\n$$\n\n### 3.2 Semiseparable Matrices\n\n$M$ in equation (3) is a particular representation of a class of matrices known as semiseparable matrices. Semiseparable matrices are a fundamental matrix structure. We first define these matrices and their properties. Definition 3.1. A (lower triangular) matrix $M$ is N -semiseparable if every submatrix contained in the lower triangular portion (i.e. on or below the diagonal) has rank at most N . We call N the order or rank of the semiseparable matrix. Definition 3.1, and other forms of related \"separable\" structure (e.g. quasiseparable matrices and other definitions of semiseparable matrices) are sometimes called structured rank matrices (or rank-structured matrices) because they are characterized by rank conditions on their submatrices. Semiseparable matrices have many structured representations including the hierarchical semiseparable (HSS), sequential semiseparable (SSS), and Bruhat forms (Pernet and Storjohann 2018). We will primarily use the SSS form. ### 3.2.1 The Sequentially Semiseparable (SSS) Representation\n\nDefinition 3.2. A lower triangular matrix $M \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{T})}$ has a N -sequentially semiseparable (SSS) representation if it can be written in the form\n\n$$\nM_{j i}=C_{j}^{\\top} A_{j} \\cdots A_{i+1} B_{i}\n$$\n\nfor vectors $B_{0}, \\ldots, B_{\\mathrm{T}-1}, C_{0}, \\ldots, C_{\\mathrm{T}-1} \\in \\mathbb{R}^{\\mathbb{N}}$ and matrices $A_{0}, \\ldots, A_{\\mathrm{T}-1} \\in \\mathbb{R}^{(\\mathrm{N}, \\mathrm{N})}$. We define the operator $\\operatorname{SSS}$ so that $M=\\operatorname{SSS}\\left(A_{0: \\mathrm{T}}, B_{0: \\mathrm{T}}, C_{0: \\mathrm{T}}\\right)$. A fundamental result of semiseparable matrices is that they are exactly equivalent to matrices with SSS representations. One direction can be deduced with a simple constructive proof. Lemma 3.3. An N -SSS matrix $M$ with representation (4) is N -semiseparable. Proof. Consider any off-diagonal block $M_{j: j^{\\prime}, i^{\\prime}: i}$ where $j^{\\prime}>j \\geq i>i^{\\prime}$. This has an explicit rank-N factorization as\n\n$$\n\\left[\\begin{array}{ccc}\nC_{j}^{\\top} A_{j: i^{\\prime}}^{\\times} B_{i^{\\prime}} & \\cdots & C_{j}^{\\top} A_{j: i-1}^{\\times} B_{i-1} \\\\\n\\vdots & & \\vdots \\\\\nC_{j^{\\prime}-1}^{\\top} A_{j^{\\prime}-1: i^{\\prime}}^{\\times} B_{i^{\\prime}} & \\cdots & C_{j^{\\prime}-1}^{\\top} A_{j^{\\prime}-1: i-1}^{\\times} B_{i-1}\n\\end{array}\\right]=\\left[\\begin{array}{c}\nC_{j}^{\\top} A_{j: j}^{\\times} \\\\\n\\vdots \\\\\nC_{j^{\\prime}-1}^{\\top} A_{j^{\\prime}-1: j}^{\\times}\n\\end{array}\\right] A_{j: i-1}^{\\times}\\left[\\begin{array}{lll}\nA_{i-1: i^{\\prime}}^{\\times} B_{i^{\\prime}} & \\cdots & A_{i-1: i-1}^{\\times} B_{i-1}\n\\end{array}\\right]\n$$\n\nEquation (5) will be used extensively in deriving our fast algorithms for sequence models.\n```\n\n#### 2. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.97)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 14/53 (Score: 0.97)*\n\n```\nAdvances in Neural Information Processing Systems, 33: $1474-1487,2020 \\mathrm{a}$. Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800-3809. PMLR, 2020b. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers, 2020. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): $1735-1780,1997$. Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pp. 87-104, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808-16820, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009. Richard Ladner and Michael Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27 (4):831-838, 1980. Sivaramakrishnan Lakshmivarahan and Sudarshan Dhall. Parallel computing using the prefix problem.\n```\n\n#### 3. Efficiently Modeling Long Sequences with Structured State Spaces (Avg. Score: 0.96)\n\n*Albert Gu, Karan Goel, Christopher R'e*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 744  (*Influential: 119*)\n\n**TL;DR:** The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.\n\n**Abstract:** A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.\n\n##### *Relevant Chunk: No. 1/45 (Score: 0.96)*\n\n```\n# Efficiently Modeling Long Sequences with Structured State Spaces \n\nAlbert Gu, Karan Goel, and Christopher R\u00e9<br>Department of Computer Science, Stanford University<br>\\{albertgu,krng\\}@stanford.edu, chrismre@cs.stanford.edu\n\n\n#### Abstract\n\nA central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of 10000 or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) $x^{\\prime}(t)=A x(t)+$ $B u(t), y(t)=C x(t)+D u(t)$, and showed that for appropriate choices of the state matrix $A$, this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning $A$ with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) $91 \\%$ accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60 \\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16 k that all prior work fails on, while being as efficient as all competitors ${ }^{1}$\n\n\n## 1 Introduction\n\nA central problem in sequence modeling is efficiently handling data that contains long-range dependencies (LRDs). Real-world time-series data often requires reasoning over tens of thousands of time steps, while few sequence models address even thousands of time steps. For instance, results from the long-range arena (LRA) benchmark 40] highlight that sequence models today perform poorly on LRD tasks, including one (Path-X) where no model performs better than random guessing. Since LRDs are perhaps the foremost challenge for sequence models, all standard model families such as continuous-time models (CTMs), RNNs, CNNs, and Transformers include many specialized variants designed to address them. Modern examples include orthogonal and Lipschitz RNNs [1, 13] to combat vanishing gradients, dilated convolutions to increase context size [3, 28, and an increasingly vast family of efficient Transformers that reduce the quadratic dependence on sequence length [8, 22]. Despite being designed for LRDs, these solutions still perform poorly on challenging benchmarks such as LRA [40] or raw audio classification 18 . An alternative approach to LRDs was recently introduced based on the state space model (SSM) (Fig. 11). SSMs are a foundational scientific model used in fields such as control theory, computational neuroscience, and many more, but have not been applicable to deep learning for concrete theoretical reasons. In particular, Gu et al. 18 showed that deep SSMs actually struggle even on simple tasks, but can perform exceptionally\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_17364ec6185c9f7478c6g-02.jpg?height=523&width=1622&top_left_y=243&top_left_x=249)\n\nFigure 1: (Left) State Space Models (SSM) parameterized by matrices $\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{D}$ map an input signal $u(t)$ to output $y(t)$ through a latent state $x(t)$. (Center) Recent theory on continuous-time memorization derives special $\\boldsymbol{A}$ matrices that allow SSMs to capture LRDs mathematically and empirically. (Right) SSMs can be computed either as a recurrence (left) or convolution (right). However, materializing these conceptual views requires utilizing different representations of its parameters (red, blue, green) which are very expensive to compute. S4 introduces a novel parameterization that efficiently swaps between these representations, allowing it to handle a wide range of tasks, be efficient at both training and inference, and excel at long sequences. well when equipped with special state matrices $\\boldsymbol{A}$ recently derived to solve a problem of continuous-time memorization [16, 45]. Their Linear State Space Layer (LSSL) conceptually unifies the strengths of CTM, RNN and CNN models, and provides a proof of concept that deep SSMs can address LRDs in principle. Unfortunately, the LSSL is infeasible to use in practice because of prohibitive computation and memory requirements induced by the state representation. For state dimension $N$ and sequence length $L$, computing the latent state requires $O\\left(N^{2} L\\right)$ operations and $O(N L)$ space - compared to a $\\Omega(L+N)$ lower bound for both. Thus for reasonably sized models (e.g. $N=256$ in Gu et al. [18]), the LSSL uses orders of magnitude more memory than comparably-sized RNNs or CNNs. Although theoretically efficient algorithms for the LSSL were proposed, we show that these are numerically unstable. In particular, the special $\\boldsymbol{A}$ matrix is highly non-normal in the linear algebraic sense, which prevents the application of conventional algorithmic techniques. Consequently, although the LSSL showed that SSMs have strong performance, they are currently computationally impractical as a general sequence modeling solution. In this work, we introduce the Structured State Space (S4) sequence model based on the SSM that solves the critical computational bottleneck in previous work. Technically, S4 reparameterizes the structured state matrices $\\boldsymbol{A}$ appearing in Gu et al. [16, Voelker et al. 45] by decomposing them as the sum of a low-rank and normal term. Additionally, instead of expanding the standard SSM in coefficient space, we compute its truncated generating function in frequency space, which can be simplified into a multipole-like evaluation. Combining these two ideas, we show that the low-rank term can be corrected by the Woodbury identity while the normal term can be diagonalized stably, ultimately reducing to a well-studied and theoretically stable Cauchy kernel [29, 30]. This results in $\\tilde{O}(N+L)$ computation and $O(N+L)$ memory usage, which is essentially tight for sequence models. Compared to the LSSL, S4 is up to $30 \\times$ faster with $400 \\times$ less memory usage, while exceeding the LSSL's performance empirically. Empirically, S4 significantly advances the state-of-the-art for LRD. On the LRA benchmark for efficient sequence models, S 4 is as fast as all baselines while outperforming them by $20+$ points on average. S 4 is the first model to solve the difficult LRA Path-X task (length-16384), achieving $\\mathbf{8 8 \\%}$ accuracy compared to $\\mathbf{5 0 \\%}$ random guessing for all prior work. On speech classification with length- 16000 sequences, S 4 halves the test error ( $1.7 \\%$ ) of specialized Speech CNNs - by contrast, all RNN and Transformer baselines fail to learn ( $\\geq 70 \\%$ error). Towards a general-purpose sequence model. Beyond LRD, a broad goal of machine learning is to develop a single model that can be used across a wide range of problems. Models today are typically\nspecialized to solve problems from a particular domain (e.g. images, audio, text, time-series), and enable a narrow range of capabilities (e.g. efficient training, fast generation, handling irregularly sampled data). This specialization is typically expressed via domain-specific preprocessing, inductive biases, and architectures. Sequence models provide a general framework for solving many of these problems with reduced specialization - e.g. Vision Transformers for image classification with less 2D information 12. However, most models such as Transformers generally still require substantial specialization per task to achieve high performance. Deep SSMs in particular have conceptual strengths that suggest they may be promising as a general sequence modeling solution. These strengths include a principled approach to handling LRDs, as well as the ability to move between continuous-time, convolutional, and recurrent model representations, each with distinct capabilities (Fig. 11). Our technical contributions enable SSMs to be applied successfully to a varied set of benchmarks with minimal modification:\n\n- Large-scale generative modeling. On CIFAR-10 density estimation, S4 is competitive with the best autoregressive models ( 2.85 bits per dim). On WikiText-103 language modeling, S 4 substantially closes the gap to Transformers (within 0.8 perplexity), setting SoTA for attention-free models. - Fast autoregressive generation. Like RNNs, S4 can use its latent state to perform $60 \\times$ faster pixel/token generation than standard autoregressive models on CIFAR-10 and WikiText-103. - Sampling resolution change. Like specialized CTMs, S4 can adapt to changes in time-series sampling frequency without retraining, e.g. at $0.5 \\times$ frequency on speech classification. - Learning with weaker inductive biases. With no architectural changes, S4 surpasses Speech CNNs on speech classification, outperforms the specialized Informer model on time-series forecasting problems, and matches a 2-D ResNet on sequential CIFAR with over $90 \\%$ accuracy. ## 2 Background: State Spaces\n\nSections 2.1 to 2.4 describe the four properties of SSMs in Fig. 1. the classic continuous-time representation, addressing LRDs with the HiPPO framework, the discrete-time recurrent representation, and the parallelizable convolution representation. In particular, Section 2.4 introduces the SSM convolution kernel $\\overline{\\boldsymbol{K}}$, which is the focus of our theoretical contributions in Section 3\n\n### 2.1 State Space Models: A Continuous-time Latent State Model\n\nThe state space model is defined by the simple equation (1). It maps a 1-D input signal $u(t)$ to an $N$-D latent state $x(t)$ before projecting to a 1-D output signal $y(t)$. $$\n\\begin{aligned}\nx^{\\prime}(t) & =\\boldsymbol{A} x(t)+\\boldsymbol{B} u(t) \\\\\ny(t) & =\\boldsymbol{C} x(t)+\\boldsymbol{D} u(t)\n\\end{aligned}\n$$\n\nSSMs are broadly used in many scientific disciplines and related to latent state models such as Hidden Markov Models (HMM). Our goal is to simply use the SSM as a black-box representation in a deep sequence model, where $\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C}, \\boldsymbol{D}$ are parameters learned by gradient descent. For the remainder of this paper, we will omit the parameter $\\boldsymbol{D}$ for exposition (or equivalently, assume $\\boldsymbol{D}=0$ ) because the term $\\boldsymbol{D} u$ can be viewed as a skip connection and is easy to compute. ### 2.2 Addressing Long-Range Dependencies with HiPPO\n\nPrior work found that the basic SSM (1) actually performs very poorly in practice. Intuitively, one explanation is that linear first-order ODEs solve to an exponential function, and thus may suffer from gradients scaling exponentially in the sequence length (i.e., the vanishing/exploding gradients problem 32]). To address this\nproblem, the LSSL leveraged the HiPPO theory of continuous-time memorization [16. HiPPO specifies a class of certain matrices $\\boldsymbol{A} \\in \\mathbb{R}^{N \\times N}$ that when incorporated into (1), allows the state $x(t)$ to memorize the history of the input $u(t)$. The most important matrix in this class is defined by equation (2), which we will call the HiPPO matrix. For example, the LSSL found that simply modifying an SSM from a random matrix $\\boldsymbol{A}$ to equation $\\sqrt[2]{2}$ improved its performance on the sequential MNIST benchmark from $60 \\%$ to $98 \\%$. $$\n(\\text { HiPPO Matrix }) \\quad \\boldsymbol{A}_{n k}=- \\begin{cases}(2 n+1)^{1 / 2}(2 k+1)^{1 / 2} & \\text { if } n>k \\\\ n+1 & \\text { if } n=k \\\\ 0 & \\text { if } n<k\\end{cases}\n$$\n\n### 2.3 Discrete-time SSM: The Recurrent Representation\n\nTo be applied on a discrete input sequence $\\left(u_{0}, u_{1}, \\ldots\\right)$ instead of continuous function $u(t)$, 11 must be discretized by a step size $\\Delta$ that represents the resolution of the input. Conceptually, the inputs $u_{k}$ can be viewed as sampling an implicit underlying continuous signal $u(t)$, where $u_{k}=u(k \\Delta)$. To discretize the continuous-time SSM, we follow prior work in using the bilinear method 43, which converts the state matrix $\\boldsymbol{A}$ into an approximation $\\overline{\\boldsymbol{A}}$. The discrete SSM is\n\n$$\n\\begin{array}{lll}\nx_{k}=\\overline{\\boldsymbol{A}} x_{k-1}+\\overline{\\boldsymbol{B}} u_{k} & \\overline{\\boldsymbol{A}}=(\\boldsymbol{I}-\\Delta / 2 \\cdot \\boldsymbol{A})^{-1}(\\boldsymbol{I}+\\Delta / 2 \\cdot \\boldsymbol{A}) & \\\\\ny_{k}=\\overline{\\boldsymbol{C}} x_{k} & \\overline{\\boldsymbol{B}}=(\\boldsymbol{I}-\\Delta / 2 \\cdot \\boldsymbol{A})^{-1} \\Delta \\boldsymbol{B} & \\overline{\\boldsymbol{C}}=\\boldsymbol{C}\n\\end{array}\n$$\n\nEquation (3) is now a sequence-to-sequence map $u_{k} \\mapsto y_{k}$ instead of function-to-function. Moreover the state equation is now a recurrence in $x_{k}$, allowing the discrete SSM to be computed like an RNN. Concretely, $x_{k} \\in \\mathbb{R}^{N}$ can be viewed as a hidden state with transition matrix $\\overline{\\boldsymbol{A}}$. Notationally, throughout this paper we use $\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\ldots$ to denote discretized SSM matrices defined by (3). Note that these matrices are a function of both $\\boldsymbol{A}$ as well as a step size $\\Delta$; we suppress this dependence for notational convenience when it is clear. ### 2.4 Training SSMs: The Convolutional Representation\n\nThe recurrent SSM (3) is not practical for training on modern hardware due to its sequentiality. Instead, there is a well-known connection between linear time-invariant (LTI) SSMs such as (1) and continuous convolutions. Correspondingly, (3) can actually be written as a discrete convolution. For simplicity let the initial state be $x_{-1}=0$. Then unrolling (3) explicitly yields\n\n$$\n\\begin{array}{lll}\nx_{0}=\\overline{\\boldsymbol{B}} u_{0} & x_{1}=\\overline{\\boldsymbol{A B}} u_{0}+\\overline{\\boldsymbol{B}} u_{1} & x_{2}=\\overline{\\boldsymbol{A}}^{2} \\overline{\\boldsymbol{B}} u_{0}+\\overline{\\boldsymbol{A B}} u_{1}+\\overline{\\boldsymbol{B}} u_{2} \\\\\ny_{0}=\\overline{\\boldsymbol{C} \\boldsymbol{B}} u_{0} & y_{1}=\\overline{\\boldsymbol{C} \\boldsymbol{A} \\boldsymbol{B}} u_{0}+\\overline{\\boldsymbol{C} \\boldsymbol{B}} u_{1} & y_{2}=\\overline{\\boldsymbol{C A}}^{2} \\overline{\\boldsymbol{B}} u_{0}+\\overline{\\boldsymbol{C} \\boldsymbol{A} \\boldsymbol{B}} u_{1}+\\overline{\\boldsymbol{C} \\boldsymbol{B}} u_{2}\n\\end{array}\n$$\n\nThis can be vectorized into a convolution (4) with an explicit formula for the convolution kernel (5). $$\n\\begin{aligned}\ny_{k} & =\\overline{\\boldsymbol{C A}}^{k} \\overline{\\boldsymbol{B}} u_{0}+\\overline{\\boldsymbol{C A}}^{k-1} \\overline{\\boldsymbol{B}} u_{1}+\\cdots+\\overline{\\boldsymbol{C A}} \\overline{\\boldsymbol{B}} u_{k-1}+\\overline{\\boldsymbol{C B}} u_{k} \\\\\ny & =\\overline{\\boldsymbol{K}} * u \\\\\n\\overline{\\boldsymbol{K}} \\in \\mathbb{R}^{L}: & =\\mathcal{K}_{L}(\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\overline{\\boldsymbol{C}}):=\\left(\\overline{\\boldsymbol{C A}}^{i} \\overline{\\boldsymbol{B}}\\right)_{i \\in[L]}=\\left(\\overline{\\boldsymbol{C} \\boldsymbol{B}}, \\overline{\\boldsymbol{C A} \\boldsymbol{B}}, \\ldots, \\overline{\\boldsymbol{C A}}^{L-1} \\overline{\\boldsymbol{B}}\\right)\n\\end{aligned}\n$$\n\nIn other words, equation (4) is a single (non-circular) convolution and can be computed very efficiently with FFTs, provided that $\\overline{\\boldsymbol{K}}$ is known. However, computing $\\overline{\\boldsymbol{K}}$ in (5) is non-trivial and is the focus of our technical contributions in Section 3. We call $\\overline{\\boldsymbol{K}}$ the SSM convolution kernel or filter. ## 3 Method: Structured State Spaces (S4)\n\nOur technical results focus on developing the S4 parameterization and showing how to efficiently compute all views of the SSM (Section 2): the continuous representation $(\\boldsymbol{A}, \\boldsymbol{B}, \\boldsymbol{C})$ (1), the recurrent representation $(\\overline{\\boldsymbol{A}}, \\overline{\\boldsymbol{B}}, \\overline{\\boldsymbol{C}})(3)$, and the convolutional representation $\\overline{\\boldsymbol{K}}(4)$. Section 3.1 motivates our approach, which is based on the linear algebraic concepts of conjugation and diagonalization, and discusses why the naive application of this approach does not work. Section 3.2 gives an overview of the key technical components of our approach and formally defines the S4 parameterization. Section 3.3 sketches the main results, showing that S 4 is asymptotically efficient (up to log factors) for sequence models. Proofs are in Appendices B and C. ### 3.1 Motivation: Diagonalization\n\nThe fundamental bottleneck in computing the discrete-time SSM (3) is that it involves repeated matrix multiplication by $\\overline{\\boldsymbol{A}}$. For example, computing (5) naively as in the LSSL involves $L$ successive multiplications by $\\overline{\\boldsymbol{A}}$, requiring $O\\left(N^{2} L\\right)$ operations and $O(N L)$ space.\n```\n\n\n\n---\n## Found 18 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: structured matrices, efficient parallel processing, memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n\n*From Search Query: structured matrices*\n\n*Changwoo Lee, Hun-Seok Kim*\n\n**TL;DR:** This paper proposes a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent based on the Gaussian-Dirichlet kernel, and defines a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters.\n\n**Abstract:** This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 2. Compute Better Spent: Replacing Dense Layers with Structured Matrices\n\n*From Search Query: structured matrices*\n\n*Shikai Qiu, Andres Potapczynski, Marc Finzi, Micah Goldblum, Andrew Gordon Wilson*\n\n**TL;DR:** This work systematically explore structured matrices as replacements for dense matrices, and proposes a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which is shown to be better than dense matrices for the same compute on multiple tasks.\n\n**Abstract:** Dense linear layers are the dominant computational bottleneck in foundation models. Identifying more efficient alternatives to dense matrices has enormous potential for building more compute-efficient models, as exemplified by the success of convolutional networks in the image domain. In this work, we systematically explore structured matrices as replacements for dense matrices. We show that different structures often require drastically different initialization scales and learning rates, which are crucial to performance, especially as models scale. Using insights from the Maximal Update Parameterization, we determine the optimal scaling for initialization and learning rates of these unconventional layers. Finally, we measure the scaling laws of different structures to compare how quickly their performance improves with compute. We propose a novel matrix family containing Monarch matrices, the Block Tensor-Train (BTT), which we show performs better than dense matrices for the same compute on multiple tasks. On CIFAR-10/100 with augmentation, BTT achieves exponentially lower training loss than dense when training MLPs and ViTs. BTT matches dense ViT-S/32 performance on ImageNet-1k with 3.8 times less compute and is more efficient than dense for training small GPT-2 language models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. E-MAPP: Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance\n\n*From Search Query: efficient parallel processing*\n\n*C. Chang, Ni Mu, Jiajun Wu, Ling Pan, Huazhe Xu*\n\n**TL;DR:** This work introduces Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance (E-MAPP), a novel framework that leverages parallel programs to guide multiple agents to efficiently accomplish goals that require planning over $10+$ stages.\n\n**Abstract:** A critical challenge in multi-agent reinforcement learning(MARL) is for multiple agents to efficiently accomplish complex, long-horizon tasks. The agents often have difficulties in cooperating on common goals, dividing complex tasks, and planning through several stages to make progress. We propose to address these challenges by guiding agents with programs designed for parallelization, since programs as a representation contain rich structural and semantic information, and are widely used as abstractions for long-horizon tasks. Specifically, we introduce Efficient Multi-Agent Reinforcement Learning with Parallel Program Guidance(E-MAPP), a novel framework that leverages parallel programs to guide multiple agents to efficiently accomplish goals that require planning over $10+$ stages. E-MAPP integrates the structural information from a parallel program, promotes the cooperative behaviors grounded in program semantics, and improves the time efficiency via a task allocator. We conduct extensive experiments on a series of challenging, long-horizon cooperative tasks in the Overcooked environment. Results show that E-MAPP outperforms strong baselines in terms of the completion rate, time efficiency, and zero-shot generalization ability by a large margin.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models\n\n*From Search Query: efficient parallel processing*\n\n*Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, R. Lee, Lidong Bing, Soujanya Poria*\n\n**TL;DR:** LLM-Adapters is presented, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks, demonstrating that using adapter- based PEFT in smaller-scale LLMs with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs in zero-shot inference on both reasoning tasks.\n\n**Abstract:** The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 144  (*Influential: 15*)\n\n#### 5. A Communication-Efficient Parallel Algorithm for Decision Tree\n\n*From Search Query: efficient parallel processing*\n\n*Qi Meng, Guolin Ke, Taifeng Wang, Wei Chen, Qiwei Ye, Zhiming Ma, Tie-Yan Liu*\n\n**TL;DR:** Experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency.\n\n**Abstract:** Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called \\emph{Parallel Voting Decision Tree (PV-Tree)}, to tackle this challenge. After partitioning the training data onto a number of (e.g., $M$) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-$k$ attributes are selected from each machine according to its local data. Then, the indices of these top attributes are aggregated by a server, and the globally top-$2k$ attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-$2k$ attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the tradeoff between accuracy and efficiency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 110  (*Influential: 7*)\n\n#### 6. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: memory optimization*\n\n*Yuzhong Wang, Xu Han, Weilin Zhao, Guoyang Zeng, Zhiyuan Liu, Maosong Sun*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 7. Memory Optimization for Deep Networks\n\n*From Search Query: memory optimization*\n\n*Aashaka Shah, Chaoxia Wu, Jayashree Mohan, Vijay Chidambaram, Philipp Krahenbuhl*\n\n**TL;DR:** MONeT is an automatic framework that minimizes both the memory footprint and computational overhead of deep networks, and is able to outperform all prior hand-tuned operations as well as automated checkpointing.\n\n**Abstract:** Deep learning is slowly, but steadily, hitting a memory bottleneck. While the tensor computation in top-of-the-line GPUs increased by 32x over the last five years, the total available memory only grew by 2.5x. This prevents researchers from exploring larger architectures, as training large networks requires more memory for storing intermediate outputs. In this paper, we present MONeT, an automatic framework that minimizes both the memory footprint and computational overhead of deep networks. MONeT jointly optimizes the checkpointing schedule and the implementation of various operators. MONeT is able to outperform all prior hand-tuned operations as well as automated checkpointing. MONeT reduces the overall memory requirement by 3x for various PyTorch models, with a 9-16% overhead in computation. For the same computation cost, MONeT requires 1.2-1.8x less memory than current state-of-the-art automated checkpointing frameworks. Our code is available at this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 21  (*Influential: 0*)\n\n### 5 related papers from ArXiv\n\n#### 1. Learning Compact Recurrent Neural Networks\n\n*From Search Query: structured matrices*\n\n*Zhiyun Lu, Vikas Sindhwani, Tara N. Sainath*\n\n**Abstract:** Recurrent neural networks (RNNs), including long short-term memory (LSTM)\nRNNs, have produced state-of-the-art results on a variety of speech recognition\ntasks. However, these models are often too large in size for deployment on\nmobile devices with memory and latency constraints. In this work, we study\nmechanisms for learning compact RNNs and LSTMs via low-rank factorizations and\nparameter sharing schemes. Our goal is to investigate redundancies in recurrent\narchitectures where compression can be admitted without losing performance. A\nhybrid strategy of using structured matrices in the bottom layers and shared\nlow-rank factors on the top layers is found to be particularly effective,\nreducing the parameters of a standard LSTM by 75%, at a small cost of 0.3%\nincrease in WER, on a 2,000-hr English Voice Search task.\n\n**Published:** 2016-04-09T19:09:22Z  (*Updated: 2016-04-09T19:09:22Z*)\n\n\n\n#### 2. Group and Shuffle: Efficient Structured Orthogonal Parametrization\n\n*From Search Query: structured matrices*\n\n*Mikhail Gorbunov, Nikolay Yudin, Vera Soboleva, Aibek Alanov, Alexey Naumov, Maxim Rakhuba*\n\n**Abstract:** The increasing size of neural networks has led to a growing demand for\nmethods of efficient fine-tuning. Recently, an orthogonal fine-tuning paradigm\nwas introduced that uses orthogonal matrices for adapting the weights of a\npretrained model. In this paper, we introduce a new class of structured\nmatrices, which unifies and generalizes structured classes from previous works.\nWe examine properties of this class and build a structured orthogonal\nparametrization upon it. We then use this parametrization to modify the\northogonal fine-tuning framework, improving parameter and computational\nefficiency. We empirically validate our method on different domains, including\nadapting of text-to-image diffusion models and downstream task fine-tuning in\nlanguage modeling. Additionally, we adapt our construction for orthogonal\nconvolutions and conduct experiments with 1-Lipschitz neural networks.\n\n**Published:** 2024-06-14T13:29:36Z  (*Updated: 2024-06-14T13:29:36Z*)\n\n\n\n#### 3. EPS-MoE: Expert Pipeline Scheduler for Cost-Efficient MoE Inference\n\n*From Search Query: efficient parallel processing*\n\n*Yulei Qian, Fengcun Li, Xiangyang Ji, Xiaoyu Zhao, Jianchao Tan, Kefeng Zhang, Xunliang Cai*\n\n**Abstract:** Large Language Model (LLM) has revolutionized the field of artificial\nintelligence, with their capabilities expanding rapidly due to advances in deep\nlearning and increased computational resources. The mixture-of-experts (MoE)\nmodel has emerged as a prominent architecture in the field of LLM, better\nbalancing the model performance and computational efficiency. MoE architecture\nallows for effective scaling and efficient parallel processing, but the GEMM\n(General Matrix Multiply) of MoE and the large parameters introduce challenges\nin terms of computation efficiency and communication overhead, which becomes\nthe throughput bottleneck during inference. Applying a single parallelism\nstrategy like EP, DP, PP, etc. to MoE architecture usually achieves sub-optimal\ninference throughput, the straightforward combinations of existing different\nparallelisms on MoE can not obtain optimal inference throughput yet. This paper\nintroduces EPS-MoE, a novel expert pipeline scheduler for MoE that goes beyond\nthe existing inference parallelism schemes. Our approach focuses on optimizing\nthe computation of MoE FFN (FeedForward Network) modules by dynamically\nselecting the best kernel implementation of GroupGemm and DenseGemm for\ndifferent loads and adaptively overlapping these computations with\n\\textit{all2all} communication, leading to a substantial increase in\nthroughput. Our experimental results demonstrate an average 21% improvement in\nprefill throughput over existing parallel inference methods. Specifically, we\nvalidated our method on DeepSeekV2, a highly optimized model claimed to achieve\na prefill throughput of 100K tokens per second. By applying EPS-MoE, we further\naccelerated it to at least 120K tokens per second.\n\n**Published:** 2024-10-16T05:17:49Z  (*Updated: 2024-10-16T05:17:49Z*)\n\n\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: memory optimization*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 5. L4Q: Parameter Efficient Quantization-Aware Fine-Tuning on Large\n  Language Models\n\n*From Search Query: memory optimization*\n\n*Hyesung Jeon, Yulhwa Kim, Jae-joon Kim*\n\n**Abstract:** Due to the high memory and computational costs associated with large language\nmodels (LLMs), model compression techniques such as quantization, which reduces\ninference costs, and parameter-efficient fine-tuning (PEFT) methods like\nLow-Rank Adaptation (LoRA), which reduce training costs, have gained\nsignificant popularity. This trend has spurred active research into\nquantization-aware PEFT techniques, aimed at maintaining model accuracy while\nminimizing memory overhead during both inference and training. Previous\nquantization-aware PEFT methods typically follow a two-step approach: first,\napplying post-training quantization (PTQ) to model weights, followed by PEFT on\nthe quantized model. However, recovering from the quantization error introduced\nby PTQ through fine-tuning has proven challenging. Additionally, most PTQ-based\nPEFT methods result in a mixture of low-precision quantized weights and\nhigh-precision adapter weights, limiting the efficiency of full quantization\nduring inference. While a previous method attempted to address these issues, it\nstill suffers from limited adaptability due to the constrained LoRA parameter\nstructure required to produce fully-quantized models. To overcome these\nchallenges, we propose L4Q, a method that integrates Quantization-Aware\nTraining (QAT) with LoRA to effectively reduce quantization error. By employing\na memory-optimized layer design, L4Q significantly reduces QAT's memory\noverhead while producing fully-quantized weights, enabling effective adaptation\nto downstream tasks. Our experiments demonstrate that this combined approach to\nquantization and fine-tuning achieves superior accuracy compared to decoupled\nfine-tuning schemes, particularly in sub-4-bit quantization, positioning L4Q as\nan efficient QAT solution. Using the LLaMA model families and instructional\ndatasets, we showcase L4Q's capabilities in language tasks and few-shot\nlearning.\n\n**Published:** 2024-02-07T14:35:05Z  (*Updated: 2024-10-28T04:41:02Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Monarch: Expressive Structured Matrices for Efficient and Accurate Training\n\n*From Search Query: structured matrices*\n\n*Christopher R\u00e9, Atri Rudra, Aniruddh Rao, Alexander Liu, Jessica Grogan, Michael Poli, Arjun Desai, Nimit Sohoni, Beidi Chen, Tri Dao*\n\n**Abstract:** Large neural networks excel in many domains, but they are expensive to train and fine-tune. A popular approach to reduce their compute or memory requirements is to replace dense weight matrices with structured ones (e.g., sparse, low-rank, Fourier transform). These methods have not seen widespread adoption (1) in end-to-end training due to unfavorable efficiency--quality tradeoffs, and (2) in dense-to-sparse fine-tuning due to lack of tractable algorithms to approximate a given dense weight matrix. To address these issues, we propose a class of matrices (Monarch) that is hardware-efficient (they are parameterized as products of two block-diagonal matrices for better hardware utilization) and expressive (they can represent many commonly used transforms). Surprisingly, the problem of approximating a dense weight matrix with a Monarch matrix, though nonconvex, has an analytical optimal solution. These properties of Monarch matrices unlock new ways to train and fine-tune sparse and dense models. We empirically validate that Monarch can achieve favorable accuracy-efficiency tradeoffs in several end-to-end sparse training applications: speeding up ViT and GPT-2 training on ImageNet classification and Wikitext-103 language modeling by 2x with comparable model quality, and reducing the error on PDE solving and MRI reconstruction tasks by 40%. In sparse-to-dense training, with a simple technique called \"reverse sparsification,\" Monarch matrices serve as a useful intermediate representation to speed up GPT-2 pretraining on OpenWebText by 2x without quality drop. The same technique brings 23% faster BERT pretraining than even the very optimized implementation from Nvidia that set the MLPerf 1.1 record. In dense-to-sparse fine-tuning, as a proof-of-concept, our Monarch approximation algorithm speeds up BERT fine-tuning on GLUE by 1.7x with comparable accuracy.\n\n**Published:** 2022-04-01\n\n\n\n#### 2. On Computing the Kronecker Structure of Polynomial and Rational Matrices using Julia\n\n*From Search Query: structured matrices*\n\n*Andreas Varga*\n\n**Abstract:** In this paper we discuss the mathematical background and the computational aspects which underly the implementation of a collection of Julia functions in the MatrixPencils package for the determination of structural properties of polynomial and rational matrices. We primarily focus on the computation of the finite and infinite spectral structures (e.g., eigenvalues, zeros, poles) as well as the left and right singular structures (e.g., Kronecker indices), which play a fundamental role in the structure of the solution of many problems involving polynomial and rational matrices. The basic analysis tool is the determination of the Kronecker structure of linear matrix pencils using numerically reliable algorithms, which is used in conjunction with several linearization techniques of polynomial and rational matrices. Examples of polynomial and rational matrices, which exhibit all relevant structural features, are considered to illustrate the main mathematical concepts and the capabilities of implemented tools.\n\n**Published:** 2020-06-09\n\n\n\n#### 3. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: efficient parallel processing*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n#### 4. Addressing Some Limitations of Transformers with Feedback Memory\n\n*From Search Query: efficient parallel processing*\n\n*Edouard Grave, Armand Joulin, Angela Fan, Thibaut Lavril, Sainbayar Sukhbaatar*\n\n**Abstract:** Transformers have been successfully applied to sequential, auto-regressive tasks despite being feedforward networks. Unlike recurrent neural networks, Transformers use attention to capture temporal relations while processing input tokens in parallel. While this parallelization makes them computationally efficient, it restricts the model from fully exploiting the sequential nature of the input. The representation at a given layer can only access representations from lower layers, rather than the higher level representations already available. In this work, we propose the Feedback Transformer architecture that exposes all previous representations to all future representations, meaning the lowest representation of the current timestep is formed from the highest-level abstract representation of the past. We demonstrate on a variety of benchmarks in language modeling, machine translation, and reinforcement learning that the increased representation capacity can create small, shallow models with much stronger performance than comparable Transformers.\n\n**Conference:** addressing-some-limitations-of-transformers\n\n**Published:** 2020-02-21\n\n\n\n#### 5. ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\n\n*From Search Query: memory optimization*\n\n*Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, Yuxiong He*\n\n**Abstract:** Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware. We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.\n\n**Published:** 2019-10-04\n\n\n\n#### 6. Memory-Efficient Adaptive Optimization\n\n*From Search Query: memory optimization*\n\n*Yoram Singer, Tomer Koren, Vineet Gupta, Rohan Anil*\n\n**Abstract:** Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.\n\n**Published:** 2019-01-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient parallel processing, memory optimization, and the use of structured matrices, here are some key insights and techniques derived from the provided sources and analysis:\n\n## Efficient Parallel Processing of Structured Matrices\n\n### Utilizing Tensor Cores\nThe use of Tensor Cores in modern GPUs can significantly accelerate matrix multiplications, which are crucial in state space models and autoregressive language models. The Tensor Core-Adapted Sparse Matrix Multiplication (TCA-SpMM) kernel is particularly relevant, as it optimizes load balancing and minimizes data movement, ensuring high utilization of Tensor Cores. This approach can be applied to structured matrices in state space models to enhance computational efficiency.\n\n### Block Decompositions and Parallelization\nBlock decompositions can enable efficient parallel processing of structured matrices. By dividing the matrix into smaller blocks and processing them in parallel, you can leverage multiple GPU threads and cores effectively. This is demonstrated in the TCA-SpMM implementation, where multiple thread blocks are assigned to handle large workloads, ensuring that each block processes a portion of the matrix efficiently.\n\n## Memory Optimization Strategies\n\n### Sparse Matrix Representations\nUsing sparse matrix representations can significantly reduce memory usage. Techniques like the Compressed Sparse Row (CSR) format can store only non-zero elements, minimizing memory footprint. The TCA-SpMM kernel utilizes the CSR format without the need for sparse matrix reordering, which further optimizes memory usage.\n\n### Distributed and Parallel Training Techniques\nFor large models, distributed and parallel training techniques are essential. Methods such as Distributed Data-Parallel (DDP), model parallelism, and techniques like gradient checkpointing can reduce memory usage. For example, gradient checkpointing selectively stores activations during the forward pass and recomputes them during the backward pass, trading computational overhead for reduced memory consumption.\n\n### Memory Offloading and Sharding\nMemory offloading strategies, such as those implemented in ZeRO-Offload, can move optimizer states and gradients to CPU memory when not actively in use, freeing up GPU memory. Fully Sharded Data Parallel (FSDP) extends this by sharding model parameters, optimizer states, and gradients, further reducing memory footprint.\n\n## Hardware-Efficient Implementations\n\n### Leveraging GPU Architectures\nModern GPU architectures, such as those utilizing Streaming Multiprocessors (SMs) and Tensor Cores, are designed for efficient matrix multiplications and other linear algebra operations. Optimizing implementations to use these specialized cores can significantly improve performance. For instance, using CUDA and libraries like cuSPARSE and CUTLASS can target sparsity-aware kernels, enhancing the efficiency of sparse matrix operations.\n\n### Optimizing Memory Layouts\nOptimizing memory layouts is crucial for performance. Techniques such as using Struct of Arrays (SOA) or Array of Structures (AOS) and ensuring proper memory alignment can improve access patterns and reduce data transfer costs. This is particularly important in parallel programming frameworks where data is partitioned and mapped across multiple processors.\n\n## Combining Gating with State Space Models\n\n### State Space Duality\nState Space Duality (SSD) offers a framework for combining state space models with attention-like mechanisms. SSD can simplify SSM parameters, enabling faster computation and providing both linear and quadratic forms. This duality allows for the integration of gating mechanisms through input-dependent state transitions, which can be represented as structured matrices with semiseparable properties[Analysis Note].\n\n### Input-Dependent Masks\nUsing input-dependent masks instead of traditional softmax-based attention can enhance gating mechanisms. These masks can be multiplied element-wise with the logits matrix, enabling efficient computation of long-range dependencies and structured state transitions[Analysis Note].\n\nBy integrating these techniques, researchers can develop more efficient and scalable autoregressive language models that leverage structured matrices, optimize memory usage, and utilize modern GPU architectures effectively.\n",
            "analysis": "Analysis Note: Integrating State Space Duality with Gating Mechanisms\n\n1. Summary of Analysis:\nThe search results reveal promising directions for enhancing the GatedTTTLinear architecture through state space duality (SSD) and efficient parallel implementations:\n\na) State Space Duality offers a theoretical framework for combining SSMs with attention-like mechanisms while maintaining computational efficiency:\n- Simplifies SSM parameters to enable faster computation\n- Provides both linear (recurrent) and quadratic (attention-like) forms\n- Enables hardware-efficient implementations\n\nb) The relationship between SSMs and attention mechanisms suggests ways to enhance gating:\n- SSMs can be viewed as structured matrices with semiseparable properties\n- Gating mechanisms can be integrated through input-dependent state transitions\n- Memory efficiency can be improved through structured sparsity\n\n2. Useful References with Excerpts:\n\nFrom \"Transformers are SSMs\":\n\"The dual form of SSD is a quadratic computation closely related to attention, defined as:\nL \u2218 QK^T \u00b7 V where L_ij = {a_i \u00d7 ... \u00d7 a_j+1  if i \u2265 j, 0 otherwise}\"\n\n\"Compared to standard softmax attention, there are two main differences:\n- The softmax is dropped\n- The attention matrix is multiplied elementwise by an additional mask matrix L\"\n\nFrom \"State Space Duality Framework\":\n\"SSD has only two main differences from standard attention:\n- SSD does not use the softmax activation\n- SSD multiplies the logits matrix by an input-dependent 1-semiseparable mask\"\n\n3. Key Insights and Analysis:\n\na) State Space Structure:\n- SSMs can be represented as structured matrices with efficient multiplication algorithms\n- The duality between linear and quadratic forms enables flexible implementation choices\n- Hardware-aware implementations can leverage modern accelerator architectures\n\nb) Gating Enhancement:\n- Input-dependent masks can replace traditional softmax-based attention\n- Semiseparable structure enables efficient computation of long-range dependencies\n- Gating mechanisms can be integrated through structured state transitions\n\nc) Implementation Considerations:\n- Block decompositions enable efficient parallel processing\n- Memory access patterns significantly impact performance\n- Trade-offs between computational complexity and model expressivity\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for efficient parallel implementation of structured matrices\n2. Methods for combining gating with state space models\n3. Memory optimization strategies for long sequences\n4. Hardware-efficient implementations of hybrid architectures\n\n5. References:\n\n[1] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv preprint.\n\n[2] Gu, A., et al. (2022). \"How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections.\" International Conference on Learning Representations.\n\n[3] Sarrof, Y., et al. (2024). \"The Expressive Capacity of State Space Models: A Formal Language Perspective.\" arXiv preprint.\n\n[4] Xu, C., et al. (2024). \"VSSD: Vision Mamba with Non-Causal State Space Duality.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "dual form computation structured matrices",
            "detail": "Search for papers discussing:\n1. Dual-form computation in neural networks and transformers\n2. Hardware-efficient structured matrix operations\n3. Memory-efficient state compression techniques\n4. Implementation strategies for parallel matrix computation\n5. Trade-offs between model compression and performance",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Dual-form computation in neural networks and transformers\n2. Hardware-efficient structured matrix operations\n3. Memory-efficient state compression techniques\n4. Implementation strategies for parallel matrix computation\n5. Trade-offs between model compression and performance\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 7/86 (Score: 0.99)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.51)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.51)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.22)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.22)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.06)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.06)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.04)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.04)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: dual form computation structured matrices\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. A Dual Algorithm for Olfactory Computation in the Locust Brain\n\n*From Search Query: dual form computation structured matrices*\n\n*Sina Tootoonian, M. Lengyel*\n\n**TL;DR:** This work proposes its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements and shows that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data.\n\n**Abstract:** We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses independent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2014\n\n**Citations:** 12  (*Influential: 2*)\n\n#### 2. SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient\n\n*From Search Query: dual form computation structured matrices*\n\n*Aaron Mishkin, Frederik Kunstner, Didrik Nielsen, Mark W. Schmidt, M. E. Khan*\n\n**TL;DR:** This work proposes a new stochastic, low-rank, approximate natural-gradient (SLANG) method for variational inference in large deep models that enables faster and more accurate estimation of uncertainty than mean-field methods, and performs comparably to state-of-the-art methods.\n\n**Abstract:** Uncertainty estimation in large deep-learning models is a computationally challenging task, where it is difficult to form even a Gaussian approximation to the posterior distribution. In such situations, existing methods usually resort to a diagonal approximation of the covariance matrix despite the fact that these matrices are known to give poor uncertainty estimates. To address this issue, we propose a new stochastic, low-rank, approximate natural-gradient (SLANG) method for variational inference in large deep models. Our method estimates a \u201cdiagonal plus low-rank\u201d structure based solely on back-propagated gradients of the network log-likelihood. This requires strictly less gradient computations than methods that compute the gradient of the whole variational objective. Empirical evaluations on standard benchmarks confirm that SLANG enables faster and more accurate estimation of uncertainty than mean-field methods, and performs comparably to state-of-the-art methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 56  (*Influential: 7*)\n\n#### 3. Learning Compressed Transforms with Low Displacement Rank\n\n*From Search Query: dual form computation structured matrices*\n\n*Anna T. Thomas, Albert Gu, Tri Dao, A. Rudra, C. R\u00e9*\n\n**TL;DR:** A rich class of LDR matrices with more general displacement operators is introduced, and explicitly learn over both the operators and the low-rank component, which exceeds the accuracy of existing compression approaches and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.\n\n**Abstract:** The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 39  (*Influential: 6*)\n\n#### 4. Distributed Dual Averaging In Networks\n\n*From Search Query: dual form computation structured matrices*\n\n*John C. Duchi, Alekh Agarwal, M. Wainwright*\n\n**TL;DR:** This work develops and analyzes distributed algorithms based on dual averaging of subgradients, and provides sharp bounds on their convergence rates as a function of the network size and topology.\n\n**Abstract:** The goal of decentralized optimization over a network is to optimize a global objective formed by a sum of local (possibly nonsmooth) convex functions using only local computation and communication. We develop and analyze distributed algorithms based on dual averaging of subgradients, and provide sharp bounds on their convergence rates as a function of the network size and topology. Our analysis clearly separates the convergence of the optimization algorithm itself from the effects of communication constraints arising from the network structure. We show that the number of iterations required by our algorithm scales inversely in the spectral gap of the network. The sharpness of this prediction is confirmed both by theoretical lower bounds and simulations for various networks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2010\n\n**Citations:** 72  (*Influential: 2*)\n\n#### 5. Walsh-Hadamard Variational Inference for Bayesian Deep Learning\n\n*From Search Query: dual form computation structured matrices*\n\n*Simone Rossi, S\u00e9bastien Marmin, M. Filippone*\n\n**TL;DR:** Walsh-Hadamard Variational Inference is proposed, which uses Walsh- hadamard-based factorization strategies to reduce the parameterization and accelerate computations, thus avoiding over-regularization issues with the variational objective.\n\n**Abstract:** Over-parameterized models, such as DeepNets and ConvNets, form a class of models that are routinely adopted in a wide variety of applications, and for which Bayesian inference is desirable but extremely challenging. Variational inference offers the tools to tackle this challenge in a scalable way and with some degree of flexibility on the approximation, but for over-parameterized models this is challenging due to the over-regularization property of the variational objective. Inspired by the literature on kernel methods, and in particular on structured approximations of distributions of random matrices, this paper proposes Walsh-Hadamard Variational Inference (WHVI), which uses Walsh-Hadamard-based factorization strategies to reduce the parameterization and accelerate computations, thus avoiding over-regularization issues with the variational objective. Extensive theoretical and empirical analyses demonstrate that WHVI yields considerable speedups and model reductions compared to other techniques to carry out approximate inference for over-parameterized models, and ultimately show how advances in kernel methods can be translated into advances in approximate Bayesian inference.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 14  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on dual-form computation, hardware-efficient structured matrix operations, memory-efficient state compression techniques, and implementation strategies for parallel matrix computation is crucial. Here are some key points and references that can help achieve these goals:\n\n## Dual-Form Computation in Neural Networks and Transformers\n\n- The concept of dual-form computation can be related to the use of different computational paradigms, such as combining traditional neural network computations with alternative methods like spiking neural networks (SNNs). For instance, the \"ViT-LCA\" approach integrates transformer architectures with neuromorphic computing, utilizing spiking self-attention mechanisms to reduce computational energy.\n\n## Hardware-Efficient Structured Matrix Operations\n\n- **Structured Sparsity and Hardware-Aware Implementations**: Techniques like FlashAttention optimize memory access patterns by using tiling to reduce the number of memory reads/writes, which is optimal for a range of SRAM sizes. This approach can significantly improve hardware utilization and performance[Analysis Note: FlashAttention].\n\n- **Neuromorphic Computing**: Integrating transformer architectures with neuromorphic computing can leverage the low-power processing capabilities and efficient matrix-vector multiplications of neuromorphic chips. This can enhance computational efficiency and reduce energy consumption.\n\n## Memory-Efficient State Compression Techniques\n\n- **Dynamic Context Pruning**: This method employs a learnable mechanism to determine which uninformative tokens can be dropped from the context, effectively pruning up to 80% of the context without significant performance degradation. This technique is crucial for reducing memory requirements in autoregressive models[Analysis Note: Dynamic Context Pruning].\n\n- **Token Compression Techniques**: Efficient state management through token compression can significantly reduce memory requirements. Techniques like dense connections in DenseSSM retain fine-grained information while maintaining training parallelizability and inference efficiency[Analysis Note: DenseMamba].\n\n## Implementation Strategies for Parallel Matrix Computation\n\n- **Blockwise Parallel Transformer**: This approach involves dividing the transformer model into blocks that can be processed in parallel, which can significantly improve the efficiency of large context models. This strategy leverages the memory hierarchy and parallel processing capabilities of modern hardware[Analysis Note: Blockwise Parallel Transformer].\n\n- **Multi-Head Attention and Parallel Processing**: The multi-head attention mechanism in transformers allows for parallel processing of different attention heads, which can be optimized further using hardware-aware designs. This parallelization is key to faster training and better performance.\n\n## Trade-Offs Between Model Compression and Performance\n\n- **Balancing Compression and Performance**: Techniques like dynamic context pruning and token compression need to balance the trade-off between model compression and performance. Efficient state management is crucial to ensure that compression does not significantly degrade the model's performance[Analysis Note: Key Insights].\n\nIn summary, to improve autoregressive language model design, researchers should focus on:\n\n- **Dual-Form Computation**: Explore integrating traditional neural network computations with alternative paradigms like neuromorphic computing and spiking neural networks.\n- **Hardware-Efficient Matrix Operations**: Utilize techniques like FlashAttention and neuromorphic computing to optimize memory access patterns and leverage structured sparsity.\n- **Memory-Efficient State Compression**: Implement dynamic context pruning and token compression techniques to reduce memory requirements without compromising performance.\n- **Parallel Matrix Computation**: Employ blockwise parallel processing and optimize multi-head attention mechanisms for better hardware utilization.\n- **Trade-Offs**: Carefully balance model compression with performance to ensure efficient and effective models.\n\nThese strategies can help in designing more efficient, scalable, and high-performance autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results provide valuable insights into hardware optimization, state space models, and memory efficiency techniques. Key findings include:\n\na) Hardware Optimization:\n- FlashAttention and similar techniques optimize memory access patterns\n- Structured sparsity patterns can improve hardware utilization\n- Hardware-aware implementations crucial for performance\n\nb) State Space Models:\n- DenseSSM shows promise for enhancing information flow\n- Mamba and similar architectures demonstrate efficient long-range modeling\n- Trade-offs between expressiveness and computational efficiency\n\nc) Memory Management:\n- Dynamic context pruning offers significant memory savings\n- Token compression techniques show promise\n- Efficient state management crucial for performance\n\n2. Useful References with Excerpts:\n\nFrom \"DenseMamba\":\n\"DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom \"Dynamic Context Pruning\":\n\"Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context [...] can effectively prune up to 80% of the context without significant performance degradation\"\n\nFrom \"FlashAttention\":\n\"Uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM [...] is optimal for a range of SRAM sizes\"\n\n3. Key Insights:\n\na) Hardware Efficiency:\n- Memory access patterns critical for performance\n- Structured computations enable better hardware utilization\n- Trade-off between flexibility and hardware efficiency\n\nb) State Management:\n- Efficient state updates crucial for long sequences\n- Compression can significantly reduce memory requirements\n- Balance needed between compression and model performance\n\nc) Implementation Considerations:\n- Hardware-aware design essential for optimal performance\n- Memory hierarchy optimization important\n- Parallel processing capabilities need consideration\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Dual-form computation implementations\n2. Hardware-specific optimizations for structured matrices\n3. State compression techniques and their impact\n\nNext search should focus on:\n- Implementation details of dual-form computation\n- Hardware-specific matrix optimization techniques\n- State compression methods in language models\n\n5. References:\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. Neural Information Processing Systems.\n\n[3] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[4] Liu, H., & Abbeel, P. (2023). Blockwise Parallel Transformer for Large Context Models. arXiv preprint.\n\n[5] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "state space duality transformers",
            "detail": "Search for papers discussing:\n1. Integration of state space models with transformer architectures\n2. Dual-form computation in neural networks\n3. Memory optimization techniques in language models\n4. Structured matrix implementations for efficient computation\n5. Hardware-aware optimization of transformer models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Integration of state space models with transformer architectures\n2. Dual-form computation in neural networks\n3. Memory optimization techniques in language models\n4. Structured matrix implementations for efficient computation\n5. Hardware-aware optimization of transformer models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.98)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.98)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.98)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.98)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. Long Range Language Modeling via Gated State Spaces (Avg. Score: 0.96)\n\n*Harsh Mehta, Ankit Gupta, Ashok Cutkosky, Behnam Neyshabur*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 134  (*Influential: 17*)\n\n**TL;DR:** This work proposes a new layer named Gated State Space (GSS) and shows that it trains significantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.\n\n**Abstract:** State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.\n\n##### *Relevant Chunk: No. 8/28 (Score: 0.96)*\n\n```\nArXiv preprint, abs/2204.02311, 2022. URL https://arxiv.org/abs/2204.02311. James W. Cooley and John W. Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of Computation, 19:297-301, 1965. Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Clifford Stein. Introduction to Algorithms. The MIT Press, 3rd edition, 2009. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, Florence, Italy, 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285. Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In Proceedings of the 34th International Conference on Machine Learning - Volume 70, ICML'17, page 933-941. JMLR.org, 2017. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171-4186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1423. URL https://aclanthology.org/N19-1423. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=YicbFdNTTy. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. ArXiv preprint, abs/2002.09402, 2020. URL https: //arxiv.org/abs/2002.09402. Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Hugo Larochelle, Marc'Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin, editors, Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/hash/ 102fObb6efb3a6128a3c750dd16729be-Abstract.html. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022a. URL https://openreview.net/forum? id=uYLFoz1vlAC. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models.\n```\n\n#### 4. LongNet: Scaling Transformers to 1,000,000,000 Tokens (Avg. Score: 0.96)\n\n*Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 84  (*Influential: 10*)\n\n**TL;DR:** This work introduces LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences, and proposes dilated attention, which expands the attentive field exponentially as the distance grows.\n\n**Abstract:** Scaling sequence length has become a critical demand in the era of large language models. However, existing methods struggle with either computational complexity or model expressivity, rendering the maximum sequence length restricted. In this work, we introduce LongNet, a Transformer variant that can scale sequence length to more than 1 billion tokens, without sacrificing the performance on shorter sequences. Specifically, we propose dilated attention, which expands the attentive field exponentially as the distance grows. LongNet has significant advantages: 1) it has a linear computation complexity and a logarithm dependency between tokens; 2) it can be served as a distributed trainer for extremely long sequences; 3) its dilated attention is a drop-in replacement for standard attention, which can be seamlessly integrated with the existing Transformer-based optimization. Experiments results demonstrate that LongNet yields strong performance on both long-sequence modeling and general language tasks. Our work opens up new possibilities for modeling very long sequences, e.g., treating a whole corpus or even the entire Internet as a sequence.\n\n##### *Relevant Chunk: No. 9/20 (Score: 0.96)*\n\n```\nArXiv, abs/2204.02311, 2022. $\\left[\\mathrm{DDM}^{+}\\right.$23] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Many Others, Xiaohua Zhai, Daniel Keysers, Jeremiah Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. CoRR, abs/2302.05442, 2023 . $\\left[\\mathrm{DFE}^{+}\\right.$22] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. $\\left[\\mathrm{DYY}^{+}\\right.$19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. In Anna Korhonen, David R. Traum, and Llu\u00eds M\u00e0rquez, editors, Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2978-2988. Association for Computational Linguistics, 2019. [FDS ${ }^{+}$23] Daniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. $\\left[\\mathrm{FPB}^{+}\\right.$23] Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, Christopher Pal, Orhan Firat, and Ross Goroshin. Block-state transformer. CoRR, abs/2306.09539, 2023. [FZS21] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity.\n```\n\n#### 5. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.93)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.93)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space duality transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: state space duality transformers*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n#### 2. Repeat After Me: Transformers are Better than State Space Models at Copying\n\n*From Search Query: state space duality transformers*\n\n*Samy Jelassi, David Brandfonbrener, S. Kakade, Eran Malach*\n\n**TL;DR:** It is proved that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state, and a fundamental gap between transformers and GSSMs on tasks of practical interest is suggested.\n\n**Abstract:** Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as\"generalized state space models\"(GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 42  (*Influential: 8*)\n\n#### 3. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: state space duality transformers*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n#### 4. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: state space duality transformers*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 274  (*Influential: 21*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: state space duality transformers*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n#### 2. VSSD: Vision Mamba with Non-Causal State Space Duality\n\n*From Search Query: state space duality transformers*\n\n*Chang Xu, Mingjia Li, Minjing Dong, Yuheng Shi*\n\n**Abstract:** Vision transformers have significantly advanced the field of computer vision, offering robust modeling capabilities and global receptive field. However, their high computational demands limit their applicability in processing long sequences. To tackle this issue, State Space Models (SSMs) have gained prominence in vision tasks as they offer linear computational complexity. Recently, State Space Duality (SSD), an improved variant of SSMs, was introduced in Mamba2 to enhance model performance and efficiency. However, the inherent causal nature of SSD/SSMs restricts their applications in non-causal vision tasks. To address this limitation, we introduce Visual State Space Duality (VSSD) model, which has a non-causal format of SSD. Specifically, we propose to discard the magnitude of interactions between the hidden state and tokens while preserving their relative weights, which relieves the dependencies of token contribution on previous tokens. Together with the involvement of multi-scan strategies, we show that the scanning results can be integrated to achieve non-causality, which not only improves the performance of SSD in vision tasks but also enhances its efficiency. We conduct extensive experiments on various benchmarks including image classification, detection, and segmentation, where VSSD surpasses existing state-of-the-art SSM-based models. Code and weights are available at \\url{https://github.com/YuHengsss/VSSD}.\n\n**Published:** 2024-07-26\n\n\n\n#### 3. SSD4Rec: A Structured State Space Duality Model for Efficient Sequential Recommendation\n\n*From Search Query: state space duality transformers*\n\n*Qing Li, Wenqi Fan, Liangbo Ning, Yifeng Zhang, Haohao Qu*\n\n**Abstract:** Sequential recommendation methods are crucial in modern recommender systems for their remarkable capability to understand a user's changing interests based on past interactions. However, a significant challenge faced by current methods (e.g., RNN- or Transformer-based models) is to effectively and efficiently capture users' preferences by modeling long behavior sequences, which impedes their various applications like short video platforms where user interactions are numerous. Recently, an emerging architecture named Mamba, built on state space models (SSM) with efficient hardware-aware designs, has showcased the tremendous potential for sequence modeling, presenting a compelling avenue for addressing the challenge effectively. Inspired by this, we propose a novel generic and efficient sequential recommendation backbone, SSD4Rec, which explores the seamless adaptation of Mamba for sequential recommendations. Specifically, SSD4Rec marks the variable- and long-length item sequences with sequence registers and processes the item representations with bidirectional Structured State Space Duality (SSD) blocks. This not only allows for hardware-aware matrix multiplication but also empowers outstanding capabilities in variable-length and long-range sequence modeling. Extensive evaluations on four benchmark datasets demonstrate that the proposed model achieves state-of-the-art performance while maintaining near-linear scalability with user sequence length. Our code is publicly available at https://github.com/ZhangYifeng1995/SSD4Rec.\n\n**Published:** 2024-09-02\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using state space duality and integrating it with transformer architectures, here are some key findings and discussions from the provided sources that align with your investigation plan:\n\n## Integration of State Space Models with Transformer Architectures\n\n- The paper \"Transformers are SSMs: Generalized models and efficient algorithms through structured state space duality\" by Tri Dao and Albert Gu discusses how state space models (SSMs), such as Mamba, can be integrated with transformer architectures. It highlights that SSMs can match or outperform transformers at small to medium scales, and the authors develop a framework connecting SSMs and transformers through structured state space duality (SSD).\n\n## Dual-Form Computation in Neural Networks\n\n- The SSD framework allows for dual-form computation, where the recurrence in SSMs can be equivalently expressed as a matrix multiplication. This duality enables the use of both linear-time and quadratic-time computations, depending on the efficiency needs. This is particularly useful as it generalizes linear attention and can be seen as a special case where the matrix \\(L\\) is the all-ones lower triangular matrix.\n\n## Memory Optimization Techniques in Language Models\n\n- Mamba-2, a variant of SSMs, offers constant memory usage during inference, which is a significant memory optimization. This is achieved through the structured state space duality, which allows for efficient computation without the need for large memory allocations. However, it still faces challenges in tasks requiring precise memory recall, similar to linear attention.\n\n## Structured Matrix Implementations for Efficient Computation\n\n- The SSD algorithm leverages block decompositions of semiseparable matrices to achieve optimal efficiency trade-offs. This involves dividing the matrix into blocks representing chunks of the input and output sequences, with diagonal blocks handling computations within each chunk and off-diagonal blocks managing interactions between chunks. This approach significantly improves hardware efficiency.\n\n## Hardware-Aware Optimization of Transformer Models\n\n- The integration of SSMs with SSD allows for hardware-aware optimizations. For instance, the SSD algorithm is designed to be 2-8X faster than previous implementations while maintaining competitiveness with transformers. This is achieved through efficient matrix multiplication techniques and the use of semiseparable matrices, which are particularly beneficial for hardware optimization.\n\n### Additional Insights\n\n- **State Space Duality Integration**: The state space duality in Mamba-2 allows for a balance between linear and quadratic forms of computation, which can be optimized based on the specific needs of the task. This duality is crucial for understanding the computational efficiency and expressive power of these models.\n\n- **Structured Matrix Optimizations**: The use of structured matrices, such as 1-semiseparable matrices, encodes temporal dependencies and content-based interactions similar to attention. This approach reduces computational complexity and memory requirements, making it more hardware-efficient.\n\n- **Memory-Efficient Implementation**: State compression techniques, such as those implied by the constant memory usage in Mamba-2, are effective in reducing memory requirements. However, there is a need to balance this with potential impacts on model performance, especially in tasks requiring precise memory recall.\n\n- **Scalability**: The linear scalability of SSMs with respect to sequence length, as demonstrated in models like SC-MAMBA2, allows for the efficient modeling of long sequences without the exponential increase in computational resources seen in traditional transformer architectures.\n\nThese findings align well with the phases of your investigation plan and highlight the potential benefits and trade-offs of integrating state space duality with transformer architectures, using structured matrices, and optimizing memory efficiency.\n",
            "analysis": "Initial Analysis of DualStateGatedTTTLinear Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) State Space Duality Integration:\n- How does state space duality compare to existing approaches in TTTLinear?\n- What are the trade-offs between linear and quadratic forms?\n- How does it affect model performance and efficiency?\n\nb) Structured Matrix Optimizations:\n- Impact on computational efficiency and memory usage\n- Trade-offs between expressiveness and sparsity\n- Hardware compatibility and implementation challenges\n\nc) Memory-Efficient Implementation:\n- Effectiveness of state compression techniques\n- Impact on model performance\n- Scalability with sequence length\n\n2. Investigation Plan:\n\nPhase 1: State Space Models and Duality\n- Research recent advances in state space models\n- Analyze dual-form computation methods\n- Evaluate efficiency gains and trade-offs\n\nPhase 2: Structured Matrices\n- Study structured matrix implementations\n- Analyze hardware-aware optimizations\n- Evaluate memory-computation trade-offs\n\nPhase 3: Memory Optimization\n- Research state compression techniques\n- Analyze efficiency-performance balance\n- Study scalability implications\n\n3. Novelty Assessment:\n\nThe proposal appears novel in several aspects:\n- Integration of state space duality with TTTLinear\n- Combination of structured matrices with dual-form computation\n- Memory-efficient implementation through state compression\n\n4. Potential Impact:\n\na) Accuracy:\n- Dual-form computation may improve model expressiveness\n- Structured matrices could affect representation capacity\n- State compression might impact model performance\n\nb) Robustness:\n- Multiple computation forms may enhance adaptability\n- Structured patterns could improve stability\n- Compression might affect noise resilience\n\nc) Efficiency:\n- Dual-form computation offers computational benefits\n- Structured matrices reduce memory requirements\n- State compression improves memory efficiency\n\nd) Scalability:\n- Linear complexity option for long sequences\n- Memory optimization enables larger models\n- Hardware-aware design supports scaling\n\n5. Research Alignment:\n- Aligns with trends in efficient transformer architectures\n- Follows recent work in state space models\n- Addresses current challenges in memory efficiency\n\nInitial search results will help validate these aspects and guide further investigation."
        },
        {
            "ready": false,
            "query": "efficient attention mechanisms memory compression",
            "detail": "Search for papers discussing:\n1. Memory compression techniques in transformer models\n2. Efficient attention mechanisms and their implementations\n3. Hardware-aware optimization strategies for language models\n4. State compression and management in neural networks\n5. Structured sparsity in transformer architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory compression techniques in transformer models\n2. Efficient attention mechanisms and their implementations\n3. Hardware-aware optimization strategies for language models\n4. State compression and management in neural networks\n5. Structured sparsity in transformer architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 1.00)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 1.00)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.97)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 12/24 (Score: 0.97)*\n\n```\narXiv preprint arXiv:1511.07289, 2015. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128 k context. arXiv preprint arXiv:2402.10171, 2024. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model.\n```\n\n#### 3. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.97)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.97)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 4. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.96)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 28/38 (Score: 0.96)*\n\n```\nArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n#### 5. Fnet: Mixing tokens with fourier transforms (Avg. Score: 0.94)\n\n*J. Lee-Thorp, J. Ainslie, Ilya Eckstein, Santiago Onta\u00f1\u00f3n*\n\n**Published in:** North American Chapter of the Association for Computational Linguistics (2021)\t**Cited by** 400  (*Influential: 56*)\n\n**TL;DR:** The FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\n\n**Abstract:** We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.\n\n##### *Relevant Chunk: No. 20/38 (Score: 0.94)*\n\n```\nNoam Shazeer. 2019. Fast transformer decoding: One write-head is all you need. arXiv preprint arXiv:1911.02150. Sam Shleifer and Alexander M Rush. 2020. Pretrained summarization distillation. arXiv preprint arXiv:2010.13002. Vikas Sindhwani, Tara N Sainath, and Sanjiv Kumar. 2015. Structured transforms for small-footprint deep learning. In Proceedings of the 28th International Conference on Neural Information Processing Systems-Volume 2, pages 3088-3096. Alex Tamkin, Dan Jurafsky, and Noah Goodman. 2020. Language through a prism: A spectral approach for multiscale language representations. In Advances in Neural Information Processing Systems, volume 33, pages 5492-5504. Curran Associates, Inc. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2020a. Synthesizer: Rethinking self-attention in transformer models. arXiv preprint arXiv:2005.00743. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020b. Sparse sinkhorn attention. In International Conference on Machine Learning, pages $9438-9447$. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021a. Long range arena: A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient attention mechanisms memory compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 2. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 3. Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Huiyin Xue, Nikolaos Aletras*\n\n**TL;DR:** This work proposes an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head, and empirically demonstrates that this MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks.\n\n**Abstract:** Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 4. Focus on the Core: Efficient Attention via Pruned Token Compression for Document Classification\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Jungmin Yun, Mihyeon Kim, Youngbin Kim*\n\n**TL;DR:** This work proposes integrating two strategies: token pruning and token combining, which not only improve the model's performance but also reduce its computational demands.\n\n**Abstract:** Transformer-based models have achieved dominant performance in numerous NLP tasks. Despite their remarkable successes, pre-trained transformers such as BERT suffer from a computationally expensive self-attention mechanism that interacts with all tokens, including the ones unfavorable to classification performance. To overcome these challenges, we propose integrating two strategies: token pruning and token combining. Token pruning eliminates less important tokens in the attention mechanism's key and value as they pass through the layers. Additionally, we adopt fuzzy logic to handle uncertainty and alleviate potential mispruning risks arising from an imbalanced distribution of each token's importance. Token combining, on the other hand, condenses input sequences into smaller sizes in order to further compress the model. By integrating these two approaches, we not only improve the model's performance but also reduce its computational demands. Experiments with various datasets demonstrate superior performance compared to baseline models, especially with the best improvement over the existing BERT model, achieving +5%p in accuracy and +5.6%p in F1 score. Additionally, memory cost is reduced to 0.61x, and a speedup of 1.64x is achieved.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2024\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 5. Efficient Attention using a Fixed-Size Memory Representation\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*D. Britz, M. Guan, Minh-Thang Luong*\n\n**TL;DR:** This work proposes an alternative attention mechanism based on a fixed size memory representation that is more efficient and predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory.\n\n**Abstract:** The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2017\n\n**Citations:** 32  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Siddharth Gopal, Manaal Faruqui, Tsendsuren Munkhdalai*\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n**Published:** 2024-04-10\n\n\n\n#### 2. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 3. Memory-augmented Dense Predictive Coding for Video Representation Learning\n\n*From Search Query: efficient attention mechanisms memory compression*\n\n*Weidi Xie, Tengda Han, Andrew Zisserman*\n\n**Abstract:** The objective of this paper is self-supervised learning from video, in particular for representations for action recognition. We make the following contributions: (i) We propose a new architecture and learning framework Memory-augmented Dense Predictive Coding (MemDPC) for the task. It is trained with a predictive attention mechanism over the set of compressed memories, such that any future states can always be constructed by a convex combination of the condense representations, allowing to make multiple hypotheses efficiently. (ii) We investigate visual-only self-supervised video representation learning from RGB frames, or from unsupervised optical flow, or both. (iii) We thoroughly evaluate the quality of learnt representation on four different downstream tasks: action recognition, video retrieval, learning with scarce annotations, and unintentional action classification. In all cases, we demonstrate state-of-the-art or comparable performance over other approaches with orders of magnitude fewer training data.\n\n**Proceeding:** eccv-2020-8\n\n**Published:** 2020-08-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on efficient attention mechanisms, memory compression, and hardware-aware optimization strategies is crucial. Here are some key findings and techniques from the provided sources and additional insights that align with your goals:\n\n## Memory Compression Techniques in Transformer Models\n\n### Quantization\nQuantization is a powerful technique for reducing the memory footprint of large language models (LLMs). It involves converting the model's weights from higher precision (e.g., 16-bit floating point) to lower precision (e.g., 8-bit integers).\n\n- **Quantization-Aware Training (QAT)**: This method trains the model with quantization in mind, leading to optimal performance. It is particularly effective when combined with low-rank adaptation (LoRA) adaptors, as seen in the QLoRA approach used for Meta's Llama models.\n- **Post-Training Quantization (PTQ)**: For existing models, PTQ offers a quicker alternative that requires minimal computational resources. Tools like PyTorch and Hugging Face's Optimum Intel library support these techniques.\n\n### BitStack\nBitStack is a novel compression technique that allows for fine-grained size control by dynamically balancing memory and performance. It decomposes weight matrices into blocks that can be selectively loaded based on performance needs or memory availability, making it highly adaptable for varying memory environments.\n\n## Efficient Attention Mechanisms\n\n### Structured Matrices and State Space Duality\nRecent research highlights the connection between transformers and state space models (SSMs), which can lead to more efficient attention mechanisms. For instance, structured semiseparable matrices can provide theoretical connections between SSMs and variants of attention, enabling hardware-efficient implementations[Gu, A., & Dao, T., 2024].\n\n### Block FFT Algorithms\nTechniques like FlashConv, which use fused block FFT algorithms, can improve the efficiency of attention mechanisms on long sequences. This approach is particularly beneficial for sequences up to 8K tokens[Dao, T., et al., 2022].\n\n## Hardware-Aware Optimization Strategies\n\n### Quantization and LoRA Adaptors\nThe use of quantization-aware training combined with LoRA adaptors (QLoRA) is optimized for mobile CPUs and can be further enhanced using NPUs. This approach ensures that the models are highly efficient in terms of both speed and memory footprint.\n\n### SpinQuant\nSpinQuant is another technique for post-training quantization that allows for flexible quantization settings. It is useful for fine-tuning models for different targets and quantization settings, ensuring optimal performance on various hardware platforms.\n\n## State Compression and Management in Neural Networks\n\n### BitStack and Dynamic Loading\nBitStack's approach to decomposing model weights into blocks that can be dynamically loaded based on memory availability is a significant advancement in state compression. This method minimizes data transfer between operational memory and storage, making it highly efficient for deployment on devices with limited memory.\n\n### Memory Management in Chatbots\nWhile not directly focused on LLMs, the memory management techniques used in chatbots, such as automatic summarization and word count-based compression, can provide insights into managing conversational context efficiently. These methods ensure that relevant context is preserved while reducing memory usage.\n\n## Structured Sparsity in Transformer Architectures\n\n### Quantization and Sparsity\nQuantization techniques inherently introduce a form of sparsity by reducing the precision of model weights. However, explicit sparsity techniques can further reduce the model's size and computational requirements. Combining quantization with structured sparsity can lead to highly efficient models that maintain performance while reducing memory and computational costs.\n\nBy integrating these techniques\u2014quantization, BitStack, efficient attention mechanisms through structured matrices and block FFT algorithms, and hardware-aware optimization strategies\u2014the researcher can develop an autoregressive language model that is highly efficient, scalable, and robust across various hardware environments.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search results provide valuable insights into state space models (SSMs), their relationship with transformers, and recent advances in dual-form computation. However, more specific information is needed about structured matrices and memory optimization techniques.\n\nKey References and Excerpts:\n\n1. From \"Transformers are SSMs\":\n\"We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices.\"\n\n2. From \"Hungry Hungry Hippos\":\n\"SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence [...] FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K\"\n\n3. From \"Repeat After Me\":\n\"GSSMs are fundamentally limited by their fixed-size latent state [...] transformers can copy strings of exponential length\"\n\nKey Insights:\n\n1. State Space Duality:\n- Enables both linear and quadratic computation forms\n- Provides theoretical connections between SSMs and attention\n- Offers potential for hardware-efficient implementations\n\n2. Memory Challenges:\n- SSMs face limitations in token recall and comparison\n- Fixed-size latent state poses constraints\n- Need for better memory management strategies\n\n3. Implementation Considerations:\n- Block FFT algorithms show promise for efficiency\n- Structured matrices can improve hardware utilization\n- Trade-offs between memory usage and model expressiveness\n\nFuture Search Plan:\n\n1. Need more information about:\n- Structured matrix implementations and optimizations\n- Memory compression techniques in language models\n- Hardware-aware optimization strategies\n\n2. Specific areas to investigate:\n- Recent advances in efficient attention mechanisms\n- State compression techniques\n- Hardware optimization approaches\n\nReferences:\n\n1. Gu, A., & Dao, T. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. ICLR 2022.\n\n3. Jelassi, S., Brandfonbrener, D., Kakade, S., & Malach, E. (2024). Repeat After Me: Transformers are Better than State Space Models at Copying. ICML 2024.\n\n4. Gu, A., Gupta, A., Goel, K., & R\u00e9, C. (2022). On the Parameterization and Initialization of Diagonal State Space Models. NeurIPS 2022."
        },
        {
            "ready": false,
            "query": "hardware optimization transformer acceleration",
            "detail": "Search for papers discussing:\n1. Hardware acceleration techniques for transformer models\n2. Implementation strategies for efficient attention mechanisms\n3. Memory optimization in neural architectures\n4. State space model architectures and implementations\n5. Empirical studies on model compression effects",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hardware acceleration techniques for transformer models\n2. Implementation strategies for efficient attention mechanisms\n3. Memory optimization in neural architectures\n4. State space model architectures and implementations\n5. Empirical studies on model compression effects\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.97)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.97)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 3. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.89)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.89)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.84)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.84)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 5. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.76)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.76)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hardware optimization transformer acceleration\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 73  (*Influential: 13*)\n\n#### 2. Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Chong Yu, Tao Chen, Zhongxue Gan*\n\n**TL;DR:** GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and de-coder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks.\n\n**Abstract:** Along with the performance improvement in NLP domain, the sizes of transformer-based language models ( TLM ) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging. This paper thoroughly designs a compression scheme named GPUSQ-TLM to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization characters . Especially, a dense TLM model is first pruned to meet the GPU\u2019s acceleration constraint of sparse patterns with FP16 type, then it is further quantized into a fixed-point one by quantization-aware training, to provide an extra speedup for integer tensors on GPU. A mixed-strategy knowledge distillation of labels, logits and feature maps is used for best accuracy compensation during pruning and quantization process. Experiment results show GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and de-coder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks. Moreover, GPUSQ-TLM can boost actual deployment performance by up to 4.08-4.25 \u00d7 times latency and 6.18-6.79 \u00d7 throughput on A100 GPU.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. DevFormer: A Symmetric Transformer for Context-Aware Device Placement\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Haeyeon Kim, Minsu Kim, Federico Berto, Joungho Kim, Jinkyoo Park*\n\n**TL;DR:** This paper applies DevFoemer to the problem of decoupling capacitor placement and shows that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\\%$.\n\n**Abstract:** In this paper, we present DevFormer, a novel transformer-based architecture for addressing the complex and computationally demanding problem of hardware design optimization. Despite the demonstrated efficacy of transformers in domains including natural language processing and computer vision, their use in hardware design has been limited by the scarcity of offline data. Our approach addresses this limitation by introducing strong inductive biases such as relative positional embeddings and action-permutation symmetricity that effectively capture the hardware context and enable efficient design optimization with limited offline data. We apply DevFoemer to the problem of decoupling capacitor placement and show that it outperforms state-of-the-art methods in both simulated and real hardware, leading to improved performances while reducing the number of components by more than $30\\%$. Finally, we show that our approach achieves promising results in other offline contextual learning-based combinatorial optimization tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 4. ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Haoran You, Huihong Shi, Yipin Guo, Yingyan Lin*\n\n**TL;DR:** This work proposes to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed ShiftAddViT, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch.\n\n**Abstract:** Vision Transformers (ViTs) have shown impressive performance and have become a unified backbone for multiple vision tasks. However, both the attention mechanism and multi-layer perceptrons (MLPs) in ViTs are not sufficiently efficient due to dense multiplications, leading to costly training and inference. To this end, we propose to reparameterize pre-trained ViTs with a mixture of multiplication primitives, e.g., bitwise shifts and additions, towards a new type of multiplication-reduced model, dubbed $\\textbf{ShiftAddViT}$, which aims to achieve end-to-end inference speedups on GPUs without requiring training from scratch. Specifically, all $\\texttt{MatMuls}$ among queries, keys, and values are reparameterized using additive kernels, after mapping queries and keys to binary codes in Hamming space. The remaining MLPs or linear layers are then reparameterized with shift kernels. We utilize TVM to implement and optimize those customized kernels for practical hardware deployment on GPUs. We find that such a reparameterization on attention maintains model accuracy, while inevitably leading to accuracy drops when being applied to MLPs. To marry the best of both worlds, we further propose a new mixture of experts (MoE) framework to reparameterize MLPs by taking multiplication or its primitives as experts, e.g., multiplication and shift, and designing a new latency-aware load-balancing loss. Such a loss helps to train a generic router for assigning a dynamic amount of input tokens to different experts according to their latency. Extensive experiments on various 2D/3D Transformer-based vision tasks consistently validate the effectiveness of our proposed ShiftAddViT, achieving up to $\\textbf{5.18$\\times$}$ latency reductions on GPUs and $\\textbf{42.9}$% energy savings, while maintaining a comparable accuracy as original or efficient ViTs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 1*)\n\n#### 5. Cheaply Estimating Inference Efficiency Metrics for Autoregressive Transformer Models\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Deepak Narayanan, Keshav Santhanam, Peter Henderson, Rishi Bommasani, Tony Lee, Percy Liang*\n\n**TL;DR:** A new metric for inference efficiency called idealized runtime is proposed, that puts models on equal footing as though they were served on uniform hardware and software without performance contention, and a cost model is proposed to efficiently estimate this metric for autoregressive Transformer models.\n\n**Abstract:** Large language models (LLMs) are highly capable but also computationally expensive. Characterizing the fundamental tradeoff between inference efficiency and model capabilities is thus important, but requires an efficiency metric that is comparable across models from different providers. Unfortunately, raw runtimes measured through black-box APIs do not satisfy this property: model providers can implement software and hardware optimizations orthogonal to the model, and shared infrastructure introduces performance contention. We propose a new metric for inference efficiency called idealized runtime , that puts models on equal footing as though they were served on uniform hardware and software without performance contention, and a cost model to efficiently estimate this metric for autoregressive Transformer models. We also propose variants of the idealized runtime that incorporate the number and type of accelerators needed to serve the model. Using these metrics, we compare ten LLMs developed in 2022 to provide the first analysis of inference efficiency-capability tradeoffs; we make several observations from this analysis, including the fact that the superior inference runtime performance of certain APIs is often a byproduct of optimizations within the API rather than the underlying model. Our code is open sourced at https:\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Rethinking Vision Transformers for MobileNet Size and Speed\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Jian Ren, Sergey Tulyakov, Yanzhi Wang, Kamyar Salahi, Georgios Evangelidis, Yang Wen, Ju Hu, Yanyu Li*\n\n**Abstract:** With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The proposed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that properly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2022-12-15\n\n\n\n#### 2. Scalable Second Order Optimization for Deep Learning\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Rohan Anil, Kevin Regan, Yoram Singer, Vineet Gupta, Tomer Koren*\n\n**Abstract:** Optimization in machine learning, both theoretical and applied, is presently dominated by first-order gradient methods such as stochastic gradient descent. Second-order optimization methods, that involve second derivatives and/or second order statistics of the data, are far less prevalent despite strong theoretical properties, due to their prohibitive computation, memory and communication costs. In an attempt to bridge this gap between theoretical and practical optimization, we present a scalable implementation of a second-order preconditioned method (concretely, a variant of full-matrix Adagrad), that along with several critical algorithmic and numerical improvements, provides significant convergence and wall-clock time improvements compared to conventional first-order methods on state-of-the-art deep models. Our novel design effectively utilizes the prevalent heterogeneous hardware architecture for training deep models, consisting of a multicore CPU coupled with multiple accelerator units. We demonstrate superior performance compared to state-of-the-art on very large learning tasks such as machine translation with Transformers, language modeling with BERT, click-through rate prediction on Criteo, and image classification on ImageNet with ResNet-50.\n\n**Published:** 2020-02-20\n\n\n\n#### 3. Q8BERT: Quantized 8Bit BERT\n\n*From Search Query: hardware optimization transformer acceleration*\n\n*Peter Izsak, Ofir Zafrir, Moshe Wasserblat, Guy Boudoukh*\n\n**Abstract:** Recently, pre-trained Transformer based language models such as BERT and GPT, have shown great improvement in many Natural Language Processing (NLP) tasks. However, these models contain a large amount of parameters. The emergence of even larger and more accurate models such as GPT2 and Megatron, suggest a trend of large pre-trained Transformer models. However, using these large models in production environments is a complex task requiring a large amount of compute, memory and power resources. In this work we show how to perform quantization-aware training during the fine-tuning phase of BERT in order to compress BERT by $4\\times$ with minimal accuracy loss. Furthermore, the produced quantized model can accelerate inference speed if it is optimized for 8bit Integer supporting hardware.\n\n**Published:** 2019-10-14\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hardware optimization, efficient attention mechanisms, memory optimization, state space model architectures, and empirical studies on model compression, here are some key findings and strategies from the provided sources and additional insights:\n\n## Hardware Acceleration Techniques for Transformer Models\n\n- **2:4 Sparsity in Transformer Training**: The use of 2:4 sparse matrix multiplication can significantly accelerate the training of transformers. This method leverages NVIDIA Ampere GPUs to execute sparse matrix operations faster than their dense equivalents. Techniques such as modifying the sparse-refined straight-through estimator, determining a feasible decay factor, and employing dense fine-tuning near the end of pre-training can preserve accuracy.\n\n- **Optimum Intel and OpenVINO**: Utilizing Optimum Intel and OpenVINO can optimize Transformer models for Intel architectures. This involves converting models to OpenVINO Intermediate Representation (IR) format and applying quantization methods like Post-Training Quantization (PTQ) and Quantization-Aware Training (QAT) to reduce memory usage and improve inference speed.\n\n- **Spatial Architecture Mapping for Transformers (SAMT)**: SAMT is a framework that optimizes the dataflow mapping of Transformer inference workloads onto spatial accelerators like TPUs. This approach reduces inference latency and energy consumption by dynamically fusing operators and co-searching optimal dataflow mapping strategies.\n\n## Implementation Strategies for Efficient Attention Mechanisms\n\n- **FlashAttention-2**: This method introduces additional parallelization over the sequence length and efficient work partitioning among GPU threads. It minimizes communication overhead and reduces shared memory reads and writes, leading to faster processing of attention mechanisms.\n\n- **Structured Sparsity and Hybrid Attention**: Implementing structured sparsity patterns in attention mechanisms can maintain performance while reducing computational costs. Hybrid attention mechanisms that combine different types of attention (e.g., local and long-term attention) can also be effective[Analysis Note].\n\n## Memory Optimization in Neural Architectures\n\n- **Quantization**: Quantizing models to lower precision (e.g., from FP32 to INT8) using frameworks like NNCF can significantly reduce memory usage and improve inference efficiency. However, it is crucial to ensure that the deployed platform supports the necessary hardware features to avoid accuracy drops.\n\n- **Dynamic Context Pruning and Token Compression**: Techniques like dynamic context pruning and token compression can reduce memory requirements. These methods involve eliminating uninformative tokens or compressing tokens to fixed-size representations, enabling more efficient scaling[Analysis Note].\n\n## State Space Model Architectures and Implementations\n\n- **Efficient State Updates**: Managing state efficiently is crucial for long sequences. Implementations that focus on compressive memory and efficient state updates can help in maintaining performance while reducing memory usage. For example, incorporating masked local attention and long-term linear attention mechanisms can be beneficial[Analysis Note].\n\n## Empirical Studies on Model Compression Effects\n\n- **Compression Impact on Performance**: Studies have shown that while compression techniques like quantization and pruning can reduce memory and computational costs, they must be balanced against potential performance losses. Techniques like dense fine-tuning after sparse training can help mitigate these losses.\n\n- **Empirical Results on SAMT**: The SAMT framework has been empirically shown to reduce inference latency by 12% to 91% and energy consumption by 3% to 23% compared to traditional spatial accelerator designs. This demonstrates the effectiveness of optimized dataflow mapping and operator fusion in real-world scenarios.\n\nBy integrating these strategies, researchers can develop more efficient, scalable, and performant autoregressive language models that leverage hardware optimizations, efficient attention mechanisms, and effective memory and state management techniques.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results provide valuable insights into memory optimization, efficient attention mechanisms, and hardware-aware implementations. Key themes emerge around:\n\na) Memory Efficiency:\n- Dynamic context pruning\n- Token compression techniques\n- Fixed-size memory representations\n\nb) Attention Optimization:\n- Structured sparsity patterns\n- Memory-efficient attention variants\n- Hardware-aware implementations\n\nc) State Management:\n- Compression techniques\n- Efficient state updates\n- Memory-bounded operations\n\n2. Useful References with Excerpts:\n\nFrom \"Dynamic Context Pruning\":\n\"Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process.\"\n\nFrom \"Leave No Context Behind\":\n\"Incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block.\"\n\nFrom \"Focus on the Core\":\n\"Token pruning eliminates less important tokens in the attention mechanism's key and value as they pass through the layers.\"\n\n3. Key Insights:\n\na) Memory Management:\n- Dynamic pruning can significantly reduce memory requirements\n- Token compression offers efficient scaling\n- Fixed-size memory representations enable predictable resource usage\n\nb) Attention Optimization:\n- Hybrid attention mechanisms show promise\n- Structured sparsity can maintain performance while reducing computation\n- Hardware-aware implementations crucial for efficiency\n\nc) Implementation Considerations:\n- Need to balance compression vs. performance\n- Hardware-specific optimizations important\n- State management crucial for long sequences\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Hardware-specific optimization techniques\n2. State space model implementations\n3. Compression impact on model performance\n\nNext search should focus on:\n- Hardware acceleration techniques\n- State space model architectures\n- Empirical studies on compression effects\n\n5. References:\n\n[1] Anagnostidis, S., et al. (2023). \"Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers.\" Neural Information Processing Systems.\n\n[2] Munkhdalai, T., et al. (2024). \"Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention.\" arXiv.org.\n\n[3] Liu, H., & Abbeel, P. (2023). \"Blockwise Parallel Transformer for Large Context Models.\"\n\n[4] Fu, T., et al. (2024). \"MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression.\" arXiv.org.\n\n[5] Lee-Thorp, J., et al. (2021). \"Fnet: Mixing tokens with fourier transforms.\" NAACL.\n\nFurther investigation needed to fully understand hardware optimization and implementation details."
        }
    ],
    "rating": 4.35,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.9388110000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.9346080000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "GatedTTTLinear",
    "proposal": "# DualStateGatedTTTLinear: Enhancing GatedTTTLinear with State Space Duality and Structured Matrix Optimizations\n\n## Motivation\n\nThe GatedTTTLinear GAU has demonstrated strong performance in test-time training scenarios by combining gating mechanisms with graph-inspired convolutions. However, its computational efficiency and ability to handle long sequences can be further improved. Recent advances in state space models and structured matrices suggest opportunities for enhancement through:\n\n1. State space duality (SSD) for efficient computation\n2. Structured matrix optimizations for parallel processing\n3. Memory-efficient implementations for handling long sequences\n\nThis proposal introduces DualStateGatedTTTLinear, which enhances the original GatedTTTLinear architecture by incorporating these advances while maintaining its core strengths in test-time adaptation.\n\n## Related Work\n\nRecent developments informing this design include:\n\n1. **State Space Duality** (Dao & Gu, 2024):\n   - Provides framework for efficient computation of state space models\n   - Enables both linear and quadratic forms of computation\n   - Demonstrates connection between SSMs and attention mechanisms\n\n2. **Structured Matrices** (Lee & Kim, 2023):\n   - Introduces differentiable learning of structured matrices\n   - Enables efficient hardware utilization\n   - Reduces memory and computational requirements\n\n3. **Memory Optimization** (Shah et al., 2020):\n   - Presents techniques for minimizing memory footprint\n   - Optimizes computational overhead\n   - Enables efficient handling of long sequences\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Computational Efficiency**:\n   - Standard implementation requires dense matrix operations\n   - Limited parallelization potential\n   - Suboptimal hardware utilization\n\n2. **Memory Usage**:\n   - High memory requirements for long sequences\n   - Inefficient storage of intermediate states\n   - Limited ability to process long contexts\n\n3. **Scalability**:\n   - Performance degradation with sequence length\n   - Increased computational cost for larger models\n   - Memory constraints limiting model size\n\n### Mathematical Perspective\n\nLet X \u2208 \u211d^(B\u00d7L\u00d7D) represent input embeddings, where:\n- B: batch size\n- L: sequence length\n- D: embedding dimension\n\nThe current GatedTTTLinear computes:\n\nY = \u03c3(XW_g + b_g) \u2299 (XW + b)\n\nwhere W_g, W are learned matrices and \u03c3 is the sigmoid function.\n\n## Design Plan\n\n### Selection of GAU to Modify\n\n```selection GatedTTTLinear```\n\n### Proposed Modifications\n\n1. **Dual-Form Computation**:\n   - Introduce state space duality for efficient computation\n   - Enable switching between linear and quadratic forms\n   - Optimize for different sequence lengths\n\n2. **Structured Matrix Integration**:\n   - Replace dense weight matrices with structured alternatives\n   - Implement block-diagonal decompositions\n   - Enable efficient parallel processing\n\n3. **Memory-Efficient Implementation**:\n   - Optimize state storage through compression\n   - Implement selective computation strategies\n   - Reduce memory footprint for long sequences\n\n### Mathematical Formulation\n\n1. **Dual State Representation**:\n   ```python\n   class DualStateGatedTTTLinear(nn.Module):\n       def __init__(self, embed_dim, state_dim):\n           self.linear_form = StateSpaceLinear(embed_dim, state_dim)\n           self.quadratic_form = StateSpaceQuadratic(embed_dim, state_dim)\n           self.form_selector = FormSelector(embed_dim)\n   ```\n\n2. **Structured Weight Matrices**:\n   ```python\n   class StructuredWeightMatrix(nn.Module):\n       def __init__(self, dim, block_size):\n           self.blocks = nn.ModuleList([\n               nn.Linear(block_size, block_size) \n               for _ in range(dim // block_size)\n           ])\n   ```\n\n3. **Memory-Efficient Processing**:\n   ```python\n   class EfficientStateProcessor(nn.Module):\n       def __init__(self, dim):\n           self.compressor = StateCompressor(dim)\n           self.processor = StreamProcessor(dim)\n   ```\n\n### Implementation Guidelines\n\n1. **Core Architecture**:\n```python\nclass DualStateGatedTTTLinear(nn.Module):\n    def __init__(self, embed_dim, state_dim, block_size):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.state_dim = state_dim\n        \n        # Structured matrices\n        self.gate_proj = StructuredWeightMatrix(embed_dim, block_size)\n        self.value_proj = StructuredWeightMatrix(embed_dim, block_size)\n        \n        # Dual state processing\n        self.state_processor = DualStateProcessor(\n            embed_dim=embed_dim,\n            state_dim=state_dim\n        )\n        \n        # Memory optimization\n        self.memory_manager = EfficientMemoryManager(\n            dim=embed_dim,\n            compression_ratio=0.5\n        )\n\n    def forward(self, X, state=None):\n        # Select computation form based on sequence length\n        if X.shape[1] > self.switch_threshold:\n            return self._linear_forward(X, state)\n        return self._quadratic_forward(X, state)\n```\n\n2. **Efficient State Processing**:\n```python\nclass DualStateProcessor(nn.Module):\n    def __init__(self, embed_dim, state_dim):\n        self.linear_processor = LinearStateProcessor(\n            embed_dim=embed_dim,\n            state_dim=state_dim\n        )\n        self.quadratic_processor = QuadraticStateProcessor(\n            embed_dim=embed_dim,\n            state_dim=state_dim\n        )\n        \n    def forward(self, X, form='linear'):\n        processor = (self.linear_processor if form == 'linear' \n                    else self.quadratic_processor)\n        return processor(X)\n```\n\n3. **Memory Optimization**:\n```python\nclass EfficientMemoryManager(nn.Module):\n    def __init__(self, dim, compression_ratio):\n        self.compressor = StateCompressor(dim)\n        self.decompressor = StateDecompressor(dim)\n        self.ratio = compression_ratio\n        \n    def compress_state(self, state):\n        return self.compressor(state, self.ratio)\n        \n    def decompress_state(self, compressed_state):\n        return self.decompressor(compressed_state)\n```\n\n### Integration Steps\n\n1. **Initialize Architecture**:\n   - Set up structured matrices\n   - Configure dual state processor\n   - Initialize memory manager\n\n2. **Forward Pass Processing**:\n   - Select computation form\n   - Process input through structured matrices\n   - Apply gating mechanism\n   - Manage memory efficiently\n\n3. **State Management**:\n   - Compress states when needed\n   - Process through appropriate form\n   - Decompress for output\n\n4. **Optimization**:\n   - Implement parallel processing\n   - Optimize memory usage\n   - Enable efficient backpropagation\n\n## Theoretical Analysis\n\n1. **Computational Complexity**:\n   - Linear form: O(N + L) operations\n   - Quadratic form: O(L\u00b2) operations\n   - Automatic selection based on sequence length\n\n2. **Memory Requirements**:\n   - Structured matrices: O(N/k) parameters\n   - State compression: O(N*c) memory\n   - Where k is block size and c is compression ratio\n\n3. **Expected Benefits**:\n   - Reduced computational complexity\n   - Lower memory usage\n   - Improved parallel processing\n   - Better hardware utilization\n\n## Conclusion\n\nDualStateGatedTTTLinear enhances the original GatedTTTLinear architecture through:\n1. Efficient dual-form computation\n2. Structured matrix optimizations\n3. Memory-efficient implementation\n4. Hardware-aware design\n\nThese improvements enable better handling of long sequences while maintaining the benefits of test-time training adaptation.\n\n## References\n\n1. Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv preprint.\n\n2. Lee, C., & Kim, H. (2023). \"Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks.\" International Conference on Learning Representations.\n\n3. Shah, A., et al. (2020). \"Memory Optimization for Deep Networks.\" International Conference on Learning Representations.\n\n4. Gu, A., et al. (2022). \"How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections.\" International Conference on Learning Representations.\n\n5. Wang, Y., et al. (2023). \"H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training.\" Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "gpt4o_0806",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "Enhancement of GatedTTTLinear GAU by integrating state space duality and structured matrix optimizations, enabling efficient parallel processing and memory optimization through dual-form computation and hardware-aware implementations.",
    "ideation": null,
    "modelname": "dualstategttt",
    "suggestions": "1. IMPLEMENTATION OPTIMIZATION:\n- Provide more specific guidelines for handling form switching\n- Detail strategies for efficient memory access patterns\n- Include benchmarking methodology for performance evaluation\n- Add specific hardware optimization guidelines\n\n2. THEORETICAL FOUNDATIONS:\n- Expand analysis of dual-form stability\n- Provide more detailed complexity analysis for different sequence lengths\n- Include theoretical bounds on compression impact\n- Add analysis of potential impact on model expressivity\n\n3. PRACTICAL CONSIDERATIONS:\n- Add guidelines for hyperparameter tuning\n- Include failure case analysis and mitigation strategies\n- Provide more detailed memory usage analysis\n- Add guidelines for handling edge cases\n\n4. HARDWARE OPTIMIZATION:\n- Detail specific optimizations for different hardware architectures\n- Include memory bandwidth analysis\n- Provide guidelines for efficient parallel implementation\n- Add specific tensor core utilization strategies\n\n5. VALIDATION AND TESTING:\n- Add unit test guidelines for critical components\n- Include integration test strategies\n- Provide performance profiling guidelines\n- Detail validation methodology for dual-form computation",
    "user_input": ""
}