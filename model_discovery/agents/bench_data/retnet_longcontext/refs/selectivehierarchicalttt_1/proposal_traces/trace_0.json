{
    "variantname": null,
    "review": "The SelectiveHierarchicalTTT proposal presents a sophisticated approach to enhancing language model blocks by combining butterfly algorithms, selective state space modeling, and hardware-efficient memory hierarchy management. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Theoretical Foundation\n- Well-grounded integration of butterfly algorithms with state space modeling\n- Clear mathematical formulation of hierarchical processing\n- Thoughtful consideration of hardware characteristics\n- Strong connection to current research in efficient language modeling\n\n2. Innovation\n- Novel combination of butterfly-based processing with selective state space modeling\n- Unique approach to memory hierarchy optimization\n- Distinguished from existing work through hardware-aware design\n- Original integration of block-based processing with butterfly algorithms\n\n3. Technical Design\n- Detailed mathematical formulations\n- Clear implementation guidelines with pseudo-code\n- Well-structured architecture with distinct components\n- Careful consideration of hardware efficiency\n\n4. Efficiency Considerations\n- Hardware-efficient butterfly algorithms reduce computation\n- Block-based processing optimizes memory usage\n- Selective computation reduces unnecessary operations\n- Memory hierarchy optimization improves cache utilization\n\nCONCERNS:\n\n1. Implementation Complexity\n- Complex interaction between butterfly and selective mechanisms\n- Challenging integration with existing architectures\n- Additional hyperparameters require careful tuning\n- Potential overhead from butterfly transforms\n\n2. Memory Management\n- Trade-off between block size and processing efficiency\n- Need for efficient state management\n- Complexity of maintaining multiple processing paths\n- Risk of memory fragmentation\n\n3. Hardware Optimization\n- Hardware-specific implementations may limit portability\n- Complex memory access patterns\n- Need for efficient parallel processing\n- Implementation challenges on different architectures\n\n4. Scalability Considerations\n- Impact of butterfly operations on very long sequences\n- Memory hierarchy efficiency at scale\n- Communication overhead in distributed settings\n- Balance between compression and model quality\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than FlashFFTConv's tensor core optimization\n2. More efficient than Block-State Transformers' hybrid approach\n3. More hardware-aware than Linear Transformers with learnable kernels\n4. Novel integration of butterfly algorithms with selective computation\n\nHowever, it shares some concepts with:\n1. Memory optimization techniques from MODeL\n2. Block-based processing from HAT\n3. Selective computation from Mamba\n4. Hardware-aware design from FlashAttention",
    "search_stack": [
        {
            "ready": false,
            "query": "selective computation routing language models",
            "detail": "Find implementations and analysis of selective computation and routing mechanisms in language models, particularly focusing on memory efficiency and adaptive processing.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind implementations and analysis of selective computation and routing mechanisms in language models, particularly focusing on memory efficiency and adaptive processing.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.14)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.14)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.07)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.07)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 3. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.04)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 23/43 (Score: 0.04)*\n\n```\nIn Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 5484-5495, 2021. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. Alan L Hodgkin and Andrew F Huxley. A quantitative description of membrane current and its application to conduction and excitation in nerve. The J. of Physiol., 117(4):500-544, 1952. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Mark Horowitz. 1.1 computing's energy problem (and what we can do about it). In 2014 IEEE international solid-state circuits conference digest of technical papers (ISSCC), pp. 10-14. IEEE, 2014. Jean Kaddour. The minipile challenge for data-efficient language models. arXiv preprint arXiv:2304.08442, 2023 . Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\n```\n\n#### 4. Efficient Beam Tree Recursion (Avg. Score: 0.01)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** These proposals standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n**Abstract:** Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n##### *Relevant Chunk: No. 19/50 (Score: 0.01)*\n\n```\nIn Proceedings of International Conference on Neural Networks (ICNN'96), volume 1, pages 347-352 vol.1, 1996. doi: 10.1109/ICNN.1996.548916. [25] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983\n[26] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [27] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022. [28] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-2017. URL https://aclanthology.org/N18-2017. [29] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11\n[30] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908-15919, 2021. [31] Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1118-1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1115. URLhttps://aclanthology org/N19-1115\n[32] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 908-921, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.74. URL https://aclanthology.org/2021 acl-long. 74\n[33] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735\n[34] Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling.\n```\n\n#### 5. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.01)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 11/24 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2001.04451, 2020. [17] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [18] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective computation routing language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models\n\n*From Search Query: selective computation routing language models*\n\n*Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou*\n\n**TL;DR:** ZOOTER is proposed, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it, and shows computation efficiency in inference.\n\n**Abstract:** The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate on it by mining latent expertise with off-the-shelf reward models. We propose ZOOTER, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. ZOOTER shows computation efficiency in inference as it only introduces minor computation overhead of a routing function compared with reward model ranking methods. We evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different domains and tasks. ZOOTER outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 2. No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models\n\n*From Search Query: selective computation routing language models*\n\n*Jean Kaddour, Oscar Key, Piotr Nawrot, Pasquale Minervini, Matt J. Kusner*\n\n**TL;DR:** An evaluation protocol is defined that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which is called reference system time and is discussed in this work.\n\n**Abstract:** The computation necessary for training Transformer-based language models has skyrocketed in recent years. This trend has motivated research on efficient training algorithms designed to improve training, validation, and downstream performance faster than standard training. In this work, we revisit three categories of such algorithms: dynamic architectures (layer stacking, layer dropping), batch selection (selective backprop, RHO loss), and efficient optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed computation budget using such methods, we find that their training, validation, and downstream gains vanish compared to a baseline with a fully-decayed learning rate. We define an evaluation protocol that enables computation to be done on arbitrary machines by mapping all computation time to a reference machine which we call reference system time. We discuss the limitations of our proposed protocol and release our code to encourage rigorous research in efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 31  (*Influential: 2*)\n\n#### 3. Compressing Context to Enhance Inference Efficiency of Large Language Models\n\n*From Search Query: selective computation routing language models*\n\n*Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin*\n\n**TL;DR:** Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used, indicating that it strikes a good balance between efficiency and performance.\n\n**Abstract:** Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 5*)\n\n#### 4. Unified Scaling Laws for Routed Language Models\n\n*From Search Query: selective computation routing language models*\n\n*Aidan Clark, Diego de Las Casas, Aurelia Guy, A. Mensch, Michela Paganini, Jordan Hoffmann, Bogdan Damoc, Blake A. Hechtman, Trevor Cai, Sebastian Borgeaud, George van den Driessche, Eliza Rutherford, T. Hennigan, Matthew G. Johnson, Katie Millican, Albin Cassirer, Chris Jones, Elena Buchatskaya, D. Budden, L. Sifre, Simon Osindero, O. Vinyals, Jack W. Rae, Erich Elsen, K. Kavukcuoglu, K. Simonyan*\n\n**TL;DR:** This work derives and justifies scaling laws defined on parameter count and computational requirement which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques.\n\n**Abstract:** The performance of a language model has been shown to be effectively modeled as a power-law in its parameter count. Here we study the scaling behaviors of Routing Networks: architectures that conditionally use only a subset of their parameters while processing an input. For these models, parameter count and computational requirement form two independent axes along which an increase leads to better performance. In this work we derive and justify scaling laws defined on these two variables which generalize those known for standard language models and describe the performance of a wide range of routing architectures trained via three different techniques. Afterwards we provide two applications of these laws: first deriving an Effective Parameter Count along which all models scale at the same rate, and then using the scaling coefficients to give a quantitative comparison of the three routing techniques considered. Our analysis derives from an extensive evaluation of Routing Networks across five orders of magnitude of size, including models with hundreds of experts and hundreds of billions of parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 140  (*Influential: 18*)\n\n#### 5. MT-PATCHER: Selective and Extendable Knowledge Distillation from Large Language Models for Machine Translation\n\n*From Search Query: selective computation routing language models*\n\n*Jiahuan Li, Shanbo Cheng, Shujian Huang, Jiajun Chen*\n\n**TL;DR:** A framework called MT-Patcher is proposed, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner, and synthesized potential errors and diverse contexts further improve MT performances on unseen contexts and words.\n\n**Abstract:** Large Language Models (LLM) have demonstrated their strong ability in the field of machine translation, yet they suffer from high computational cost and latency. Therefore, transferring translation knowledge from giant LLMs to medium-sized machine translation models is a promising research direction. However, traditional knowledge distillation methods ignore the capability of student and teacher models, therefore repeatedly teaching student models on the knowledge they have learned, and failing to extend to novel contexts and knowledge. In this paper, we propose a framework called MT-Patcher, which transfers knowledge from LLMs to existing MT models in a selective, comprehensive and proactive manner. Considering the current translation ability of student MT models, we only identify and correct their translation errors, instead of distilling the whole translation from the teacher. Leveraging the strong language abilities of LLMs, we instruct LLM teachers to synthesize diverse contexts and anticipate more potential errors for the student. Experiment results on translating both specific language phenomena and general MT benchmarks demonstrate that finetuning the MT model on about 10% examples can achieve comparable results to the traditional knowledge distillation method, and synthesized potential errors and diverse contexts further improve MT performances on unseen contexts and words.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n\n*From Search Query: selective computation routing language models*\n\n*Noam Shazeer, Barret Zoph, William Fedus*\n\n**Abstract:** In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n\n**Published:** 2021-01-11\n\n\n\n#### 2. VMamba: Visual State Space Model\n\n*From Search Query: selective computation routing language models*\n\n*Yunfan Liu, Qixiang Ye, YaoWei Wang, Lingxi Xie, Hongtian Yu, Yuzhong Zhao, Yunjie Tian, Yue Liu*\n\n**Abstract:** Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity. At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D helps bridge the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the gathering of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments showcase VMamba's promising performance across diverse visual perception tasks, highlighting its advantages in input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba.\n\n**Published:** 2024-01-18\n\n\n\n#### 3. Image Synthesis with Adversarial Networks: a Comprehensive Survey and Case Studies\n\n*From Search Query: selective computation routing language models*\n\n*Jie Yang, M. Emre Celebi, Ruili Wang, Huiyu Zhou, Eric Granger, Masoumeh Zareapoor, Pourya Shamsolmoali*\n\n**Abstract:** Generative Adversarial Networks (GANs) have been extremely successful in various application domains such as computer vision, medicine, and natural language processing. Moreover, transforming an object or person to a desired shape become a well-studied research in the GANs. GANs are powerful models for learning complex distributions to synthesize semantically meaningful samples. However, there is a lack of comprehensive review in this field, especially lack of a collection of GANs loss-variant, evaluation metrics, remedies for diverse image generation, and stable training. Given the current fast GANs development, in this survey, we provide a comprehensive review of adversarial models for image synthesis. We summarize the synthetic image generation methods, and discuss the categories including image-to-image translation, fusion image generation, label-to-image mapping, and text-to-image translation. We organize the literature based on their base models, developed ideas related to architectures, constraints, loss functions, evaluation metrics, and training datasets. We present milestones of adversarial models, review an extensive selection of previous works in various categories, and present insights on the development route from the model-based to data-driven methods. Further, we highlight a range of potential future research directions. One of the unique features of this review is that all software implementations of these GAN methods and datasets have been collected and made available in one place at https://github.com/pshams55/GAN-Case-Study.\n\n**Published:** 2020-12-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on selective computation, routing mechanisms, memory efficiency, and adaptive processing, here are some key findings and implementations from the provided sources:\n\n## Selective Computation Mechanisms\n\n### XMoE: Sparse Models with Fine-grained and Adaptive Expert Selection\nThe XMoE model introduces a sparse Mixture-of-Experts (MoE) approach that enhances both the efficacy and efficiency of sparse MoE models. XMoE uses small experts and a threshold-based router to enable tokens to selectively engage only essential parameters, reducing computational load by over 50% without sacrificing performance. This approach is particularly relevant for reducing unnecessary computations and improving memory efficiency.\n\n### FiRST: Finetuning Router-Selective Transformers\nFiRST proposes an algorithm that reduces inference latency by using layer-specific routers to select a subset of transformer layers adaptively for each input sequence. This method allows for input-adaptive latency reduction while preserving compatibility with KV caching, which is crucial for speed-ups in modern frameworks. This adaptive layer selection can be integrated into existing autoregressive models to enhance efficiency and reduce latency.\n\n## Adaptive Routing Mechanisms\n\n### XMoE\nIn addition to selective computation, XMoE's threshold-based router enables adaptive routing by dynamically selecting which experts (or parameters) to engage based on the input tokens. This mechanism ensures that only the most relevant parameters are involved in computations, making the model more efficient and adaptive.\n\n### FiRST\nFiRST's layer-specific routers adaptively select which transformer layers to use for each input sequence, based on the prompt during the prefill stage. This adaptive routing mechanism is critical for reducing latency and maintaining performance, as different task-specific middle layers play a crucial role in evolving hidden representations depending on the task.\n\n## Memory-Efficient Architectures\n\n### Hierarchical and Sparse Models\nThe integration of hierarchical processing (as seen in HierarchicalFastTTTLinear and HierarchicalMamba) with sparse models like XMoE can enhance memory efficiency. XMoE's approach to using small experts and selective engagement of parameters can be combined with hierarchical state space modeling to achieve better memory efficiency and adaptive processing.\n\n### Temporal Latent Bottlenecks\nThe concept of temporal latent bottlenecks from HierarchicalFastTTTLinear can be combined with adaptive routing mechanisms to further optimize memory usage. These bottlenecks help in compressing states at different temporal scales, which can be integrated with the selective computation and routing mechanisms discussed above[References: Didolkar et al., 2022].\n\n## Practical Implementations and Analysis\n\n### Retrieval-Augmented Generation (RAG)\nWhile RAG is primarily focused on augmenting LLMs with external knowledge, the modular RAG approach and techniques like iterative retrieval and recursive retrieval can provide insights into how to design more adaptive and efficient retrieval and processing mechanisms. These techniques can be adapted to improve the selective computation and routing in autoregressive models, especially in scenarios where external knowledge is crucial.\n\n### Selective Retrieval in Code Completion\nThe approach of using selective retrieval, as seen in the Repoformer model for code completion, demonstrates how to determine whether retrieval is necessary and how to emit special tokens to decide on retrieval. This selective retrieval mechanism can be generalized to other autoregressive tasks, ensuring that computations and retrievals are only performed when necessary, thus enhancing efficiency and reducing latency.\n\nBy integrating these concepts\u2014selective computation through sparse MoE models, adaptive routing with layer-specific routers, memory-efficient architectures using hierarchical and temporal latent bottlenecks, and adaptive retrieval mechanisms\u2014the researcher can develop an autoregressive language model that is more efficient, adaptive, and scalable.\n",
            "analysis": "Summary of Analysis:\n\nI have analyzed both parent designs and identified their key characteristics:\n\nParent 1 - HierarchicalFastTTTLinear:\n- Uses hierarchical multi-scale processing with fast and slow streams\n- Implements temporal latent bottlenecks for memory efficiency\n- Employs adaptive state compression\n- Focuses on test-time training capabilities\n\nParent 2 - HierarchicalMamba:\n- Integrates hierarchical state space modeling into Mamba2Layer\n- Uses multiple temporal scales for processing\n- Implements efficient gating mechanisms\n- Maintains linear computational complexity\n\nKey Insights:\n\n1. Complementary Strengths:\n- HierarchicalFastTTTLinear excels at multi-scale processing and memory efficiency\n- HierarchicalMamba provides strong state space modeling and gating mechanisms\n- Both use hierarchical structures but in different ways\n\n2. Potential Integration Points:\n- Could combine FastTTT's temporal bottlenecks with Mamba's state space modeling\n- May integrate adaptive compression with hierarchical state updates\n- Opportunity to merge gating mechanisms from both approaches\n\n3. Areas for Improvement:\n- Memory efficiency could be enhanced by selective state updates\n- Processing streams could be made more adaptive\n- Gating mechanisms could be unified for better control\n\n4. Research Gaps:\n- Need to explore recent advances in selective computation\n- Should investigate memory-efficient state space models\n- Could benefit from studying adaptive routing mechanisms\n\nFuture Search Plan:\n1. Investigate selective computation and routing in language models\n2. Research memory-efficient state space architectures\n3. Explore adaptive gating mechanisms\n\nReferences:\n\n1. Didolkar, A., et al. (2022). \"Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning\". Neural Information Processing Systems.\n\n2. Bhirangi, R. M., et al. (2024). \"Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\". International Conference on Machine Learning.\n\n3. He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models\". arXiv preprint.\n\n4. Qin, Z., et al. (2023). \"Hierarchically Gated Recurrent Neural Network for Sequence Modeling\". Advances in Neural Information Processing Systems.\n\nNext Steps:\n1. Search for selective computation mechanisms\n2. Investigate memory-efficient architectures\n3. Study adaptive routing techniques"
        },
        {
            "ready": false,
            "query": "selective state space implementation memory efficiency",
            "detail": "Find specific implementations and technical details of selective state space models, focusing on memory efficiency and practical implementation approaches.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and technical details of selective state space models, focusing on memory efficiency and practical implementation approaches.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.97)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 2. Theoretical Foundations of Deep Selective State-Space Models (Avg. Score: 0.97)\n\n*Nicola Muca Cirone, Antonio Orvieto, Benjamin Walker, C. Salvi, Terry Lyons*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** Theoretical grounding is given to this recent finding that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\n\n**Abstract:** Structured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data. Deep SSMs demonstrate outstanding performance across a diverse set of domains, at a reduced training and inference cost compared to attention-based transformers. Recent developments show that if the linear recurrence powering SSMs allows for multiplicative interactions between inputs and hidden states (e.g. GateLoop, Mamba, GLA), then the resulting architecture can surpass in both in accuracy and efficiency attention-powered foundation models trained on text, at scales of billion parameters. In this paper, we give theoretical grounding to this recent finding using tools from Rough Path Theory: we show that when random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales. Our theory not only motivates the success of modern selective state-space models such as Mamba but also provides a solid framework to understand the expressive power of future SSM variants.\n\n##### *Relevant Chunk: No. 1/45 (Score: 0.97)*\n\n```\n# Theoretical Foundations of Deep Selective State-Space Models \n\nNicola Muca Cirone ${ }^{1}$ Antonio Orvieto ${ }^{2}$ Benjamin Walker ${ }^{3}$ Cristopher Salvi ${ }^{1}$ Terry Lyons ${ }^{3}$\n\n\n#### Abstract\n\nStructured state-space models (SSMs) such as S4, stemming from the seminal work of Gu et al., are gaining popularity as effective approaches for modeling sequential data.\n```\n\n#### 3. MambaLRP: Explaining Selective State Space Sequence Models (Avg. Score: 0.89)\n\n*F. Jafari, G. Montavon, Klaus-Robert M\u00fcller, Oliver Eberle*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work identifies specific components in the Mamba architecture, which cause unfaithful explanations and proposes MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components.\n\n**Abstract:** Recent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.\n\n##### *Relevant Chunk: No. 1/59 (Score: 0.89)*\n\n```\n# MambaLRP: Explaining Selective State Space Sequence Models \n\nFarnoush Rezaei Jafari ${ }^{1,2}$ Gr\u00e9goire Montavon ${ }^{3,2,1}$ Klaus-Robert M\u00fcller ${ }^{1,2,4,5,6} \\quad$ Oliver Eberle $^{1,2}$<br>${ }^{1}$ Machine Learning Group, Technische Universit\u00e4t Berlin, 10587 Berlin, Germany<br>${ }^{2}$ BIFOLD - Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany<br>${ }^{3}$ Department of Mathematics and Computer Science, Freie Universit\u00e4t Berlin,<br>Arnimallee 14, 14195 Berlin, Germany<br>${ }^{4}$ Department of Artificial Intelligence, Korea University, Seoul 136-713, South Korea<br>${ }^{5}$ Max Planck Institute for Informatics, Stuhlsatzenhausweg 4, 66123 Saarbr\u00fccken, Germany<br>${ }^{6}$ Google DeepMind, Berlin, Germany<br>Correspondence to: rezaeijafari@campus.tu-berlin.de, oliver.eberle@tu-berlin.de\n\n\n#### Abstract\n\nRecent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest.\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.74)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.74)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space implementation memory efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 363  (*Influential: 58*)\n\n#### 2. Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, A. Rudra, Christopher R'e*\n\n**TL;DR:** A simple sequence model inspired by control systems that generalizes RNN heuristics, temporal convolutions, and neural differential equations while addressing their shortcomings, and introduces a trainable subset of structured matrices that endow LSSLs with long-range memory.\n\n**Abstract:** Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 357  (*Influential: 22*)\n\n#### 4. Simplified State Space Layers for Sequence Modeling\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 334  (*Influential: 32*)\n\n#### 5. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs and ASICs\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Yue Wang, Zetong Guan, Yongan Zhang, Yingyan Lin, Deming Chen, Cong Hao, Xiaofan Zhang, Pengfei Xu, Chaojian Li, Yang Zhao*\n\n**Abstract:** Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing demand for DNN chips. However, designing DNN chips is non-trivial because: (1) mainstream DNNs have millions of parameters and operations; (2) the large design space due to the numerous design choices of dataflows, processing elements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is needed to allow the same DNN functionality to have a different decomposition, which would require different hardware IPs to meet the application specifications. Therefore, DNN chips take a long time to design and require cross-disciplinary experts. To enable fast and effective DNN chip design, we propose AutoDNNchip - a DNN chip generator that can automatically generate both FPGA- and ASIC-based DNN chip implementation given DNNs from machine learning frameworks (e.g., PyTorch) for a designated application and dataset. Specifically, AutoDNNchip consists of two integrated enablers: (1) a Chip Predictor, built on top of a graph-based accelerator representation, which can accurately and efficiently predict a DNN accelerator's energy, throughput, and area based on the DNN model parameters, hardware configuration, technology-based IPs, and platform constraints; and (2) a Chip Builder, which can automatically explore the design space of DNN chips (including IP selection, block configuration, resource balancing, etc.), optimize chip design via the Chip Predictor, and then generate optimized synthesizable RTL to achieve the target design metrics. Experimental results show that our Chip Predictor's predicted performance differs from real-measured ones by < 10% when validated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC). Furthermore, accelerators generated by our AutoDNNchip can achieve better (up to 3.86X improvement) performance than that of expert-crafted state-of-the-art accelerators.\n\n**Published:** 2020-01-06\n\n\n\n#### 3. New Insights on Reducing Abrupt Representation Change in Online Continual Learning\n\n*From Search Query: selective state space implementation memory efficiency*\n\n*Eugene Belilovsky, Joelle Pineau, Tinne Tuytelaars, Nader Asadi, Rahaf Aljundi, Lucas Caccia*\n\n**Abstract:** In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work, we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes must be distinguished from previous ones. We shed new light on this question by showing that applying ER causes the newly added classes' representations to overlap significantly with the previous classes, leading to highly disruptive parameter updates. Based on this empirical analysis, we propose a new method which mitigates this issue by shielding the learned representations from drastic adaptation to accommodate new classes. We show that using an asymmetric update rule pushes new classes to adapt to the older ones (rather than the reverse), which is more effective especially at task boundaries, where much of the forgetting typically occurs. Empirical results show significant gains over strong baselines on standard continual learning benchmarks.\n\n**Published:** 2021-04-11\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design with a focus on selective state space models, memory efficiency, and practical implementation approaches, here are some key findings and technical details from the provided sources:\n\n## Selective State Space Models\n\n### Mamba Model\nThe Mamba model, as described in the sources, is a selective state space model that offers several advantages:\n- **Input-Dependent Parameters**: Mamba allows its state space model (SSM) parameters to be functions of the input, enabling content-based reasoning and selective propagation or forgetting of information along the sequence length dimension.\n- **Linear Scaling**: Mamba scales linearly in the sequence length, which is a significant improvement over the quadratic complexity of traditional Transformer models. This makes it more efficient for long sequences and larger inputs.\n- **Efficient Inference**: Mamba executes in recurrent mode, which is advantageous at inference time, showing better scalability than Transformer models like MAT in terms of the number of agents or sequence length.\n\n### Implementation Details\n- **Discretization**: To incorporate continuous SSMs into deep learning architectures, discretization is essential. This involves introducing a time step parameter and applying the zero-order hold (ZOH) technique to transform continuous matrices into their discrete counterparts.\n- **Convolutional Form**: The S4 model, related to Mamba, can be transformed into a convolutional form under linear time invariance assumptions, allowing for parallel training and improving efficiency.\n\n## Memory Efficiency\n\n### Hybrid Architectures\n- **Taipan Model**: Taipan combines Mamba-2 with Selective Attention Layers (SALs) to balance Mamba's efficiency with Transformer-like performance. This approach constrains the attention budget, allowing accurate predictions up to context lengths of 1 million tokens while preserving computational efficiency.\n- **Selective Attention**: SALs identify tokens requiring long-range interactions, remove less important features, and augment their representations using the attention module. This selective attention mechanism helps in reducing memory usage and computational costs.\n\n### Sparse Models\n- **Sparse Sequence-to-Sequence Models**: These models, such as the Sparse Transformer, reduce computational complexity from O(n\u00b2) to O(n\u221an) by applying sparse factorizations to self-attention matrices. This approach minimizes memory consumption and speeds up processing times, making them suitable for large datasets and real-world applications.\n\n## Practical Implementation Approaches\n\n### Combining Hierarchical and Selective Processing\n- Integrating hierarchical processing with selective state spaces can help manage long-range dependencies efficiently. For example, combining the strengths of HierarchicalFastTTTLinear with Mamba's selective state space processing can address temporal bottlenecks and improve overall efficiency[Analysis Summary].\n\n### Efficient Routing Mechanisms\n- **Adaptive Routing**: Mechanisms like those in ZOOTER and Switch Transformers can be adapted to route inputs to specialized components based on expertise, ensuring efficient sparse activation with constant computational cost. This can be integrated with selective state space models to further optimize performance and memory usage[Analysis Summary].\n\n### Memory Optimization Techniques\n- **Compressed Contexts**: Techniques such as those described in \"Compressing Context to Enhance Inference Efficiency\" can significantly reduce memory cost and generation latency while maintaining comparable performance. This involves selective context compression, which can achieve a 50% reduction in context cost and a 36% reduction in inference memory usage[Analysis Summary].\n\nBy leveraging these approaches, researchers can design autoregressive language models that are more efficient, scalable, and memory-efficient, while maintaining or improving performance compared to existing state-of-the-art models.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for combining the strengths of HierarchicalFastTTTLinear and HierarchicalMamba while addressing their limitations:\n\n1. Selective State Space Processing\nFrom Mamba paper, selective state space models show strong potential for efficient sequence processing:\n\"A key weakness of such models is their inability to perform content-based reasoning... letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\n2. Adaptive Routing Mechanisms\nThe ZOOTER and Switch Transformer papers demonstrate effective routing strategies:\n- ZOOTER shows how to route inputs to specialized components based on expertise\n- Switch Transformers demonstrate efficient sparse activation with constant computational cost\n\n3. Memory Optimization Techniques\nFrom \"Compressing Context to Enhance Inference Efficiency\":\n\"Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance... achieve a 50% reduction in context cost, resulting in a 36% reduction in inference memory usage\"\n\nKey Insights:\n\n1. Selective Processing\n- Content-based selection of state space parameters can improve discrete sequence modeling\n- Adaptive routing can help specialize different components for different types of inputs\n- Selective activation can maintain performance while reducing computation\n\n2. Memory Efficiency\n- Temporal bottlenecks from HierarchicalFastTTTLinear can be combined with selective state spaces\n- Compressed contexts can significantly reduce memory usage without major performance loss\n- Hierarchical processing helps manage long-range dependencies efficiently\n\n3. Integration Opportunities\n- Combine selective state spaces with hierarchical processing\n- Use routing mechanisms to specialize different temporal scales\n- Integrate compressed contexts with selective computation\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Specific implementations of selective state space models\n- Methods for combining hierarchical and selective processing\n- Techniques for efficient routing in state space models\n\n2. Search queries needed:\n- Selective state space implementation details\n- Hierarchical routing mechanisms\n- Memory-efficient state compression techniques\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint.\n\n2. Lu, K., Yuan, H., Lin, R., Lin, J., Yuan, Z., Zhou, C., & Zhou, J. (2023). \"Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models\". NAACL.\n\n3. Shazeer, N., Zoph, B., & Fedus, W. (2021). \"Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\". arXiv preprint.\n\n4. Li, Y., Dong, B., Lin, C., & Guerin, F. (2023). \"Compressing Context to Enhance Inference Efficiency of Large Language Models\". EMNLP.\n\n5. Clark, A., et al. (2022). \"Unified Scaling Laws for Routed Language Models\". ICML."
        },
        {
            "ready": false,
            "query": "hardware efficient implementation parallel processing state space models",
            "detail": "Find specific implementations and optimization techniques for parallel processing in state space models, focusing on hardware efficiency and practical implementation approaches.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and optimization techniques for parallel processing in state space models, focusing on hardware efficiency and practical implementation approaches.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. From generalization analysis to optimization designs for state space models (Avg. Score: 0.95)\n\n*Fusheng Liu, Qianxiao Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.95)*\n\n```\n# From Generalization Analysis to Optimization Designs for State Space Models \n\nFusheng Liu<br>National University of Singapore<br>fusheng@u.nus.edu\n\nQianxiao Li<br>National University of Singapore<br>qianxiao@nus.edu.sg\n\n\n#### Abstract\n\nA State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.47)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.47)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 3. Simplified State Space Layers for Sequence Modeling (Avg. Score: 0.32)\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 232  (*Influential: 28*)\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 1/53 (Score: 0.33)*\n\n```\n# Simplified State Space LayERS FOR SEQUENCE MODELING \n\nJimmy T.H. Smith ${ }^{*}$, 1,2 , Andrew Warrington ${ }^{*, 2,3}$, Scott W. Linderman ${ }^{2,3}$<br>*Equal contribution.<br>${ }^{1}$ Institute for Computational and Mathematical Engineering, Stanford University.<br>${ }^{2} \\mathrm{Wu}$ Tsai Neurosciences Institute, Stanford University.<br>${ }^{3}$ Department of Statistics, Stanford University.<br>\\{jsmith14, awarring, scott.linderman\\}@stanford.edu. #### Abstract\n\nModels using structured state space sequence (S4) layers have achieved state-ofthe-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the $S 5$ layer. Whereas an $S 4$ layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S 5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages $87.4 \\%$ on the long range arena benchmark, and $98.5 \\%$ on the most difficult Path-X task.\n```\n\n##### *Relevant Chunk: No. 14/53 (Score: 0.32)*\n\n```\nAdvances in Neural Information Processing Systems, 33: $1474-1487,2020 \\mathrm{a}$. Albert Gu, Caglar Gulcehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800-3809. PMLR, 2020b. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021a. Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021b. Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations, 2023. Ankit Gupta and Jonathan Berant. Gmat: Global memory augmentation for transformers, 2020. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8): $1735-1780,1997$. Arieh Iserles. A first course in the numerical analysis of differential equations. 44. Cambridge university press, 2009. Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pp. 87-104, 2022. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Shiva Kaul. Linear dynamical systems as a core computational primitive. Advances in Neural Information Processing Systems, 33:16808-16820, 2020. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Alex Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009. Richard Ladner and Michael Fischer. Parallel prefix computation. Journal of the ACM (JACM), 27 (4):831-838, 1980. Sivaramakrishnan Lakshmivarahan and Sudarshan Dhall. Parallel computing using the prefix problem.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.03)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 7/86 (Score: 0.03)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hardware efficient implementation parallel processing state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient implementation parallel processing state space models*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: hardware efficient implementation parallel processing state space models*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: hardware efficient implementation parallel processing state space models*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 4. Simplified State Space Layers for Sequence Modeling\n\n*From Search Query: hardware efficient implementation parallel processing state space models*\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 334  (*Influential: 32*)\n\n#### 5. State Space Expectation Propagation: Efficient Inference Schemes for Temporal Gaussian Processes\n\n*From Search Query: hardware efficient implementation parallel processing state space models*\n\n*William J. Wilkinson, Paul E. Chang, M. R. Andersen, A. Solin*\n\n**TL;DR:** This work forms approximate Bayesian inference in non-conjugate temporal and spatio-temporal Gaussian process models as a simple parameter update rule applied during Kalman smoothing, providing extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework.\n\n**Abstract:** We formulate approximate Bayesian inference in non-conjugate temporal and spatio-temporal Gaussian process models as a simple parameter update rule applied during Kalman smoothing. This viewpoint encompasses most inference schemes, including expectation propagation (EP), the classical (Extended, Unscented, etc.) Kalman smoothers, and variational inference. We provide a unifying perspective on these algorithms, showing how replacing the power EP moment matching step with linearisation recovers the classical smoothers. EP provides some benefits over the traditional methods via introduction of the so-called cavity distribution, and we combine these benefits with the computational efficiency of linearisation, providing extensive empirical analysis demonstrating the efficacy of various algorithms under this unifying framework. We provide a fast implementation of all methods in JAX.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 12  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hardware-efficient implementations, parallel processing, and state space models, here are some key insights and techniques from the provided sources and additional context:\n\n## Parallel Processing in State Space Models\n\n### Selective State Space Models\nState Space Models (SSMs) like Mamba offer significant advantages in terms of parallel processing and hardware efficiency. The Mamba architecture allows for dynamic parameter adjustment based on input, which enhances sequence modeling performance. It also employs a selective scan mechanism that improves computational efficiency, making it viable for real-time applications and enabling parallel processing without sacrificing performance.\n\n### Linear Complexity and Parallel Training\nSSMs maintain linear complexity with respect to sequence length, which is crucial for scalable and efficient parallel processing. This linear complexity allows for training in parallel, significantly reducing training time and improving overall efficiency.\n\n## Hardware-Efficient Implementations\n\n### Simplified RNN Architectures\nThe work on minimal versions of GRUs and LSTMs (minGRUs and minLSTMs) shows that by removing hidden state dependencies, these models can be trained in parallel using the parallel scan algorithm. This simplification results in significant parameter efficiency and training speedup, making them more hardware-efficient compared to traditional RNNs.\n\n### Memristor-Based Accelerators\nFor transformer models, memristor-based in-memory computing can provide a hardware-efficient implementation. This approach leverages memristor devices to perform matrix-matrix multiplications efficiently, resulting in a 10\u00d7 acceleration in transformer self-attention compared to digital counterparts. This method also reduces power consumption and latency, making it suitable for real-time applications.\n\n## Memory Optimization Techniques\n\n### Selective Mechanisms\nIntegrating selective mechanisms, as seen in Mamba, allows for content-based reasoning and selective information propagation. This can be combined with other architectures to optimize state representation and reduce memory usage. Selective compression can be used for efficient state representation, which is crucial for deep learning architectures.\n\n### Hardware-Aware Algorithms\nImplementing hardware-aware algorithms for state expansion and compression can optimize memory hierarchy usage. For example, the partitioning and rotation methods used in FPGA implementations for eigen-decomposition can be adapted to optimize resource usage and execution time in SSMs.\n\n## Practical Implementation Approaches\n\n### Unified Gating Mechanisms\nCombining the gating mechanisms from different architectures (e.g., FastTTT and Mamba) can help in unifying the selective state updates while maintaining hardware efficiency. This unified approach can simplify the overall architecture and balance complexity with performance[Analysis Summary].\n\n### Parallel and Causal Processing\nEnsuring that parallel processing maintains causal properties is essential for autoregressive models. SSMs like Mamba are designed to handle this by allowing parallel computation while preserving the causal structure of the sequence data.\n\nBy integrating these insights, researchers can develop autoregressive language models that are highly efficient in terms of hardware usage, parallel processing capabilities, and memory optimization, ultimately leading to better scalability and performance.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal important insights about selective state space models and memory efficiency that can help guide the combination of HierarchicalFastTTTLinear and HierarchicalMamba:\n\n1. Key Findings from Selective State Space Models:\n\n- Mamba's selective mechanism allows input-dependent parameter adjustment, enabling content-based reasoning and selective information propagation\n- Hardware-aware implementation is crucial for efficiency\n- Linear scaling with sequence length while maintaining performance\n- Simplified architecture without attention or MLP blocks\n\n2. Memory Optimization Techniques:\n\n- State compression through selective mechanisms\n- Efficient hardware-aware algorithms for state expansion\n- Parallel processing capabilities while maintaining causal properties\n- Memory hierarchy optimization for GPU implementation\n\nUseful References with Excerpts:\n\n1. From \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\n2. From \"Theoretical Foundations of Deep Selective State-Space Models\":\n\"When random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\"\n\n3. From \"Simplified State Space Layers for Sequence Modeling\":\n\"Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM... allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance.\"\n\nKey Insights for Design:\n\n1. Selective Mechanism Integration:\n- Can combine FastTTT's temporal bottlenecks with Mamba's selective state updates\n- Need to maintain hardware efficiency while adding selectivity\n- Opportunity to unify gating mechanisms from both approaches\n\n2. Memory Optimization:\n- Use selective compression for efficient state representation\n- Implement hardware-aware parallel processing\n- Optimize memory hierarchy usage\n\n3. Architecture Design:\n- Potential to simplify the overall architecture\n- Need to balance complexity with performance\n- Opportunity for efficient parallel implementation\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Hardware-efficient implementations of selective mechanisms\n2. Parallel processing strategies for state space models\n3. Memory optimization techniques for deep learning architectures\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint.\n\n2. Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., & Lyons, T. (2024). \"Theoretical Foundations of Deep Selective State-Space Models\". arXiv preprint.\n\n3. Smith, J., Warrington, A., & Linderman, S. W. (2022). \"Simplified State Space Layers for Sequence Modeling\". International Conference on Learning Representations.\n\n4. Jafari, F. R., Montavon, G., M\u00fcller, K. R., & Eberle, O. (2024). \"MambaLRP: Explaining Selective State Space Sequence Models\". arXiv preprint.\n\n5. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). \"Spectral State Space Models\". arXiv preprint."
        },
        {
            "ready": false,
            "query": "butterfly algorithm implementation memory hierarchy optimization",
            "detail": "Find specific implementations and optimization techniques for butterfly algorithms and memory hierarchy management in deep learning models, focusing on practical implementation approaches and performance optimization.",
            "search_ret": "\n---\n## Found 1 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and optimization techniques for butterfly algorithms and memory hierarchy management in deep learning models, focusing on practical implementation approaches and performance optimization.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 1 papers in Internal Library\n\n#### 1. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.56)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.96)*\n\n```\nHowever, we found that these tricks are not necessary. For example, the learned transforms in Section 4.1 typically put weight at least 0.99 on a permutation. Initialization As the BP or BPBP construction is a product of many matrices, proper initialization is crucial to avoid exponential blowup in the size of the entries or condition numbers (i.e., the exploding/vanishing gradient problem [37]). We aim to initialize each butterfly factor to be close to unitary or orthogonal, so that the magnitude of the inputs and outputs to the transform are preserved. This is easy since each of the factors $B_{N}, \\ldots, B_{2}$ has exactly two nonzeros in each row and column; for example in the real case, initializing each entry of $B_{k}$ as $\\mathcal{N}(0,1 / 2)$ guarantees $\\mathbb{E} B_{k}^{*} B_{k}=I_{N}$. Comparison to related methods Some previous works have examined similar butterfly matrices in numerical algebra or machine learning [36, 20, 32, mainly motivated by trying to parametrize cheap orthogonal matrices. Our parametrization, motivated by the goal of learning recursive transforms, differs in several ways from all previous works: 1 . We explicitly model and learn a permutation matrix $P$. 2. Our relaxation does not enforce the matrix to be orthogonal. 3. Our butterfly factors are ordered so that closer elements interact first (Figure 1), whereas some works (e.g. [32]) reverse the order. 4. Every work has a different weight-tying scheme; ours ties the blocks in each butterfly factor, leading to fewer parameters and a tighter recursive interpretation than for example [20]. Our main baseline for deep learning experiments is Thomas et al. 42, who define a special matrix class with a complicated recursive algorithm. While our BP method and theirs share some overlap (e.g., they both capture circulant matrices), they have a distinct parametrization, and the exact relation between the BP hierarchy and their LDR-SD or LDR-TD classes is unknown. From a practical standpoint, BP is significantly faster and simpler to implement than their methods. ### 3.3 Expressivity and the butterfly hierarchy\n\nThe butterfly matrix $B$ has a total of $4 N$ learnable parameters (the butterfly factors $B_{N}, B_{N / 2}, \\ldots, B_{2}$ have $2 N, N, \\ldots, 4$ entries respectively). The overall permutation $P$ has $3 \\log _{2} N$ learnable parameters; we can also tie the logits of the $\\log _{2} N$ probabilistic permutations-reflecting the fact that for some algorithms the reduction from size $N$ to $N / 2$ is self-similar to the reduction from size $N / 2^{k}$ to $N / 2^{k+1}$ reducing this to just 3 parameters. We can define a natural hierarchy of matrix classes built on the BP primitive. This hierarchy covers a spectrum ranging from extremely structured matrices with a linear number of parameters, to the entire space of square matrices. Definition 1. For any dimension $N$, let $(\\mathrm{BP})_{r}^{k}(k, r \\in \\mathbb{N})$ denote the classes of matrices that can be expressed as\n\n$$\nS\\left(\\prod_{i=1}^{k} B_{i} P_{i}\\right) S^{T}\n$$\n\nwhere each $B_{i} P_{i} \\in \\mathbb{F}^{r N \\times r N}$ is a BP module as in equation (2), and $S \\in \\mathbb{F}^{N \\times r N}=\\left[\\begin{array}{llll}I_{N} & 0 & \\ldots & 0\\end{array}\\right]$ (that is, $S$ and $S^{T}$ select the upper left $N \\times N$ entries of the BP product matrix). The subscript $r$ is understood to be 1 if omitted. Note that the BP and BPBP classes are equivalent to (BP) ${ }^{1}$ and (BP) ${ }^{2}$ respectively. We remark that $B$ and $P$ are both capable of being the identity, and thus $(\\mathrm{BP})^{k} \\subseteq(\\mathrm{BP})^{k+1}$. The BP hierarchy is expressive enough to theoretically represent many important transforms with low depth, as well as all matrices with linear depth:\n\nProposition 1. $(B P)^{1}$ captures the fast Fourier transform, the fast Hadamard transform, and their inverses exactly. $(B P)^{2}$ captures the DCT, DST, and convolution exactly. All $N \\times N$ matrices are contained in $(B P)_{2}^{4 N+10}$. Proposition 1 is shown in Appendix B. We suggest some additional conjectures about the expressiveness of the BP hierarchy in Appendix D. Even though the BP parameterization is expressive, it still retains the learnability characteristic of compressed parameterizations. In fact, neural networks comprising layers of BP and BPBP matrices still have VC dimension that is almost linear in the number of parameters (Appendix B), similar to networks with fully-connected layers [1, 17] and LDR 42, which implies a corresponding sample complexity bound. ## 4 Empirical Evaluation\n\nWe evaluate the proposed approach to verify that our butterfly parameterization can both recover fast transforms and be integrated as an effective component in ML pipelines ${ }^{3}$, In Section 4.1, we confirm that it automatically learns the fast algorithms for many discrete transforms commonly used in signal processing and machine learning. Section 4.2 further shows that it can be a useful component to increase the performance of deep learning models while ensuring fast multiplication and few parameters by design. ### 4.1 Discrete Transforms\n\nBelow we list several important classes of structured matrices. Some of them are directly captured by our parametrization and we expect that they can be recovered close to perfectly, thus providing a $O(N \\log N)$ algorithm that closely approximates the naive $O\\left(N^{2}\\right)$ matrix multiplication. Others are not perfectly captured by the BPBP class but still have recursive structure; for these, we expect that our method reconstructs them better than standard matrix compression methods (sparse, low-rank, and combinations) can. Transforms We describe the matrices we evaluate on and their applications; a standard reference is Proakis 38.\n```\n\n##### *Relevant Chunk: No. 2/35 (Score: 0.83)*\n\n```\nAll of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of matrices with fast matrix-vector multiplication as factoring into products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \\log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4 X faster inference speed and 40 X fewer parameters. ## 1 Introduction\n\nStructured linear transformations, such as the discrete Fourier transform (DFT), discrete cosine transform (DCT), and Hadamard transform, are a workhorse of machine learning, with applications ranging from data preprocessing, feature generation, and kernel approximation, to image and language modeling (convolutions). To date, these transformations rely on carefully designed algorithms, such as the famous fast Fourier transform (FFT) algorithm, and on specialized implementations (e.g., FFTW and cuFFT). Moreover, each specific transform requires hand-crafted implementations for every platform (e.g., Tensorflow and PyTorch lack the fast Hadamard transform), and it can be difficult to know when they are useful. Ideally, these barriers would be addressed by automatically learning the most effective transform for a given task and dataset, along with an efficient implementation of it. Such a method should be capable of recovering a range of fast transforms with high accuracy and realistic sizes given limited prior knowledge. It is also preferably composed of differentiable primitives and basic operations common to linear algebra/machine learning libraries, that allow it to run on any platform and be integrated into modern ML frameworks such as PyTorch/Tensorflow. More fundamentally, this problem ties into the foundational question of understanding the minimal prior knowledge needed to learn high-speed systems, in the spirit of modern trends toward relaxing manually imposed structure (i.e., AutoML). Recent progress in this vein of learning computational primitives includes addition/multiplication gates [43], the Strassen $2 \\times 2$ matrix multiplication algorithm [44, and PDE solvers [19]. We propose a method that addresses this problem for a class of important transforms that includes the\naforementioned examples. A key challenge lies in defining or parameterizing the space of transforms and corresponding fast algorithms, which requires using a minimal amount of prior knowledge that captures important and interesting transforms while remaining learnable and efficient. Egner \\& P\u00fcschel [13, 14] previously posed this question and a novel combinatorial approach, but their solution only addresses a limited set of transforms (primarily DFT) and only on limited problem sizes. In particular, these approaches search through an exponentially large discrete space using a symbolic form of the matrix [13, 14] and recover the solution only up to dimensions $8 \\times 8$. We instead draw two key lessons from the work of De Sa et al. 8, who characterize matrices with efficient matrix-vector multiplication algorithms as being factorizable into products of sparse matrices ${ }^{1}$ Thus, the task of learning algorithms can be reduced to finding appropriate sparse matrix product representations of the transforms. They further show that divide-and-conquer schemes lead to fast multiplication algorithms for a surprisingly general set of structured matrices. Motivated by the broad applicability of this recursive structure, we propose a particular factorization using sequences of special block diagonal matrices, called butterfly matrices. Specific instances of butterfly structure have been used before - for example as a random orthogonal preconditioner [36] or in matrix approximation [28 - but we use a relaxed representation that captures a larger class of structures and can learn from data. These form a class of structured matrices with $O(N)$ parameters and automatic fast multiplication in $O(N \\log N)$ operations. We empirically validate our method in two ways. First, we consider a specification of a transform (e.g., $N$ input-output pairs) and attempt to factorize it. We successfully recover a fast algorithm up to machine precision for several important transforms such as the DFT, Hadamard, DCT, and convolution for realistic sizes (dimensions up to $N=1024$ ), while standard sparse and low-rank baselines cannot (Section 4.1). Beyond recovering famous transforms, we additionally incorporate this method in end-to-end ML pipelines to learn fast and compressible latent transformations (Section 4.2). On the benchmark single hidden layer network, this parameterization exceeds the classification accuracy of a baseline fully connected layer on several datasets - such as by 3.9 points on CIFAR- 10 while using 40X fewer parameters-which is to our knowledge the first time a structured model has outperformed the unconstrained model for this task on a realistic dataset 42. We also find that the addition of a lightweight butterfly layer improves the accuracy of a modern ResNet architecture by 0.43 points. Finally, our method is simple with an easily implementable fast algorithm. We compare the training and inference speed of our implementation to specialized implementations of discrete transforms (Section 4.3). Our generic representation comes within 3-5X of implementations for specific transforms such as the DFT and DCT, while still being capable of learning a rich class of more general transforms. ## 2 Related Work\n\nFast transforms are crucial and ubiquitous in the machine learning pipelines, from data preprocessing, feature generation, and dimensionality reduction to compressing models. For example, the DFT and DCT form the basis of the mel-frequency cepstral coefficients (MFCCs), a standard feature representation for speech recognition [21]. State-of-the-art kernel approximation methods leverage circulant matrices (i.e., convolution) 49] and the DFT and Hadamard transform [24, 50] for fast projection. Structured matrices, which are matrix representations of fast transforms, play a crucial role in designing fast neural network layers with few parameters 40, 10. Given their importance, there have been significant efforts in finding more and more general classes of fast transforms. Traditional classes of structured matrices such as the Toeplitz, Hankel, Vandermonde, and Cauchy matrices are ubiquitous in engineering and signal processing [35], and more recently have found use in deep learning. These were generalized under the seminal notion of low displacement rank (LDR) introduced by Kailath et al. [22], and were later unified under a single class of displacement structure (the confluent Cauchy-like matrices) introduced by Olshevsky \\& Shokrollahi 34 to solve the Nevanlinna-Pick interpolation problem. Another class of fast transforms that directly generalize the DFT and DCT are based on orthogonal polynomials [7], which find usage in areas from differential equations to optics. Both orthogonal polynomial transforms [12], and all of the previously introduced matrices with displacement rank structure, were further\n\n[^0]significantly generalized under a single class by De Sa et al. 8. Notably, almost all of the structured matrix classes mentioned here exhibit a form of recursive structure in their construction and superfast algorithms. Since the product of sparse matrices immediately has a fast multiplication algorithm, the problem of sparse matrix factorization has been tackled in many settings. Sparse PCA 51] and dictionary learning [29] factor a matrix into two components, one of which is sparse. Sparse matrix factorization with more than two factors has also been considered, for example in the setting where the true matrix is the product of random sparse matrices 33, or in the context of learning multi-layer sparse approximations 25, 26. Our approach differs from these in that we focus on the recursive structure of the transforms - not just the sparsity of their factors-leading to sparse and structured transforms, and avoiding the discreteness problem inherent to learning sparsity. Since most distinct transforms typically require significant work both to design fast algorithms and to efficiently implement them on different platforms, there have been attempts to automatically learn these fast algorithms. The field of algebraic signal processing [39] uses methods from representation theory of groups and algebras to automatically generate fast algorithms from the symbolic form of the transform matrix. However, these methods require search over a combinatorially-large discrete space, limiting their approaches to small matrices of size up to $8 \\times 8$ [14, 45]. Attempts to learn general algorithms such as matching 31, sorting [16], and traveling salesman [2] using differentiable architectures face a similar challenge of having to effectively explore a large discrete space. Thus, they only work for problems of size at most 100. By contrast, our approach simplifies the discreteness of the problem into learning a simpler set of permutations, allowing us to recover fast algorithms for realistic dimensions. Independently, there has been growing interest in compressed deep learning models, motivated by the goal of adapting them to resource-constrained environments. A common approach for learning compressed models involves replacing the unconstrained weight matrices with a class of structured matrices and learning directly on the parametrization of that class. The most effective methods use matrix classes that are explicitly related to Fourier transforms 40, or employ highly specialized and complicated recursive algorithms 42]. As our method also implicitly defines a highly compressible subclass of matrices with linear parameter count and efficient multiplication, it can be used as a drop-in replacement for matrices in such end-to-end ML models. ## 3 Recovering Fast Transforms\n\nWe now set up and describe our approach. We first reiterate the connection between fast algorithms and sparse matrix factorization, and briefly outline a quintessential divide-and-conquer algorithm (the FFT) as motivation. We then elaborate the details of our method for learning particular recursive algorithms, including a core permutation-learning step that enables it to capture a wider range of structures. We also discuss the expressive power of these matrices, including which transforms they capture perfectly, and define a hierarchy of matrix classes built on butterflies that can theoretically capture richer recursive structures. ### 3.1 Preliminaries\n\nSparse factorizations One method of constructing matrices with obvious fast matrix-vector multiplication is as a product of sparse matrices, so that multiplication by an arbitrary vector will have cost proportional to the total number of nonzeros of the matrices in the product. Surprisingly, the converse is also true. The notion of sparse product width (SPW) [8, which roughly corresponds to the total sparsity of a factorization of a matrix, turns out to be equivalent to the length of the shortest linear straight-line program describing a matrix (up to a constant). Hence, it is an optimal descriptor of the algorithmic complexity of matrix-vector multiplication on these types of models [3. Given the general correspondence between sparse factorization and fast algorithms, we consider specific types of discrete transforms and their recursive factorizations. This is a prototype for our parameterization of fast recursive algorithms in Section 3.2\n\nCase study: DFT The Discrete Fourier Transform (DFT) transforms a complex input vector $x=$ $\\left[x_{0}, \\ldots, x_{N-1}\\right]$ into a complex output vector $X=\\left[X_{0}, \\ldots, X_{N-1}\\right]$ by expressing the input in the basis of the complex exponentials:\n\n$$\nX_{k}=\\sum_{n=0}^{N-1} x_{n} e^{-\\frac{2 \\pi i}{N} k n}, \\quad k=0, \\ldots, N-1, N=2^{m}\n$$\n\nLet $\\omega_{N}:=e^{2 \\pi i / N}$ denote a primitive $N$-th root of unity. The DFT can be expressed as matrix multiplication by the DFT matrix $F_{N} \\in \\mathbb{C}^{N \\times N}$, where $\\left(F_{N}\\right)_{k n}=\\omega_{N}^{-k n}$. The DFT of size $N$ can be reduced to two DFTs of size $N / 2$ on the even indices and the odd indices:\n\n$$\nF_{N} x=\\left[\\begin{array}{l}\nF_{N / 2} x_{\\mathrm{even}}+\\Omega_{N / 2} F_{N / 2} x_{\\text {odd }} \\\\\nF_{N / 2} x_{\\text {even }}-\\Omega_{N / 2} F_{N / 2} x_{\\text {odd }}\n\\end{array}\\right]\n$$\n\nwhere $x_{\\text {even }}=\\left[x_{0}, x_{2}, \\ldots, x_{N-2}\\right], x_{\\text {odd }}=\\left[x_{1}, x_{3}, \\ldots, x_{N-1}\\right]$, and $\\Omega_{N / 2}$ is the diagonal matrix with entries $1, \\omega_{N}^{-1}, \\ldots, \\omega_{N}^{-(N / 2-1)}$. This recursive structure yields the efficient recursive Cooley-Tukey Fast Fourier Transform (FFT) algorithm. This computation can be written as a matrix factorization\n\n$$\nF_{N}=\\left[\\begin{array}{cc}\nI_{N / 2} & \\Omega_{N / 2} \\\\\nI_{N / 2} & -\\Omega_{N / 2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nF_{N / 2} & 0 \\\\\n0 & F_{N / 2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\text { Sort the even } \\\\\n\\text { and odd indices }\n\\end{array}\\right]\n$$\n\nwhere $I_{N / 2}$ is the identity matrix, and the last factor is the permutation matrix $P_{N}$ that separates the even and odd indices (e.g., mapping $[0,1,2,3]$ to $[0,2,1,3]$ ) (see Figure 2]. Unrolling the recursion, we obtain:\n\n$$\n\\begin{aligned}\nF_{N} & =B_{N}\\left[\\begin{array}{cc}\nF_{N / 2} & 0 \\\\\n0 & F_{N / 2}\n\\end{array}\\right] P_{N} \\\\\n& =B_{N}\\left[\\begin{array}{cc}\nB_{N / 2} & 0 \\\\\n0 & B_{N / 2}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\nF_{N / 4} & 0 & 0 & 0 \\\\\n0 & F_{N / 4} & 0 & 0 \\\\\n0 & 0 & F_{N / 4} & 0 \\\\\n0 & 0 & 0 & F_{N / 4}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nP_{N / 2} & 0 \\\\\n0 & P_{N / 2}\n\\end{array}\\right] P_{N} \\\\\n& =\\cdots \\\\\n& =\\left(B_{N} \\ldots\\left[\\begin{array}{ccc}\nB_{2} & \\ldots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\ldots & B_{2}\n\\end{array}\\right]\\right)\\left(\\left[\\begin{array}{ccc}\nP_{2} & \\ldots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\ldots & P_{2}\n\\end{array}\\right] \\ldots P_{N}\\right)\n\\end{aligned}\n$$\n\nThe product of all the $B_{N / 2^{k}}$ matrices on the left is called a butterfly matrix, and each factor $B_{N / 2^{k}}$ is a $2 \\times 2$ block matrix of diagonal matrices called a butterfly factor. Figure 1 illustrates the sparsity pattern of the structured butterfly factors. One can also combine the product of permutation matrices on the right to obtain a single permutation called the bit-reversal permutation, which sorts the indices by the reverse of their binary representation (e.g. $[0, \\ldots, 7] \\rightarrow[0,4,2,6,1,5,3,7]$ ). Other transforms have similar recursive structure but differ in the entries of $B_{N / 2^{k}}$, and in the permutation. For example, the DCT involves separating the even and the odd indices, and then reversing the second half (e.g., $[0,1,2,3] \\rightarrow[0,2,1,3] \\rightarrow[0,2,3,1]$ ). Appendix A provides some examples of how important transforms, such as the DFT, DCT, Hadamard, and convolutions, can factor as similar products of sparse matrices. ### 3.2 Recovering Fast Transform Algorithms\n\nMany previous works attempt to compress generic matrices by sparsifying them. We note that allowing for products of matrices with a total sparsity budget is strictly more expressive than a single matrix with that sparsity, while retaining the same compression and computation complexity. Therefore one can hope to recover all fast algorithms by learning over the set of matrix products with a total sparsity budget. However, this is infeasible to learn due to the discreteness of the sparsity constraint (Section 122). We instead use a class of matrices built as products of specific factors that captures the recursive nature of many fast algorithms. ![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-05.jpg?height=201&width=825&top_left_y=249&top_left_x=647)\n\nFigure 1: Butterfly matrix for $N=16$. From left to right: single copy of $B_{16}$, blocks of $B_{8}$, blocks of $B_{4}$, blocks of $B_{2}$. A butterfly parametrization Let $x=\\left[x_{0}, \\ldots, x_{N-1}\\right]$ be an input vector ${ }^{2}$ Let $\\mathcal{T}_{N}$ be a linear transform of size $N$ with matrix representation $T_{N} \\in \\mathbb{F}^{N \\times N}$, where $\\mathbb{F} \\in\\{\\mathbb{R}, \\mathbb{C}\\}$. A general recursive structure is to separate the input vector into two halves by some permutation, apply the transform on each half, and combine the result in a linear manner by scaling by an diagonal matrix and adding the results. Written as a matrix factorization:\n\n$$\nT_{N}=\\left[\\begin{array}{ll}\nD_{1} & D_{2} \\\\\nD_{3} & D_{4}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{N / 2} & 0_{N / 2 \\times N / 2} \\\\\n0_{N / 2 \\times N / 2} & T_{N / 2}\n\\end{array}\\right] P_{N}\n$$\n\nwhere $P_{N}$ is some permutation matrix and $D_{1}, \\ldots, D_{4} \\in \\mathbb{F}^{N / 2}$ are diagonal matrices. Inspired by the factors of the FFT, we call the matrix $\\left[\\begin{array}{ll}D_{1} & D_{2} \\\\ D_{3} & D_{4}\\end{array}\\right]$ a butterfly factor, denoted by $B_{N}$. Unrolling the recursion as in equation (1) gives the factorization $T_{N}=B^{(N)} P^{(N)}$, where $B^{(N)}$ is a butterfly matrix and $P^{(N)}$ is a permutation that can be written as the product of $\\log _{2}(N)$ simpler block permutations. We also consider composing this module, hence learn either\n\n$$\nT_{N}=B^{(N)} P^{(N)} \\quad T_{N}=B_{2}^{(N)} P_{2}^{(N)} B_{1}^{(N)} P_{1}^{(N)}\n$$\n\nwhich we term the BP and the BPBP parametrization respectively. One dimensional convolutions (i.e. circulant matrices) are notably captured by BPBP, since they can be computed via an FFT, a component-wise product, then an inverse FFT (see Appendix A). In the case of the FFT, as in Section 3.1 the entries of the butterfly factors are called twiddle factors, and the combined permutation $P^{(N)}$ is called the bit-reversal permutation. Learning a recursive permutation The butterfly blocks in the BP or BPBP parametrization have a fixed sparsity pattern and their parameters can be directly optimized. However, the transforms we are interested in capturing frequently require different permutations as part of the \"divide\" step, which form a set of discrete objects that we must consider. We will restrict to learning over permutations that have a simple structure often encountered in these algorithms: we assume that the distribution factors into $\\log _{2} N$ steps following the $\\log _{2} N$ recursive layers. At each step in the recursion, the permutation $P_{N / 2^{k}}$ is allowed to either keep the first half and second half intact or separate the even and the odd indices (e.g., $[0,1,2,3] \\rightarrow[0,2,1,3]$ ). Then, it can choose to reverse the first half (e.g., $[0,1] \\rightarrow[1,0]$ ) and can choose to reverse the second half (e.g., $[2,3] \\rightarrow[3,2]$ ). Thus at each step, there are 3 binary choices and hence 8 possible permutations. These are illustrated in Figure 2, where $P_{N}^{a}$ denotes the permutation matrix on $N$ elements that separates the even and odd elements, $P_{N}^{b}$ denotes the permutation matrix that reverses the first half, and $P_{N}^{c}$ denotes the permutation matrix that reverses the second half. Instead of searching over $8^{\\log _{2} N}$ discrete permutations, we parameterize the permutation $P^{(N)}$ as a categorical distribution of these $8^{\\log _{2} N}$ permutations. The permutation $P_{N / 2^{k}}$ at step $k$ is thus chosen as a convex combination of the 8 possible choices:\n\n$$\nP_{N / 2^{k}}=p_{c b a} P_{N / 2^{k}}^{c} P_{N / 2^{k}}^{b} P_{N / 2^{k}}^{a}+p_{c b} P_{N / 2^{k}}^{c} P_{N / 2^{k}}^{b}+\\ldots\n$$\n\nThis can be learned by representing this probability distribution $\\left\\{p_{c b a}, p_{c b}, \\ldots\\right\\}$ for example via logits and the softmax. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-06.jpg?height=320&width=833&top_left_y=247&top_left_x=644)\n\nFigure 2: Three binary choices for constructing the permutation used at every step of the recursive process. One of 8 possible permutations can be constructed by multiplying a subset of these matrices in the presented order. We make the further simplification that the probabilities $p_{c b a}$ factor into the three components; conceptually, that the choices of choosing $P_{N / 2^{k}}^{c}, P_{N / 2^{k}}^{b}, P_{N / 2^{k}}^{a}$ to be part of the product are independent of each other. This results in the representation\n\n$$\nP_{N / 2^{k}}=\\prod_{s=c, b, a}\\left(p_{s} P_{N / 2^{k}}^{s}+\\left(1-p_{s}\\right) I\\right)\n$$\n\nThus we learn the permutation $P_{N / 2^{k}}$ via equation 3y optimizing over 3 logits $\\ell_{a}, \\ell_{b}, \\ell_{c}$ and setting $p_{s}=\\sigma\\left(\\ell_{s}\\right)$, where $\\sigma$ is the sigmoid function. To encourage the distribution over permutations to be peaked, one can add entropy regularization [15] or semantic loss [46.\n```\n\n##### *Relevant Chunk: No. 9/35 (Score: 0.60)*\n\n```\nOur implementation is built on top of the publicly available implementation of Thomas et al. 42 with the same hyperparameters, and we report their numbers for the competing baseline methods directly. We test on the three main datasets from Thomas et al. 42]: two challenging variants of MNIST-one with randomly rotated images and random background, the other with correlated background noise-and the standard CIFAR-10 dataset. Table 1: Test accuracy when replacing the hidden layer with structured classes. For the BPBP methods, the permutations $P$ have been fixed to the bit-reversal permutation. The butterfly parameterization achieves higher accuracy than the unstructured layer on all datasets. | Method | MNIST-bg-rot | MNIST-noise | CIFAR-10 | Compression factor |\n| :--- | :--- | :--- | :--- | :--- |\n| Unstructured | 44.08 | 65.15 | 46.03 | 1 |\n| BPBP (complex, fixed permutation) | $\\mathbf{4 6 . 2 6}$ | 77.00 | $\\mathbf{4 9 . 9 3}$ | 39.4 |\n| BPBP (real, fixed permutation) | 46.16 | 75.00 | 48.69 | 56.9 |\n| LDR-TD 42] | 45.81 | $\\mathbf{7 8 . 4 5}$ | 45.33 | 56.9 |\n| Toeplitz-like [40] | 42.67 | 75.75 | 41.78 | 56.9 |\n| Fastfood 47] | 38.13 | 63.55 | 39.64 | 78.7 |\n| Circulant 6] | 34.46 | 65.35 | 34.28 | 93.0 |\n| Low-rank [9] | 35.67 | 52.25 | 32.28 | 56.9 |\n\nTable 1 reports results for variants of our butterfly parametrization, compared to the unstructured matrix baseline and other structured matrix approaches. Notably, the butterfly methods achieve higher classification accuracy than the fully-connected layer on all datasets and are highly competitive with the other approaches. We note that improvements over unconstrained matrices can arise from lower generalization error due to fewer parameters (relating to VC bounds, Proposition 2), or better inductive bias encoded by the structured class. For example, convolutions are important in image tasks due to encoding shift equivariance, and Thomas et al. 42 hypothesize that their structured classes improve over FC layers through imposing approximate equivariance to more general transformations. Since our BP parametrization can represent arbitrary convolutions, it can encode these important priors. ResNet In addition to the standard single hidden layer benchmarks, we test the effect of using butterfly layers in a standard ResNet18 [18] implementation on the CIFAR-10 dataset. This architecture is normally fully convolutional, ending with a FC layer of dimensions $512 \\times 10$ before the softmax. However, we experiment with adding an additional FC or structured layer right before this final FC layer. Table 2 shows that the ResNet18 architecture can benefit from an additional fully connected layer, and using a BPBP layer instead improves performance even more while adding a negligible ( $0.07 \\%$ increase) number of parameters to the original model. Table 2: Classification accuracy for the ResNet18 architecture with different layers inserted before the final FC/softmax layer. | Last layer | None | FC | BPBP |\n| :--- | :--- | :--- | :--- |\n| Accuracy | $93.58 \\pm 0.15$ | $93.89 \\pm 0.19$ | $\\mathbf{9 4 . 0 1} \\pm 0.09$ |\n\n### 4.3 Training and Inference Speed Comparison\n\nBy design, the BP parameterization yields a fast algorithm of complexity $O(N \\log N)$, no matter which transform it learns. Moreover, given the parameters of the BP model, it is easy to implement this fast algorithm (this can be done in 5 lines of Python, and our code provides a function to do this automatically). The BP parameterization captures many common transforms (Section 4.1), and its implementation makes no transform-specific optimizations. Nevertheless, our simple implementation is surprisingly competitive with hand-tuned kernels both for training and for inference (after the parameters of the BP model are learned and we wish to evaluate $B P x$ for new input $x$ ). In Figure 4, we compare the speed of the BP fast multiplication against specialized implementation of common transforms such as the FFT, DCT, and DST (all have complexity $O(N \\log N)$ ), using dense matrix-vector multiply (GEMV, complexity $O\\left(N^{2}\\right)$ ) as a baseline. For training with realistic input sizes $N=1024$ and batch size 256 on GPU, the training time (forward and\nbackward) of butterfly matrix is $15 \\%$ faster than dense matrix multiply (GEMM from cuBLAS) and within $40 \\%$ of FFT (from cuFFT). For inference on CPU, the BP fast multiplication can be one or two orders of magnitude faster than GEMV, is within a factor of 5 of the FFT, and is within a factor of 3 of the DCT and the DST, across a range of input sizes. The GEMM/GEMV and the FFT are two of the most heavily tuned numerical routines. ![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-11.jpg?height=732&width=1612&top_left_y=490&top_left_x=243)\n\nFigure 4: Speedup of FFT and Butterfly against dense matrix-matrix multiply (GEMM) for training, and FFT, DCT, DST, and BP against dense matrix-vector multiply (GEMV) for inference. Butterfly's performance is constant with respect to any of the possible transforms it can learn, in contrast to the highly tuned implementations for specific transforms. ## 5 Conclusion\n\nWe address the problem of automatically learning fast algorithms for a class of important linear transforms, through a parameterization of recursive algorithms via butterfly factorizations. We validate our method by learning transforms including the DFT, DCT, Hadamard transform, and convolutions up to machine precision and dimension $N=1024$. Finally, we show that the same method yields consistent performance improvements and substantial compression and speed increases as a component of end-to-end ML models. ## Acknowledgments\n\nWe thank Maximilian Lam for his help with early experiments. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M) and FA86501827865 (SDH), NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, Google Cloud, Swiss Re, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, Facebook, Google, Ant Financial, NEC, SAP, VMWare, and Infosys.\n```\n\n##### *Relevant Chunk: No. 35/35 (Score: 0.32)*\n\n```\nWith the exception of learning rate, hyperparameters such as batch size 50, validation set comprising $15 \\%$ of training data, and fixed momentum at 0.9 are fixed as reported in Appendix F. 1 of their paper. For the BP methods, the learning rate was tested for the values $\\{0.005,0.01,0.02,0.05,0.1,0.2\\}$; parameters outside this range were found to be ineffective. For each method, Table 1 reports the test accuracy of the model with the highest validation accuracy. ## C. 3 Resnet\n\nWe build on top of the standard ResNet18 model from PyTorch. ${ }^{6}$ The model is modified for CIFAR-10 by reducing the kernel size and stride for the initial convolution to 3 and 1 respectively, and removing the first max pool layer. Weight decay of $\\lambda=0.0002$ was used. The learning rate was initialized in $\\{0.1,0.2\\}$,\n\n[^4]and decayed by $\\{0.1,0.2\\}$ every 25 epochs for 100 epochs total. For each method, Table 2 reports the mean and standard deviation of the test accuracies for the hyperparameters with the highest average validation accuracy. ## C. 4 Speed Comparison\n\nIn Section 4.3, we benchmark the speed of training and inference of butterfly factorizations. For training, we compare our CUDA implementation of the fast algorithm for butterfly matrices with dense matrix-matrix multiply (GEMM from cuBLAS) and FFT (from cuFFT). The batch size is 256, and we measure the total time of the forward and backward pass. The experiment is run on a Tesla P100 GPU with 16 GB of memory. For inference, we compare our simple Python implementation of the fast algorithm for the BP parameterization, against dense matrix-vector multiplication (GEMV), FFT, DCT, and DST. Our BP parameterization here refers to the product of a butterfly matrix $B^{(N)}$ and a fixed permutation $P^{(N)}$ (say, learned from data). We use the standard dense matrix-vector multiplication implementation in Numpy (BLAS binding), the FFT implementation from Numpy and the DCT and DST implementation from Scipy (FFTPACK binding). We compare their speed in single-threaded mode, running on a server Intel Xeon CPU E5-2690 v4 at 2.60GHz. Results are shown in Figure 4\n\n## D BP Hierarchy\n\nIn Definition 1. we defined the notion of a BP hierarchy, which we believes captures a natural class of matrices. To this point, we offer the following observations, the latter left as a conjecture, about the expressiveness of this hierarchy, supplementing the inclusion results of Proposition 1. Proposition 3. For every fixed $c \\geq 1$, there is a sufficiently large $N$ such that there is an $N \\times N$ matrix $M_{N}$ that is in $(B P)^{c+1}$ but not in $(B P)^{c}$. Proof. Given $c$, fix $N$ such that $N$ is even and such that $c<\\frac{N}{8 \\log _{2} N}$. For sake of contradiction, assume that every $N \\times N$ matrix in (BP) ${ }^{c+1}$ is also in $(\\mathrm{BP})^{c}$. Let $A$ be an arbitrary $\\frac{N}{2} \\times \\frac{N}{2}$ matrix. Then, from Proposition 1, $A$ is in $(\\mathrm{BP})_{2}^{2 N+10}$. Therefore, from Definition 1, there is some $N \\times N$ matrix $M \\in(\\mathrm{BP})^{2 N+10}$ such that the upper-left $\\frac{N}{2} \\times \\frac{N}{2}$ entries are $A$. From our assumption, we can replace the first $c+1$ BP factors in $M$ with $c$ (possibly different) BP factors. We can repeat this process until we are left with $c$ (BP) factors, so $M$ in $(\\mathrm{BP})^{c}$. This representation for $M$ has $c \\cdot 2 N \\log _{2} N$ parameters, which must be less than $\\frac{N}{8 \\log _{2} N} \\cdot 2 N \\log _{2} N=\\frac{N^{2}}{4}$ based on how we fixed $N$ above. However, $A$ (and therefore $M$ ) has $\\frac{N^{2}}{4}$ arbitrary entries, contradicting that it can be represented with fewer than $\\frac{N^{2}}{4}$ parameters. Hence, there must be some $N \\times N$ matrix in $(\\mathrm{BP})^{c+1}$ that is not in $(\\mathrm{BP})^{c}$. Conjecture 1. Let $M$ be an $N \\times N$ matrix such that for any $x \\in \\mathcal{F}^{N}, M x$ can be computed with an arithmetic circuit of size $N$ poly $\\log (N)$ and depth poly $\\log (N)$. Then, $M$ is in $(B P)_{O(1)}^{\\mathrm{poly} \\log N}$. We believe that we can prove an approximation of the above using known approximations of the Jacobi transform by the DCT (up to some scaling) 41. It is known that such transforms have an arithmetic circuit of the kind mentioned in the conjecture above $[12$. [^0]:    ${ }^{1}$ This characterization was equivalently known in the language of arithmetic circuits [3]. [^1]:    ${ }^{2}$ For simplicity, we assume that $N$ is a power of 2 . Otherwise, the input can be padded with zeros. [^2]:    ${ }^{3}$ Code to reproduce experiments and plots is available at https://github.com/HazyResearch/butterfly\n\n[^3]:    ${ }^{4}$ Although there is an extra addition, this can also be written as a sparse product of 3 matrices by adding auxiliary identity blocks. [^4]:    ${ }^{5}$ Available at https://github.com/HazyResearch/structured-nets\n    ${ }^{6}$ Available at https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\n\n```\n\n##### *Relevant Chunk: No. 1/35 (Score: 0.10)*\n\n```\n# Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations \n\nTri Dao ${ }^{1}$, Albert Gu ${ }^{1}$, Matthew Eichhorn ${ }^{2}$, Atri Rudra ${ }^{2}$, and Christopher R\u00e9 ${ }^{1}$<br>${ }^{1}$ Department of Computer Science, Stanford University<br>${ }^{2}$ Department of Computer Science and Engineering, University at Buffalo, SUNY<br>\\{trid,albertgu\\}@stanford.edu, \\{maeichho,atri\\}@buffalo.edu,<br>chrismre@cs.stanford.edu\n\nJanuary 1, 2021\n\n\n#### Abstract\n\nFast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions.\n```\n\n\n\n---\n## Found 6 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: butterfly algorithm implementation memory hierarchy optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Mitigating Catastrophic Forgetting in Online Continual Learning by Modeling Previous Task Interrelations via Pareto Optimization\n\n*From Search Query: butterfly algorithm implementation memory hierarchy optimization*\n\n*Yichen Wu, Hong Wang, Peilin Zhao, Yefeng Zheng, Ying Wei, Long-Kai Huang*\n\n**TL;DR:** This work reformulates replay-based CL methods as a unified hierarchical gradient aggregation framework and incorporates the Pareto optimization to capture the interrelationship among previously learned tasks and de-sign a Pareto-Optimized CL algorithm (POCL), which effectively enhances the overall performance of past tasks while ensuring the performance of the current task.\n\n**Abstract:** Catastrophic forgetting remains a core challenge in continual learning (CL), where the models struggle to retain previous knowledge when learning new tasks. While existing replay-based CL methods have been proposed to tackle this challenge by utilizing a memory buffer to store data from previous tasks, they generally overlook the interdependence between previously learned tasks and fail to encapsulate the optimally integrated knowledge in previous tasks, leading to sub-optimal performance of the previous tasks. Against this issue, we first reformulate replay-based CL methods as a unified hierarchical gradient aggregation framework. We then incorporate the Pareto optimization to capture the interrelationship among previously learned tasks and de-sign a Pareto-Optimized CL algorithm (POCL), which effectively enhances the overall performance of past tasks while ensuring the performance of the current task. To further stabilize the gradients of different tasks, we carefully devise a hyper-gradient-based implementation manner for POCL. Comprehensive empirical results demonstrate that the proposed POCL outperforms current state-of-the-art CL methods across multiple datasets and different settings.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. On-Device Training Under 256KB Memory\n\n*From Search Query: butterfly algorithm implementation memory hierarchy optimization*\n\n*Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, Song Han*\n\n**TL;DR:** This framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW.\n\n**Abstract:** On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/0pUFZYdoMY8.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 141  (*Influential: 17*)\n\n#### 3. Constrained Predictive Coding as a Biologically Plausible Model of the Cortical Hierarchy\n\n*From Search Query: butterfly algorithm implementation memory hierarchy optimization*\n\n*Siavash Golkar, Tiberiu Te\u015fileanu, Yanis Bahroun, Anirvan M. Sengupta, D. Chklovskii*\n\n**TL;DR:** This work shows that the PC framework in the linear regime can be modified to map faithfully onto the cortical hierarchy in a manner compatible with empirical observations by employing a disentangling-inspired constraint on hidden-layer neural activities and derives an upper bound for the PC objective.\n\n**Abstract:** Predictive coding has emerged as an influential normative model of neural computation, with numerous extensions and applications. As such, much effort has been put into mapping PC faithfully onto the cortex, but there are issues that remain unresolved or controversial. In particular, current implementations often involve separate value and error neurons and require symmetric forward and backward weights across different brain regions. These features have not been experimentally confirmed. In this work, we show that the PC framework in the linear regime can be modified to map faithfully onto the cortical hierarchy in a manner compatible with empirical observations. By employing a disentangling-inspired constraint on hidden-layer neural activities, we derive an upper bound for the PC objective. Optimization of this upper bound leads to an algorithm that shows the same performance as the original objective and maps onto a biologically plausible network. The units of this network can be interpreted as multi-compartmental neurons with non-Hebbian learning rules, with a remarkable resemblance to recent experimental findings. There exist prior models which also capture these features, but they are phenomenological, while our work is a normative derivation. The network we derive does not involve one-to-one connectivity or signal multiplexing, which the phenomenological models required, indicating that these features are not necessary for learning in the cortex. The normative nature of our algorithm in the simplified linear case allows us to prove interesting properties of the framework and analytically understand the computational role of our network's components. The parameters of our network have natural interpretations as physiological quantities in a multi-compartmental model of pyramidal neurons, providing a concrete link between PC and experimental measurements carried out in the cortex.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 4. Differentially Private Optimization on Large Model at Small Cost\n\n*From Search Query: butterfly algorithm implementation memory hierarchy optimization*\n\n*Zhiqi Bu, Yu-Xiang Wang, Sheng Zha, G. Karypis*\n\n**TL;DR:** A novel Book-Keeping (BK) technique is developed that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost.\n\n**Abstract:** Differentially private (DP) optimization is the standard paradigm to learn large neural networks that are accurate and privacy-preserving. The computational cost for DP deep learning, however, is notoriously heavy due to the per-sample gradient clipping. Existing DP implementations are 2-1000X more costly in time and space complexity than the standard (non-private) training. In this work, we develop a novel Book-Keeping (BK) technique that implements existing DP optimizers (thus achieving the same accuracy), with a substantial improvement on the computational cost. Specifically, BK enables DP training on large models and high dimensional data to be roughly as fast and memory-saving as the standard training, whereas previous DP algorithms can be inefficient or incapable of training due to memory error. The computational advantage of BK is supported by the complexity analysis as well as extensive experiments on vision and language tasks. Our implementation achieves state-of-the-art (SOTA) accuracy with very small extra cost: on GPT2 and at almost the same memory cost (<1% overhead), BK has 1.03X the time complexity of the standard training (0.83X training speed in practice), and 0.61X the time complexity of the most efficient DP implementation (1.36X training speed in practice). We open-source the codebase for the BK algorithm at the FastDP library (https://github.com/awslabs/fast-differential-privacy).\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 40  (*Influential: 4*)\n\n#### 5. Betty: An Automatic Differentiation Library for Multilevel Optimization\n\n*From Search Query: butterfly algorithm implementation memory hierarchy optimization*\n\n*Sang Keun Choe, W. Neiswanger, P. Xie, Eric P. Xing*\n\n**TL;DR:** Betty, a software library for large-scale MLO, is introduced with a novel dataflow graph that allows to develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O( d^2), and enables scaling MLO to models with hundreds of millions of parameters.\n\n**Abstract:** Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that Betty can be used to implement an array of MLO programs, while also observing up to 11% increase in test accuracy, 14% decrease in GPU memory usage, and 20% decrease in training wall time over existing implementations on multiple benchmarks. We also showcase that Betty enables scaling MLO to models with hundreds of millions of parameters. We open-source the code at https://github.com/leopard-ai/betty.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 23  (*Influential: 6*)\n\n### 1 related papers from Papers with Code\n\n#### 1. RECEIPT: REfine CoarsE-grained IndePendent Tasks for Parallel Tip decomposition of Bipartite Graphs\n\n*From Search Query: butterfly algorithm implementation memory hierarchy optimization*\n\n*Anonymous*\n\n**Abstract:** Tip decomposition is a crucial kernel for mining dense subgraphs in bipartite\nnetworks, with applications in spam detection, analysis of affiliation networks\netc. It creates a hierarchy of vertex-induced subgraphs with varying densities\ndetermined by the participation of vertices in butterflies (2,2-bicliques). To\nbuild the hierarchy, existing algorithms iteratively follow a\ndelete-update(peeling) process: deleting vertices with the minimum number of\nbutterflies and correspondingly updating the butterfly count of their 2-hop\nneighbors. The need to explore 2-hop neighborhood renders tip-decomposition\ncomputationally very expensive. Furthermore, the inherent sequentiality in\npeeling only minimum butterfly vertices makes derived parallel algorithms prone\nto heavy synchronization.\n  In this paper, we propose a novel parallel tip-decomposition algorithm --\nREfine CoarsE-grained Independent Tasks (RECEIPT) that relaxes the peeling\norder restrictions by partitioning the vertices into multiple independent\nsubsets that can be concurrently peeled. This enables RECEIPT to simultaneously\nachieve a high degree of parallelism and dramatic reduction in\nsynchronizations. Further, RECEIPT employs a hybrid peeling strategy along with\nother optimizations that drastically reduce the amount of wedge exploration and\nexecution time.\n  We perform detailed experimental evaluation of RECEIPT on a shared-memory\nmulticore server. It can process some of the largest publicly available\nbipartite datasets orders of magnitude faster than the state-of-the-art\nalgorithms -- achieving up to 1100x and 64x reduction in the number of thread\nsynchronizations and traversed wedges, respectively. Using 36 threads, RECEIPT\ncan provide up to 17.1x self-relative speedup. Our implementation of RECEIPT is\navailable at https://github.com/kartiklakhotia/RECEIPT.\n\n**Published:** 2020-10-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the implementation and optimization of butterfly algorithms and memory hierarchy management, here are some key insights and techniques derived from the provided sources and additional relevant information:\n\n## Butterfly Algorithm Implementations\n\n### Butterfly Decomposition\nButterfly decomposition is a crucial technique for optimizing convolutional and transform operations. For example, the paper on GPU-based FHE scheme switching acceleration, Chameleon, discusses the use of butterfly decomposition for efficient NTT (Number Theoretic Transform) computations. It highlights the use of decimation-in-time (DIT) and decimation-in-frequency (DIF) NTT based on the Cooley-Tukey (CT) and Gentleman-Sande (GS) butterflies, respectively. This approach can be adapted for efficient convolutional operations in autoregressive models by reducing memory IO and increasing FLOP utilization, as seen in the \"Simple Hardware-Efficient Long Convolutions\" reference.\n\n### FlashButterfly\nThe FlashButterfly approach, mentioned in the context of \"Simple Hardware-Efficient Long Convolutions,\" leverages classic butterfly decompositions to speed up convolutions. This method can be integrated into autoregressive models to reduce GPU memory IO and enhance computational efficiency.\n\n## Memory Hierarchy Optimization\n\n### Hierarchical Memory Access\nEfficient memory hierarchy utilization is critical for optimizing autoregressive models. Techniques such as hierarchical memory access patterns can be employed to minimize synchronization and optimize data layout. For instance, the Chameleon paper proposes a scalable NTT acceleration design that includes stage fusion methods, polynomial coefficient shuffling, and an SM-aware synchronization combination strategy to optimize memory hierarchy usage.\n\n### Selective State Compression\nSelective state compression, as mentioned in the context of Mamba's selective state space mechanisms, can help in reducing memory usage. By compressing state representations selectively, autoregressive models can achieve better memory efficiency and faster computation times.\n\n### Block-Based Processing\nBlock-based processing is another technique to optimize memory hierarchy usage. This approach ensures better cache utilization by processing data in blocks, which can significantly reduce memory access overhead. The \"Short-Long Convolutions Help Hardware-Efficient Linear Attention\" reference and the \"Mamba: Linear-Time Sequence Modeling\" paper emphasize the importance of block-based processing for efficient computation.\n\n## Efficient Parallel Scan Implementations\n\n### Parallel Scan Algorithms\nParallel scan algorithms can be applied to achieve subquadratic parallelization and fast autoregressive generation. The \"Convolutional State Space Models\" reference demonstrates how parallel scans can be used in convolutional recurrences to enhance computational efficiency. This technique can be integrated into autoregressive models to maintain linear complexity while leveraging parallel processing.\n\n### Hardware-Aware Parallel Algorithms\nDesigning hardware-aware parallel algorithms, as seen in Mamba's architecture, is essential for optimizing autoregressive models. By incorporating selective state spaces and simplified neural network architectures, these models can efficiently utilize hardware resources and reduce computational overhead.\n\n## Integration and Future Directions\n\n### Combining FastTTT and Mamba\nCombining the temporal bottlenecks of FastTTT with Mamba's selective mechanisms can lead to a unified and efficient architecture. This integration can leverage hardware-efficient parallel processing and implement unified gating for better control over the model's computations.\n\n### Unified Gating and State Management\nImplementing unified gating and efficient state management can further optimize the performance of autoregressive models. By selectively managing state spaces and using hierarchical memory access patterns, these models can achieve better performance and scalability.\n\nIn summary, the key techniques to improve autoregressive language model design include:\n\n- **Butterfly Decomposition**: Use DIT and DIF NTT based on CT and GS butterflies to optimize convolutional operations.\n- **Hierarchical Memory Access**: Optimize memory hierarchy usage through stage fusion, polynomial coefficient shuffling, and SM-aware synchronization.\n- **Selective State Compression**: Employ selective state compression to reduce memory usage.\n- **Block-Based Processing**: Use block-based processing to ensure better cache utilization.\n- **Parallel Scan Algorithms**: Apply parallel scan algorithms to achieve subquadratic parallelization.\n- **Hardware-Aware Parallel Algorithms**: Design hardware-aware parallel algorithms to efficiently utilize hardware resources.\n\nThese techniques can be integrated to create a highly efficient and scalable autoregressive language model.\n",
            "analysis": "Summary of Analysis:\n\nAfter analyzing the search results and parent architectures, I've identified key opportunities for combining HierarchicalFastTTTLinear and HierarchicalMamba while addressing hardware efficiency and parallel processing:\n\n1. Hardware-Efficient Implementation Insights:\n- Mamba's selective state space mechanism provides efficient parallel processing\n- Hardware-aware algorithms can optimize memory hierarchy usage\n- Simplified architectures can leverage parallel scans effectively\n\n2. Memory Optimization Opportunities:\n- Selective compression for state representation\n- Efficient memory hierarchy utilization\n- Block-based processing for better cache usage\n\n3. Parallel Processing Techniques:\n- Linear complexity maintenance through careful algorithm design\n- Parallel scan algorithms for efficient computation\n- Hardware-aware block decomposition\n\nUseful References with Excerpts:\n\n1. From \"Short-Long Convolutions Help Hardware-Efficient Linear Attention\":\n\"CHELA leverages the thought of tiling and hierarchy to implement linear attention in a divide-and-conquer manner... enjoys global abstraction and data-dependent selection while maintaining real linear complexity.\"\n\n2. From \"Convolutional State Space Models\":\n\"We demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation.\"\n\n3. From \"Simple Hardware-Efficient Long Convolutions\":\n\"FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization... speeds up convolutions by 2.2\u00d7\"\n\n4. From \"Mamba: Linear-Time Sequence Modeling\":\n\"We design a hardware-aware parallel algorithm in recurrent mode... selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks\"\n\nKey Insights for Design:\n\n1. Hardware-Aware Processing:\n- Use block-based processing for better cache utilization\n- Implement parallel scan algorithms for efficient computation\n- Leverage butterfly decompositions for reduced memory IO\n\n2. Memory Optimization:\n- Employ selective state compression\n- Use hierarchical memory access patterns\n- Implement efficient state management\n\n3. Architecture Integration:\n- Combine FastTTT's temporal bottlenecks with Mamba's selective mechanisms\n- Use hardware-efficient parallel processing\n- Implement unified gating for better control\n\nFuture Search Plan:\nNeed to investigate:\n1. Specific implementations of butterfly algorithms\n2. Memory hierarchy optimization techniques\n3. Efficient parallel scan implementations\n\nReferences:\n\n1. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n2. Smith, J. T. H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n3. Fu, D. Y., Epstein, E. L., Nguyen, E. N. D., Thomas, A., Zhang, M., Dao, T., Rudra, A., & R\u00e9, C. (2023). Simple Hardware-Efficient Long Convolutions for Sequence Modeling. International Conference on Machine Learning.\n\n4. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n5. Smith, J., Warrington, A., & Linderman, S. W. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations."
        },
        {
            "ready": true,
            "query": "hardware efficient butterfly algorithms, memory optimization transformers, selective computation neural networks",
            "detail": "Search for papers discussing:\n1. Hardware-efficient implementations of butterfly algorithms in neural networks\n2. Memory optimization techniques for transformer architectures\n3. Selective computation mechanisms in deep learning\n4. Integration of multiple optimization techniques\n5. Performance analysis of hardware-optimized implementations\nFocus on papers from 2022-2024 that provide detailed implementation strategies and performance metrics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hardware-efficient implementations of butterfly algorithms in neural networks\n2. Memory optimization techniques for transformer architectures\n3. Selective computation mechanisms in deep learning\n4. Integration of multiple optimization techniques\n5. Performance analysis of hardware-optimized implementations\nFocus on papers from 2022-2024 that provide detailed implementation strategies and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.80)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.80)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 2. A Faster and Better Large Language Model with Improved TransNormer (Avg. Score: 0.73)\n\n*Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Y. Qiao, Yiran Zhong*\n\n**Published in:**  (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** TransNormerLLM is presented, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency and develops a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length.\n\n**Abstract:** We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanisms, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism for smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over $20\\%$. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM.\n\n##### *Relevant Chunk: No. 19/32 (Score: 0.73)*\n\n```\narXiv preprint arXiv:2202.10447, 2022. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models, 2023. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=09-528y2Fgf\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023. Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955, 2022. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training.\n```\n\n#### 3. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.65)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.65)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 4. Block-State Transformers (Avg. Score: 0.63)\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n##### *Relevant Chunk: No. 14/28 (Score: 0.63)*\n\n```\n[11] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling, 2023. [12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020. [13] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [14] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022. [15] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022. [16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. [18] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107-116, 1998. [19] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. [20] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9099-9117. PMLR, 17-23 Jul 2022. [21] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 5. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.62)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.62)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hardware efficient butterfly algorithms, memory optimization transformers, selective computation neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: hardware efficient butterfly algorithms*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 2. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient butterfly algorithms*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 70  (*Influential: 13*)\n\n#### 3. Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization\n\n*From Search Query: hardware efficient butterfly algorithms*\n\n*Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Sch\u00f6lkopf*\n\n**TL;DR:** Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, an efficient orthogonal parameterization using butterfly structures is proposed, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT).\n\n**Abstract:** Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 7*)\n\n#### 4. Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model\n\n*From Search Query: memory optimization transformers*\n\n*Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, D. Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, V. Chaudhary, Shuai Xu, Xia Hu*\n\n**TL;DR:** This work proposes a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient in a stochastic manner.\n\n**Abstract:** With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, neural networks are usually trained using stochastic gradient descent. We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance. Following this motivation, we propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones. By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7$\\times$ peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size. Under the same hardware, WTA-CRS enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 5. SAViT: Structure-Aware Vision Transformer Pruning via Collaborative Optimization\n\n*From Search Query: memory optimization transformers*\n\n*Chuanyang Zheng, Zheyang Li, Kai Zhang, Zhi Yang, Wenming Tan, Jun Xiao, Ye Ren, Shiliang Pu*\n\n**TL;DR:** Joint importance is introduced, which integrates essential structural-aware interactions between components for the first time, to perform collaborative pruning in ViTs, and outperforms the existing state-of-the-art approaches on ImageNet.\n\n**Abstract:** Vision Transformers (ViTs) yield impressive performance across various vision tasks. However, heavy computation and memory footprint make them inaccessible for edge devices. Previous works apply importance criteria determined independently by each individual component to prune ViTs. Considering that heterogeneous components in ViTs play distinct roles, these approaches lead to suboptimal performance. In this paper, we introduce joint importance, which integrates essential structural-aware interactions between components for the first time, to perform collaborative pruning. Based on the theoretical analysis, we construct a Taylor-based approximation to evaluate the joint importance. This guides pruning toward a more balanced reduction across all components. To further reduce the algorithm complexity, we incorporate the interactions into the optimization function under some mild assumptions. Moreover, the proposed method can be seamlessly applied to various tasks including object detection. Extensive experiments demonstrate the effectiveness of our method. Notably, the proposed approach outperforms the existing state-of-the-art approaches on ImageNet, increasing accuracy by 0.7% over the DeiT-Base baseline while saving 50% FLOPs. On COCO, we are the first to show that 70% FLOPs of Faster R-CNN with ViT backbone can be removed with only 0.3% mAP drop. The code is available at https://github.com/hikvision-research/SAViT .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 29  (*Influential: 3*)\n\n#### 6. LightSeq: A High Performance Inference Library for Transformers\n\n*From Search Query: memory optimization transformers*\n\n*Xiaohui Wang, Ying Xiong, Yang Wei, Mingxuan Wang, Lei Li*\n\n**TL;DR:** A highly efficient inference library for models in the Transformer family that includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint.\n\n**Abstract:** Transformer and its variants have achieved great success in natural language processing. Since Transformer models are huge in size, serving these models is a challenge for real industrial applications. In this paper, we propose , a highly efficient inference library for models in the Transformer family. includes a series of GPU optimization techniques to both streamline the computation of Transformer layers and reduce memory footprint. supports models trained using PyTorch and Tensorflow. Experimental results on standard machine translation benchmarks show that achieves up to 14x speedup compared with TensorFlow and 1.4x speedup compared with , a concurrent CUDA implementation. The code will be released publicly after the review.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 50  (*Influential: 4*)\n\n#### 7. Understanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition\n\n*From Search Query: selective computation neural networks*\n\n*Tasha Nagamine, N. Mesgarani*\n\n**TL;DR:** This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform.\n\n**Abstract:** Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform. These methods are used to discern and quantify properties of feedforward neural networks trained to map acoustic features to phoneme labels. We show a selective and nonlinear warping of the feature space, achieved by forming prototypical functions to account for the possible variation of each class. This study provides a joint framework where the properties of node activations and the functions implemented by the network can be linked together.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 29  (*Influential: 1*)\n\n#### 8. MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition\n\n*From Search Query: selective computation neural networks*\n\n*Nicolas Menet, Michael Hersche, G. Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi*\n\n**TL;DR:** This work proposes Multiple-Input-Multiple-Output Neural Networks capable of handling many inputs at once, and applies the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMoFormer, respectively.\n\n**Abstract:** With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition. To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 9. Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis\n\n*From Search Query: selective computation neural networks*\n\n*Mitchell Ostrow, Adam Eisen, L. Kozachkov, I. Fiete*\n\n**TL;DR:** This work introduces a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA), and opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.\n\n**Abstract:** How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 5*)\n\n### 4 related papers from Papers with Code\n\n#### 1. H3T: Efficient Integration of Memory Optimization and Parallelism for Large-scale Transformer Training\n\n*From Search Query: memory optimization transformers*\n\n*Anonymous*\n\n**Abstract:** In recent years, big models based on Transformers have achieved state-of-the-art performance on many artificial intelligence (AI) tasks.\nDespite the success of these Transformer-based models, their huge parameter size poses a serious challenge to their training, both from the storage and computation perspectives.\nTo this end, memory optimization (e.g., rematerialization and offloading) and parallelism (e.g., data parallelism and model parallelism) are widely explored to make training Transformers more efficient.\nIn this paper, we propose a framework to automatically find an efficient integration of memory optimization and parallelism for High-Throughput Transformer Training (named H3T), which is rarely considered by existing efforts for training big Transformer-based models.\nSpecifically, we design search algorithms to combine appropriate memory optimization strategies and parallelism schemes to achieve a balance between memory overhead and training efficiency.\nWe implement H3T based on an open-source toolkit BMTrain and then use H3T to train the Transformers of different sizes to evaluate the efficiency of H3T.\nThe experimental results show that H3T outperforms the most popular deep learning (DL) toolkit Megatron-DeepSpeed by $1.2\\times \\sim 4.3\\times$ training speed while reducing $34.6\\% \\sim 80.5\\%$ of memory overhead.\nMoreover, H3T can use only 64 NVIDIA A100 GPUs to train GPT-3-175B, which is very difficult for existing DL toolkits. The source code is available at https://github.com/OpenBMB/BMTrain/tree/h3t.Submission Number: 3012\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-09-21\n\n\n\n#### 2. Optimizing the Memory Hierarchy by Compositing Automatic Transformations on Computations and Data\n\n*From Search Query: memory optimization transformers*\n\n*Peng Di, Jie Zhao*\n\n**Abstract:** Optimizing compilers exploit the memory hierarchy using loop tiling and fusion, but these two transformations usually interfere with each other due to the oversight of transformations on data in memories. We present a novel composition of loop tiling and fusion in this paper. Unlike existing tiling-after-fusion algorithms that only transform computation spaces, our approach first applies rectangular/parallelogram tiling to live-out computation spaces for fitting the memory hierarchy, followed by the computation of the memory footprints required by each tile. The upwards exposed data extracted from the memory footprints are used to determine the tile shapes of intermediate computation spaces, allowing the construction of arbitrary tile shapes. Finally, our technique implements a post-tiling fusion strategy for maximizing data locality without losing tilability or parallelism of live-out computation spaces, thereby enabling storage reduction and reuse, and optimizing the memory hierarchy. We demonstrate that our approach can achieve superior performance on both CPU and GPU architectures over the state of the art by experimenting on 11 benchmarks extracted from numerous domains including neural networks, image processing, sparse matrix computation and linear algebra. Also, the results of the ResNet-50 model on an AI accelerator show that our approach can obtain 16% performance improvement.\n\n**Proceeding:** 53rd-annual-ieee-acm-international-symposium\n\n**Published:** 2020-11-11\n\n\n\n#### 3. Loss-aware automatic selection of structured pruning criteria for deep neural network acceleration\n\n*From Search Query: selective computation neural networks*\n\n*Seong-heum Kim, Kilho Lee, Deepak Ghimire*\n\n**Abstract:** Structured pruning is a well-established technique for compressing neural networks, making them suitable for deployment in resource-limited edge devices. This study presents an efficient loss-aware automatic selection of structured pruning (LAASP) criteria for slimming and accelerating deep neural networks. The majority of pruning methods employ a sequential process consisting of three stages, 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed pruning technique adopts a pruning-while-training approach that eliminates the first stage and integrates the second and third stages into a single cycle. The automatic selection of magnitude or similarity-based filter pruning criteria from a specified pool of criteria and the specific pruning layer at each pruning iteration is guided by the network's overall loss on a small subset of training data. To mitigate the abrupt accuracy drop due to pruning, the network is retrained briefly after each reduction of a predefined number of floating-point operations (FLOPs). The optimal pruning rates for each layer in the network are automatically determined, eliminating the need for manual allocation of fixed or variable pruning rates for each layer. Experiments on the VGGNet, ResNet, and MobileNet models on the CIFAR-10 and ImageNet benchmark datasets demonstrate the effectiveness of the proposed method. In particular, the ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the top-1 accuracy compared to state-of-the-art methods while reducing the network FLOPs by 52%. Furthermore, pruning the ResNet50 model on the ImageNet dataset reduces FLOPs by more than 42% with a negligible 0.33% drop in the top-5 accuracy. The source code of this study is publicly available on GitHub: https://github.com/ghimiredhikura/laasp.\n\n**Proceeding:** image-and-vision-computing-2023-7\n\n**Published:** 2023-07-06\n\n\n\n#### 4. Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n\n*From Search Query: selective computation neural networks*\n\n*Woonghyun Ka, Junmo Kim, Sehwan Chun, Donggyu Joo, Pyungwhan Ahn, Doyeon Kim*\n\n**Abstract:** Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.\n\n**Published:** 2022-01-19\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hardware-efficient implementations, memory optimization, selective computation, and integration of multiple optimization techniques, here are some key papers and insights from 2022-2024 that align with your requirements:\n\n## Hardware-Efficient Implementations of Butterfly Algorithms\n- **FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores** by Fu, D. Y., et al. (2023): This paper discusses the use of tensor cores to efficiently implement convolutions, which can be adapted for butterfly algorithms. It highlights the potential for significant speedup using hardware-specific optimizations.\n\n## Memory Optimization Techniques for Transformer Architectures\n- **MODeL: Memory Optimizations for Deep Learning** by Steiner, B., et al. (2023): This work focuses on memory optimizations for deep learning models, including transformer architectures. It introduces strategies such as block-based processing, tiling, and memory lifetime optimization, which are crucial for reducing resource requirements and improving performance.\n- **Block-State Transformers** by Fathi, M., et al. (2023): This paper combines state space modeling with block transformer layers, demonstrating a more than tenfold increase in speed. It emphasizes the importance of memory hierarchy optimization and block-based processing for efficient transformer implementations.\n\n## Selective Computation Mechanisms in Deep Learning\n- **You Only Scan Once** by Qin, Z., et al. (2024): This paper proposes an efficient alternative to traditional linear recurrences by using additive linear recurrences, which can handle multi-dimensional data within a single scan. This approach can be integrated into selective computation mechanisms to improve efficiency.\n- **Block-State Transformers** also discusses selective state space modeling, which allows for more efficient computation by selectively updating the state space, thereby reducing computational overhead.\n\n## Integration of Multiple Optimization Techniques\n- **HAT: Hardware-Aware Transformers** by Wang, H., et al. (2020), although slightly older, provides a foundational framework for integrating hardware-aware optimizations with transformer architectures. This can be combined with more recent techniques to achieve balanced performance and memory efficiency.\n- **Linear Transformers with Learnable Kernel Functions** by Aksenov, Y., et al. (2024): This work advances subquadratic architectures by combining linear transformers with learnable kernel functions. It suggests that integrating multiple techniques, such as kernel functions and convolutional networks, can enhance performance and efficiency.\n\n## Performance Analysis of Hardware-Optimized Implementations\n- **Short-Long Convolutions Help Hardware-Efficient Linear Attention** by Liu, Z., et al. (2024): This paper analyzes the performance of using short-long convolutions in linear attention mechanisms, which can be optimized for hardware efficiency. It provides detailed performance metrics and implementation strategies.\n\n### Key Insights and References\n\n- **Hardware-aware algorithm design**: Papers like \"FlashFFTConv\" and \"HAT\" highlight the importance of designing algorithms that leverage hardware capabilities such as tensor cores.\n- **Memory hierarchy optimization**: Works like \"MODeL\" and \"Block-State Transformers\" emphasize the need for careful memory hierarchy optimization and block-based processing to reduce resource requirements.\n- **Selective computation**: Approaches such as those in \"You Only Scan Once\" and \"Block-State Transformers\" show how selective computation can improve efficiency by reducing unnecessary computations.\n- **Integration challenges**: Balancing computation and memory access, as well as managing multiple optimization techniques, is crucial. Papers like \"Linear Transformers with Learnable Kernel Functions\" and \"Block-State Transformers\" provide insights into integrating multiple techniques effectively.\n\nThese papers and insights should help researchers in designing and optimizing autoregressive language models with a focus on hardware efficiency, memory optimization, selective computation, and the integration of multiple optimization techniques.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\nThe SelectiveHierarchicalTTT proposal combines butterfly algorithms, selective state space modeling, and hardware-efficient memory management. The third round of search results provides crucial insights about implementation feasibility and performance optimization:\n\na) Hardware-Efficient Implementation:\n- Butterfly algorithms can be efficiently implemented using tensor cores\n- Memory hierarchy optimization is crucial for performance\n- Hardware-specific optimizations show significant speedup potential\n\nb) Memory Management:\n- Block-based processing with tiling strategies is effective\n- Memory lifetime optimization can significantly reduce resource requirements\n- Hierarchical memory access patterns need careful design\n\n2. Useful References with Key Excerpts:\n\nFrom \"Linear Transformers with Learnable Kernel Functions\" (Aksenov et al., 2024):\n\"advancing the frontier of subquadratic architectures [...] combining Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks\"\n\nFrom \"You Only Scan Once\" (Qin et al., 2024):\n\"identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence [...] can handle multi-dimensional data within a single scan\"\n\nFrom \"MambaForGCN\" (Lawan et al., 2024):\n\"introduces syntax-based Graph Convolutional Network (SynGCN) and MambaFormer modules to encode input with dependency relations and semantic information\"\n\nFrom \"Block-State Transformers\" (Fathi et al., 2023):\n\"combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation [...] demonstrates more than tenfold increase in speed\"\n\n3. Key Insights:\n\na) Implementation Strategies:\n- Hardware-aware algorithm design is crucial for performance\n- Memory hierarchy optimization needs careful consideration\n- Integration of multiple techniques requires balanced approach\n\nb) Performance Optimization:\n- Block-based processing shows promise for memory efficiency\n- Butterfly algorithms can leverage hardware acceleration\n- Selective computation mechanisms improve efficiency\n\nc) Integration Challenges:\n- Balancing computation and memory access\n- Managing multiple optimization techniques\n- Ensuring hardware compatibility\n\n4. Next Steps:\nReady for final review and rating. The collected information provides comprehensive coverage of:\n- Hardware implementation feasibility\n- Memory optimization strategies\n- Performance characteristics\n- Integration challenges and solutions\n\n5. References:\n\n[1] Aksenov, Y., et al. (2024). \"Linear Transformers with Learnable Kernel Functions are Better In-Context Models.\" arXiv preprint.\n\n[2] Qin, Z., et al. (2024). \"You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet.\" arXiv preprint.\n\n[3] Lawan, A., et al. (2024). \"MambaForGCN: Enhancing Long-Range Dependency with State Space Model.\" arXiv preprint.\n\n[4] Fathi, M., et al. (2023). \"Block-State Transformers.\" Neural Information Processing Systems.\n\n[5] Fu, D. Y., et al. (2023). \"FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores.\" arXiv preprint.\n\n[6] Steiner, B., et al. (2023). \"MODeL: Memory Optimizations for Deep Learning.\" International Conference on Machine Learning.\n\n[7] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention.\" International Conference on Machine Learning.\n\n[8] Wang, H., et al. (2020). \"HAT: Hardware-Aware Transformers.\" Annual Meeting of the Association for Computational Linguistics."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "butterfly algorithm neural networks, hardware efficient transformers, selective state space models",
            "detail": "Search for papers discussing:\n1. Implementation details of butterfly algorithms in neural networks, particularly for sequence processing\n2. Hardware-efficient designs for transformers and state space models\n3. Memory hierarchy optimization techniques in deep learning\n4. Integration of selective computation with hierarchical processing\n5. Block-based processing for efficient memory usage\nFocus on papers from 2020-2024 that specifically address computational efficiency and hardware optimization.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Implementation details of butterfly algorithms in neural networks, particularly for sequence processing\n2. Hardware-efficient designs for transformers and state space models\n3. Memory hierarchy optimization techniques in deep learning\n4. Integration of selective computation with hierarchical processing\n5. Block-based processing for efficient memory usage\nFocus on papers from 2020-2024 that specifically address computational efficiency and hardware optimization.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Block-State Transformers (Avg. Score: 0.98)\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n##### *Relevant Chunk: No. 14/28 (Score: 0.98)*\n\n```\n[11] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling, 2023. [12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020. [13] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [14] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022. [15] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022. [16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. [18] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107-116, 1998. [19] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. [20] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9099-9117. PMLR, 17-23 Jul 2022. [21] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.97)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 8/54 (Score: 0.97)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] George B Arfken and Hans J Weber. Mathematical methods for physicists. Elsevier Academic Press, 2005 . [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016. [4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. [7] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2(1):24-47, 2020. [8] John P Boyd. Chebyshev and Fourier spectral methods.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.95)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.95)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.82)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 45/64 (Score: 0.82)*\n\n```\nPlease refer to Tay et al. (2022) and Wan et al. (2023) for a comprehensive and in-depth survey of efficient transformers. Recurrent architectures Before the advent of transformers, recurrent neural networks, especially Long Short-Term Memory (LSTM) (Hochreiter \\& Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014), were the dominant architectures in NLP for sequence processing. However, traditional RNNs are hard, if not impossible, to parallelize across the time dimension, susceptible to gradient vanishing and explosion, and ineffective in capturing long-range dependencies, which are ubiquitous in natural language. These shortcomings contributed to the rapid decline of traditional RNNs in NLP. There has been a revival of RNNs in NLP research (Tiezzi et al., 2024) in recent years. Compared to transformers with quadratic complexity, RNNs are highly efficient in autoregressive inference with $\\mathrm{O}(1)$ time complexity per step, making them an attractive architecture for large language models. Many efforts have been devoted to parallelized recurrent models and improving their capability to capture long-range dependency, while maintaining the low inference complexity. The Legendre Memory Unit (LMU) (Voelker et al., 2019) was designed to efficiently handle longrange dependencies with a new type of memory cell for recurrent neural networks. Unlike LSTM units, which struggle with remembering information over very long sequences, LMU use Legendre polynomials to create a memory system that can maintain and process information over extended time periods more effectively. High-order polynomial projection operators (HiPPO) (Gu et al., 2020) generalizes LMU by providing a flexible framework for online compression of signals through polynomial projections, accommodating various polynomial bases beyond Legendre polynomials. It optimizes function approximation over time, adapting to different data timescales without needing predefined hyperparameters. SSMs have inspired a range of follow-up research to incorporate SSMs, or modified SSMs into end-to-end architectures for language modeling, including MEGA (Ma et al., 2022), DSS (Gupta et al., 2022), H3 (Fu et al., 2022), and Linear Recurrent Unit (LRU) (Orvieto et al., 2023). Mamba (Gu \\& Dao, 2023) is a selective SSM that introduces time-dependent selective mechanism to enhance the long-range modeling ability of SSMs. The selectivity removes the linear time-variance property of the SSM, making it no longer possible to parallelize Mamba as a long convolution kernel. Yet Mamba can still be effectively parallelized using parallel associative scan\n(Blelloch, 1990; Martin \\& Cundy, 2018; Smith et al., 2023) with a hardware-aware implementation. Recently proposed GateLoop (Katsch, 2023) also adopts a similar data-dependent state transitions. The data-dependent states, also concurrently proposed in GLA (Yang et al., 2023), are similar to the Weighted Key-Value State in Finch. A contemporary but independent work also proposes recurrent models named as Hawk and Griffin (De et al., 2024). Hawk is a recurrent model with the Real-Gated Linear Recurrent Unit (RG-LRU), whereas Griffin mixes the RG-LRU with local multi-query attention, thereby achieving long-context extrapolation efficiently. Please see Tiezzi et al.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.78)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.78)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: butterfly algorithm neural networks, hardware efficient transformers, selective state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. A Communication-Efficient Distributed Gradient Clipping Algorithm for Training Deep Neural Networks\n\n*From Search Query: butterfly algorithm neural networks*\n\n*Mingrui Liu, Zhenxun Zhuang, Yunwei Lei, Chunyang Liao*\n\n**TL;DR:** A relaxed-smoothness assumption of the loss landscape which LSTM was shown to satisfy in previous works is explored, and a communication-efficient gradient clipping algorithm is designed that enjoys linear speedup and reduced communication rounds in the homogeneous data setting.\n\n**Abstract:** In distributed training of deep neural networks, people usually run Stochastic Gradient Descent (SGD) or its variants on each machine and communicate with other machines periodically. However, SGD might converge slowly in training some deep neural networks (e.g., RNN, LSTM) because of the exploding gradient issue. Gradient clipping is usually employed to address this issue in the single machine setting, but exploring this technique in the distributed setting is still in its infancy: it remains mysterious whether the gradient clipping scheme can take advantage of multiple machines to enjoy parallel speedup. The main technical difficulty lies in dealing with nonconvex loss function, non-Lipschitz continuous gradient, and skipping communication rounds simultaneously. In this paper, we explore a relaxed-smoothness assumption of the loss landscape which LSTM was shown to satisfy in previous works, and design a communication-efficient gradient clipping algorithm. This algorithm can be run on multiple machines, where each machine employs a gradient clipping scheme and communicate with other machines after multiple steps of gradient-based updates. Our algorithm is proved to have $O\\left(\\frac{1}{N\\epsilon^4}\\right)$ iteration complexity and $O(\\frac{1}{\\epsilon^3})$ communication complexity for finding an $\\epsilon$-stationary point in the homogeneous data setting, where $N$ is the number of machines. This indicates that our algorithm enjoys linear speedup and reduced communication rounds. Our proof relies on novel analysis techniques of estimating truncated random variables, which we believe are of independent interest. Our experiments on several benchmark datasets and various scenarios demonstrate that our algorithm indeed exhibits fast convergence speed in practice and thus validates our theory.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 2*)\n\n#### 2. The Clock and the Pizza: Two Stories in Mechanistic Explanation of Neural Networks\n\n*From Search Query: butterfly algorithm neural networks*\n\n*Ziqian Zhong, Ziming Liu, Max Tegmark, Jacob Andreas*\n\n**TL;DR:** The results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.\n\n**Abstract:** Do neural networks, trained on well-understood algorithmic tasks, reliably rediscover known algorithms for solving those tasks? Several recent studies, on tasks ranging from group arithmetic to in-context linear regression, have suggested that the answer is yes. Using modular addition as a prototypical problem, we show that algorithm discovery in neural networks is sometimes more complex. Small changes to model hyperparameters and initializations can induce the discovery of qualitatively different algorithms from a fixed training set, and even parallel implementations of multiple such algorithms. Some networks trained to perform modular addition implement a familiar Clock algorithm; others implement a previously undescribed, less intuitive, but comprehensible procedure which we term the Pizza algorithm, or a variety of even more complex procedures. Our results show that even simple learning problems can admit a surprising diversity of solutions, motivating the development of new tools for characterizing the behavior of neural networks across their algorithmic phase space.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 66  (*Influential: 8*)\n\n#### 3. Separable Physics-Informed Neural Networks\n\n*From Search Query: butterfly algorithm neural networks*\n\n*Junwoo Cho, Seungtae Nam, Hyunmo Yang, S. Yun, Youngjoon Hong, Eunbyung Park*\n\n**TL;DR:** The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs, and can solve a chaotic (2+1)-d Navier-Stokes equation significantly faster than the best-performing prior method, maintaining accuracy.\n\n**Abstract:** Physics-informed neural networks (PINNs) have recently emerged as promising data-driven PDE solvers showing encouraging results on various PDEs. However, there is a fundamental limitation of training PINNs to solve multi-dimensional PDEs and approximate highly complex solution functions. The number of training points (collocation points) required on these challenging PDEs grows substantially, but it is severely limited due to the expensive computational costs and heavy memory overhead. To overcome this issue, we propose a network architecture and training algorithm for PINNs. The proposed method, separable PINN (SPINN), operates on a per-axis basis to significantly reduce the number of network propagations in multi-dimensional PDEs unlike point-wise processing in conventional PINNs. We also propose using forward-mode automatic differentiation to reduce the computational cost of computing PDE residuals, enabling a large number of collocation points (>10^7) on a single commodity GPU. The experimental results show drastically reduced computational costs (62x in wall-clock time, 1,394x in FLOPs given the same number of collocation points) in multi-dimensional PDEs while achieving better accuracy. Furthermore, we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation significantly faster than the best-performing prior method (9 minutes vs 10 hours in a single GPU), maintaining accuracy. Finally, we showcase that SPINN can accurately obtain the solution of a highly nonlinear and multi-dimensional PDE, a (3+1)-d Navier-Stokes equation. For visualized results and code, please see https://jwcho5576.github.io/spinn.github.io/.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 3*)\n\n#### 4. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers\n\n*From Search Query: hardware efficient transformers*\n\n*Z. Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He*\n\n**TL;DR:** This work is able to show that ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference.\n\n**Abstract:** How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 312  (*Influential: 37*)\n\n#### 5. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: selective state space models*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 6. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space models*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: selective state space models*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 64  (*Influential: 6*)\n\n### 2 related papers from ArXiv\n\n#### 1. MambaByte: Token-free Selective State Space Model\n\n*From Search Query: selective state space models*\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the\ninductive bias of subword tokenization. Operating on bytes, however, results in\nsignificantly longer sequences. In this setting, standard autoregressive\nTransformers scale poorly as the effective memory required grows with sequence\nlength. The recent development of the Mamba state space model (SSM) offers an\nappealing alternative approach with a fixed-sized memory state and efficient\ndecoding. We propose MambaByte, a token-free adaptation of the Mamba SSM\ntrained autoregressively on byte sequences. In terms of modeling, we show\nMambaByte to be competitive with, and even to outperform, state-of-the-art\nsubword Transformers on language modeling tasks while maintaining the benefits\nof token-free language models, such as robustness to noise. In terms of\nefficiency, we develop an adaptation of speculative decoding with tokenized\ndrafting and byte-level verification. This results in a $2.6\\times$ inference\nspeedup to the standard MambaByte implementation, showing similar decoding\nefficiency as the subword Mamba. These findings establish the viability of SSMs\nin enabling token-free language modeling.\n\n**Published:** 2024-01-24T18:53:53Z  (*Updated: 2024-08-09T20:18:57Z*)\n\n\n\n#### 2. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context\n  Language Modeling\n\n*From Search Query: selective state space models*\n\n*Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, Weizhu Chen*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a\nlong-standing problem. Past works suffer from either the quadratic computation\ncomplexity or the limited extrapolation ability on length generalization. In\nthis work, we present Samba, a simple hybrid architecture that layer-wise\ncombines Mamba, a selective State Space Model (SSM), with Sliding Window\nAttention (SWA). Samba selectively compresses a given sequence into recurrent\nhidden states while still maintaining the ability to precisely recall memories\nwith the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T\ntraining tokens and show that Samba substantially outperforms the\nstate-of-the-art models based on pure attention or SSMs on a wide range of\nbenchmarks. When trained on 4K length sequences, Samba can be efficiently\nextrapolated to 256K context length with perfect memory recall and show\nimproved token predictions up to 1M context length. As a linear-time sequence\nmodel, Samba enjoys a 3.73x higher throughput compared to Transformers with\ngrouped-query attention when processing user prompts of 128K length, and 3.64x\nspeedup when generating 64K tokens with unlimited streaming. A sample\nimplementation of Samba is publicly available in\nhttps://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11T17:50:51Z  (*Updated: 2024-06-11T17:50:51Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Discovery of sparse hysteresis models for piezoelectric materials\n\n*From Search Query: butterfly algorithm neural networks*\n\n*Daniel M. Tartakovsky, Elena A. Lomonova, Koen Tiels, Mitrofan Curti, Bram Daniels, Abhishek Chandra*\n\n**Abstract:** This article presents an approach for modelling hysteresis in piezoelectric materials, that leverages recent advancements in machine learning, particularly in sparse-regression techniques. While sparse regression has previously been used to model various scientific and engineering phenomena, its application to nonlinear hysteresis modelling in piezoelectric materials has yet to be explored. The study employs the least-squares algorithm with a sequential threshold to model the dynamic system responsible for hysteresis, resulting in a concise model that accurately predicts hysteresis for both simulated and experimental piezoelectric material data. Several numerical experiments are performed, including learning butterfly-shaped hysteresis and modelling real-world hysteresis data for a piezoelectric actuator. The presented approach is compared to traditional regression-based and neural network methods, demonstrating its efficiency and robustness. Source code is available at https://github.com/chandratue/SmartHysteresis\n\n**Published:** 2023-02-10\n\n\n\n#### 2. Symmetry Detection and Classification in Drawings of Graphs\n\n*From Search Query: butterfly algorithm neural networks*\n\n*Md Iqbal Hossain, Stephen Kobourov, Felice De Luca*\n\n**Abstract:** Symmetry is a key feature observed in nature (from flowers and leaves, to butterflies and birds) and in human-made objects (from paintings and sculptures, to manufactured objects and architectural design). Rotational, translational, and especially reflectional symmetries, are also important in drawings of graphs. Detecting and classifying symmetries can be very useful in algorithms that aim to create symmetric graph drawings and in this paper we present a machine learning approach for these tasks. Specifically, we show that deep neural networks can be used to detect reflectional symmetries with 92% accuracy. We also build a multi-class classifier to distinguish between reflectional horizontal, reflectional vertical, rotational, and translational symmetries. Finally, we make available a collection of images of graph drawings with specific symmetric features that can be used in machine learning systems for training, testing and validation purposes. Our datasets, best trained ML models, source code are available online.\n\n**Published:** 2019-07-01\n\n\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient transformers*\n\n*Yoon Kim, Rameswar Panda, Yikang Shen, Bailin Wang, Songlin Yang*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Published:** 2023-12-11\n\n\n\n#### 4. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: hardware efficient transformers*\n\n*Zhanghao Wu, Hanrui Wang, Zhijian Liu, Song Han, Ligeng Zhu, Han Cai, Chuang Gan*\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with $\\textit{arbitrary encoder-decoder attention}$ and $\\textit{heterogeneous layers}$. Then we train a $\\textit{SuperTransformer}$ that covers all candidates in the design space, and efficiently produces many $\\textit{SubTransformers}$ with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized $\\textit{SubTransformer}$ dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT'14 translation task on Raspberry Pi-4, HAT can achieve $\\textbf{3}\\times$ speedup, $\\textbf{3.7}\\times$ smaller size over baseline Transformer; $\\textbf{2.7}\\times$ speedup, $\\textbf{3.6}\\times$ smaller size over Evolved Transformer with $\\textbf{12,041}\\times$ less search cost and no performance loss. HAT code is https://github.com/mit-han-lab/hardware-aware-transformers.git\n\n**Conference:** hat-hardware-aware-transformers-for-efficient-1\n\n**Published:** 2020-05-28\n\n\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: selective state space models*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 6. PyramidMamba: Rethinking Pyramid Feature Fusion with Selective Space State Model for Semantic Segmentation of Remote Sensing Imagery\n\n*From Search Query: selective state space models*\n\n*Danfeng Hong, Xiaokang Zhang, Xiaoliang Meng, Sijun Dong, Dongxu Li, Libo Wang*\n\n**Abstract:** Semantic segmentation, as a basic tool for intelligent interpretation of remote sensing images, plays a vital role in many Earth Observation (EO) applications. Nowadays, accurate semantic segmentation of remote sensing images remains a challenge due to the complex spatial-temporal scenes and multi-scale geo-objects. Driven by the wave of deep learning (DL), CNN- and Transformer-based semantic segmentation methods have been explored widely, and these two architectures both revealed the importance of multi-scale feature representation for strengthening semantic information of geo-objects. However, the actual multi-scale feature fusion often comes with the semantic redundancy issue due to homogeneous semantic contents in pyramid features. To handle this issue, we propose a novel Mamba-based segmentation network, namely PyramidMamba. Specifically, we design a plug-and-play decoder, which develops a dense spatial pyramid pooling (DSPP) to encode rich multi-scale semantic features and a pyramid fusion Mamba (PFM) to reduce semantic redundancy in multi-scale feature fusion. Comprehensive ablation experiments illustrate the effectiveness and superiority of the proposed method in enhancing multi-scale feature representation as well as the great potential for real-time semantic segmentation. Moreover, our PyramidMamba yields state-of-the-art performance on three publicly available datasets, i.e. the OpenEarthMap (70.8% mIoU), ISPRS Vaihingen (84.8% mIoU) and Potsdam (88.0% mIoU) datasets. The code will be available at https://github.com/WangLibo1995/GeoSeg.\n\n**Published:** 2024-06-16\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models with a focus on computational efficiency, hardware optimization, and the integration of novel techniques such as butterfly algorithms and selective state space models, here are some key points and references that align with the specified details:\n\n## Implementation Details of Butterfly Algorithms in Neural Networks\n\n- The work by Dao et al. (2019) on \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\" is a foundational piece that discusses how butterfly factorizations can be used to speed up linear transforms, which is crucial for sequence processing in neural networks. This approach can be integrated into hierarchical processing to enhance efficiency.\n\n## Hardware-Efficient Designs for Transformers and State Space Models\n\n- The paper by Fu et al. (2023) titled \"Simple Hardware-Efficient Long Convolutions for Sequence Modeling\" addresses hardware-efficient designs specifically for sequence modeling. This work focuses on optimizing long convolutions, which can be adapted for transformer architectures to improve their hardware efficiency.\n- Gu and Dao (2023) in \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\" introduce a model that uses selective state spaces and linear-time sequence modeling, which can be highly efficient in terms of both computation and memory usage. This work integrates well with the concept of hardware-efficient transformers and state space models.\n\n## Memory Hierarchy Optimization Techniques in Deep Learning\n\n- The guide on advances in deep learning efficiency highlights the importance of memory hierarchy optimization, including techniques such as pruning and the use of sparsity-aware kernels. It notes that while pruning can reduce memory load, it does not necessarily provide significant speed-ups due to the lack of locality in weight matrix multiplication. However, clever implementations can target sparsity-aware kernels to improve efficiency.\n\n## Integration of Selective Computation with Hierarchical Processing\n\n- The work on \"Mamba\" by Gu and Dao (2023) is particularly relevant here as it integrates selective state spaces with linear-time sequence modeling. This integration allows for efficient hierarchical processing by selectively updating and managing the state space, which can be crucial for reducing computational and memory costs.\n\n## Block-Based Processing for Efficient Memory Usage\n\n- Block-based processing is a technique that can be highly effective for optimizing memory usage. This approach is mentioned in the context of optimizing deep learning models, where dividing the data into blocks can improve cache utilization and reduce memory access overhead. The guide on deep learning efficiency touches on this by discussing how each parameter in the model might not be equally important, suggesting that block-based processing can help in managing this efficiently.\n\n## Additional Considerations\n\n- For hardware-specific optimizations, the guide on deep learning efficiency discusses the importance of optimizing optimizers and the use of efficient data structures. It also mentions that clever implementations of pruning can target sparsity-aware kernels, which is relevant for block-based processing and memory hierarchy optimization.\n\nIn summary, the works by Dao et al. (2019), Gu and Dao (2023), and Fu et al. (2023) provide significant insights into how butterfly algorithms, selective state space models, and hardware-efficient designs can be integrated to improve the efficiency and scalability of autoregressive language models. These papers, along with the general guide on deep learning efficiency, offer practical strategies for optimizing memory hierarchy, block-based processing, and hardware-specific optimizations.\n",
            "analysis": "Initial Analysis of SelectiveHierarchicalTTT Proposal\n\n1. Key Components Requiring Investigation:\n\na) Butterfly-Based Processing:\n- Novel integration with hierarchical processing\n- Hardware efficiency claims\n- Implementation feasibility\n- Performance implications\n\nb) Selective State Management:\n- Comparison with existing state compression techniques\n- Memory-computation trade-offs\n- Integration with temporal bottlenecks\n- Scalability characteristics\n\nc) Memory Hierarchy Optimization:\n- Block-based processing effectiveness\n- Cache utilization strategies\n- Memory access patterns\n- Hardware-specific optimizations\n\n2. Comparison with Siblings and Similar Designs:\n\nThe proposal differs from AdaptiveHierarchicalTTT (sibling) in several ways:\n- Focuses on butterfly algorithms vs adaptive computation\n- Emphasizes hardware efficiency vs adaptive activation\n- Uses block-based processing vs dynamic routing\n- Integrates selective state space modeling differently\n\n3. Potential Impact Areas:\n\na) Accuracy:\n- Multi-scale processing may improve feature capture\n- Selective mechanisms could enhance precision\n- Butterfly transforms might affect numerical stability\n\nb) Robustness:\n- Hierarchical processing may improve generalization\n- Selective mechanisms could enhance adaptation\n- Memory optimization might affect stability\n\nc) Efficiency:\n- Butterfly algorithms promise computational benefits\n- Block-based processing suggests memory improvements\n- Hardware-aware design indicates optimization potential\n\nd) Scalability:\n- Linear complexity claims need verification\n- Memory hierarchy impact on scaling\n- Hardware efficiency at scale\n\n4. Investigation Plan:\n\nRound 1:\n- Focus on butterfly algorithms in language models\n- Examine hardware-efficient implementations\n- Study memory hierarchy optimizations\n\nRound 2:\n- Investigate selective state space modeling\n- Research temporal bottleneck implementations\n- Analyze block-based processing techniques\n\nRound 3:\n- Study integration approaches\n- Examine scaling characteristics\n- Research hardware optimization strategies\n\nReferences from Initial Review:\n1. Dao, T., et al. (2019). \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\"\n2. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\"\n3. Fu, D. Y., et al. (2023). \"Simple Hardware-Efficient Long Convolutions for Sequence Modeling\"\n\nThe proposal appears to be novel in its specific combination of butterfly algorithms, selective state space modeling, and hardware-efficient memory hierarchy optimization. While individual components have precedent, their integration and focus on hardware efficiency presents a unique approach."
        },
        {
            "ready": false,
            "query": "butterfly algorithm optimization neural networks, memory hierarchy deep learning optimization, hardware efficient selective computation",
            "detail": "Search for papers discussing:\n1. Specific implementations and optimizations of butterfly algorithms in neural networks\n2. Memory hierarchy design and optimization techniques for deep learning models\n3. Hardware-efficient implementations of selective computation mechanisms\n4. Integration strategies for combining multiple optimization techniques\n5. Performance analysis and benchmarking of hybrid architectures\nFocus on papers from 2022-2024 that provide detailed technical implementations and performance analyses.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Specific implementations and optimizations of butterfly algorithms in neural networks\n2. Memory hierarchy design and optimization techniques for deep learning models\n3. Hardware-efficient implementations of selective computation mechanisms\n4. Integration strategies for combining multiple optimization techniques\n5. Performance analysis and benchmarking of hybrid architectures\nFocus on papers from 2022-2024 that provide detailed technical implementations and performance analyses.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.59)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 16/46 (Score: 0.80)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [25] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations.\n```\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.38)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 2. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.49)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 9/35 (Score: 0.49)*\n\n```\nOur implementation is built on top of the publicly available implementation of Thomas et al. 42 with the same hyperparameters, and we report their numbers for the competing baseline methods directly. We test on the three main datasets from Thomas et al. 42]: two challenging variants of MNIST-one with randomly rotated images and random background, the other with correlated background noise-and the standard CIFAR-10 dataset. Table 1: Test accuracy when replacing the hidden layer with structured classes. For the BPBP methods, the permutations $P$ have been fixed to the bit-reversal permutation. The butterfly parameterization achieves higher accuracy than the unstructured layer on all datasets. | Method | MNIST-bg-rot | MNIST-noise | CIFAR-10 | Compression factor |\n| :--- | :--- | :--- | :--- | :--- |\n| Unstructured | 44.08 | 65.15 | 46.03 | 1 |\n| BPBP (complex, fixed permutation) | $\\mathbf{4 6 . 2 6}$ | 77.00 | $\\mathbf{4 9 . 9 3}$ | 39.4 |\n| BPBP (real, fixed permutation) | 46.16 | 75.00 | 48.69 | 56.9 |\n| LDR-TD 42] | 45.81 | $\\mathbf{7 8 . 4 5}$ | 45.33 | 56.9 |\n| Toeplitz-like [40] | 42.67 | 75.75 | 41.78 | 56.9 |\n| Fastfood 47] | 38.13 | 63.55 | 39.64 | 78.7 |\n| Circulant 6] | 34.46 | 65.35 | 34.28 | 93.0 |\n| Low-rank [9] | 35.67 | 52.25 | 32.28 | 56.9 |\n\nTable 1 reports results for variants of our butterfly parametrization, compared to the unstructured matrix baseline and other structured matrix approaches. Notably, the butterfly methods achieve higher classification accuracy than the fully-connected layer on all datasets and are highly competitive with the other approaches. We note that improvements over unconstrained matrices can arise from lower generalization error due to fewer parameters (relating to VC bounds, Proposition 2), or better inductive bias encoded by the structured class. For example, convolutions are important in image tasks due to encoding shift equivariance, and Thomas et al. 42 hypothesize that their structured classes improve over FC layers through imposing approximate equivariance to more general transformations. Since our BP parametrization can represent arbitrary convolutions, it can encode these important priors. ResNet In addition to the standard single hidden layer benchmarks, we test the effect of using butterfly layers in a standard ResNet18 [18] implementation on the CIFAR-10 dataset. This architecture is normally fully convolutional, ending with a FC layer of dimensions $512 \\times 10$ before the softmax. However, we experiment with adding an additional FC or structured layer right before this final FC layer. Table 2 shows that the ResNet18 architecture can benefit from an additional fully connected layer, and using a BPBP layer instead improves performance even more while adding a negligible ( $0.07 \\%$ increase) number of parameters to the original model. Table 2: Classification accuracy for the ResNet18 architecture with different layers inserted before the final FC/softmax layer. | Last layer | None | FC | BPBP |\n| :--- | :--- | :--- | :--- |\n| Accuracy | $93.58 \\pm 0.15$ | $93.89 \\pm 0.19$ | $\\mathbf{9 4 . 0 1} \\pm 0.09$ |\n\n### 4.3 Training and Inference Speed Comparison\n\nBy design, the BP parameterization yields a fast algorithm of complexity $O(N \\log N)$, no matter which transform it learns. Moreover, given the parameters of the BP model, it is easy to implement this fast algorithm (this can be done in 5 lines of Python, and our code provides a function to do this automatically). The BP parameterization captures many common transforms (Section 4.1), and its implementation makes no transform-specific optimizations. Nevertheless, our simple implementation is surprisingly competitive with hand-tuned kernels both for training and for inference (after the parameters of the BP model are learned and we wish to evaluate $B P x$ for new input $x$ ). In Figure 4, we compare the speed of the BP fast multiplication against specialized implementation of common transforms such as the FFT, DCT, and DST (all have complexity $O(N \\log N)$ ), using dense matrix-vector multiply (GEMV, complexity $O\\left(N^{2}\\right)$ ) as a baseline. For training with realistic input sizes $N=1024$ and batch size 256 on GPU, the training time (forward and\nbackward) of butterfly matrix is $15 \\%$ faster than dense matrix multiply (GEMM from cuBLAS) and within $40 \\%$ of FFT (from cuFFT). For inference on CPU, the BP fast multiplication can be one or two orders of magnitude faster than GEMV, is within a factor of 5 of the FFT, and is within a factor of 3 of the DCT and the DST, across a range of input sizes. The GEMM/GEMV and the FFT are two of the most heavily tuned numerical routines. ![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-11.jpg?height=732&width=1612&top_left_y=490&top_left_x=243)\n\nFigure 4: Speedup of FFT and Butterfly against dense matrix-matrix multiply (GEMM) for training, and FFT, DCT, DST, and BP against dense matrix-vector multiply (GEMV) for inference. Butterfly's performance is constant with respect to any of the possible transforms it can learn, in contrast to the highly tuned implementations for specific transforms. ## 5 Conclusion\n\nWe address the problem of automatically learning fast algorithms for a class of important linear transforms, through a parameterization of recursive algorithms via butterfly factorizations. We validate our method by learning transforms including the DFT, DCT, Hadamard transform, and convolutions up to machine precision and dimension $N=1024$. Finally, we show that the same method yields consistent performance improvements and substantial compression and speed increases as a component of end-to-end ML models. ## Acknowledgments\n\nWe thank Maximilian Lam for his help with early experiments. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M) and FA86501827865 (SDH), NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, Google Cloud, Swiss Re, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, Facebook, Google, Ant Financial, NEC, SAP, VMWare, and Infosys.\n```\n\n#### 3. Scatterbrain: Unifying Sparse and Low-rank Attention Approximation (Avg. Score: 0.28)\n\n*Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 93  (*Influential: 11*)\n\n**TL;DR:** Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, Scatterbrain is proposed, a novel way to unify sparse and low-rank attention for accurate and efficient approximation and is unbiased with provably low error.\n\n**Abstract:** Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.\n\n##### *Relevant Chunk: No. 10/51 (Score: 0.28)*\n\n```\nThe U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of NIH, ONR, or the U.S. Government. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] Alexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya Razenshteyn, and Ludwig Schmidt. Practical and optimal lsh for angular distance.\n```\n\n#### 4. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.28)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 8/54 (Score: 0.28)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] George B Arfken and Hans J Weber. Mathematical methods for physicists. Elsevier Academic Press, 2005 . [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016. [4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. [7] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2(1):24-47, 2020. [8] John P Boyd. Chebyshev and Fourier spectral methods.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: butterfly algorithm optimization neural networks, memory hierarchy deep learning optimization, hardware efficient selective computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Optimization and Bayes: A Trade-off for Overparameterized Neural Networks\n\n*From Search Query: butterfly algorithm optimization neural networks*\n\n*Zhengmian Hu, Heng Huang*\n\n**TL;DR:** The proposed algorithm TransBL enables a trade-off between generalization and sampling efficiency, and derives the first algorithm-dependent PAC-Bayesian generalization bound for infinitely wide networks based on an exact KL divergence between the trained posterior distribution obtained by infinitesimal step size gradient descent and a Gaussian prior.\n\n**Abstract:** This paper proposes a novel algorithm, Transformative Bayesian Learning (TransBL), which bridges the gap between empirical risk minimization (ERM) and Bayesian learning for neural networks. We compare ERM, which uses gradient descent to optimize, and Bayesian learning with importance sampling for their generalization and computational complexity. We derive the first algorithm-dependent PAC-Bayesian generalization bound for infinitely wide networks based on an exact KL divergence between the trained posterior distribution obtained by infinitesimal step size gradient descent and a Gaussian prior. Moreover, we show how to transform gradient-based optimization into importance sampling by incorporating a weight. While Bayesian learning has better generalization, it suffers from low sampling efficiency. Optimization methods, on the other hand, have good sampling efficiency but poor generalization. Our proposed algorithm TransBL enables a trade-off between generalization and sampling efficiency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time\n\n*From Search Query: butterfly algorithm optimization neural networks*\n\n*Arvind V. Mahankali, Jeff Z. HaoChen, Kefan Dong, Margalit Glasgow, Tengyu Ma*\n\n**TL;DR:** It is proved that with sample size $n = O(d^{3.1})$ where $d$ is the dimension of the inputs, the network trained with projected gradient flow converges in $\\text{poly}(d)$ time to a non-trivial error that is not achievable by kernel methods using n samples, hence demonstrating a clear separation between unmodified gradient descent and NTK.\n\n**Abstract:** Despite recent theoretical progress on the non-convex optimization of two-layer neural networks, it is still an open question whether gradient descent on neural networks without unnatural modifications can achieve better sample complexity than kernel methods. This paper provides a clean mean-field analysis of projected gradient flow on polynomial-width two-layer neural networks. Different from prior works, our analysis does not require unnatural modifications of the optimization algorithm. We prove that with sample size $n = O(d^{3.1})$ where $d$ is the dimension of the inputs, the network trained with projected gradient flow converges in $\\text{poly}(d)$ time to a non-trivial error that is not achievable by kernel methods using $n \\ll d^4$ samples, hence demonstrating a clear separation between unmodified gradient descent and NTK. As a corollary, we show that projected gradient descent with a positive learning rate and a polynomial number of iterations converges to low error with the same sample complexity.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 3. Hypernetwork-based Meta-Learning for Low-Rank Physics-Informed Neural Networks\n\n*From Search Query: butterfly algorithm optimization neural networks*\n\n*Woojin Cho, Kookjin Lee, Donsub Rim, Noseong Park*\n\n**TL;DR:** A lightweight low-rank PINNs containing only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm, which allows efficient approximation of solutions of PDEs for varying ranges of Pde input parameters, is proposed.\n\n**Abstract:** In various engineering and applied science applications, repetitive numerical simulations of partial differential equations (PDEs) for varying input parameters are often required (e.g., aircraft shape optimization over many design parameters) and solvers are required to perform rapid execution. In this study, we suggest a path that potentially opens up a possibility for physics-informed neural networks (PINNs), emerging deep-learning-based solvers, to be considered as one such solver. Although PINNs have pioneered a proper integration of deep-learning and scientific computing, they require repetitive time-consuming training of neural networks, which is not suitable for many-query scenarios. To address this issue, we propose a lightweight low-rank PINNs containing only hundreds of model parameters and an associated hypernetwork-based meta-learning algorithm, which allows efficient approximation of solutions of PDEs for varying ranges of PDE input parameters. Moreover, we show that the proposed method is effective in overcoming a challenging issue, known as\"failure modes\"of PINNs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 4. Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping\n\n*From Search Query: memory hierarchy deep learning optimization*\n\n*Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Y. Lee, A. Backurs, Nenghai Yu, J. Bian*\n\n**TL;DR:** It is shown that per-layer clipping, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization, which results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest.\n\n**Abstract:** Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \\emph{group-wise clipping}. To reduce the compute time overhead of private learning, we show that \\emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest. While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3. We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \\emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 38  (*Influential: 5*)\n\n#### 5. MODeL: Memory Optimizations for Deep Learning\n\n*From Search Query: memory hierarchy deep learning optimization*\n\n*Benoit Steiner, Mostafa Elhoushi, Jacob Kahn, James Hegarty*\n\n**TL;DR:** This work presents MODeL, an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks, and presents several techniques to simplify the encoding of the problem, and enables the approach to scale to the size of state-of- the-art neural networks using an off-the-shelf ILP solver.\n\n**Abstract:** The size of deep neural networks has grown exponentially in recent years. Unfortunately, hardware devices have not kept pace with the rapidly increasing memory requirements. To cope with this, researchers have proposed various techniques including spilling, recomputation, reduced precision training, model pruning, and so on. However, these approaches suffer from various limitations: they can increase training time, affect model accuracy, or require extensive manual modifications to the neural networks. We present MODeL , an algorithm that optimizes the lifetime and memory location of the tensors used to train neural networks. Our method automatically reduces the memory usage of existing neural networks without any of the drawbacks of other techniques. We formulate the problem as a joint integer linear program (ILP). We present several techniques to simplify the encoding of the problem, and enable our approach to scale to the size of state-of-the-art neural networks using an off-the-shelf ILP solver. We experimentally demonstrate that MODeL only takes seconds to allow the training of neural networks using 30% less memory on average. MODeL is an open-source project available at https://github.com/facebookresearch/model opt.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 2*)\n\n#### 6. MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning\n\n*From Search Query: memory hierarchy deep learning optimization*\n\n*Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, Song Han*\n\n**TL;DR:** This study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.\n\n**Abstract:** Tiny deep learning on microcontroller units (MCUs) is challenging due to the limited memory size. We find that the memory bottleneck is due to the imbalanced memory distribution in convolutional neural network (CNN) designs: the first several blocks have an order of magnitude larger memory usage than the rest of the network. To alleviate this issue, we propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. However, naive implementation brings overlapping patches and computation overhead. We further propose network redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. Manually redistributing the receptive field is difficult. We automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the peak memory usage of existing networks by 4-8x. Co-designed with neural networks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves>90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2 also unblocks object detection on tiny devices, achieving 16.9% higher mAP on Pascal VOC compared to the state-of-the-art result. Our study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 123  (*Influential: 17*)\n\n#### 7. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient selective computation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. HAT: Hardware-Aware Transformers for Efficient Natural Language Processing\n\n*From Search Query: hardware efficient selective computation*\n\n*Hanrui Wang, Zhanghao Wu, Zhijian Liu, Han Cai, Ligeng Zhu, Chuang Gan, Song Han*\n\n**TL;DR:** This work designs Hardware-Aware Transformers with neural architecture search, and trains a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing, and performs an evolutionary search with a hardware latency constraint.\n\n**Abstract:** Transformers are ubiquitous in Natural Language Processing (NLP) tasks, but they are difficult to be deployed on hardware due to the intensive computation. To enable low-latency inference on resource-constrained hardware platforms, we propose to design Hardware-Aware Transformers (HAT) with neural architecture search. We first construct a large design space with arbitrary encoder-decoder attention and heterogeneous layers. Then we train a SuperTransformer that covers all candidates in the design space, and efficiently produces many SubTransformers with weight sharing. Finally, we perform an evolutionary search with a hardware latency constraint to find a specialized SubTransformer dedicated to run fast on the target hardware. Extensive experiments on four machine translation tasks demonstrate that HAT can discover efficient models for different hardware (CPU, GPU, IoT device). When running WMT\u201914 translation task on Raspberry Pi-4, HAT can achieve 3\u00d7 speedup, 3.7\u00d7 smaller size over baseline Transformer; 2.7\u00d7 speedup, 3.6\u00d7 smaller size over Evolved Transformer with 12,041\u00d7 less search cost and no performance loss. HAT is open-sourced at https://github.com/mit-han-lab/hardware-aware-transformers.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 243  (*Influential: 45*)\n\n#### 9. Towards Robust and Efficient Cloud-Edge Elastic Model Adaptation via Selective Entropy Distillation\n\n*From Search Query: hardware efficient selective computation*\n\n*Yaofo Chen, Shuaicheng Niu, Shoukai Xu, Hengjie Song, Yaowei Wang, Mingkui Tan*\n\n**TL;DR:** A Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online and to reduce the communication burden is established.\n\n**Abstract:** The conventional deep learning paradigm often involves training a deep model on a server and then deploying the model or its distilled ones to resource-limited edge devices. Usually, the models shall remain fixed once deployed (at least for some period) due to the potential high cost of model adaptation for both the server and edge sides. However, in many real-world scenarios, the test environments may change dynamically (known as distribution shifts), which often results in degraded performance. Thus, one has to adapt the edge models promptly to attain promising performance. Moreover, with the increasing data collected at the edge, this paradigm also fails to further adapt the cloud model for better performance. To address these, we encounter two primary challenges: 1) the edge model has limited computation power and may only support forward propagation; 2) the data transmission budget between cloud and edge devices is limited in latency-sensitive scenarios. In this paper, we establish a Cloud-Edge Elastic Model Adaptation (CEMA) paradigm in which the edge models only need to perform forward propagation and the edge models can be adapted online. In our CEMA, to reduce the communication burden, we devise two criteria to exclude unnecessary samples from uploading to the cloud, i.e., dynamic unreliable and low-informative sample exclusion. Based on the uploaded samples, we update and distribute the affine parameters of normalization layers by distilling from the stronger foundation model to the edge model with a sample replay strategy. Extensive experimental results on ImageNet-C and ImageNet-R verify the effectiveness of our CEMA.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Allo: A Programming Model for Composable Accelerator Design\n\n*From Search Query: memory hierarchy deep learning optimization*\n\n*Zhiru Zhang, Mengjia Dai, Zhichen Zeng, Shaojie Xiang, Niansong Zhang, Hongzheng Chen*\n\n**Abstract:** Special-purpose hardware accelerators are increasingly pivotal for sustaining performance improvements in emerging applications, especially as the benefits of technology scaling continue to diminish. However, designers currently lack effective tools and methodologies to construct complex, high-performance accelerator architectures in a productive manner. Existing high-level synthesis (HLS) tools often require intrusive source-level changes to attain satisfactory quality of results. Despite the introduction of several new accelerator design languages (ADLs) aiming to enhance or replace HLS, their advantages are more evident in relatively simple applications with a single kernel. Existing ADLs prove less effective for realistic hierarchical designs with multiple kernels, even if the design hierarchy is flattened. In this paper, we introduce Allo, a composable programming model for efficient spatial accelerator design. Allo decouples hardware customizations, including compute, memory, communication, and data type from algorithm specification, and encapsulates them as a set of customization primitives. Allo preserves the hierarchical structure of an input program by combining customizations from different functions in a bottom-up, type-safe manner. This approach facilitates holistic optimizations that span across function boundaries. We conduct comprehensive experiments on commonly-used HLS benchmarks and several realistic deep learning models. Our evaluation shows that Allo can outperform state-of-the-art HLS tools and ADLs on all test cases in the PolyBench. For the GPT2 model, the inference latency of the Allo generated accelerator is 1.7x faster than the NVIDIA A100 GPU with 5.4x higher energy efficiency, demonstrating the capability of Allo to handle large-scale designs.\n\n**Published:** 2024-04-07\n\n\n\n#### 2. AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs and ASICs\n\n*From Search Query: memory hierarchy deep learning optimization*\n\n*Yue Wang, Zetong Guan, Yongan Zhang, Yingyan Lin, Deming Chen, Cong Hao, Xiaofan Zhang, Pengfei Xu, Chaojian Li, Yang Zhao*\n\n**Abstract:** Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing demand for DNN chips. However, designing DNN chips is non-trivial because: (1) mainstream DNNs have millions of parameters and operations; (2) the large design space due to the numerous design choices of dataflows, processing elements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is needed to allow the same DNN functionality to have a different decomposition, which would require different hardware IPs to meet the application specifications. Therefore, DNN chips take a long time to design and require cross-disciplinary experts. To enable fast and effective DNN chip design, we propose AutoDNNchip - a DNN chip generator that can automatically generate both FPGA- and ASIC-based DNN chip implementation given DNNs from machine learning frameworks (e.g., PyTorch) for a designated application and dataset. Specifically, AutoDNNchip consists of two integrated enablers: (1) a Chip Predictor, built on top of a graph-based accelerator representation, which can accurately and efficiently predict a DNN accelerator's energy, throughput, and area based on the DNN model parameters, hardware configuration, technology-based IPs, and platform constraints; and (2) a Chip Builder, which can automatically explore the design space of DNN chips (including IP selection, block configuration, resource balancing, etc.), optimize chip design via the Chip Predictor, and then generate optimized synthesizable RTL to achieve the target design metrics. Experimental results show that our Chip Predictor's predicted performance differs from real-measured ones by < 10% when validated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC). Furthermore, accelerators generated by our AutoDNNchip can achieve better (up to 3.86X improvement) performance than that of expert-crafted state-of-the-art accelerators.\n\n**Published:** 2020-01-06\n\n\n\n#### 3. Tune: A Research Platform for Distributed Model Selection and Training\n\n*From Search Query: hardware efficient selective computation*\n\n*Ion Stoica, Philipp Moritz, Robert Nishihara, Joseph E. Gonzalez, Eric Liang, Richard Liaw*\n\n**Abstract:** Modern machine learning algorithms are increasingly computationally\ndemanding, requiring specialized hardware and distributed computation to\nachieve high performance in a reasonable time frame. Many hyperparameter search\nalgorithms have been proposed for improving the efficiency of model selection,\nhowever their adaptation to the distributed compute environment is often\nad-hoc. We propose Tune, a unified framework for model selection and training\nthat provides a narrow-waist interface between training scripts and search\nalgorithms. We show that this interface meets the requirements for a broad\nrange of hyperparameter search algorithms, allows straightforward scaling of\nsearch to large clusters, and simplifies algorithm implementation. We\ndemonstrate the implementation of several state-of-the-art hyperparameter\nsearch algorithms in Tune. Tune is available at\nhttp://ray.readthedocs.io/en/latest/tune.html.\n\n**Published:** 2018-07-13\n\n\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: hardware efficient selective computation*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the specified keywords and analysis, here are some key points and references that can guide the researcher:\n\n## Specific Implementations and Optimizations of Butterfly Algorithms in Neural Networks\n\n- The work by Fathi et al. on \"Block-State Transformers\" (2023) is relevant, as it combines butterfly factorizations with hierarchical processing. This approach demonstrates significant speed improvements by integrating an SSM (Selective State Space) sublayer for long-range contextualization and a Block Transformer sublayer for short-term representation of sequences.\n\n## Memory Hierarchy Design and Optimization Techniques for Deep Learning Models\n\n- The \"FlashAttention\" paper by Dao et al. (2022) introduces an IO-aware exact attention algorithm that uses tiling to reduce memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. This tiling strategy can be adapted for butterfly algorithms to optimize memory access.\n\n- The \"Transformers are SSMs\" paper by Dao and Gu (2024) discusses a state space duality framework that allows for designing architectures with refined selective SSMs, which are 2-8X faster. This work emphasizes the importance of hardware-aware implementation for real performance gains.\n\n## Hardware-Efficient Implementations of Selective Computation Mechanisms\n\n- The \"MambaByte\" paper by Wang et al. (2024) presents a token-free selective state space model that is optimized for hardware efficiency. This model leverages selective computation to reduce computational overhead while maintaining performance.\n\n- The \"Samba\" paper by Ren et al. (2024) proposes simple hybrid state space models for efficient unlimited context language modeling. These models are designed to be hardware-efficient and can handle selective computation effectively.\n\n## Integration Strategies for Combining Multiple Optimization Techniques\n\n- Combining butterfly factorizations with selective state space models and block-based processing is a promising direction. The \"Block-State Transformers\" and \"Transformers are SSMs\" papers provide insights into how these techniques can be integrated to achieve significant performance improvements.\n\n## Performance Analysis and Benchmarking of Hybrid Architectures\n\n- The performance analysis in the mentioned papers highlights the importance of hardware-aware design and memory hierarchy optimization. For example, \"FlashAttention\" shows that IO-aware algorithms can significantly reduce memory access overhead, and \"Transformers are SSMs\" demonstrates that refined selective SSMs can be 2-8X faster.\n\n### Key Takeaways\n\n- **Butterfly Algorithms**: Integrating butterfly factorizations with hierarchical processing can lead to efficient linear transforms and significant speed improvements.\n- **Memory Hierarchy Optimization**: Tiling strategies and block-based processing can reduce memory access overhead, and hardware-aware implementation is crucial for real performance gains.\n- **Selective Computation**: Token-free selective state space models and hybrid state space models can be optimized for hardware efficiency, reducing computational overhead while maintaining performance.\n- **Integration and Performance**: Combining multiple optimization techniques and ensuring hardware-aware design are key for achieving better overall performance and scalability.\n\nBy focusing on these areas and leveraging the insights from the referenced papers, the researcher can develop a more efficient and scalable autoregressive language model.\n",
            "analysis": "Analysis Note - Round 1\n\n1. Summary of Analysis:\nThe SelectiveHierarchicalTTT proposal combines butterfly algorithms, selective state space modeling, and hardware-efficient memory management. Initial search results reveal several promising directions and key considerations:\n\na) Novel Integration Aspects:\n- Unique combination of butterfly factorizations with hierarchical processing\n- Hardware-aware implementation of selective state space models\n- Block-based memory optimization with butterfly algorithms\n\nb) Technical Feasibility:\n- Butterfly algorithms show promise for efficient linear transforms\n- Selective state space models demonstrate linear-time complexity\n- Block-based processing offers memory hierarchy optimization potential\n\n2. Useful References with Key Excerpts:\n\nFrom Block-State Transformers (Fathi et al., 2023):\n\"combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences [...] demonstrates more than tenfold increase in speed at the layer level\"\n\nFrom FlashAttention (Dao et al., 2022):\n\"IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\"\n\nFrom Transformers are SSMs (Dao & Gu, 2024):\n\"state space duality (SSD) framework allows us to design a new architecture [...] whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster\"\n\n3. Key Insights:\n\na) Memory Optimization:\n- Block-based processing can significantly reduce memory access overhead\n- Tiling strategies from FlashAttention could be adapted for butterfly algorithms\n- Hardware-aware implementation crucial for real performance gains\n\nb) Computational Efficiency:\n- Butterfly factorizations offer potential for fast linear transforms\n- Selective state space models provide linear-time complexity\n- Integration needs careful consideration of hardware characteristics\n\nc) Architecture Design:\n- Hybrid approaches combining multiple techniques show promise\n- Hardware-aware algorithm design critical for performance\n- Memory hierarchy optimization key for scalability\n\n4. Future Search Plan:\n\nRound 2 Focus:\n- Detailed investigation of butterfly algorithm implementations\n- Memory hierarchy optimization techniques\n- Hardware-specific optimizations for selective computation\n\nSearch Keywords:\n- \"butterfly algorithm optimization neural networks\"\n- \"memory hierarchy deep learning optimization\"\n- \"hardware efficient selective computation\"\n\n5. References:\n\n[1] Fathi, M., Pilault, J., Bacon, P. L., Pal, C., Firat, O., & Goroshin, R. (2023). Block-State Transformers. Neural Information Processing Systems.\n\n[2] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[3] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. International Conference on Machine Learning.\n\n[4] Wang, J., Gangavarapu, T., Yan, J. N., & Rush, A. M. (2024). MambaByte: Token-free Selective State Space Model. arXiv preprint.\n\n[5] Ren, L., Liu, Y., Lu, Y., Shen, Y., Liang, C., & Chen, W. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint."
        },
        {
            "ready": false,
            "query": "selective state space model implementation, hardware optimization neural networks, long sequence memory management",
            "detail": "Search for papers discussing:\n1. Implementation details and optimizations of selective state space models\n2. Hardware-specific optimization techniques for neural network operations\n3. Memory management strategies for processing long sequences\n4. Integration of multiple optimization techniques in neural architectures\n5. Performance analysis of hardware-optimized implementations\nFocus on papers from 2022-2024 that provide concrete implementation details and performance metrics.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Implementation details and optimizations of selective state space models\n2. Hardware-specific optimization techniques for neural network operations\n3. Memory management strategies for processing long sequences\n4. Integration of multiple optimization techniques in neural architectures\n5. Performance analysis of hardware-optimized implementations\nFocus on papers from 2022-2024 that provide concrete implementation details and performance metrics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.99)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.99)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 2. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.99)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.99)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 3. MambaForGCN: Enhancing Long-Range Dependency with State Space Model and Kolmogorov-Arnold Networks for Aspect-Based Sentiment Analysis (Avg. Score: 0.98)\n\n*A. Lawan, Juhua Pu, Haruna Yunusa, Aliyu Umar, Muhammad Lawan*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents MambaForGCN, a novel approach to enhance short and long-range dependencies between aspect and opinion words in ABSA, which incorporates syntax-based Graph Convolutional Network (SynGCN) and MambaFormer (Mamba-Transformer) modules to encode input with dependency relations and semantic information.\n\n**Abstract:** Aspect-based sentiment Analysis (ABSA) identifies and evaluates sentiments toward specific aspects of entities within text, providing detailed insights beyond overall sentiment. However, Attention mechanisms and neural network models struggle with syntactic constraints, and the quadratic complexity of attention mechanisms hinders their adoption for capturing long-range dependencies between aspect and opinion words in ABSA. This complexity can lead to the misinterpretation of irrelevant con-textual words, restricting their effectiveness to short-range dependencies. Some studies have investigated merging semantic and syntactic approaches but face challenges in effectively integrating these methods. To address the above problems, we present MambaForGCN, a novel approach to enhance short and long-range dependencies between aspect and opinion words in ABSA. This innovative approach incorporates syntax-based Graph Convolutional Network (SynGCN) and MambaFormer (Mamba-Transformer) modules to encode input with dependency relations and semantic information. The Multihead Attention (MHA) and Mamba blocks in the MambaFormer module serve as channels to enhance the model with short and long-range dependencies between aspect and opinion words. We also introduce the Kolmogorov-Arnold Networks (KANs) gated fusion, an adaptively integrated feature representation system combining SynGCN and MambaFormer representations. Experimental results on three benchmark datasets demonstrate MambaForGCN's effectiveness, outperforming state-of-the-art (SOTA) baseline models.\n\n##### *Relevant Chunk: No. 20/29 (Score: 0.98)*\n\n```\n274, Aug. 2023, doi: 10.1016/j.knosys. 2023.110648\n[19] A. Gu and T. Dao, \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces,\" Dec. 2023, [Online]. Available: http://arxiv.org/abs/2312.00752\n[20] A. Gu, T. Dao, S. Ermon, A. Rudra, and C. R\u00e9, \"HiPPO: Recurrent Memory with Optimal Polynomial Projections,\" 2020. [Online]. Available: https://github.com/HazyResearch/hippo-code. [21] A. Gu, K. Goel, and C. R\u00e9, \"Efficiently Modeling Long Sequences with Structured State Spaces,\" Oct. 2021, [Online]. Available: http://arxiv.org/abs/2111.00396\n[22] Y. Tang, P. Dong, Z. Tang, X. Chu, and J. Liang, \"VMRNN: Integrating Vision Mamba and LSTM for Efficient and Accurate Spatiotemporal Forecasting,\" Mar. 2024, [Online]. Available: http://arxiv.org/abs/2403.16536\n[23] L. Zhu, B. Liao, Q. Zhang, X.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.97)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.97)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 5. Block-State Transformers (Avg. Score: 0.95)\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n##### *Relevant Chunk: No. 14/28 (Score: 0.95)*\n\n```\n[11] Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. Simple hardware-efficient long convolutions for sequence modeling, 2023. [12] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher Re. Hippo: Recurrent memory with optimal polynomial projections, 2020. [13] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [14] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022. [15] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models, 2022. [16] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [17] Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for JAX, 2023. [18] Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems, 6(2):107-116, 1998. [19] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9(8):1735-1780, 1997. [20] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 9099-9117. PMLR, 17-23 Jul 2022. [21] DeLesley Hutchins, Imanol Schlag, Yuhuai Wu, Ethan Dyer, and Behnam Neyshabur. Blockrecurrent transformers. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. [22] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: selective state space model implementation, hardware optimization neural networks, long sequence memory management\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Simplified State Space Layers for Sequence Modeling\n\n*From Search Query: selective state space model implementation*\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 334  (*Influential: 32*)\n\n#### 2. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: selective state space model implementation*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 216  (*Influential: 38*)\n\n#### 3. Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n\n*From Search Query: selective state space model implementation*\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. DepthShrinker: A New Compression Paradigm Towards Boosting Real-Hardware Efficiency of Compact Neural Networks\n\n*From Search Query: hardware optimization neural networks*\n\n*Y. Fu, Haichuan Yang, Jiayi Yuan, Meng Li, Cheng Wan, Raghuraman Krishnamoorthi, Vikas Chandra, Yingyan Lin*\n\n**TL;DR:** This work opens up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy, and proposes a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient Dnns that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real- hardware efficiency.\n\n**Abstract:** Efficient deep neural network (DNN) models equipped with compact operators (e.g., depthwise convolutions) have shown great potential in reducing DNNs' theoretical complexity (e.g., the total number of weights/operations) while maintaining a decent model accuracy. However, existing efficient DNNs are still limited in fulfilling their promise in boosting real-hardware efficiency, due to their commonly adopted compact operators' low hardware utilization. In this work, we open up a new compression paradigm for developing real-hardware efficient DNNs, leading to boosted hardware efficiency while maintaining model accuracy. Interestingly, we observe that while some DNN layers' activation functions help DNNs' training optimization and achievable accuracy, they can be properly removed after training without compromising the model accuracy. Inspired by this observation, we propose a framework dubbed DepthShrinker, which develops hardware-friendly compact networks via shrinking the basic building blocks of existing efficient DNNs that feature irregular computation patterns into dense ones with much improved hardware utilization and thus real-hardware efficiency. Excitingly, our DepthShrinker framework delivers hardware-friendly compact networks that outperform both state-of-the-art efficient DNNs and compression techniques, e.g., a 3.06% higher accuracy and 1.53$\\times$ throughput on Tesla V100 over SOTA channel-wise pruning method MetaPruning. Our codes are available at: https://github.com/facebookresearch/DepthShrinker.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 5. L2ight: Enabling On-Chip Learning for Optical Neural Networks via Efficient in-situ Subspace Optimization\n\n*From Search Query: hardware optimization neural networks*\n\n*Jiaqi Gu, Hanqing Zhu, Chenghao Feng, Zixuan Jiang, Ray T. Chen, D. Pan*\n\n**TL;DR:** This synergistic framework L2ight is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips.\n\n**Abstract:** Silicon-photonics-based optical neural network (ONN) is a promising hardware platform that could represent a paradigm shift in efficient AI with its CMOS-compatibility, flexibility, ultra-low execution latency, and high energy efficiency. In-situ training on the online programmable photonic chips is appealing but still encounters challenging issues in on-chip implementability, scalability, and efficiency. In this work, we propose a closed-loop ONN on-chip learning framework L2ight to enable scalable ONN mapping and efficient in-situ learning. L2ight adopts a three-stage learning flow that first calibrates the complicated photonic circuit states under challenging physical constraints, then performs photonic core mapping via combined analytical solving and zeroth-order optimization. A subspace learning procedure with multi-level sparsity is integrated into L2ight to enable in-situ gradient evaluation and fast adaptation, unleashing the power of optics for real on-chip intelligence. Extensive experiments demonstrate our proposed L2ight outperforms prior ONN training protocols with 3-order-of-magnitude higher scalability and over 30X better efficiency, when benchmarked on various models and learning tasks. This synergistic framework is the first scalable on-chip learning solution that pushes this emerging field from intractable to scalable and further to efficient for next-generation self-learnable photonic neural chips. From a co-design perspective, L2ight also provides essential insights for hardware-restricted unitary subspace optimization and efficient sparse training. We open-source our framework at https://github.com/JeremieMelo/L2ight.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 6. Temporal Effective Batch Normalization in Spiking Neural Networks\n\n*From Search Query: hardware optimization neural networks*\n\n*Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, Tiejun Huang*\n\n**TL;DR:** Experimental results show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Abstract:** Spiking Neural Networks (SNNs) are promising in neuromorphic hardware owing to utilizing spatio-temporal information and sparse event-driven signal processing. However, it is challenging to train SNNs due to the non-differentiable nature of the binary firing function. The surrogate gradients alleviate the training problem and make SNNs obtain comparable performance as Artificial Neural Networks (ANNs) with the same structure. Unfortunately, batch normalization, contributing to the success of ANNs, does not play a prominent role in SNNs because of the additional temporal dimension. To this end, we propose an effective normalization method called temporal effective batch normalization (TEBN). By rescaling the presynaptic inputs with different weights at every time-step, temporal distributions become smoother and uniform. Theoretical analysis shows that TEBN can be viewed as a smoother of SNN\u2019s optimization landscape and could help stabilize the gradient norm. Experimental results on both static and neuromorphic datasets show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 63  (*Influential: 7*)\n\n#### 7. Long Sequence Hopfield Memory\n\n*From Search Query: long sequence memory management*\n\n*Hamza Chaudhry, Jacob A. Zavatone-Veth, D. Krotov, C. Pehlevan*\n\n**TL;DR:** This work derives novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and introduces a generalized pseudoinverse rule to recall sequences of highly correlated patterns.\n\n**Abstract:** \n Sequence memory is an essential attribute of natural and artificial intelligence that enables agents to encode, store, and retrieve complex sequences of stimuli and actions. Computational models of sequence memory have been proposed where recurrent Hopfield-like neural networks are trained with temporally asymmetric Hebbian rules. However, these networks suffer from limited sequence capacity (maximal length of the stored sequence) due to interference between the memories. Inspired by recent work on Dense Associative Memories, we expand the sequence capacity of these models by introducing a nonlinear interaction term, enhancing separation between the patterns. We derive novel scaling laws for sequence capacity with respect to network size, significantly outperforming existing scaling laws for models based on traditional Hopfield networks, and verify these theoretical results with numerical simulation. Moreover, we introduce a generalized pseudoinverse rule to recall sequences of highly correlated patterns. Finally, we extend this model to store sequences with variable timing between states\u2019 transitions and describe a biologically-plausible implementation, with connections to motor neuroscience.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 2*)\n\n#### 8. Keep Me Updated! Memory Management in Long-term Conversations\n\n*From Search Query: long sequence memory management*\n\n*Sanghwan Bae, Donghyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, Sang-Woo Lee, W. Park, Nako Sung*\n\n**TL;DR:** A novel task and a corresponding dataset of memory management in long-term conversations are presented, in which bots keep track of and bring up the latest information about users while conversing through multiple sessions, to support more precise and interpretable memory.\n\n**Abstract:** Remembering important information from the past and continuing to talk about it in the present are crucial in long-term conversations. However, previous literature does not deal with cases where the memorized information is outdated, which may cause confusion in later conversations. To address this issue, we present a novel task and a corresponding dataset of memory management in long-term conversations, in which bots keep track of and bring up the latest information about users while conversing through multiple sessions. In order to support more precise and interpretable memory, we represent memory as unstructured text descriptions of key information and propose a new mechanism of memory management that selectively eliminates invalidated or redundant information. Experimental results show that our approach outperforms the baselines that leave the stored memory unchanged in terms of engagingness and humanness, with larger performance gap especially in the later sessions.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 36  (*Influential: 3*)\n\n#### 9. SMR: State Memory Replay for Long Sequence Modeling\n\n*From Search Query: long sequence memory management*\n\n*Biqing Qi, Junqi Gao, Kaiyan Zhang, Dong Li, Jianxing Liu, Ligang Wu, Bowen Zhou*\n\n**TL;DR:** A novel non-recursive non-uniform sample processing strategy, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data, enables SSMs to stably model varying sampling points.\n\n**Abstract:** Despite the promising performance of state space models (SSMs) in long sequence modeling, limitations still exist. Advanced SSMs like S5 and S6 (Mamba) in addressing non-uniform sampling, their recursive structures impede efficient SSM computation via convolution. To overcome compatibility limitations in parallel convolutional computation, this paper proposes a novel non-recursive non-uniform sample processing strategy. Theoretical analysis of SSMs through the lens of Event-Triggered Control (ETC) theory reveals the Non-Stable State (NSS) problem, where deviations from sampling point requirements lead to error transmission and accumulation, causing the divergence of the SSM's hidden state. Our analysis further reveals that adjustments of input sequences with early memories can mitigate the NSS problem, achieving Sampling Step Adaptation (SSA). Building on this insight, we introduce a simple yet effective plug-and-play mechanism, State Memory Replay (SMR), which utilizes learnable memories to adjust the current state with multi-step information for generalization at sampling points different from those in the training data. This enables SSMs to stably model varying sampling points. Experiments on long-range modeling tasks in autoregressive language modeling and Long Range Arena demonstrate the general effectiveness of the SMR mechanism for a series of SSM models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. STG-Mamba: Spatial-Temporal Graph Learning via Selective State Space Model\n\n*From Search Query: selective state space model implementation*\n\n*Adelle Coster, Wenjie Zhang, Hanchen Wang, Lincan Li*\n\n**Abstract:** Spatial-Temporal Graph (STG) data is characterized as dynamic, heterogenous, and non-stationary, leading to the continuous challenge of spatial-temporal graph learning. In the past few years, various GNN-based methods have been proposed to solely focus on mimicking the relationships among node individuals of the STG network, ignoring the significance of modeling the intrinsic features that exist in STG system over time. In contrast, modern Selective State Space Models (SSSMs) present a new approach which treat STG Network as a system, and meticulously explore the STG system's dynamic state evolution across temporal dimension. In this work, we introduce Spatial-Temporal Graph Mamba (STG-Mamba) as the first exploration of leveraging the powerful selective state space models for STG learning by treating STG Network as a system, and employing the Spatial-Temporal Selective State Space Module (ST-S3M) to precisely focus on the selected STG latent features. Furthermore, to strengthen GNN's ability of modeling STG data under the setting of selective state space models, we propose Kalman Filtering Graph Neural Networks (KFGN) for dynamically integrate and upgrade the STG embeddings from different temporal granularities through a learnable Kalman Filtering statistical theory-based approach. Extensive empirical studies are conducted on three benchmark STG forecasting datasets, demonstrating the performance superiority and computational efficiency of STG-Mamba. It not only surpasses existing state-of-the-art methods in terms of STG forecasting performance, but also effectively alleviate the computational bottleneck of large-scale graph networks in reducing the computational cost of FLOPs and test inference time. The implementation code is available at: \\url{https://github.com/LincanLi98/STG-Mamba}.\n\n**Published:** 2024-03-19\n\n\n\n#### 2. I2I-Mamba: Multi-modal medical image synthesis via selective state space modeling\n\n*From Search Query: selective state space model implementation*\n\n*Tolga \u00c7ukur, Onat Dalmaz, Mahmut Yurt, Fuat Arslan, Bilal Kabas, Omer F. Atli*\n\n**Abstract:** In recent years, deep learning models comprising transformer components have pushed the performance envelope in medical image synthesis tasks. Contrary to convolutional neural networks (CNNs) that use static, local filters, transformers use self-attention mechanisms to permit adaptive, non-local filtering to sensitively capture long-range context. However, this sensitivity comes at the expense of substantial model complexity, which can compromise learning efficacy particularly on relatively modest-sized imaging datasets. Here, we propose a novel adversarial model for multi-modal medical image synthesis, I2I-Mamba, that leverages selective state space modeling (SSM) to efficiently capture long-range context while maintaining local precision. To do this, I2I-Mamba injects channel-mixed Mamba (cmMamba) blocks in the bottleneck of a convolutional backbone. In cmMamba blocks, SSM layers are used to learn context across the spatial dimension and channel-mixing layers are used to learn context across the channel dimension of feature maps. Comprehensive demonstrations are reported for imputing missing images in multi-contrast MRI and MRI-CT protocols. Our results indicate that I2I-Mamba offers superior performance against state-of-the-art CNN- and transformer-based methods in synthesizing target-modality images.\n\n**Published:** 2024-05-22\n\n\n\n#### 3. Multi-Component Optimization and Efficient Deployment of Neural-Networks on Resource-Constrained IoT Hardware\n\n*From Search Query: hardware optimization neural networks*\n\n*Rajiv Ranjan, Albert Zomaya, Schahram Dustdar, Muhammad Intizar Ali, John G. Breslin, Pankesh Patel, Dineshkumar Sundaram, Bharath Sudharsan*\n\n**Abstract:** The majority of IoT devices like smartwatches, smart plugs, HVAC controllers, etc., are powered by hardware with a constrained specification (low memory, clock speed and processor) which is insufficient to accommodate and execute large, high-quality models. On such resource-constrained devices, manufacturers still manage to provide attractive functionalities (to boost sales) by following the traditional approach of programming IoT devices/products to collect and transmit data (image, audio, sensor readings, etc.) to their cloud-based ML analytics platforms. For decades, this online approach has been facing issues such as compromised data streams, non-real-time analytics due to latency, bandwidth constraints, costly subscriptions, recent privacy issues raised by users and the GDPR guidelines, etc. In this paper, to enable ultra-fast and accurate AI-based offline analytics on resource-constrained IoT devices, we present an end-to-end multi-component model optimization sequence and open-source its implementation. Researchers and developers can use our optimization sequence to optimize high memory, computation demanding models in multiple aspects in order to produce small size, low latency, low-power consuming models that can comfortably fit and execute on resource-constrained hardware. The experimental results show that our optimization components can produce models that are; (i) 12.06 x times compressed; (ii) 0.13% to 0.27% more accurate; (iii) Orders of magnitude faster unit inference at 0.06 ms. Our optimization sequence is generic and can be applied to any state-of-the-art models trained for anomaly detection, predictive maintenance, robotics, voice recognition, and machine vision.\n\n**Published:** 2022-04-20\n\n\n\n#### 4. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: hardware optimization neural networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 5. Pyraformer: Low-Complexity Pyramidal Attention for Long-Range Time Series Modeling and Forecasting\n\n*From Search Query: long sequence memory management*\n\n*Schahram Dustdar, Alex X. Liu, Weiyao Lin, Jianguo Li, Cong Liao, Hang Yu, Shizhan Liu*\n\n**Abstract:** Accurate prediction of the future given the past based on time series data is of paramount importance, since it opens the door for decision making and risk management ahead of time. In practice, the challenge is to build a flexible but parsimonious model that can capture a wide range of temporal dependencies. In this paper, we propose Pyraformer by exploring the multiresolution representation of the time series. Specifically, we introduce the pyramidal attention module (PAM) in which the inter-scale tree structure summarizes features at different resolutions and the intra-scale neighboring connections model the temporal dependencies of different ranges. Under mild conditions, the maximum length of the signal traversing path in Pyraformer is a constant (i.e., $\\mathcal O(1)$) with regard to the sequence length $L$, while its time and space complexity scale linearly with $L$. Extensive numerical results show that Pyraformer typically achieves the highest prediction accuracy in both single-step and long-range forecasting tasks with the least amount of time and memory consumption, especially when the sequence is long. Full code will be available at the time of publication.\n\n**Proceeding:** iclr-2022-4\n\n**Published:** 2021-09-29\n\n\n\n#### 6. Formal Verification of Long Short-Term Memory based Audio Classifiers: A Star based Approach\n\n*From Search Query: long sequence memory management*\n\n*Taylor T Johnson, Neelanjana Pal*\n\n**Abstract:** Formally verifying audio classification systems is essential to ensure accurate signal classification across real-world applications like surveillance, automotive voice commands, and multimedia content management, preventing potential errors with serious consequences. Drawing from recent research, this study advances the utilization of star-set-based formal verification, extended through reachability analysis, tailored explicitly for Long Short-Term Memory architectures and their Convolutional variations within the audio classification domain. By conceptualizing the classification process as a sequence of set operations, the star set-based reachability approach streamlines the exploration of potential operational states attainable by the system. The paper serves as an encompassing case study, validating and verifying sequence audio classification analytics within real-world contexts. It accentuates the necessity for robustness verification to ensure precise and dependable predictions, particularly in light of the impact of noise on the accuracy of output classifications.\n\n**Published:** 2023-11-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on selective state space models, hardware optimization, and memory management for long sequences, here are some relevant findings and references from recent research:\n\n## Selective State Space Model Implementation\n\n- The paper \"Short-Long Convolutions\" by Liu et al. (2024) is particularly relevant. It discusses the implementation of linear attention in a divide-and-conquer manner, which incorporates selective state space modeling (SSM) and linear attention. This approach maintains real linear complexity and is efficient for processing long sequences.\n\n## Hardware Optimization for Neural Networks\n\n- \"HAT: Hardware-Aware Transformers\" by Wang et al. (2020) provides insights into architecture-specific optimizations for transformers. This work highlights the importance of designing models that are aware of the underlying hardware, which can significantly improve efficiency in natural language processing tasks.\n- \"FlashFFTConv\" by Fu et al. (2023) demonstrates efficient implementation of butterfly-like operations using tensor cores. This method leverages matrix decomposition to compute FFT using matrix multiply units, which can speed up exact FFT convolutions and is beneficial for long sequence processing.\n\n## Memory Management for Long Sequences\n\n- \"MODeL: Memory Optimizations for Deep Learning\" by Steiner et al. (2023) focuses on optimizing the lifetime and memory location of tensors used in neural networks. This approach automatically reduces memory usage without compromising performance, which is crucial for managing long sequences.\n- \"MCUNetV2: Memory-Efficient Patch-based Inference\" by Lin et al. (2021) discusses memory-efficient patch-based inference techniques. These techniques can be adapted for processing long sequences by using block-based processing with tiling strategies, which helps in managing the memory hierarchy efficiently.\n\n## Integration of Multiple Optimization Techniques\n\n- Combining multiple optimization techniques, such as those discussed in \"FlashFFTConv,\" \"MODeL,\" and \"HAT,\" requires careful orchestration. For instance, integrating matrix decomposition techniques with memory lifetime optimization and hardware-aware design can lead to significant improvements in both computation speed and memory efficiency.\n- The \"Short-Long Convolutions\" paper also integrates selective state space modeling with linear attention, showing how different optimization techniques can be combined to achieve better performance on long sequences.\n\n## Performance Analysis of Hardware-Optimized Implementations\n\n- The performance metrics from \"FlashFFTConv\" show that hardware-specific implementations can achieve speedups of up to 7.93\u00d7 over standard PyTorch implementations for exact FFT convolutions.\n- \"HAT\" and \"MODeL\" also provide performance analyses that highlight the benefits of hardware-aware design and memory optimization, respectively. These analyses emphasize the importance of balancing computation and memory access to achieve optimal performance.\n\nBy focusing on these papers and their techniques, researchers can gain valuable insights into how to improve the design of autoregressive language models, particularly in terms of selective state space modeling, hardware optimization, and efficient memory management for long sequences.\n",
            "analysis": "Analysis Note - Round 2\n\n1. Summary of Analysis:\nThe SelectiveHierarchicalTTT proposal combines butterfly algorithms, selective state space modeling, and hardware-efficient memory management. The second round of search results reveals important insights about implementation strategies and potential optimizations:\n\na) Hardware Efficiency:\n- FlashFFTConv demonstrates efficient implementation of butterfly-like operations using tensor cores\n- Hardware-aware transformers (HAT) show the importance of architecture-specific optimizations\n- Memory optimization techniques from MODeL provide insights for efficient tensor management\n\nb) Memory Hierarchy:\n- Block-based processing with tiling strategies shows promise for memory efficiency\n- Memory-efficient patch-based inference techniques could be adapted for the proposed design\n- Hierarchical memory access patterns need careful consideration\n\n2. Useful References with Key Excerpts:\n\nFrom FlashFFTConv (Fu et al., 2023):\n\"uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O [...] speeds up exact FFT convolutions by up to 7.93\u00d7 over PyTorch\"\n\nFrom MODeL (Steiner et al., 2023):\n\"optimizes the lifetime and memory location of the tensors used to train neural networks [...] automatically reduces the memory usage of existing neural networks without any of the drawbacks of other techniques\"\n\nFrom Short-Long Convolutions (Liu et al., 2024):\n\"implements linear attention in a divide-and-conquer manner [...] enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity\"\n\n3. Key Insights:\n\na) Implementation Strategies:\n- Matrix decomposition techniques can leverage hardware-specific optimizations\n- Memory lifetime optimization can significantly reduce resource requirements\n- Divide-and-conquer approaches show promise for efficient attention mechanisms\n\nb) Performance Considerations:\n- Hardware-specific implementations can achieve significant speedups\n- Memory optimization needs to consider both spatial and temporal aspects\n- Careful balance needed between computation and memory access\n\nc) Integration Challenges:\n- Combining multiple optimization techniques requires careful orchestration\n- Hardware-specific features need to be leveraged effectively\n- Memory hierarchy management crucial for performance\n\n4. Future Search Plan:\n\nRound 3 Focus:\n- Detailed investigation of selective state space implementations\n- Hardware-specific optimization techniques for butterfly operations\n- Memory hierarchy management strategies for long sequences\n\nSearch Keywords:\n- \"selective state space model implementation\"\n- \"hardware optimization neural networks\"\n- \"long sequence memory management\"\n\n5. References:\n\n[1] Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[2] Steiner, B., Elhoushi, M., Kahn, J., & Hegarty, J. (2023). MODeL: Memory Optimizations for Deep Learning. International Conference on Machine Learning.\n\n[3] Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning.\n\n[4] Wang, H., Wu, Z., Liu, Z., Cai, H., Zhu, L., Gan, C., & Han, S. (2020). HAT: Hardware-Aware Transformers for Efficient Natural Language Processing. Annual Meeting of the Association for Computational Linguistics.\n\n[5] Lin, J., Chen, W. M., Cai, H., Gan, C., & Han, S. (2021). MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning. Neural Information Processing Systems."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.698397,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.746583,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "# SelectiveHierarchicalTTT: Enhancing Language Model Performance through Hardware-Efficient Integration of Temporal Bottlenecks and Selective State Space Modeling\n\n## Motivation\n\nModern language models face significant challenges in efficiently processing long sequences while maintaining expressiveness and adaptability. The two parent designs, HierarchicalFastTTTLinear and HierarchicalMamba, each offer unique strengths:\n\n- HierarchicalFastTTTLinear provides efficient temporal bottlenecks and adaptive state compression\n- HierarchicalMamba offers selective state space modeling and efficient gating mechanisms\n\nThe motivation behind SelectiveHierarchicalTTT is to combine these strengths while introducing hardware-efficient butterfly algorithms and optimized memory hierarchy management to create a more powerful and efficient architecture.\n\n## Problem Analysis\n\n### Parent Design Analysis\n\n1. HierarchicalFastTTTLinear:\n   - Strengths:\n     - Efficient temporal bottlenecks\n     - Adaptive state compression\n     - Multi-scale processing\n   - Limitations:\n     - Limited selective computation\n     - Sub-optimal memory hierarchy usage\n     - Fixed processing patterns\n\n2. HierarchicalMamba:\n   - Strengths:\n     - Selective state space modeling\n     - Efficient gating mechanisms\n     - Linear computational complexity\n   - Limitations:\n     - Less efficient temporal processing\n     - Limited compression capabilities\n     - Higher memory requirements\n\n### Integration Opportunities\n\n1. Memory Optimization:\n   - Combine FastTTT's temporal bottlenecks with Mamba's selective mechanisms\n   - Implement hardware-efficient butterfly algorithms\n   - Optimize memory hierarchy through block-based processing\n\n2. Computational Efficiency:\n   - Integrate selective computation with hierarchical processing\n   - Implement parallel scan algorithms\n   - Utilize butterfly decomposition for efficient transforms\n\n3. Architecture Enhancement:\n   - Unify gating mechanisms from both parents\n   - Implement selective state compression\n   - Optimize memory access patterns\n\n## Core Idea and Philosophy\n\nThe core idea is to create a hybrid architecture that:\n1. Leverages butterfly algorithms for efficient computation\n2. Uses selective state space modeling for adaptive processing\n3. Implements optimized memory hierarchy management\n4. Maintains hardware efficiency through careful algorithm design\n\n## Design Plan\n\n### Selected GAU to Modify\n\n```selection\nFastTTTLinear\n```\n\n### Modification Overview\n\n1. **Butterfly-Based Processing**:\n   - Implement butterfly decomposition for efficient transforms\n   - Use hardware-aware parallel algorithms\n   - Optimize memory access patterns\n\n2. **Selective State Management**:\n   - Integrate selective state space modeling\n   - Implement adaptive compression\n   - Optimize state updates\n\n3. **Memory Hierarchy Optimization**:\n   - Implement block-based processing\n   - Use hierarchical memory access\n   - Optimize cache utilization\n\n### Detailed Modifications\n\n#### 1. Butterfly-Based Processing\n\nMathematical formulation:\n\n\\[\n\\begin{aligned}\nB(x) &= \\prod_{i=1}^{\\log N} B_i P_i x \\\\\nB_i &= \\text{diag}(D_1^i, D_2^i, ..., D_{N/2}^i) \\\\\nP_i &= \\text{butterfly\\_permutation}(i)\n\\end{aligned}\n\\]\n\nwhere:\n- $B_i$ are butterfly factors\n- $P_i$ are permutation matrices\n- $D_j^i$ are diagonal matrices\n\n#### 2. Selective State Management\n\n\\[\n\\begin{aligned}\nh_t &= \\text{SelectiveUpdate}(x_t, h_{t-1}) \\\\\n&= \\sigma(W_s x_t + U_s h_{t-1}) \\odot \\text{tanh}(W_h x_t + U_h h_{t-1}) \\\\\nc_t &= \\text{Compress}(h_t) = \\text{ButterflyProject}(h_t)\n\\end{aligned}\n\\]\n\nwhere:\n- $h_t$ is the hidden state\n- $c_t$ is the compressed state\n- $\\sigma$ is the sigmoid function\n- $\\odot$ denotes element-wise multiplication\n\n#### 3. Memory Hierarchy Optimization\n\nImplement block-based processing:\n\n\\[\n\\begin{aligned}\nX_b &= \\text{BlockPartition}(X, b) \\\\\nY_b &= \\text{ProcessBlock}(X_b) \\\\\nY &= \\text{BlockMerge}(Y_b)\n\\end{aligned}\n\\]\n\nwhere:\n- $b$ is the block size\n- $X_b$ represents blocked input\n- $Y_b$ represents processed blocks\n\n## Implementation Guidelines\n\n### Pseudo-code for SelectiveHierarchicalTTT\n\n```python\nclass SelectiveHierarchicalTTT(GAUBase):\n    def __init__(self, embed_dim, block_loc, kwarg_all, **kwargs):\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \n        # Butterfly processing components\n        self.butterfly = ButterflyProcessor(embed_dim)\n        \n        # Selective state components\n        self.selective_state = SelectiveStateManager(embed_dim)\n        \n        # Memory hierarchy components\n        self.block_processor = BlockProcessor(embed_dim)\n        \n    def _forward(self, X, **Z):\n        # Block-based processing\n        X_blocks = self.block_processor.partition(X)\n        \n        outputs = []\n        for block in X_blocks:\n            # Butterfly transform\n            B = self.butterfly(block)\n            \n            # Selective state update\n            h = self.selective_state(B, Z.get('state', None))\n            Z['state'] = h\n            \n            # Process and compress\n            out = self.process_block(B, h)\n            outputs.append(out)\n            \n        # Merge blocks\n        Y = self.block_processor.merge(outputs)\n        return Y, Z\n\nclass ButterflyProcessor(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.factors = nn.ModuleList([\n            ButterflyFactor(dim, i) \n            for i in range(int(math.log2(dim)))\n        ])\n        \n    def forward(self, x):\n        for factor in self.factors:\n            x = factor(x)\n        return x\n\nclass SelectiveStateManager(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.update = nn.Linear(2*dim, dim)\n        self.compress = ButterflyProjection(dim)\n        \n    def forward(self, x, state):\n        if state is None:\n            state = torch.zeros_like(x)\n        # Selective update\n        combined = torch.cat([x, state], dim=-1)\n        gate = torch.sigmoid(self.update(combined))\n        new_state = gate * state + (1-gate) * x\n        # Compress state\n        compressed = self.compress(new_state)\n        return compressed\n```\n\n### Integration Steps\n\n1. **Initialize Components**:\n   - Set up butterfly processing modules\n   - Initialize selective state management\n   - Configure block processing\n\n2. **Memory Management**:\n   - Implement block-based processing\n   - Configure memory hierarchy\n   - Set up cache optimization\n\n3. **Optimization**:\n   - Enable parallel processing\n   - Implement butterfly algorithms\n   - Configure selective mechanisms\n\n## Conclusion\n\nSelectiveHierarchicalTTT combines the strengths of both parent designs while introducing hardware-efficient butterfly algorithms and optimized memory hierarchy management. The design achieves:\n\n1. Improved computational efficiency through butterfly-based processing\n2. Enhanced adaptability through selective state space modeling\n3. Optimized memory usage through hierarchical management\n4. Maintained hardware efficiency through careful algorithm design\n\nThese improvements make SelectiveHierarchicalTTT a powerful and efficient architecture for modern language modeling tasks.\n\n## References\n\n1. Dao, T., et al. (2019). \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\". International Conference on Machine Learning.\n\n2. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\". arXiv preprint.\n\n3. Fu, D. Y., et al. (2023). \"Simple Hardware-Efficient Long Convolutions for Sequence Modeling\". International Conference on Machine Learning.\n\n4. Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\". International Conference on Machine Learning.\n\n5. Smith, J. T. H., et al. (2023). \"Convolutional State Space Models for Long-Range Spatiotemporal Modeling\". Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel design that combines HierarchicalFastTTTLinear's temporal bottlenecks with HierarchicalMamba's selective state space modeling, introducing hardware-efficient butterfly algorithms and optimized memory hierarchy management to enhance model performance while maintaining computational efficiency.",
    "ideation": null,
    "modelname": "SelectiveHierarchicalTTT",
    "suggestions": "1. Implementation Refinements:\n- Provide more detailed analysis of butterfly algorithm implementation on different hardware\n- Include concrete examples of memory hierarchy optimization\n- Add benchmarking guidelines for different components\n- Clarify initialization strategies for butterfly parameters\n\n2. Theoretical Enhancements:\n- Develop formal proof of computational complexity\n- Analyze impact on model capacity more rigorously\n- Clarify relationship between block size and performance\n- Provide stability analysis for butterfly transforms\n\n3. Practical Improvements:\n- Add guidelines for hardware-specific optimizations\n- Include stability analysis for different sequence lengths\n- Provide more detailed memory management strategies\n- Outline specific parallelization techniques\n\n4. Documentation:\n- Include more detailed performance expectations\n- Add failure mode analysis\n- Provide integration guidelines with existing codebases\n- Document hardware requirements and optimization strategies\n\n5. Evaluation Framework:\n- Define specific metrics for measuring efficiency gains\n- Include guidelines for comparing with baseline implementations\n- Add ablation study recommendations\n- Outline benchmark suite for different hardware configurations",
    "user_input": ""
}