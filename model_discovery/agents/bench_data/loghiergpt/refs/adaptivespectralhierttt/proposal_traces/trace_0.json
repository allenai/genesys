{
    "variantname": "AdaptiveSpectralHRMSNorm",
    "review": "The SpectralHierRMSNorm proposal presents an innovative approach to enhancing hierarchical RMS normalization through spectral filtering and adaptive state compression. After thorough analysis of the proposal, existing research, and similar approaches, here is a comprehensive review:\n\nCLARITY (4.5/5):\n+ Well-structured presentation with clear motivation and objectives\n+ Detailed mathematical formulations and implementation guidelines\n+ Comprehensive theoretical analysis\n- Some implementation details about memory hierarchy optimization could be more explicit\n- Hardware-specific optimizations need more detailed explanation\n\nINNOVATION (4.0/5):\n+ Novel integration of spectral filtering with hierarchical RMS normalization\n+ Unique approach to adaptive state compression based on input complexity\n+ Innovative multi-scale processing with stability guarantees\n- Some concepts build on existing work in spectral methods and hierarchical processing\n- Similar ideas exist in recent papers about spectral state space models\n\nFEASIBILITY (3.8/5):\n+ Clear implementation path with detailed mathematical formulations\n+ Careful consideration of stability issues\n+ Thoughtful approach to memory management\nConcerns:\n- FFT operations have poor hardware utilization (as shown in FlashFFTConv research)\n- Complex integration of spectral filtering with normalization\n- Memory hierarchy optimization needs careful implementation\n\nSCALABILITY (4.2/5):\n+ Linear complexity maintained through efficient spectral operations\n+ Adaptive compression reduces memory requirements\n+ Multi-scale processing enables efficient handling of long sequences\nConcerns:\n- Memory requirements for FFT operations\n- Potential bottlenecks in very long sequences\n- Trade-offs between compression and model quality\n\nACCURACY AND ROBUSTNESS (4.3/5):\n+ Enhanced feature representation through spectral filtering\n+ Improved stability through careful normalization design\n+ Better handling of varying sequence lengths\n+ Adaptive compression maintains model quality\nConcerns:\n- Impact of compression on model quality needs validation\n- Stability under different input distributions requires testing\n- Trade-offs between efficiency and accuracy need empirical validation\n\nEFFICIENCY (4.0/5):\n+ Reduced memory requirements through adaptive compression\n+ Efficient parallel processing capabilities\n+ Hardware-aware implementation strategy\nLimitations:\n- FFT operations have computational overhead\n- Memory hierarchy optimization is crucial for performance\n- Implementation complexity may affect practical efficiency\n\nCOMPARATIVE ANALYSIS:\nThe proposal differs significantly from existing approaches:\n- More sophisticated than standard hierarchical normalization\n- More efficient than traditional spectral methods\n- Novel combination of adaptive compression with spectral filtering\n- Distinct from sibling proposals in implementation approach\n\nTECHNICAL DEPTH:\nThe proposal demonstrates strong technical understanding:\n+ Comprehensive mathematical formulations\n+ Detailed stability analysis\n+ Thorough consideration of implementation aspects\n+ Clear analysis of trade-offs",
    "search_stack": [
        {
            "ready": false,
            "query": "state space models spectral filtering",
            "detail": "Find papers and technical details about state space models and spectral filtering techniques in language models, focusing on efficiency and scalability improvements.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers and technical details about state space models and spectral filtering techniques in language models, focusing on efficiency and scalability improvements.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.99)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.99)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.97)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.97)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n#### 3. Linear Transformers with Learnable Kernel Functions are Better In-Context Models (Avg. Score: 0.97)\n\n*Yaroslav Aksenov, Nikita Balagansky, Sofia Maria Lo Cicero Vaina, Boris Shaposhnikov, Alexey Gorbatovski, Daniil Gavrilov*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A singular, elegant alteration to the Based kernel is presented that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n**Abstract:** Advancing the frontier of subquadratic architectures for Language Models (LMs) is crucial in the rapidly evolving field of natural language processing. Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities - a domain where the Transformer traditionally shines. The Based model emerged as a hybrid solution, blending a Linear Transformer with a kernel inspired by the Taylor expansion of exponential functions, augmented by convolutional networks. Mirroring the Transformer's in-context adeptness, it became a strong contender in the field. In our work, we present a singular, elegant alteration to the Based kernel that amplifies its In-Context Learning abilities evaluated with the Multi-Query Associative Recall task and overall language modeling process, as demonstrated on the Pile dataset.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.97)*\n\n```\nDaniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. 2023a. Hungry Hungry Hippos: Towards language modeling with state space models. In International Conference on Learning Representations. Daniel Y. Fu, Elliot L. Epstein, Eric Nguyen, Armin W. Thomas, Michael Zhang, Tri Dao, Atri Rudra, and Christopher R\u00e9. 2023b. Simple hardware-efficient long convolutions for sequence modeling. International Conference on Machine Learning. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint arXiv:2101.00027. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2023. A framework for few-shot language model evaluation. Albert Gu and Tri Dao. 2023. Mamba: Linear-time sequence modeling with selective state spaces. Albert Gu, Karan Goel, and Christopher Re. 2022. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations. Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. 2023. How to train your HIPPO: State space models with generalized orthogonal basis projections. In International Conference on Learning Representations. Alex Henry, Prudhvi Raj Dachapally, S. Pawar, and Yuxuan Chen. 2020. Query-key normalization for transformers. FINDINGS. Sepp Hochreiter and J\u00fcrgen Schmidhuber. 1997. Long short-term memory. Neural Computation, 9(8):17351780 . Samy Jelassi, David Brandfonbrener, Sham M. Kakade, and Eran Malach. 2024. Repeat after me: Transformers are better than state space models at copying.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.95)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.95)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space models spectral filtering\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Streaming Coresets for Symmetric Tensor Factorization\n\n*From Search Query: state space models spectral filtering*\n\n*Rachit Chhaya, Jayesh Choudhari, A. Dasgupta, Supratim Shit*\n\n**TL;DR:** Six algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the p-moment tensor of the coreset approximates the corresponding decomposing of the $p-moments tensor computed from the full data.\n\n**Abstract:** Factorizing tensors has recently become an important optimization module in a number of machine learning pipelines, especially in latent variable models. We show how to do this efficiently in the streaming setting. Given a set of $n$ vectors, each in $\\mathbb{R}^d$, we present algorithms to select a sublinear number of these vectors as coreset, while guaranteeing that the CP decomposition of the $p$-moment tensor of the coreset approximates the corresponding decomposition of the $p$-moment tensor computed from the full data. We introduce two novel algorithmic techniques: online filtering and kernelization. Using these two, we present six algorithms that achieve different tradeoffs of coreset size, update time and working space, beating or matching various state of the art algorithms. In the case of matrices ($2$-ordered tensor), our online row sampling algorithm guarantees $(1 \\pm \\epsilon)$ relative error spectral approximation. We show applications of our algorithms in learning single topic modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 2. Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series\n\n*From Search Query: state space models spectral filtering*\n\n*Abdul Fatir Ansari, Alvin Heng, Andre Lim, Harold Soh*\n\n**TL;DR:** This work proposes the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations and proposes three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference.\n\n**Abstract:** Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved imputation and forecasting performance of NCDSSM over existing models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Latent Matters: Learning Deep State-Space Models\n\n*From Search Query: state space models spectral filtering*\n\n*Alexej Klushyn, Richard Kurle, Maximilian Soelch, Botond Cseke, Patrick van der Smagt*\n\n**TL;DR:** The EKVAE out-performs previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.\n\n**Abstract:** Deep state-space models (DSSMs) enable temporal predictions by learning the underlying dynamics of observed sequence data. They are often trained by max-imising the evidence lower bound. However, as we show, this does not ensure the model actually learns the underlying dynamics. We therefore propose a constrained optimisation framework as a general approach for training DSSMs. Building upon this, we introduce the extended Kalman VAE (EKVAE), which combines amortised variational inference with classic Bayesian \ufb01ltering/smoothing to model dynamics more accurately than RNN-based DSSMs. Our results show that the constrained optimisation framework signi\ufb01cantly improves system identi\ufb01cation and prediction accuracy on the example of established state-of-the-art DSSMs. The EKVAE out-performs previous models w.r.t. prediction accuracy, achieves remarkable results in identifying dynamical systems, and can furthermore successfully learn state-space representations where static and dynamic features are disentangled.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 31  (*Influential: 3*)\n\n#### 4. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: state space models spectral filtering*\n\n*Raunaq M. Bhirangi, Chenyu Wang, Venkatesh Pattabiraman, Carmel Majidi, Abhinav Gupta, T. Hellebrekers, Lerrel Pinto*\n\n**TL;DR:** Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction that stacks structured state-space models on top of each other to create a temporal hierarchy, outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba on MSE.\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. Self-Supervised Inference in State-Space Models\n\n*From Search Query: state space models spectral filtering*\n\n*David Ruhe, Patrick Forr'e*\n\n**TL;DR:** This work performs approximate inference in state-space models with nonlinear state transitions using a local linearity approximation parameterized by neural networks, accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states.\n\n**Abstract:** We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 3  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. EchoMamba4Rec: Harmonizing Bidirectional State Space Models with Spectral Filtering for Advanced Sequential Recommendation\n\n*From Search Query: state space models spectral filtering*\n\n*Shengxin Zhu, Xuxin He, Yuda Wang*\n\n**Abstract:** Predicting user preferences and sequential dependencies based on historical behavior is the core goal of sequential recommendation. Although attention-based models have shown effectiveness in this field, they often struggle with inference inefficiency due to the quadratic computational complexity inherent in attention mechanisms, especially with long-range behavior sequences. Drawing inspiration from the recent advancements of state space models (SSMs) in control theory, which provide a robust framework for modeling and controlling dynamic systems, we introduce EchoMamba4Rec. Control theory emphasizes the use of SSMs for managing long-range dependencies and maintaining inferential efficiency through structured state matrices. EchoMamba4Rec leverages these control relationships in sequential recommendation and integrates bi-directional processing with frequency-domain filtering to capture complex patterns and dependencies in user interaction data more effectively. Our model benefits from the ability of state space models (SSMs) to learn and perform parallel computations, significantly enhancing computational efficiency and scalability. It features a bi-directional Mamba module that incorporates both forward and reverse Mamba components, leveraging information from both past and future interactions. Additionally, a filter layer operates in the frequency domain using learnable Fast Fourier Transform (FFT) and learnable filters, followed by an inverse FFT to refine item embeddings and reduce noise. We also integrate Gate Linear Units (GLU) to dynamically control information flow, enhancing the model's expressiveness and training stability. Experimental results demonstrate that EchoMamba significantly outperforms existing models, providing more accurate and personalized recommendations.\n\n**Published:** 2024-06-04\n\n\n\n#### 2. Harmonic Convolutional Networks based on Discrete Cosine Transform\n\n*From Search Query: state space models spectral filtering*\n\n*Matej Ulicny, Rozenn Dahyot, Vladimir A. Krylov*\n\n**Abstract:** Convolutional neural networks (CNNs) learn filters in order to capture local correlation patterns in feature space. We propose to learn these filters as combinations of preset spectral filters defined by the Discrete Cosine Transform (DCT). Our proposed DCT-based harmonic blocks replace conventional convolutional layers to produce partially or fully harmonic versions of new or existing CNN architectures. Using DCT energy compaction properties, we demonstrate how the harmonic networks can be efficiently compressed by truncating high-frequency information in harmonic blocks thanks to the redundancies in the spectral domain. We report extensive experimental validation demonstrating benefits of the introduction of harmonic blocks into state-of-the-art CNN models in image classification, object detection and semantic segmentation applications.\n\n**Published:** 2020-01-18\n\n\n\n#### 3. Spectral State Space Models\n\n*From Search Query: state space models spectral filtering*\n\n*Elad Hazan, Xinyi Chen, Daniel Suo, Naman Agarwal*\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n**Published:** 2023-12-11\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using state space models and spectral filtering techniques, several key points and references are relevant:\n\n## State Space Models in Language Modeling\n\n### Competitive with Transformers\nRecent research indicates that state space models (SSMs) can be competitive with Transformer models for language modeling, offering advantageous deployment characteristics. For example, the Mamba model, which is a type of SSM, has been shown to be competitive with Transformers while providing benefits in terms of deployment efficiency.\n\n### Selective State-Space Models\nThe Mamba architecture and its variants use selective SSMs, which process time sequences through learned mappings of the input, effectively implementing time-varying and nonlinear SSMs. This approach can be particularly useful for handling long sequences and reducing computational overhead.\n\n### Bidirectional State Space Models\nThe Hydra model, a bidirectional extension of Mamba, uses quasiseparable matrix mixers and demonstrates superior performance over other sequence models, including Transformers, on non-causal tasks. This bidirectional approach can enhance the model's ability to capture complex dependencies in sequences.\n\n## Spectral Filtering Techniques\n\n### Graph State Space Networks\nWhile not directly applied to language models, the concept of using state space models for spectral filtering is illustrated in the Graph State Space Network (GrassNet). GrassNet uses structured state space models to design and learn arbitrary graph spectral filters, which can be more expressive than traditional polynomial filters. This idea can be adapted to language models by applying similar spectral filtering techniques to process sequence data efficiently.\n\n### Efficient Feature Processing\nSpectral methods have shown promise in efficient feature processing. By integrating spectral techniques with state space models, researchers can potentially develop more efficient ways to handle long sequences and reduce computational overhead. This integration can leverage the strengths of both approaches to improve memory efficiency and scalability.\n\n## Efficiency and Scalability Improvements\n\n### Memory Management and State Updates\nCombining different types of memory mechanisms (e.g., eidetic and fading memory) and using selective state updates can improve efficiency. These approaches can reduce computational overhead and make the models more scalable.\n\n### Hardware-Aware Optimizations\nHardware-aware design is crucial for practical implementation. By optimizing the architecture for hardware, researchers can ensure that the models are not only theoretically efficient but also practically deployable. This includes leveraging techniques like sparse computation and linear attention mechanisms to reduce computational overhead.\n\n### Modular Architecture Design\nModular approaches allow for flexible composition of different components, which can lead to significant improvements in efficiency and scalability. Simple modifications, such as replacing softmax with ReLU in attention mechanisms, can also emerge as effective sparse computation methods.\n\nIn summary, integrating state space models with spectral filtering techniques, leveraging selective state updates, and focusing on hardware-aware optimizations can significantly improve the efficiency, scalability, and overall performance of autoregressive language models. The Mamba, Hydra, and GrassNet models provide valuable insights and methodologies that can be adapted and extended for this purpose.\n",
            "analysis": "Initial Analysis of Seed Design and Research Direction\n\n1. Summary of Analysis\n\nThe seed design SpectralHierRMSNorm introduces spectral filtering and selective state integration into hierarchical normalization. From analyzing the seed design and initial references, several key observations emerge:\n\na) Current Landscape:\n- Hierarchical normalization approaches face challenges in capturing long-range dependencies\n- State space models (SSMs) are emerging as efficient alternatives to attention\n- Spectral methods show promise for efficient feature processing\n- Memory efficiency and computational overhead remain key challenges\n\nb) Opportunities for Innovation:\n- Integration of state space models with spectral techniques\n- Novel approaches to memory and state management\n- More efficient ways to handle long sequences\n- Hardware-aware optimizations\n\n2. Key References and Excerpts\n\nFrom B'MOJO paper:\n\"We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module.\"\n\nThis suggests potential for combining different types of memory mechanisms.\n\nFrom Infini-gram paper:\n\"Instead of pre-computing n-gram count tables (which would be very expensive), we develop an engine named infini-gram -- powered by suffix arrays -- that can compute \u221e-gram probabilities with millisecond-level latency.\"\n\nShows innovative approaches to efficient sequence processing.\n\nFrom Sparse Attention with Linear Units:\n\"We introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation.\"\n\nDemonstrates potential for simplified yet effective sparse computation.\n\n3. Key Insights\n\na) Memory Management:\n- Combining different memory types (eidetic, fading) could improve efficiency\n- Selective state updates may reduce computational overhead\n- Sparse computation can emerge from simple modifications\n\nb) Computational Efficiency:\n- Linear attention mechanisms show promise\n- Spectral methods can provide efficient feature processing\n- Hardware-aware design is crucial for practical implementation\n\nc) Architecture Design:\n- Modular approaches allow flexible composition\n- Simple modifications can lead to significant improvements\n- Balance between complexity and effectiveness is key\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in state space models and their applications\n2. Efficient implementations of spectral operations\n3. Hardware optimization techniques for LM blocks\n\n5. References\n\n[1] Zancato, L., et al. (2024). \"B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory.\" arXiv preprint.\n\n[2] Liu, J., et al. (2024). \"Infini-gram: Scaling Unbounded n-gram Language Models to a Trillion Tokens.\" arXiv preprint.\n\n[3] Zhang, B., et al. (2021). \"Sparse Attention with Linear Units.\" Conference on Empirical Methods in Natural Language Processing.\n\n[4] Qin, Z., et al. (2023). \"Exploring Transformer Extrapolation.\" AAAI Conference on Artificial Intelligence."
        },
        {
            "ready": false,
            "query": "hybrid architectures SSM attention",
            "detail": "Find papers discussing hybrid architectures that combine state space models with attention or other mechanisms, focusing on improving model capabilities while maintaining efficiency.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing hybrid architectures that combine state space models with attention or other mechanisms, focusing on improving model capabilities while maintaining efficiency.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 1.00)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 2. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 1.00)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 1.00)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 0.99)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 5. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.99)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.99)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid architectures SSM attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Block-State Transformers\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. CoAtNet: Marrying Convolution and Attention for All Data Sizes\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan*\n\n**TL;DR:** This work presents CoAtNets, a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention and (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency.\n\n**Abstract:** Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced\"coat\"nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 1000  (*Influential: 105*)\n\n#### 4. Pretraining Without Attention\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Junxiong Wang, J. Yan, Albert Gu, Alexander M. Rush*\n\n**TL;DR:** This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs), and proposes a proposed model, Bidirectional Gated SSM (BiGS), that combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures.\n\n**Abstract:** Transformers have been essential to pretraining success in NLP. While other architectures have been used, downstream accuracy is either significantly worse, or requires attention layers to match standard benchmarks such as GLUE. This work explores pretraining without attention by using recent advances in sequence routing based on state-space models (SSMs). Our proposed model, Bidirectional Gated SSM (BiGS), combines SSM layers with a multiplicative gating architecture that has been effective in simplified sequence modeling architectures. The model learns static layers that do not consider pair-wise interactions. Even so, BiGS is able to match BERT pretraining accuracy on GLUE and can be extended to long-form pretraining of 4096 tokens without approximation. Analysis shows that while the models have similar average accuracy, the approach has different inductive biases than BERT in terms of interactions and syntactic representations. All models from this work are available at https://github.com/jxiw/BiGS.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 38  (*Influential: 0*)\n\n#### 5. LambdaNetworks: Modeling Long-Range Interactions Without Attention\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Irwan Bello*\n\n**TL;DR:** The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, C OCO object detection and COCO instance segmentation, while being more computationally efficient.\n\n**Abstract:** We present lambda layers -- an alternative framework to self-attention -- for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Lambda layers capture such interactions by transforming available contexts into linear functions, termed lambdas, and applying these linear functions to each input separately. Similar to linear attention, lambda layers bypass expensive attention maps, but in contrast, they model both content and position-based interactions which enables their application to large structured inputs such as images. The resulting neural network architectures, LambdaNetworks, significantly outperform their convolutional and attentional counterparts on ImageNet classification, COCO object detection and COCO instance segmentation, while being more computationally efficient. Additionally, we design LambdaResNets, a family of hybrid architectures across different scales, that considerably improves the speed-accuracy tradeoff of image classification models. LambdaResNets reach excellent accuracies on ImageNet while being 3.2 - 4.4x faster than the popular EfficientNets on modern machine learning accelerators. When training with an additional 130M pseudo-labeled images, LambdaResNets achieve up to a 9.5x speed-up over the corresponding EfficientNet checkpoints.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 167  (*Influential: 11*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid architectures SSM attention*\n\n*ChengXiang Zhai, Chenguang Zhu, Yichong Xu, Shuohang Wang, Yang Liu, Liliang Ren*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.\n\n**Conference:** sparse-modular-activation-for-efficient\n\n**Published:** 2023-06-19\n\n\n\n#### 3. HMT-UNet: A hybird Mamba-Transformer Vision UNet for Medical Image Segmentation\n\n*From Search Query: hybrid architectures SSM attention*\n\n*Yiyuan Ge, Zhihao Chen, Xianping Tao, Mingya Zhang*\n\n**Abstract:** In the field of medical image segmentation, models based on both CNN and Transformer have been thoroughly investigated. However, CNNs have limited modeling capabilities for long-range dependencies, making it challenging to exploit the semantic information within images fully. On the other hand, the quadratic computational complexity poses a challenge for Transformers. State Space Models (SSMs), such as Mamba, have been recognized as a promising method. They not only demonstrate superior performance in modeling long-range interactions, but also preserve a linear computational complexity. The hybrid mechanism of SSM (State Space Model) and Transformer, after meticulous design, can enhance its capability for efficient modeling of visual features. Extensive experiments have demonstrated that integrating the self-attention mechanism into the hybrid part behind the layers of Mamba's architecture can greatly improve the modeling capacity to capture long-range spatial dependencies. In this paper, leveraging the hybrid mechanism of SSM, we propose a U-shape architecture model for medical image segmentation, named Hybird Transformer vision Mamba UNet (HTM-UNet). We conduct comprehensive experiments on the ISIC17, ISIC18, CVC-300, CVC-ClinicDB, Kvasir, CVC-ColonDB, ETIS-Larib PolypDB public datasets and ZD-LCI-GIM private dataset. The results indicate that HTM-UNet exhibits competitive performance in medical image segmentation tasks. Our code is available at https://github.com/simzhangbest/HMT-Unet.\n\n**Published:** 2024-08-21\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by incorporating hybrid architectures that combine State Space Models (SSMs) with attention or other mechanisms, several key points and models are worth considering:\n\n## Hybrid SSM-Attention Architectures\n\n### Taipan Architecture\nThe Taipan architecture combines the efficiency of Mamba (a type of SSM) with Selective Attention Layers (SALs) to enhance long-range dependency handling. This hybrid design balances the computational efficiency of Mamba with the expressive power of SALs, allowing the model to excel in tasks requiring both speed and accurate information retrieval. The SALs employ a lightweight gating network to dynamically determine which tokens should undergo softmax attention processing, maintaining overall efficiency.\n\n### Zamba2-7B Architecture\nThe Zamba2-7B model is another example of a hybrid architecture, combining Mamba2 layers (a variant of SSMs) with shared transformer blocks. This model includes several improvements such as the use of Mamba2 blocks, LoRA projection matrices for depth specialization, and rotary position embeddings in the shared attention layers. These enhancements allow Zamba2-7B to achieve state-of-the-art performance among models with \u22648B parameters, while also offering low inference latency and a smaller memory footprint compared to pure transformer models.\n\n## Efficiency and Performance\n\n### Memory and State Management\nHybrid models like Zamba2-7B and Taipan focus on efficient memory and state management. For instance, Zamba2-7B uses a backbone of Mamba layers interleaved with shared attention layers, which minimizes parameter cost and improves information maintenance across depth. The use of LoRA projection matrices allows for additional expressivity in each block without significant parameter overhead.\n\n### Sequence Length Handling\nSSMs in these hybrid models are particularly efficient for handling long sequences. Unlike transformers, which require storing key-value caches that scale with context length, SSMs only store and attend to their recurrent state, reducing memory requirements. Techniques like Sequential Prefill can further mitigate memory scaling issues, allowing SSMs to process arbitrarily long prompts efficiently.\n\n## Improving In-Context Learning\n\n### Selective Attention Layers\nTaipan's use of Selective Attention Layers (SALs) is designed to enhance the model's ability to focus on critical tokens while maintaining efficiency. This approach dynamically allocates computational resources based on token importance, addressing some of the limitations of pure SSMs in handling complex long-range dependencies.\n\n### Hybrid Design Benefits\nThe hybrid design of models like Zamba2-7B combines the general sequence-to-sequence mapping capabilities of SSMs with the fast retrieval properties of attention layers. This combination can outperform pure Transformer or SSM models by leveraging the complementary features of both architectures.\n\n## Future Directions\n\n### Spectral Methods and Dense Connections\nFuture research could explore integrating spectral methods into these hybrid architectures to provide robustness guarantees and efficient feature processing. Additionally, dense connections between layers, as seen in models like DenseMamba, could further enhance the flow of hidden information and improve overall performance[Analysis Note].\n\n### Hardware-Aware Design\nImplementing these models with hardware-aware design considerations is crucial for practical deployment. Balancing efficiency with in-context learning capabilities and using modular design to flexibly compose components will be key to achieving scalable and high-performance autoregressive language models[Analysis Note].\n\nBy combining these insights, researchers can develop more efficient, scalable, and high-performance autoregressive language models that leverage the strengths of both SSMs and attention mechanisms.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for improving LM block design:\n\na) State Space Models (SSMs):\n- Emerging as efficient alternatives to attention mechanisms\n- Offer linear complexity and strong performance on long sequences\n- Recent innovations like Mamba and spectral SSMs show competitive results\n\nb) Spectral Methods:\n- Enable efficient feature processing and state updates\n- Can improve robustness to varying sequence lengths\n- Provide theoretical guarantees independent of spectral gap\n\nc) Memory and State Management:\n- Hybrid approaches combining different memory types show promise\n- Selective state updates can improve efficiency\n- Adaptive compression techniques can reduce memory overhead\n\n2. Useful References with Excerpts\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.\"\n\nFrom \"DenseMamba\":\n\"While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs.\"\n\nFrom \"Linear Transformers with Learnable Kernel Functions\":\n\"Current innovations, including State Space Models, were initially celebrated for surpassing Transformer performance on language modeling tasks. However, these models have revealed deficiencies in essential In-Context Learning capabilities.\"\n\n3. Key Insights\n\na) Spectral Filtering Benefits:\n- Can provide robustness guarantees independent of model parameters\n- Enables efficient processing of long sequences\n- Fixed convolutional filters reduce parameter count\n\nb) State Management:\n- Dense connections between layers can improve information flow\n- Selective state updates balance efficiency and expressiveness\n- Hybrid memory mechanisms show promise\n\nc) Implementation Considerations:\n- Hardware-aware design is crucial for practical deployment\n- Balance needed between efficiency and in-context learning capabilities\n- Modular design allows flexible composition of components\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in hybrid architectures combining SSMs with other techniques\n2. Efficient implementations of spectral operations in neural networks\n3. Methods for improving in-context learning in SSM-based models\n\n5. References\n\n[1] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n[2] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Aksenov, Y., et al. (2024). Linear Transformers with Learnable Kernel Functions are Better In-Context Models. arXiv preprint.\n\n[4] Smith, J.T.H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n[5] Bhirangi, R.M., et al. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "spectral methods neural networks",
            "detail": "Find papers discussing spectral methods in neural networks, focusing on applications to sequence modeling and state space models.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing spectral methods in neural networks, focusing on applications to sequence modeling and state space models.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 1/31 (Score: 1.00)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.97)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.97)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 3. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.97)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.97)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n#### 4. State-Free Inference of State-Space Models: The Transfer Function Approach (Avg. Score: 0.94)\n\n*Rom N. Parnichkun, Stefano Massaroli, Alessandro Moro, Jimmy T.H. Smith, Ramin M. Hasani, Mathias Lechner, Qi An, Christopher R'e, Hajime Asama, Stefano Ermon, Taiji Suzuki, Atsushi Yamashita, Michael Poli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work uncovers a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size.\n\n**Abstract:** We approach designing a state-space model for deep learning applications through its dual representation, the transfer function, and uncover a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF.\n\n##### *Relevant Chunk: No. 31/44 (Score: 0.94)*\n\n```\nIn Proc. NeurIPS, 2020. Smith, J. T., Warrington, A., and Linderman, S. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview. net/forum?id=Ai8Hw3AXqks. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=qVyeW-grC2k. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. Attention is all you need. In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips. cc/paper_files/paper/2017/file/ 3f5ee243547dee91fbd053c1c4a845aa-Paper. pdf. Zhang, M., Saab, K., Poli, M., Dao, T., Goel, K., and R\u00e9, C. Effectively modeling time series with simple discrete state spaces. International Conference on Learning Representations, 2023. ## Supplementary Material\n\n## Contents\n\n1 Introduction ..... 1\n2 Preliminaries and Related Work ..... 2\n2.1 Sequence Modeling with Convolutions ..... 2\n2.2 State-Space Realization of Convolutions ..... 3\n3 Training SSMs in the frequency domain .....\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: spectral methods neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods\n\n*From Search Query: spectral methods neural networks*\n\n*A. Gautier, Quynh N. Nguyen, Matthias Hein*\n\n**TL;DR:** This work shows under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with a nonlinear spectral method, the first practically feasible method which achieves such a guarantee.\n\n**Abstract:** The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 32  (*Influential: 1*)\n\n#### 2. A Unified Algebraic Perspective on Lipschitz Neural Networks\n\n*From Search Query: spectral methods neural networks*\n\n*Alexandre Araujo, Aaron J. Havens, Blaise Delattre, A. Allauzen, Bin Hu*\n\n**TL;DR:** A novel algebraic perspective unifies various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods, and shows that SLLs outperform previous approaches on certified robust accuracy.\n\n**Abstract:** Important research efforts have focused on the design and training of neural networks with a controlled Lipschitz constant. The goal is to increase and sometimes guarantee the robustness against adversarial attacks. Recent promising techniques draw inspirations from different backgrounds to design 1-Lipschitz neural networks, just to name a few: convex potential layers derive from the discretization of continuous dynamical systems, Almost-Orthogonal-Layer proposes a tailored method for matrix rescaling. However, it is today important to consider the recent and promising contributions in the field under a common theoretical lens to better design new and improved layers. This paper introduces a novel algebraic perspective unifying various types of 1-Lipschitz neural networks, including the ones previously mentioned, along with methods based on orthogonality and spectral methods. Interestingly, we show that many existing techniques can be derived and generalized via finding analytical solutions of a common semidefinite programming (SDP) condition. We also prove that AOL biases the scaled weight to the ones which are close to the set of orthogonal matrices in a certain mathematical manner. Moreover, our algebraic condition, combined with the Gershgorin circle theorem, readily leads to new and diverse parameterizations for 1-Lipschitz network layers. Our approach, called SDP-based Lipschitz Layers (SLL), allows us to design non-trivial yet efficient generalization of convex potential layers. Finally, the comprehensive set of experiments on image classification shows that SLLs outperform previous approaches on certified robust accuracy. Code is available at https://github.com/araujoalexandre/Lipschitz-SLL-Networks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 36  (*Influential: 7*)\n\n#### 3. Spectral Clustering with Graph Neural Networks for Graph Pooling\n\n*From Search Query: spectral methods neural networks*\n\n*F. Bianchi, Daniele Grattarola, C. Alippi*\n\n**TL;DR:** This paper forms a continuous relaxation of the normalized minCUT problem and trains a GNN to compute cluster assignments that minimize this objective, and designs a graph pooling operator that overcomes some important limitations of state-of-the-art graph Pooling techniques and achieves the best performance in several supervised and unsupervised tasks.\n\n**Abstract:** Spectral clustering (SC) is a popular clustering technique to find strongly connected communities on a graph. SC can be used in Graph Neural Networks (GNNs) to implement pooling operations that aggregate nodes belonging to the same cluster. However, the eigendecomposition of the Laplacian is expensive and, since clustering results are graph-specific, pooling methods based on SC must perform a new optimization for each new sample. In this paper, we propose a graph clustering approach that addresses these limitations of SC. We formulate a continuous relaxation of the normalized minCUT problem and train a GNN to compute cluster assignments that minimize this objective. Our GNN-based implementation is differentiable, does not require to compute the spectral decomposition, and learns a clustering function that can be quickly evaluated on out-of-sample graphs. From the proposed clustering method, we design a graph pooling operator that overcomes some important limitations of state-of-the-art graph pooling techniques and achieves the best performance in several supervised and unsupervised tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 373  (*Influential: 45*)\n\n#### 4. Facilitating Graph Neural Networks with Random Walk on Simplicial Complexes\n\n*From Search Query: spectral methods neural networks*\n\n*Cai Zhou, Xiyuan Wang, Muhan Zhang*\n\n**TL;DR:** This paper systematically analyzes how random walk on different orders of simplicial complexes (SC) facilitates GNNs in their theoretical expressivity and proposes Hodge1Lap, a permutation equivariant and expressive edge-level positional encoding.\n\n**Abstract:** Node-level random walk has been widely used to improve Graph Neural Networks. However, there is limited attention to random walk on edge and, more generally, on $k$-simplices. This paper systematically analyzes how random walk on different orders of simplicial complexes (SC) facilitates GNNs in their theoretical expressivity. First, on $0$-simplices or node level, we establish a connection between existing positional encoding (PE) and structure encoding (SE) methods through the bridge of random walk. Second, on $1$-simplices or edge level, we bridge edge-level random walk and Hodge $1$-Laplacians and design corresponding edge PE respectively. In the spatial domain, we directly make use of edge level random walk to construct EdgeRWSE. Based on the spectral analysis of Hodge $1$-Laplcians, we propose Hodge1Lap, a permutation equivariant and expressive edge-level positional encoding. Third, we generalize our theory to random walk on higher-order simplices and propose the general principle to design PE on simplices based on random walk and Hodge Laplacians. Inter-level random walk is also introduced to unify a wide range of simplicial networks. Extensive experiments verify the effectiveness of our random walk-based methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 5. Feature Expansion for Graph Neural Networks\n\n*From Search Query: spectral methods neural networks*\n\n*Jiaqi Sun, Lin Zhang, Guan-Hong Chen, Kun Zhang, Peng Xu, Yujiu Yang*\n\n**TL;DR:** This paper decomposes graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis, and theoretically finds that thefeature space tends to be linearly correlated due to repeated aggregations.\n\n**Abstract:** Graph neural networks aim to learn representations for graph-structured data and show impressive performance, particularly in node classification. Recently, many methods have studied the representations of GNNs from the perspective of optimization goals and spectral graph theory. However, the feature space that dominates representation learning has not been systematically studied in graph neural networks. In this paper, we propose to fill this gap by analyzing the feature space of both spatial and spectral models. We decompose graph neural networks into determined feature spaces and trainable weights, providing the convenience of studying the feature space explicitly using matrix space analysis. In particular, we theoretically find that the feature space tends to be linearly correlated due to repeated aggregations. Motivated by these findings, we propose 1) feature subspaces flattening and 2) structural principal components to expand the feature space. Extensive experiments verify the effectiveness of our proposed more comprehensive feature space, with comparable inference time to the baseline, and demonstrate its efficient convergence capability.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Graph Attention Networks\n\n*From Search Query: spectral methods neural networks*\n\n*Pietro Li\u00f2, Arantxa Casanova, Petar Veli\u010dkovi\u0107, Yoshua Bengio, Guillem Cucurull, Adriana Romero*\n\n**Abstract:** We present graph attention networks (GATs), novel neural network\narchitectures that operate on graph-structured data, leveraging masked\nself-attentional layers to address the shortcomings of prior methods based on\ngraph convolutions or their approximations. By stacking layers in which nodes\nare able to attend over their neighborhoods' features, we enable (implicitly)\nspecifying different weights to different nodes in a neighborhood, without\nrequiring any kind of costly matrix operation (such as inversion) or depending\non knowing the graph structure upfront. In this way, we address several key\nchallenges of spectral-based graph neural networks simultaneously, and make our\nmodel readily applicable to inductive as well as transductive problems. Our GAT\nmodels have achieved or matched state-of-the-art results across four\nestablished transductive and inductive graph benchmarks: the Cora, Citeseer and\nPubmed citation network datasets, as well as a protein-protein interaction\ndataset (wherein test graphs remain unseen during training).\n\n**Conference:** graph-attention-networks-1\n\n**Published:** 2017-10-30\n\n\n\n#### 2. SplineCNN: Fast Geometric Deep Learning with Continuous B-Spline Kernels\n\n*From Search Query: spectral methods neural networks*\n\n*Heinrich M\u00fcller, Matthias Fey, Jan Eric Lenssen, Frank Weichert*\n\n**Abstract:** We present Spline-based Convolutional Neural Networks (SplineCNNs), a variant\nof deep neural networks for irregular structured and geometric input, e.g.,\ngraphs or meshes. Our main contribution is a novel convolution operator based\non B-splines, that makes the computation time independent from the kernel size\ndue to the local support property of the B-spline basis functions. As a result,\nwe obtain a generalization of the traditional CNN convolution operator by using\ncontinuous kernel functions parametrized by a fixed number of trainable\nweights. In contrast to related approaches that filter in the spectral domain,\nthe proposed method aggregates features purely in the spatial domain. In\naddition, SplineCNN allows entire end-to-end training of deep architectures,\nusing only the geometric structure as input, instead of handcrafted feature\ndescriptors. For validation, we apply our method on tasks from the fields of\nimage graph classification, shape correspondence and graph node classification,\nand show that it outperforms or pars state-of-the-art approaches while being\nsignificantly faster and having favorable properties like domain-independence.\n\n**Conference:** splinecnn-fast-geometric-deep-learning-with-1\n\n**Published:** 2017-11-24\n\n\n\n#### 3. Topology Adaptive Graph Convolutional Networks\n\n*From Search Query: spectral methods neural networks*\n\n*Jose M. F.  Moura, Soummya Kar, Shanghang Zhang, Jian Du, Guanhang Wu*\n\n**Abstract:** Spectral graph convolutional neural networks (CNNs) require approximation to\nthe convolution to alleviate the computational complexity, resulting in\nperformance loss. This paper proposes the topology adaptive graph convolutional\nnetwork (TAGCN), a novel graph convolutional network defined in the vertex\ndomain. We provide a systematic way to design a set of fixed-size learnable\nfilters to perform convolutions on graphs. The topologies of these filters are\nadaptive to the topology of the graph when they scan the graph to perform\nconvolution. The TAGCN not only inherits the properties of convolutions in CNN\nfor grid-structured data, but it is also consistent with convolution as defined\nin graph signal processing. Since no approximation to the convolution is\nneeded, TAGCN exhibits better performance than existing spectral CNNs on a\nnumber of data sets and is also computationally simpler than other recent\nmethods.\n\n**Conference:** topology-adaptive-graph-convolutional-1\n\n**Published:** 2017-10-28\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on the integration of spectral methods with state space models (SSMs) and their applications to sequence modeling, here are some relevant findings and directions:\n\n## Spectral Attention and Its Applications\nThe concept of spectral attention, as introduced in the paper \"Spectral Attention for Long-Range Dependency in Time\", could be highly relevant. This method preserves long-period trends through a low-pass filter and facilitates gradient flow between samples, which is crucial for sequence modeling. Integrating spectral attention into SSMs could enhance the model's ability to capture long-range dependencies efficiently.\n\n## Spectral Geometry in Neural Modeling\nThe paper \"Latent Functional Maps: a spectral framework for representation learning\" discusses integrating spectral geometry principles into neural modeling. This framework could be adapted to improve the representation learning capabilities of SSMs, particularly in handling complex sequence data. By leveraging spectral methods, the model can better mitigate issues related to long-range dependencies and state management.\n\n## State Space Models and Spectral Methods\nThe analysis note highlights the potential of combining SSMs with other mechanisms, such as attention and spectral methods. For instance, the \"LOCOST\" architecture, which is based on state-space models for conditional text generation, shows promising results in terms of performance and memory efficiency[Analysis Note]. Integrating spectral methods into such architectures could further enhance their capabilities in handling long sequences and improving overall model expressiveness.\n\n## Adaptive and Selective Computation\nThe future search plan emphasizes the need to investigate techniques for adaptive and selective computation in neural networks. Spectral methods can be part of this strategy by allowing for more efficient and selective processing of sequence data. For example, using spectral attention or spectral geometry principles can help in identifying and focusing on the most relevant parts of the input sequence, thus optimizing computational resources.\n\n## Hardware Optimization\nGiven the importance of hardware-aware implementations, integrating spectral methods into SSMs should also consider hardware optimization strategies. This could involve designing spectral attention mechanisms or spectral geometry-based representations that are computationally efficient and can be optimized for specific hardware architectures.\n\nIn summary, the integration of spectral methods with SSMs holds significant promise for improving autoregressive language model design. Key areas to focus on include:\n\n- **Spectral Attention**: Implementing spectral attention to capture long-range dependencies efficiently.\n- **Spectral Geometry**: Leveraging spectral geometry principles to enhance representation learning in SSMs.\n- **Adaptive Computation**: Using spectral methods to enable adaptive and selective computation in sequence modeling.\n- **Hardware Optimization**: Designing spectral-based mechanisms that are optimized for hardware efficiency.\n\nThese approaches can help in balancing computational efficiency with model expressiveness, managing memory and state effectively, and ensuring stability and robustness in hybrid architectures.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe search results reveal several key trends and opportunities in hybrid architectures combining SSMs with other mechanisms:\n\na) Architectural Innovations:\n- Hybrid models combining SSMs with attention show promise in balancing efficiency and expressiveness\n- Novel approaches to state management and compression are emerging\n- Hardware-aware implementations are crucial for practical deployment\n\nb) Key Challenges:\n- Balancing computational efficiency with model expressiveness\n- Managing memory and state effectively for long sequences\n- Ensuring stability and robustness in hybrid architectures\n\nc) Promising Directions:\n- Integration of spectral methods with state space models\n- Selective and adaptive computation mechanisms\n- Hardware-optimized implementations\n\n2. Useful References with Excerpts\n\nFrom \"CHELA\":\n\"We leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner.\"\n\nFrom \"LOCOST\":\n\"We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs... reaching a performance level that is 93-96% comparable to the top-performing sparse transformers while saving up to 50% memory.\"\n\nFrom \"Transformers are SSMs\":\n\"Our state space duality (SSD) framework allows us to design a new architecture whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers.\"\n\nFrom \"Mamba\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning... letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\n\n3. Key Insights\n\na) Architectural Design:\n- Combining SSMs with selective attention mechanisms can provide both efficiency and expressiveness\n- Short-long convolutions offer an efficient alternative to standard SSMs\n- Hardware-aware implementation is crucial for practical performance\n\nb) Memory and State Management:\n- Adaptive state sizing based on input complexity\n- Selective computation through gating mechanisms\n- Efficient compression techniques for state representation\n\nc) Performance Considerations:\n- Linear complexity achievable through careful design\n- Memory savings of 50-87% compared to transformer baselines\n- Potential for 2-8x speed improvements with optimized implementations\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in spectral methods and their applications to SSMs\n2. Techniques for adaptive and selective computation in neural networks\n3. Hardware optimization strategies for hybrid architectures\n\n5. References\n\n[1] Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\" arXiv preprint.\n\n[2] Le Bronnec, F., et al. (2024). \"LOCOST: State-Space Models for Long Document Abstractive Summarization.\" Conference of the European Chapter of the Association for Computational Linguistics.\n\n[3] Dao, T., & Gu, A. (2024). \"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\" arXiv preprint.\n\n[4] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv preprint.\n\n[5] Sieber, J., et al. (2024). \"Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "adaptive spectral methods compression",
            "detail": "Find papers discussing adaptive spectral methods and compression techniques in neural networks, particularly focusing on applications to state space models and sequence processing.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing adaptive spectral methods and compression techniques in neural networks, particularly focusing on applications to state space models and sequence processing.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.99)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.95)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.94)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.94)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.87)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.87)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n#### 4. Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model (Avg. Score: 0.84)\n\n*Wenbing Li, Hang Zhou, Junqing Yu, Zikai Song, Wei Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Coupled SSM model is proposed, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes, and demonstrates that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n**Abstract:** The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n##### *Relevant Chunk: No. 21/22 (Score: 0.84)*\n\n```\nIn Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3718-3727, 2020. [38] Amir Zadeh, Paul Pu Liang, Navonil Mazumder, Soujanya Poria, Erik Cambria, and LouisPhilippe Morency. Memory fusion network for multi-view sequential learning. Proceedings of the AAAI Conference on Artificial Intelligence, Jun 2022. [39] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022. [40] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. [41] Albert Gu, Karan Goel, Ankit Gupta, and Christopher Re. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [42] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model. arXiv preprint arXiv:2401.09417, 2024. [43] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive spectral methods compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Debiased Distribution Compression\n\n*From Search Query: adaptive spectral methods compression*\n\n*Lingxiao Li, Raaz Dwivedi, Lester Mackey*\n\n**TL;DR:** New suite of compression methods suitable for compression with biased input sequences are introduced, providing succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.\n\n**Abstract:** Modern compression methods can summarize a target distribution $\\mathbb{P}$ more succinctly than i.i.d. sampling but require access to a low-bias input sequence like a Markov chain converging quickly to $\\mathbb{P}$. We introduce a new suite of compression methods suitable for compression with biased input sequences. Given $n$ points targeting the wrong distribution and quadratic time, Stein kernel thinning (SKT) returns $\\sqrt{n}$ equal-weighted points with $\\widetilde{O}(n^{-1/2})$ maximum mean discrepancy (MMD) to $\\mathbb{P}$. For larger-scale compression tasks, low-rank SKT achieves the same feat in sub-quadratic time using an adaptive low-rank debiasing procedure that may be of independent interest. For downstream tasks that support simplex or constant-preserving weights, Stein recombination and Stein Cholesky achieve even greater parsimony, matching the guarantees of SKT with as few as $\\text{poly-log}(n)$ weighted points. Underlying these advances are new guarantees for the quality of simplex-weighted coresets, the spectral decay of kernel matrices, and the covering numbers of Stein kernel Hilbert spaces. In our experiments, our techniques provide succinct and accurate posterior summaries while overcoming biases due to burn-in, approximate Markov chain Monte Carlo, and tempering.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 1*)\n\n#### 2. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n*From Search Query: adaptive spectral methods compression*\n\n*Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao*\n\n**TL;DR:** Adaptive KV cache compression is introduced, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs) and demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss.\n\n**Abstract:** In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 105  (*Influential: 13*)\n\n#### 3. Adaptive Contrastive Knowledge Distillation for BERT Compression\n\n*From Search Query: adaptive spectral methods compression*\n\n*Jinyang Guo, Jiaheng Liu, Z. Wang, Yuqing Ma, Ruihao Gong, Ke Xu, Xianglong Liu*\n\n**TL;DR:** This paper introduces a novel contrastive distillation loss based on hidden state features in BERT as the explicit supervision to learn discriminative student features and incorporates this strategy into a new strategy called sample adaptive reweighting (SAR) to adaptively pay more attention to these hard samples and strengthen their discrimination abilities.\n\n**Abstract:** In this paper, we propose a new knowledge distillation approach called adaptive contrastive knowledge distillation (ACKD) for BERT compression. Different from existing knowledge distillation methods for BERT that implicitly learn discriminative student features by mimicking the teacher features, we first introduce a novel contrastive distillation loss (CDL) based on hidden state features in BERT as the explicit supervision to learn discriminative student features. We further observe sentences with similar features may have completely different meanings, which makes them hard to distinguish. Existing methods do not pay sufficient attention to these hard samples with less discriminative features. Therefore, we propose a new strategy called sample adaptive reweighting (SAR) to adaptively pay more attention to these hard samples and strengthen their discrimination abilities. We incorporate our SAR strategy into our CDL and form the adaptive contrastive distillation loss, based on which we construct our ACKD framework. Comprehensive experiments on multiple natural language processing tasks demonstrate the effectiveness of our ACKD framework.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 4. On Distributed Adaptive Optimization with Gradient Compression\n\n*From Search Query: adaptive spectral methods compression*\n\n*Xiaoyun Li, Belhal Karimi, Ping Li*\n\n**Abstract:** We study COMP-AMS, a distributed optimization framework based on gradient averaging and adaptive AMSGrad algorithm. Gradient compression with error feedback is applied to reduce the communication cost in the gradient transmission process. Our convergence analysis of COMP-AMS shows that such compressed gradient averaging strategy yields same convergence rate as standard AMSGrad, and also exhibits the linear speedup effect w.r.t. the number of local workers. Compared with recently proposed protocols on distributed adaptive methods, COMP-AMS is simple and convenient. Numerical experiments are conducted to justify the theoretical findings, and demonstrate that the proposed method can achieve same test accuracy as the full-gradient AMSGrad with substantial communication savings. With its simplicity and efficiency, COMP-AMS can serve as a useful distributed training framework for adaptive gradient methods.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 5. Neural Image Compression: Generalization, Robustness, and Spectral Biases\n\n*From Search Query: adaptive spectral methods compression*\n\n*Kelsey Lieberman, James Diffenderfer, Charles Godfrey, B. Kailkhura*\n\n**TL;DR:** A comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods, a detailed performance comparison of several classic codecs and NIC variants, and spectrally-inspired inspection tools to gain deeper insight into errors introduced by image compression method as well as their OOD performance.\n\n**Abstract:** Recent advances in neural image compression (NIC) have produced models that are starting to outperform classic codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to the popular CLIC and Kodak benchmarks. Next, we propose spectrally-inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of several classic codecs and NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Binarized Spectral Compressive Imaging\n\n*From Search Query: adaptive spectral methods compression*\n\n*Haoqian Wang, Yulun Zhang, Xin Yuan, Jing Lin, Yuxin Zheng, Yuanhao Cai*\n\n**Abstract:** Existing deep learning models for hyperspectral image (HSI) reconstruction achieve good performance but require powerful hardwares with enormous memory and computational resources. Consequently, these methods can hardly be deployed on resource-limited mobile devices. In this paper, we propose a novel method, Binarized Spectral-Redistribution Network (BiSRNet), for efficient and practical HSI restoration from compressed measurement in snapshot compressive imaging (SCI) systems. Firstly, we redesign a compact and easy-to-deploy base model to be binarized. Then we present the basic unit, Binarized Spectral-Redistribution Convolution (BiSR-Conv). BiSR-Conv can adaptively redistribute the HSI representations before binarizing activation and uses a scalable hyperbolic tangent function to closer approximate the Sign function in backpropagation. Based on our BiSR-Conv, we customize four binarized convolutional modules to address the dimension mismatch and propagate full-precision information throughout the whole network. Finally, our BiSRNet is derived by using the proposed techniques to binarize the base model. Comprehensive quantitative and qualitative experiments manifest that our proposed BiSRNet outperforms state-of-the-art binarization methods and achieves comparable performance with full-precision algorithms. Code and models are publicly available at https://github.com/caiyuanhao1998/BiSCI and https://github.com/caiyuanhao1998/MST\n\n**Conference:** binarized-spectral-compressive-imaging\n\n**Published:** 2023-05-17\n\n\n\n#### 2. Adaptive spectral proper orthogonal decomposition of tonal flows\n\n*From Search Query: adaptive spectral methods compression*\n\n*Anonymous*\n\n**Abstract:** An adaptive algorithm for spectral proper orthogonal decomposition (SPOD) of mixed broadband-tonal turbulent flows is developed. Sharp peak resolution at tonal frequencies is achieved by locally minimizing the bias of the spectrum. Smooth spectrum estimates of broadband regions are achieved by locally reducing the variance of the spectrum. The method utilizes multitaper estimation with sine tapers. An iterative criterion based on modal convergence is introduced to enable the SPOD to adapt to spectral features. For tonal flows, the adaptivity is controlled by a single user input; for broadband flows, a constant number of sine tapers is recommended without adaptivity. The discrete version of Parseval's theorem for SPOD is stated. Proper normalization of the tapers ensures that Parseval's theorem is satisfied in expectation. Drastic savings in computational complexity and memory usage are facilitated by two aspects: (i) sine tapers, which permit post hoc windowing of a single Fourier transform; and (ii) time-domain lossless compression using a QR or eigenvalue decomposition. Sine-taper SPOD is demonstrated on time-resolved particle image velocimetry (TR-PIV) data from an open cavity flow and high-fidelity large-eddy simulation (LES) data from a round jet, with and without adaptivity. For the tonal cavity flow, the adaptive algorithm outperforms Slepian-based multitaper SPOD in terms of variance and local bias of the spectrum, mode convergence, and memory usage.\n\n**Published:** 2023-12-04\n\n\n\n#### 3. Parameter-Efficient Fine-Tuning with Discrete Fourier Transform\n\n*From Search Query: adaptive spectral methods compression*\n\n*Jia Li, Liang Chen, Bingzhe Wu, Zijing Liu, Aochuan Chen, Qichao Wang, Ziqi Gao*\n\n**Abstract:** Low-rank adaptation~(LoRA) has recently gained much interest in fine-tuning foundation models. It effectively reduces the number of trainable parameters by incorporating low-rank matrices $A$ and $B$ to represent the weight change, i.e., $\\Delta W=BA$. Despite LoRA's progress, it faces storage challenges when handling extensive customization adaptations or larger base models. In this work, we aim to further compress trainable parameters by enjoying the powerful expressiveness of the Fourier transform. Specifically, we introduce FourierFT, which treats $\\Delta W$ as a matrix in the spatial domain and learns only a small fraction of its spectral coefficients. With the trained spectral coefficients, we implement the inverse discrete Fourier transform to recover $\\Delta W$. Empirically, our FourierFT method shows comparable or better performance with fewer parameters than LoRA on various tasks, including natural language understanding, natural language generation, instruction tuning, and image classification. For example, when performing instruction tuning on the LLaMA2-7B model, FourierFT surpasses LoRA with only 0.064M trainable parameters, compared to LoRA's 33.5M. Our code is released at \\url{https://github.com/Chaos96/fourierft}.\n\n**Published:** 2024-05-05\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model (LM) design using adaptive spectral methods and compression techniques, here are some key insights and relevant information from the provided sources and additional context:\n\n## Adaptive Spectral Methods\n\n### Frequency Principle and Spectral Bias\nThe frequency principle or spectral bias in deep neural networks, as described in, highlights that DNNs tend to fit target functions from low to high frequencies during training. This principle can be leveraged to design spectral methods that prioritize learning low-frequency components first, which are often more critical for capturing long-range dependencies in sequence data.\n\n### Phase Shift DNN and Adaptive Activation Functions\nTechniques like PhaseDNN, which converts high-frequency components to low-frequency spectra for learning, and adaptive activation functions can be adapted for LM design. These methods can help in efficiently capturing both low and high-frequency patterns in sequence data, enhancing the model's ability to handle long-range dependencies.\n\n## Compression Techniques\n\n### Developmental Plasticity-Inspired Adaptive Pruning\nThe paper on developmental plasticity-inspired adaptive pruning (DPAP) for deep neural networks (DNNs) and spiking neural networks (SNNs) provides insights into efficient compression methods. This approach dynamically prunes redundant synapses and neurons based on their activity levels, which can be applied to LMs to reduce computational and memory costs while maintaining performance.\n\n### Efficient State Compression\nFor state space models, efficient state compression can be achieved through methods that reduce the dimensionality of the state space while preserving critical information. Techniques such as structured pruning, as mentioned in the DPAP method, can be adapted for LM state compression. This involves treating each channel as an overall neuron population to prune, ensuring that the model remains efficient and compact.\n\n## Hybrid Architectures and Adaptive Computation\n\n### Spectral State Space Models\nIntegrating spectral methods with state space models can provide robustness and efficiency. Spectral state space models use fixed convolutional filters that do not require learning, which can be beneficial for processing long sequences efficiently. This approach can be combined with adaptive computation strategies to balance between efficiency and expressiveness (Analysis Note).\n\n### Robustifying State-Space Models\nThe \"perturb-then-diagonalize\" (PTD) methodology, based on pseudospectral theory, can be used to robustify state-space models against noise-perturbed inputs. This methodology can be applied to LM design to enhance resilience to varying input conditions, a crucial aspect for handling diverse and noisy data (Analysis Note).\n\n## Future Search Plan\n\n### Recent Advances in Adaptive Spectral Methods\nInvestigate recent papers and preprints that focus on adaptive spectral methods, such as those using phase shift techniques or adaptive activation functions, to see how these can be applied to LM design.\n\n### Techniques for Efficient State Compression\nLook into state-of-the-art compression techniques, including pruning methods inspired by developmental plasticity, to optimize the state size and computational cost of LMs.\n\n### Hardware Optimization Strategies\nExplore hardware optimization strategies for spectral operations, such as efficient FFT implementations, to ensure that the proposed models can be efficiently executed on various hardware platforms.\n\nBy integrating these insights, researchers can develop novel autoregressive LM blocks that leverage adaptive spectral methods and efficient compression techniques, enhancing robustness, efficiency, and scalability.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for improving LM block design through the integration of spectral methods and state space models:\n\na) Spectral State Space Models:\n- Provide robustness guarantees independent of spectral gap\n- Enable efficient processing through fixed convolutional filters\n- Show strong performance on long-range tasks\n\nb) Hardware-Efficient Implementation:\n- Need for efficient spectral operations\n- Importance of parallel processing capabilities\n- Memory optimization strategies\n\nc) Hybrid Architectures:\n- Potential for combining spectral methods with other mechanisms\n- Balance between efficiency and expressiveness\n- Adaptive computation strategies\n\n2. Useful References with Excerpts\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.\"\n\nFrom \"Robustifying State-space Models\":\n\"We introduce a generic, backward-stable 'perturb-then-diagonalize' (PTD) methodology, which is based on the pseudospectral theory of non-normal operators... Our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models.\"\n\nFrom \"State-Free Inference\":\n\"We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform.\"\n\n3. Key Insights\n\na) Spectral Methods Benefits:\n- Robustness to varying input conditions\n- Efficient computation through fixed filters\n- Theoretical guarantees on performance\n\nb) Implementation Considerations:\n- Need for backward-stable operations\n- Importance of efficient FFT implementations\n- Balance between state size and computational cost\n\nc) Architecture Design:\n- Potential for hybrid spectral-state space models\n- Importance of adaptive computation\n- Role of transfer functions in optimization\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in adaptive spectral methods\n2. Techniques for efficient state compression\n3. Hardware optimization strategies for spectral operations\n\n5. References\n\n[1] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). \"Spectral State Space Models.\" arXiv preprint.\n\n[2] Yu, A., Nigmetov, A., Morozov, D., Mahoney, M.W., & Erichson, N.B. (2023). \"Robustifying State-space Models for Long Sequences via Approximate Diagonalization.\" arXiv preprint.\n\n[3] Parnichkun, R.N., et al. (2024). \"State-Free Inference of State-Space Models: The Transfer Function Approach.\" arXiv preprint.\n\n[4] Qin, Z., et al. (2024). \"You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet.\" arXiv preprint."
        },
        {
            "ready": false,
            "query": "spectral normalization implementation deep learning",
            "detail": "Find papers discussing:\n1. Practical implementations of spectral normalization in deep networks\n2. Efficient computation strategies for spectral operations\n3. Stability analysis methods for deep neural networks\n4. Memory-efficient implementations of hierarchical processing\n5. Integration of spectral methods with normalization layers",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Practical implementations of spectral normalization in deep networks\n2. Efficient computation strategies for spectral operations\n3. Stability analysis methods for deep neural networks\n4. Memory-efficient implementations of hierarchical processing\n5. Integration of spectral methods with normalization layers\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.30)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.30)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 2. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.07)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.07)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n#### 3. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.04)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.04)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 4. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.03)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 11/63 (Score: 0.03)*\n\n```\nE. Hinton. Layer normalization. stat, 1050:21, 2016. [5] D. A. M. Barrington, K. Compton, H. Straubing, and D. Th\u00e9rien. Regular languages in nc1. Journal of Computer and System Sciences, 44(3):478-499, 1992. [6] S. Bhattamishra, K. Ahuja, and N. Goyal. On the ability and limitations of transformers to recognize formal languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7096-7116, 2020. [7] D. Blasi, R. Cotterell, L. Wolf-Sonkin, S. Stoll, B. Bickel, and M. Baroni. On the distribution of deep clausal embeddings: A large cross-linguistic study. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3938-3943, 2019. [8] J. Bradbury, S. Merity, C. Xiong, and R. Socher. Quasi-recurrent neural networks. In International Conference on Learning Representations, 2016. [9] D. Chiang and P. Cholak. Overcoming a theoretical limitation of self-attention. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7654-7664, 2022. [10] D. Chiang, P. Cholak, and A. Pillay. Tighter bounds on the expressivity of transformer encoders.\n```\n\n#### 5. Resurrecting Recurrent Neural Networks for Long Sequences  (Avg. Score: 0.03)\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 146  (*Influential: 26*)\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n##### *Relevant Chunk: No. 17/71 (Score: 0.03)*\n\n```\nE. Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. S. Bai, J. Z. Kolter, and V. Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 1994. N. Bordin, C. Dallago, M. Heinzinger, S. Kim, M. Littmann, C. Rauer, M. Steinegger, B. Rost, and C. Orengo. Novel machine learning approaches revolutionize protein knowledge. Trends in Biochemical Sciences, 2022. J. Bradbury, R. Frostig, P. Hawkins, M. J. Johnson, C. Leary, D. Maclaurin, G. Necula, A. Paszke, J. VanderPlas, S. Wanderman-Milne, et al. JAX: composable transformations of python+ numpy programs, 2018.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: spectral normalization implementation deep learning\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Simple and Principled Uncertainty Estimation with Deterministic Deep Learning via Distance Awareness\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Jeremiah Zhe Liu, Zi Lin, Shreyas Padhy, Dustin Tran, Tania Bedrax-Weiss, Balaji Lakshminarayanan*\n\n**TL;DR:** Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process and outperforms the other single-model approaches.\n\n**Abstract:** Bayesian neural networks (BNN) and deep ensembles are principled approaches to estimate the predictive uncertainty of a deep learning model. However their practicality in real-time, industrial-scale applications are limited due to their heavy memory and inference cost. This motivates us to study principled approaches to high-quality uncertainty estimation that require only a single deep neural network (DNN). By formalizing the uncertainty quantification as a minimax learning problem, we first identify input distance awareness, i.e., the model's ability to quantify the distance of a testing example from the training data in the input space, as a necessary condition for a DNN to achieve high-quality (i.e., minimax optimal) uncertainty estimation. We then propose Spectral-normalized Neural Gaussian Process (SNGP), a simple method that improves the distance-awareness ability of modern DNNs, by adding a weight normalization step during training and replacing the output layer with a Gaussian process. On a suite of vision and language understanding tasks and on modern architectures (Wide-ResNet and BERT), SNGP is competitive with deep ensembles in prediction, calibration and out-of-domain detection, and outperforms the other single-model approaches.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 394  (*Influential: 82*)\n\n#### 2. Why Spectral Normalization Stabilizes GANs: Analysis and Improvements\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Zinan Lin, Vyas Sekar, G. Fanti*\n\n**TL;DR:** This work shows that SN controls two important failure modes of GAN training: exploding and vanishing gradients, and proposes Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization.\n\n**Abstract:** Spectral normalization (SN) is a widely-used technique for improving the stability of Generative Adversarial Networks (GANs) by forcing each layer of the discriminator to have unit spectral norm. This approach controls the Lipschitz constant of the discriminator, and is empirically known to improve sample quality in many GAN architectures. However, there is currently little understanding of why SN is so effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization technique, proposed over two decades ago to control gradients in the training of deep neural networks. This connection helps to explain why the most popular implementation of SN for GANs requires no hyperparameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, we show that SN tends to preserve this property throughout training. Finally, building on this theoretical understanding, we propose Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization. Theoretically, we show that BSN gives better gradient control than SN. Empirically, we demonstrate that BSN outperforms SN in sample quality on several benchmark datasets, while also exhibiting better training stability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 35  (*Influential: 3*)\n\n#### 3. A Temporal Kernel Approach for Deep Learning with Continuous-time Information\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, Kannan Achan*\n\n**TL;DR:** This paper provides a principled way to characterize continuous-time systems using deep learning tools by composing neural networks with a temporal kernel, and introduces the random feature approach and converts the kernel learning problem to spectral density estimation under reparameterization.\n\n**Abstract:** Sequential deep learning models such as RNN, causal CNN and attention mechanism do not readily consume continuous-time information. Discretizing the temporal data, as we show, causes inconsistency even for simple continuous-time processes. Current approaches often handle time in a heuristic manner to be consistent with the existing deep learning architectures and implementations. In this paper, we provide a principled way to characterize continuous-time systems using deep learning tools. Notably, the proposed approach applies to all the major deep learning architectures and requires little modifications to the implementation. The critical insight is to represent the continuous-time system by composing neural networks with a temporal kernel, where we gain our intuition from the recent advancements in understanding deep learning with Gaussian process and neural tangent kernel. To represent the temporal kernel, we introduce the random feature approach and convert the kernel learning problem to spectral density estimation under reparameterization. We further prove the convergence and consistency results even when the temporal kernel is non-stationary, and the spectral density is misspecified. The simulations and real-data experiments demonstrate the empirical effectiveness of our temporal kernel approach in a broad range of settings.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 4. Generalizable Adversarial Training via Spectral Normalization\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Farzan Farnia, Jesse M. Zhang, David Tse*\n\n**TL;DR:** This work extends the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN's weight matrices.\n\n**Abstract:** Deep neural networks (DNNs) have set benchmarks on a wide array of supervised learning tasks. Trained DNNs, however, often lack robustness to minor adversarial perturbations to the input, which undermines their true practicality. Recent works have increased the robustness of DNNs by fitting networks using adversarially-perturbed training samples, but the improved performance can still be far below the performance seen in non-adversarial settings. A significant portion of this gap can be attributed to the decrease in generalization performance due to adversarial training. In this work, we extend the notion of margin loss to adversarial settings and bound the generalization error for DNNs trained under several well-known gradient-based attack schemes, motivating an effective regularization scheme based on spectral normalization of the DNN's weight matrices. We also provide a computationally-efficient method for normalizing the spectral norm of convolutional layers with arbitrary stride and padding schemes in deep convolutional networks. We evaluate the power of spectral normalization extensively on combinations of datasets, network architectures, and adversarial training schemes. The code is available at this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 130  (*Influential: 15*)\n\n#### 5. Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, Liping Jing*\n\n**TL;DR:** This work focuses on the most popular Batch Normalization and provides an in-depth theoretical analysis of its sub-optimality in continual learning, and proposes Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages.\n\n**Abstract:** Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n### 2 related papers from Papers with Code\n\n#### 1. Hyperspectral Image Super-resolution via Deep Progressive Zero-centric Residual Learning\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Jie Chen, Huanqiang Zeng, Zhiyu Zhu, Junhui Hou, Jiantao Zhou*\n\n**Abstract:** This paper explores the problem of hyperspectral image (HSI) super-resolution that merges a low resolution HSI (LR-HSI) and a high resolution multispectral image (HR-MSI). The cross-modality distribution of the spatial and spectral information makes the problem challenging. Inspired by the classic wavelet decomposition-based image fusion, we propose a novel \\textit{lightweight} deep neural network-based framework, namely progressive zero-centric residual network (PZRes-Net), to address this problem efficiently and effectively. Specifically, PZRes-Net learns a high resolution and \\textit{zero-centric} residual image, which contains high-frequency spatial details of the scene across all spectral bands, from both inputs in a progressive fashion along the spectral dimension. And the resulting residual image is then superimposed onto the up-sampled LR-HSI in a \\textit{mean-value invariant} manner, leading to a coarse HR-HSI, which is further refined by exploring the coherence across all spectral bands simultaneously. To learn the residual image efficiently and effectively, we employ spectral-spatial separable convolution with dense connections. In addition, we propose zero-mean normalization implemented on the feature maps of each layer to realize the zero-mean characteristic of the residual image. Extensive experiments over both real and synthetic benchmark datasets demonstrate that our PZRes-Net outperforms state-of-the-art methods to a \\textit{significant} extent in terms of both 4 quantitative metrics and visual quality, e.g., our PZRes-Net improves the PSNR more than 3dB, while saving 2.3$\\times$ parameters and consuming 15$\\times$ less FLOPs. The code is publicly available at https://github.com/zbzhzhy/PZRes-Net .\n\n**Published:** 2020-06-18\n\n\n\n#### 2. Efficient Training of Probabilistic Neural Networks for Survival Analysis\n\n*From Search Query: spectral normalization implementation deep learning*\n\n*Christian Fischer Pedersen, Martin Magris, Christian Marius Lillelund*\n\n**Abstract:** Variational Inference (VI) is a commonly used technique for approximate Bayesian inference and uncertainty estimation in deep learning models, yet it comes at a computational cost, as it doubles the number of trainable parameters to represent uncertainty. This rapidly becomes challenging in high-dimensional settings and motivates the use of alternative techniques for inference, such as Monte Carlo Dropout (MCD) or Spectral-normalized Neural Gaussian Process (SNGP). However, such methods have seen little adoption in survival analysis, and VI remains the prevalent approach for training probabilistic neural networks. In this paper, we investigate how to train deep probabilistic survival models in large datasets without introducing additional overhead in model complexity. To achieve this, we adopt three probabilistic approaches, namely VI, MCD, and SNGP, and evaluate them in terms of their prediction performance, calibration performance, and model complexity. In the context of probabilistic survival analysis, we investigate whether non-VI techniques can offer comparable or possibly improved prediction performance and uncertainty calibration compared to VI. In the MIMIC-IV dataset, we find that MCD aligns with VI in terms of the concordance index (0.748 vs. 0.743) and mean absolute error (254.9 vs. 254.7) using hinge loss, while providing C-calibrated uncertainty estimates. Moreover, our SNGP implementation provides D-calibrated survival functions in all datasets compared to VI (4/4 vs. 2/4, respectively). Our work encourages the use of techniques alternative to VI for survival analysis in high-dimensional datasets, where computational efficiency and overhead are of concern.\n\n**Published:** 2024-04-09\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral methods, here are some key findings and references that address the specified details:\n\n## Practical Implementations of Spectral Normalization\n\n- **Spectral normalization** is a technique that stabilizes the training of neural networks by controlling the Lipschitz constant of the weight matrices. This method is particularly useful in preventing overfitting and ensuring consistent performance throughout training. In the context of deep learning, spectral normalization can be applied to each layer's weight matrix to maintain a consistent spectral norm, which helps in preventing issues like mode collapse and unstable training.\n\n- The paper \"Learning Continually by Spectral Regularization\" discusses a related concept, spectral regularization, which controls the deviation of the singular values of each layer. This approach is shown to be more performant and less sensitive to hyperparameters than other regularizers, and it can improve generalization across various architectures and datasets. While this is not exactly spectral normalization, it shares similar principles and could be adapted for autoregressive models.\n\n## Efficient Computation Strategies for Spectral Operations\n\n- Efficient computation of spectral operations is crucial for practical implementation. However, the current sources do not delve deeply into specialized computational strategies for spectral normalization. Generally, spectral normalization involves calculating the spectral norm (the maximum singular value) of the weight matrices, which can be computationally intensive. Optimizations might include using approximations or iterative methods to compute the singular values efficiently, but these are not explicitly covered in the provided sources.\n\n## Stability Analysis Methods for Deep Neural Networks\n\n- Stability in deep neural networks, particularly in transformer-based models, is a critical issue. Spectral normalization can help in preventing attention entropy collapse and over-smoothing. For instance, the analysis note mentions that spectral normalization can prevent attention entropy collapse, which is a common stability issue in transformer training[Analysis Note].\n\n- The paper \"Self-attention Networks Localize When QK-eigenspectrum Concentrates\" provides insights into how the eigenspectrum of the attention matrices affects the stability and localization of self-attention mechanisms. This can be relevant for designing stable autoregressive models.\n\n## Memory-Efficient Implementations of Hierarchical Processing\n\n- Memory hierarchy optimization is crucial for efficient hierarchical processing. While the provided sources do not specifically address memory-efficient implementations of hierarchical processing, the analysis note highlights the importance of memory hierarchy optimization for performance. This suggests that researchers should focus on optimizing the memory usage of their models, possibly by using techniques like model pruning, quantization, or efficient data loading strategies[Analysis Note].\n\n## Integration of Spectral Methods with Normalization Layers\n\n- Integrating spectral methods with normalization layers can enhance stability and performance. The analysis note suggests integrating spectral filtering with hierarchical RMS norm, which could be a novel approach to stabilize autoregressive models. Layer normalization plays a key role in preventing over-smoothing, and combining it with spectral normalization could provide additional stability benefits[Analysis Note].\n\nIn summary, to improve autoregressive language model design:\n\n- **Implement spectral normalization** to control the Lipschitz constant of weight matrices, ensuring stable training and preventing overfitting.\n- **Optimize spectral operations** by exploring efficient computational methods, such as approximations or iterative methods, though this is not explicitly covered in the sources.\n- **Use stability analysis methods** like preventing attention entropy collapse and ensuring the eigenspectrum of attention matrices is well-behaved[Analysis Note].\n- **Optimize memory usage** through techniques like model pruning, quantization, or efficient data loading strategies to enable memory-efficient hierarchical processing[Analysis Note].\n- **Integrate spectral methods with normalization layers** to enhance stability, such as combining spectral filtering with hierarchical RMS norm and layer normalization[Analysis Note].\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nAfter three rounds of searches, we have gathered substantial information about spectral methods, hierarchical normalization, and stability considerations. Key findings include:\n\na) Stability Considerations:\n- Attention entropy collapse is a critical issue in transformer training\n- Layer normalization plays a key role in preventing over-smoothing\n- Spectral normalization can provide stability benefits\n\nb) Implementation Challenges:\n- FFT operations have poor hardware utilization\n- Memory hierarchy optimization is crucial\n- Trade-offs between computational efficiency and stability\n\n2. Useful References with Excerpts:\n\nFrom \"Stabilizing Transformer Training by Preventing Attention Entropy Collapse\" (Zhai et al., 2023):\n\"We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability\" - Critical for understanding stability issues\n\nFrom \"Revisiting Over-smoothing in BERT\" (Shi et al., 2022):\n\"layer normalization plays a key role in the over-smoothing issue of Transformer-based models\" - Important for normalization design\n\nFrom \"Self-attention Networks Localize When QK-eigenspectrum Concentrates\" (Bao et al., 2024):\n\"small eigenspectrum variance leads attention to be localized\" - Relevant for spectral stability\n\n3. Key Insights:\n\na) Stability Mechanisms:\n- Spectral normalization can prevent attention entropy collapse\n- Hierarchical processing needs careful normalization design\n- Layer normalization placement affects model stability\n\nb) Implementation Efficiency:\n- Memory hierarchy optimization is crucial for performance\n- FFT operations need specialized implementation\n- Trade-off between stability and computational efficiency\n\nc) Novel Elements in Proposal:\n- Integration of spectral filtering with hierarchical RMS norm\n- Adaptive compression based on input complexity\n- Multi-scale processing with stability guarantees\n\n4. Future Search Plan:\n\nNeed final round of search focusing on:\n1. Practical implementations of spectral normalization\n2. Memory-efficient hierarchical processing\n3. Stability analysis methods for deep networks\n\n5. References:\n\n[1] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., & Susskind, J. (2023). Stabilizing Transformer Training by Preventing Attention Entropy Collapse. ICML.\n\n[2] Shi, H., Gao, J., Xu, H., Liang, X., Li, Z., Kong, L., Lee, S. M. S., & Kwok, J. (2022). Revisiting Over-smoothing in BERT from the Perspective of Graph. ICLR.\n\n[3] Bao, H., Hataya, R., & Karakida, R. (2024). Self-attention Networks Localize When QK-eigenspectrum Concentrates. arXiv.\n\n[4] Chen, W., Grangier, D., & Auli, M. (2015). Strategies for Training Large Vocabulary Neural Language Models. ACL.\n\n[5] Riley, P., et al. (2024). Finding Replicable Human Evaluations via Stable Ranking Probability. NAACL."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "spectral filtering normalization neural networks",
            "detail": "Find papers discussing:\n1. Integration of spectral filtering in normalization layers\n2. Techniques for adaptive or hierarchical normalization\n3. Methods for efficient FFT operations in neural networks\n4. Approaches to dynamic compression in language models\n5. Stability analysis of spectral methods in deep learning",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of spectral filtering in normalization layers\n2. Techniques for adaptive or hierarchical normalization\n3. Methods for efficient FFT operations in neural networks\n4. Approaches to dynamic compression in language models\n5. Stability analysis of spectral methods in deep learning\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.03)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.03)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 2. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.03)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.03)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n#### 3. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.02)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.02)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n#### 4. Normalized Attention Without Probability Cage (Avg. Score: 0.02)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.02)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 5. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.01)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.01)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: spectral filtering normalization neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*M. Defferrard, X. Bresson, P. Vandergheynst*\n\n**TL;DR:** This work presents a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs.\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks (CNNs) from low-dimensional regular grids, where image, video and speech are represented, to high-dimensional irregular domains, such as social networks, brain connectomes or words' embedding, represented by graphs. We present a formulation of CNNs in the context of spectral graph theory, which provides the necessary mathematical background and efficient numerical schemes to design fast localized convolutional filters on graphs. Importantly, the proposed technique offers the same linear computational complexity and constant learning complexity as classical CNNs, while being universal to any graph structure. Experiments on MNIST and 20NEWS demonstrate the ability of this novel deep learning system to learn local, stationary, and compositional features on graphs.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 7134  (*Influential: 768*)\n\n#### 2. Specformer: Spectral Graph Neural Networks Meet Transformers\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*Deyu Bo, Chuan Shi, Lele Wang, Renjie Liao*\n\n**TL;DR:** This work introduces Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter and design a decoder with learnable bases to enable non-local graph convolution.\n\n**Abstract:** Spectral graph neural networks (GNNs) learn graph representations via spectral-domain graph convolutions. However, most existing spectral graph filters are scalar-to-scalar functions, i.e., mapping a single eigenvalue to a single filtered value, thus ignoring the global pattern of the spectrum. Furthermore, these filters are often constructed based on some fixed-order polynomials, which have limited expressiveness and flexibility. To tackle these issues, we introduce Specformer, which effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter. We also design a decoder with learnable bases to enable non-local graph convolution. Importantly, Specformer is equivariant to permutation. By stacking multiple Specformer layers, one can build a powerful spectral GNN. On synthetic datasets, we show that our Specformer can better recover ground-truth spectral filters than other spectral GNNs. Extensive experiments of both node-level and graph-level tasks on real-world graph datasets show that our Specformer outperforms state-of-the-art GNNs and learns meaningful spectrum patterns. Code and data are available at https://github.com/bdy9527/Specformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 57  (*Influential: 10*)\n\n#### 3. How Powerful are Spectral Graph Neural Networks\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*Xiyuan Wang, Muhan Zhang*\n\n**TL;DR:** It is proved that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality, and JacobiConv is proposed, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions.\n\n**Abstract:** Spectral Graph Neural Network is a kind of Graph Neural Network (GNN) based on graph signal filters. Some models able to learn arbitrary spectral filters have emerged recently. However, few works analyze the expressive power of spectral GNNs. This paper studies spectral GNNs' expressive power theoretically. We first prove that even spectral GNNs without nonlinearity can produce arbitrary graph signals and give two conditions for reaching universality. They are: 1) no multiple eigenvalues of graph Laplacian, and 2) no missing frequency components in node features. We also establish a connection between the expressive power of spectral GNNs and Graph Isomorphism (GI) testing, the latter of which is often used to characterize spatial GNNs' expressive power. Moreover, we study the difference in empirical performance among different spectral GNNs with the same expressive power from an optimization perspective, and motivate the use of an orthogonal basis whose weight function corresponds to the graph signal density in the spectrum. Inspired by the analysis, we propose JacobiConv, which uses Jacobi basis due to its orthogonality and flexibility to adapt to a wide range of weight functions. JacobiConv deserts nonlinearity while outperforming all baselines on both synthetic and real-world datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 145  (*Influential: 29*)\n\n#### 4. Graph Neural Networks with Learnable and Optimal Polynomial Bases\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*Y. Guo, Zhewei Wei*\n\n**TL;DR:** This paper proposes two spectral GNN models, inspired by Favard's Theorem, which learns a polynomial basis from the space of all possible orthonormal bases and proposes a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal.\n\n**Abstract:** Polynomial filters, a kind of Graph Neural Networks, typically use a predetermined polynomial basis and learn the coefficients from the training data. It has been observed that the effectiveness of the model is highly dependent on the property of the polynomial basis. Consequently, two natural and fundamental questions arise: Can we learn a suitable polynomial basis from the training data? Can we determine the optimal polynomial basis for a given graph and node features? In this paper, we propose two spectral GNN models that provide positive answers to the questions posed above. First, inspired by Favard's Theorem, we propose the FavardGNN model, which learns a polynomial basis from the space of all possible orthonormal bases. Second, we examine the supposedly unsolvable definition of optimal polynomial basis from Wang&Zhang (2022) and propose a simple model, OptBasisGNN, which computes the optimal basis for a given graph structure and graph signal. Extensive experiments are conducted to demonstrate the effectiveness of our proposed models. Our code is available at https://github.com/yuziGuo/FarOptBasis.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 5*)\n\n#### 5. Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*M. Balcilar, G. Renton, P. H\u00e9roux, Benoit Ga\u00fcz\u00e8re, S\u00e9bastien Adam, P. Honeine*\n\n**TL;DR:** It is argued that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of Gnns, and theoretically demonstrate some equivalence of the graph convolution process regardless of whether it is designed in the spatial or the spectral domain.\n\n**Abstract:** In the recent literature of Graph Neural Networks (GNN), the expressive power of models has been studied through their capability to distinguish if two given graphs are isomorphic or not. Since the graph isomorphism problem is NP-intermediate, and Weisfeiler-Lehman (WL) test can give sufficient but not enough evidence in polynomial time, the theoretical power of GNNs is usually evaluated by the equivalence of WL-test order, followed by an empirical analysis of the models on some reference inductive and transductive datasets. However, such analysis does not account the signal processing pipeline, whose capability is generally evaluated in the spectral domain. In this paper, we argue that a spectral analysis of GNNs behavior can provide a complementary point of view to go one step further in the understanding of GNNs. By bridging the gap between the spectral and spatial design of graph convolutions, we theoretically demonstrate some equivalence of the graph convolution process regardless it is designed in the spatial or the spectral domain. Using this connection, we managed to re-formulate most of the state-of-the-art graph neural networks into one common framework. This general framework allows to lead a spectral analysis of the most popular GNNs, explaining their performance and showing their limits according to spectral point of view. Our theoretical spectral analysis is confirmed by experiments on various graph databases. Furthermore, we demonstrate the necessity of high and/or band-pass filters on a graph dataset, while the majority of GNN is limited to only low-pass and inevitably it fails. Code available at https://github.com/balcilar/gnn-spectral-expressive-power.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 145  (*Influential: 19*)\n\n### 3 related papers from Papers with Code\n\n#### 1. iSEGAN: Improved Speech Enhancement Generative Adversarial Networks\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*Deepak Baby*\n\n**Abstract:** Popular neural network-based speech enhancement systems operate on the magnitude spectrogram and ignore the phase mismatch between the noisy and clean speech signals. Conditional generative adversarial networks (cGANs) show promise in addressing the phase mismatch problem by directly mapping the raw noisy speech waveform to the underlying clean speech signal. However, stabilizing and training cGAN systems is difficult and they still fall short of the performance achieved by the spectral enhancement approaches. This paper investigates whether different normalization strategies and one-sided label smoothing can further stabilize the cGAN-based speech enhancement model. In addition, we propose incorporating a Gammatone-based auditory filtering layer and a trainable pre-emphasis layer to further improve the performance of the cGAN framework. Simulation results show that the proposed approaches improve the speech enhancement performance of cGAN systems in addition to yielding improved stability and reduced computational effort.\n\n**Published:** 2020-02-20\n\n\n\n#### 2. Operational Neural Networks for Parameter-Efficient Hyperspectral Single-Image Super-Resolution\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*Nour Aburaed, Mehmet Yamac, Serkan Kiranyaz, Moncef Gabbouj, Stephen Marshall, Paul Murray, Alexander Ulrichsen*\n\n**Abstract:** Hyperspectral Imaging is a crucial tool in remote sensing which captures far more spectral information than standard color images. However, the increase in spectral information comes at the cost of spatial resolution. Super-resolution is a popular technique where the goal is to generate a high-resolution version of a given low-resolution input. The majority of modern super-resolution approaches use convolutional neural networks. However, convolution itself is a linear operation and the networks rely on the non-linear activation functions after each layer to provide the necessary non-linearity to learn the complex underlying function. This means that convolutional neural networks tend to be very deep to achieve the desired results. Recently, self-organized operational neural networks have been proposed that aim to overcome this limitation by replacing the convolutional filters with learnable non-linear functions through the use of MacLaurin series expansions. This work focuses on extending the convolutional filters of a popular super-resolution model to more powerful operational filters to enhance the model performance on hyperspectral images. We also investigate the effects that residual connections and different normalization types have on this type of enhanced network. Despite having fewer parameters than their convolutional network equivalents, our results show that operational neural networks achieve superior super-resolution performance on small hyperspectral image datasets. Our code is made available on Github: https://github.com/aulrichsen/SRONN.\n\n**Published:** 2023-03-29\n\n\n\n#### 3. BernNet: Learning Arbitrary Graph Spectral Filters via Bernstein Approximation\n\n*From Search Query: spectral filtering normalization neural networks*\n\n*Hongteng Xu, Zengfeng Huang, Zhewei Wei, Mingguo He*\n\n**Abstract:** Many representative graph neural networks, e.g., GPR-GNN and ChebNet, approximate graph convolutions with graph spectral filters. However, existing work either applies predefined filter weights or learns them without necessary constraints, which may lead to oversimplified or ill-posed filters. To overcome these issues, we propose BernNet, a novel graph neural network with theoretical support that provides a simple but effective scheme for designing and learning arbitrary graph spectral filters. In particular, for any filter over the normalized Laplacian spectrum of a graph, our BernNet estimates it by an order-$K$ Bernstein polynomial approximation and designs its spectral property by setting the coefficients of the Bernstein basis. Moreover, we can learn the coefficients (and the corresponding filter weights) based on observed graphs and their associated signals and thus achieve the BernNet specialized for the data. Our experiments demonstrate that BernNet can learn arbitrary spectral filters, including complicated band-rejection and comb filters, and it achieves superior performance in real-world graph modeling tasks. Code is available at https://github.com/ivam-he/BernNet.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model incorporating spectral filtering, hierarchical normalization, and other advanced techniques, here are some key findings and references that align with your detailed requirements:\n\n## Integration of Spectral Filtering in Normalization Layers\n\n- The concept of spectral filtering in neural networks is not directly addressed in the provided sources, but spectral methods are discussed in the context of other domains. However, the idea of using spectral techniques in normalization layers can be inferred from the use of spectral methods in other areas. For instance, spectral regularization is discussed in the context of maintaining trainability and preventing exploding gradients in neural networks.\n\n## Techniques for Adaptive or Hierarchical Normalization\n\n- Hierarchical normalization is a concept that can be related to the use of multi-scale processing and adaptive weights. While the sources do not explicitly discuss hierarchical normalization in language models, the idea of adaptive processing is seen in the context of graph neural networks, where self-interaction and neighbor-interaction terms are separated to improve accuracy and generalization.\n- Adaptive normalization techniques can be inferred from the use of spectral regularization, which maintains the beneficial initialization properties throughout training, ensuring gradient diversity and trainability.\n\n## Methods for Efficient FFT Operations in Neural Networks\n\n- Efficient FFT operations are crucial for spectral methods. Although the provided sources do not specifically address FFT operations in neural networks, the computational efficiency of spectral methods is a general concern. For example, the use of spectral regularization involves controlling the spectral norm of each layer, which could imply efficient handling of spectral operations.\n\n## Approaches to Dynamic Compression in Language Models\n\n- Dynamic compression based on input complexity is an area of interest. While the sources do not directly discuss dynamic compression in language models, the concept of adaptive processing and efficiency can be related. For instance, the use of graph neural networks to predict complex network dynamics involves handling irregularly sampled and noisy data, which might provide insights into adaptive compression techniques.\n\n## Stability Analysis of Spectral Methods in Deep Learning\n\n- Stability analysis of spectral methods is crucial. The paper on spectral regularization provides insights into how maintaining the spectral norm of each layer close to one ensures stability and promotes continual trainability. This approach minimally interferes with performance and is robust across different datasets and architectures.\n- The stability of spectral filtering in practice can be inferred from its application in other domains, such as audio classification, where mel-scaled spectrograms are used effectively without stability issues.\n\n### Additional Considerations\n\n- **Computational Overhead and Memory Efficiency**: The computational overhead of FFT operations and memory efficiency are critical. While not directly addressed, the use of spectral regularization and other spectral methods suggests that these operations can be managed efficiently to maintain trainability and performance.\n- **Adaptive Scaling Approaches**: Multi-scale processing with adaptive weights can be related to the hierarchical processing seen in graph neural networks. This approach separates self-interaction and neighbor-interaction terms, which could be analogous to adaptive scaling in language models.\n\n### References to Track\n\n- For spectral regularization and its impact on trainability and stability:\n- For insights into adaptive processing and efficiency in handling complex data:\n- For understanding the effectiveness of spectral methods in other domains:\n\nBy exploring these areas and references, the researchers can gain a deeper understanding of how to integrate spectral filtering, hierarchical normalization, and dynamic compression into autoregressive language models, ensuring improved efficiency, stability, and performance.\n",
            "analysis": "Initial Analysis of SpectralHierRMSNorm Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Spectral Filtering Integration:\n- The proposal combines spectral filtering with hierarchical RMS normalization\n- Need to investigate existing spectral methods in normalization layers\n- Research needed on stability and efficiency of FFT operations in normalization\n\nb) Adaptive State Compression:\n- Dynamic compression based on input complexity\n- Need to examine existing approaches to adaptive compression in LMs\n- Research impact on model quality vs. memory efficiency\n\nc) Scale-Adaptive Normalization:\n- Multi-scale processing with adaptive weights\n- Need to study existing hierarchical normalization techniques\n- Research efficiency of parallel scale processing\n\n2. Critical Questions:\n\na) Novelty Assessment:\n- How does this differ from existing spectral approaches?\n- What makes the combination of spectral filtering and hierarchical RMS norm unique?\n- Are there similar adaptive compression techniques in current research?\n\nb) Technical Feasibility:\n- What is the computational overhead of FFT operations?\n- How stable is the spectral filtering in practice?\n- What are the memory-computation tradeoffs?\n\nc) Performance Impact:\n- How does spectral filtering affect gradient flow?\n- What is the impact on model convergence?\n- How does adaptive compression affect model quality?\n\n3. Investigation Plan:\n\nRound 1:\n- Search for existing spectral filtering approaches in normalization\n- Examine state-of-the-art in hierarchical normalization\n- Review papers on adaptive compression in LMs\n\nRound 2:\n- Deep dive into spectral methods stability\n- Research memory-efficient normalization techniques\n- Study adaptive scaling approaches\n\nRound 3:\n- Investigate implementation aspects\n- Research hardware optimization techniques\n- Examine parallel processing strategies\n\n4. Initial Findings:\n\nThe proposal appears to introduce several novel elements:\n- Integration of spectral filtering with hierarchical RMS norm\n- Dynamic state compression based on input complexity\n- Adaptive multi-scale processing\n\nHowever, some components have precedent in existing research:\n- Spectral filtering in neural networks\n- Hierarchical normalization approaches\n- Adaptive compression techniques\n\n5. Comparison with Previous Designs:\n\nLooking at the sibling designs:\n- spectralhierrmsnorm: Similar core idea but different approach to spectral filtering\n- an_improved_rotarypositionalembe: Different focus on positional embeddings\n- adaptivemamba2: Different architecture focusing on state space models\n\nThe current proposal appears to be distinct in:\n- Its specific combination of spectral filtering and hierarchical normalization\n- The approach to adaptive compression\n- The implementation of multi-scale processing\n\n6. References to Track:\n\nFrom the proposal:\n[1] Agarwal, N., et al. (2023). Spectral State Space Models\n[2] Yu, A., et al. (2023). Robustifying State-space Models\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling\n[4] Li, W., et al. (2024). Coupled Mamba\n\nNeed to investigate these and related works in detail.\n\n7. Initial Technical Assessment:\n\nStrengths:\n+ Novel combination of techniques\n+ Potential for improved efficiency\n+ Theoretically sound approach\n\nConcerns:\n- Computational overhead of FFT operations\n- Implementation complexity\n- Stability considerations\n\n8. Next Steps:\n\nProceed with detailed searches to:\n1. Validate novelty claims\n2. Assess technical feasibility\n3. Evaluate potential performance impact\n4. Identify similar approaches in literature"
        },
        {
            "ready": false,
            "query": "FFT optimization deep learning memory efficiency",
            "detail": "Find papers discussing:\n1. FFT optimization techniques in deep learning\n2. Memory-efficient implementations of spectral operations\n3. Applications of spectral methods in language models\n4. Hierarchical normalization in transformer architectures\n5. Stability analysis of FFT operations in neural networks",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. FFT optimization techniques in deep learning\n2. Memory-efficient implementations of spectral operations\n3. Applications of spectral methods in language models\n4. Hierarchical normalization in transformer architectures\n5. Stability analysis of FFT operations in neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.76)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 8/54 (Score: 0.76)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Keivan Alizadeh, Ali Farhadi, and Mohammad Rastegari. Butterfly transform: An efficient FFT based neural architecture design. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2020 . [2] George B Arfken and Hans J Weber. Mathematical methods for physicists. Elsevier Academic Press, 2005 . [3] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In The International Conference on Machine Learning (ICML), pages 1120-1128, 2016. [4] Anthony Bagnall, Hoang Anh Dau, Jason Lines, Michael Flynn, James Large, Aaron Bostrom, Paul Southam, and Eamonn Keogh. The UEA multivariate time series classification archive, 2018. arXiv preprint arXiv:1811.00075, 2018. [5] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [6] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. Trellis networks for sequence modeling. In The International Conference on Learning Representations (ICLR), 2019. [7] Rapha\u00ebl Berthier, Francis Bach, and Pierre Gaillard. Accelerated gossip in networks of given dimension using Jacobi polynomial iterations. SIAM Journal on Mathematics of Data Science, 2(1):24-47, 2020. [8] John P Boyd. Chebyshev and Fourier spectral methods.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.53)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.80)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 11/46 (Score: 0.26)*\n\n```\narXiv preprint arXiv:2108.07258, 2021. [12] E Oran Brigham. The fast Fourier transform and its applications. Prentice-Hall, Inc., 1988. [13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [14] Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R\u00e9. Pixelated butterfly: Simple and efficient sparse training for neural network models.\n```\n\n#### 3. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.33)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.33)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n#### 4. Fnet: Mixing tokens with fourier transforms (Avg. Score: 0.20)\n\n*J. Lee-Thorp, J. Ainslie, Ilya Eckstein, Santiago Onta\u00f1\u00f3n*\n\n**Published in:** North American Chapter of the Association for Computational Linguistics (2021)\t**Cited by** 400  (*Influential: 56*)\n\n**TL;DR:** The FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs).\n\n**Abstract:** We show that Transformer encoder architectures can be sped up, with limited accuracy costs, by replacing the self-attention sublayers with simple linear transformations that \u201cmix\u201d input tokens. Most surprisingly, we find that replacing the self-attention sublayer in a Transformer encoder with a standard, unparameterized Fourier Transform achieves 92-97% of the accuracy of BERT counterparts on the GLUE benchmark, but trains 80% faster on GPUs and 70% faster on TPUs at standard 512 input lengths. At longer input lengths, our FNet model is significantly faster: when compared to the \u201cefficient Transformers\u201d on the Long Range Arena benchmark, FNet matches the accuracy of the most accurate models, while outpacing the fastest models across all sequence lengths on GPUs (and across relatively shorter lengths on TPUs). Finally, FNet has a light memory footprint and is particularly efficient at smaller model sizes; for a fixed speed and accuracy budget, small FNet models outperform Transformer counterparts.\n\n##### *Relevant Chunk: No. 17/38 (Score: 0.20)*\n\n```\nRen\u00e9e Koplon and Eduardo D Sontag. 1997. Using fourier-neural recurrent networks to fit sequential input/output data. Neurocomputing, 15(3-4):225-248. Taku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66-71, Brussels, Belgium. Association for Computational Linguistics. Henry O. Kunz. 1979. On the equivalence between one-dimensional discrete walsh-hadamard and multidimensional discrete fourier transforms. IEEE Computer Architecture Letters, 28(03):267-268. Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. Gshard: Scaling giant models with conditional computation and automatic sharding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. 2021. Fourier neural operator for parametric partial differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. Sheng Lin, Ning Liu, Mahdi Nazemi, Hongjia Li, Caiwen Ding, Yanzhi Wang, and Massoud Pedram. 2018. Fft-based deep learning deployment in embedded systems. In 2018 Design, Automation \\& Test in Europe Conference \\& Exhibition (DATE), pages $1045-1050$.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: FFT optimization deep learning memory efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. PriorBand: Practical Hyperparameter Optimization in the Age of Deep Learning\n\n*From Search Query: FFT optimization deep learning memory efficiency*\n\n*Neeratyoy Mallik, Eddie Bergman, Carl Hvarfner, Daniel Stoll, Maciej Janowski, M. Lindauer, Luigi Nardi, F. Hutter*\n\n**TL;DR:** This work proposes PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks, and demonstrates its efficiency across a range of DL benchmarks and shows its gains under informative expert input and robustness against poor expert beliefs.\n\n**Abstract:** Hyperparameters of Deep Learning (DL) pipelines are crucial for their downstream performance. While a large number of methods for Hyperparameter Optimization (HPO) have been developed, their incurred costs are often untenable for modern DL. Consequently, manual experimentation is still the most prevalent approach to optimize hyperparameters, relying on the researcher's intuition, domain knowledge, and cheap preliminary explorations. To resolve this misalignment between HPO algorithms and DL researchers, we propose PriorBand, an HPO algorithm tailored to DL, able to utilize both expert beliefs and cheap proxy tasks. Empirically, we demonstrate PriorBand's efficiency across a range of DL benchmarks and show its gains under informative expert input and robustness against poor expert beliefs\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 2. Exploring the Limits of Differentially Private Deep Learning with Group-wise Clipping\n\n*From Search Query: FFT optimization deep learning memory efficiency*\n\n*Jiyan He, Xuechen Li, Da Yu, Huishuai Zhang, Janardhan Kulkarni, Y. Lee, A. Backurs, Nenghai Yu, J. Bian*\n\n**TL;DR:** It is shown that per-layer clipping, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization, which results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest.\n\n**Abstract:** Differentially private deep learning has recently witnessed advances in computational efficiency and privacy-utility trade-off. We explore whether further improvements along the two axes are possible and provide affirmative answers leveraging two instantiations of \\emph{group-wise clipping}. To reduce the compute time overhead of private learning, we show that \\emph{per-layer clipping}, where the gradient of each neural network layer is clipped separately, allows clipping to be performed in conjunction with backpropagation in differentially private optimization. This results in private learning that is as memory-efficient and almost as fast per training update as non-private learning for many workflows of interest. While per-layer clipping with constant thresholds tends to underperform standard flat clipping, per-layer clipping with adaptive thresholds matches or outperforms flat clipping under given training epoch constraints, hence attaining similar or better task performance within less wall time. To explore the limits of scaling (pretrained) models in differentially private deep learning, we privately fine-tune the 175 billion-parameter GPT-3. We bypass scaling challenges associated with clipping gradients that are distributed across multiple devices with \\emph{per-device clipping} that clips the gradient of each model piece separately on its host device. Privately fine-tuning GPT-3 with per-device clipping achieves a task performance at $\\epsilon=1$ better than what is attainable by non-privately fine-tuning the largest GPT-2 on a summarization task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 38  (*Influential: 5*)\n\n#### 3. Symmetric Replay Training: Enhancing Sample Efficiency in Deep Reinforcement Learning for Combinatorial Optimization\n\n*From Search Query: FFT optimization deep learning memory efficiency*\n\n*Hyeon-Seob Kim, Minsu Kim, Sungsoo Ahn, Jinkyoo Park*\n\n**TL;DR:** This work proposes a simple but effective method, called symmetric replay training (SRT), which can be easily integrated into various DRL methods and leverages high-reward samples to encourage exploration of the under-explored symmetric regions without additional online interactions - free.\n\n**Abstract:** Deep reinforcement learning (DRL) has significantly advanced the field of combinatorial optimization (CO). However, its practicality is hindered by the necessity for a large number of reward evaluations, especially in scenarios involving computationally intensive function assessments. To enhance the sample efficiency, we propose a simple but effective method, called symmetric replay training (SRT), which can be easily integrated into various DRL methods. Our method leverages high-reward samples to encourage exploration of the under-explored symmetric regions without additional online interactions - free. Through replay training, the policy is trained to maximize the likelihood of the symmetric trajectories of discovered high-rewarded samples. Experimental results demonstrate the consistent improvement of our method in sample efficiency across diverse DRL methods applied to real-world tasks, such as molecular optimization and hardware design.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: FFT optimization deep learning memory efficiency*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**TL;DR:** The low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter, is introduced and achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 3*)\n\n#### 5. D4FT: A Deep Learning Approach to Kohn-Sham Density Functional Theory\n\n*From Search Query: FFT optimization deep learning memory efficiency*\n\n*Tianbo Li, Min Lin, Zheyuan Hu, Kunhao Zheng, G. Vignale, Kenji Kawaguchi, A. Neto, K. Novoselov, Shuicheng Yan*\n\n**TL;DR:** A deep learning approach to KS-DFT is proposed to directly minimize the total energy by reparameterizing the orthogonal constraint as a feed-forward computation, and it is proved that such an approach has the same expressivity as the SCF method, yet reduces the computational complexity from O(N^4) to O(3).\n\n**Abstract:** Kohn-Sham Density Functional Theory (KS-DFT) has been traditionally solved by the Self-Consistent Field (SCF) method. Behind the SCF loop is the physics intuition of solving a system of non-interactive single-electron wave functions under an effective potential. In this work, we propose a deep learning approach to KS-DFT. First, in contrast to the conventional SCF loop, we propose to directly minimize the total energy by reparameterizing the orthogonal constraint as a feed-forward computation. We prove that such an approach has the same expressivity as the SCF method, yet reduces the computational complexity from O(N^4) to O(N^3). Second, the numerical integration which involves a summation over the quadrature grids can be amortized to the optimization steps. At each step, stochastic gradient descent (SGD) is performed with a sampled minibatch of the grids. Extensive experiments are carried out to demonstrate the advantage of our approach in terms of efficiency and stability. In addition, we show that our approach enables us to explore more complex neural-based wave functions.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using the specified keywords and details, here are some key points and references that can be useful:\n\n## FFT Optimization Techniques in Deep Learning\n\n- While the provided sources do not directly discuss FFT optimization in deep learning, it is important to note that FFTs can be optimized using efficient algorithms and hardware implementations. For instance, the AMD Vitis tutorials on implementing FFT and DFT designs on AI Engines highlight how FFT algorithms can be optimized for compute-intensive tasks using very-long instruction word (VLIW) processors and single instruction multiple data (SIMD) vector units.\n- In the context of deep learning, FFT optimization can involve leveraging these efficient computational methods to speed up spectral operations, which can be crucial for models that incorporate spectral filtering or transformations.\n\n## Memory-Efficient Implementations of Spectral Operations\n\n- Memory efficiency in spectral operations can be achieved through specialized data structures and optimized algorithms. For example, using orthogonal bases, as mentioned in the analysis, can provide stability advantages and reduce computational overhead.\n- The use of hierarchical structures and adaptive processing can also help in balancing computational overhead with model expressiveness. This is evident in works like \"Graph Neural Networks with Learnable and Optimal Polynomial Bases\" which discusses learning a polynomial basis from the space of all possible orthonormal bases, which can be memory-efficient[Analysis Note].\n\n## Applications of Spectral Methods in Language Models\n\n- Spectral methods have been successfully applied in graph neural networks, and there is potential for their application in language models. For instance, the Specformer model effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, which could be adapted for language models[Analysis Note: Specformer (Bo et al., 2023)].\n- Integrating spectral filtering into language models, particularly in the normalization layers, could enhance the model's ability to capture long-range dependencies and improve overall performance.\n\n## Hierarchical Normalization in Transformer Architectures\n\n- Hierarchical normalization techniques can improve model efficiency and expressiveness. This involves using multi-scale processing with adaptive weights, which is an active research area. Dynamic adaptation based on input complexity shows promise in maintaining stability and improving performance[Analysis Note].\n- Implementing hierarchical normalization in transformer architectures could involve layer-wise normalization schemes that adapt to the complexity of the input data, ensuring better stability and computational efficiency.\n\n## Stability Analysis of FFT Operations in Neural Networks\n\n- Stability in spectral operations, including FFTs, can be maintained through proper normalization and the use of orthogonal bases. This is crucial for ensuring that the spectral filters do not introduce instability into the neural network[Analysis Note: Key Insights].\n- The stability analysis would involve examining how the spectral operations interact with other components of the neural network, such as activation functions and weight updates, to ensure that the overall system remains stable and converges properly.\n\n### Additional Considerations\n\n- **Efficient Implementation**: Leveraging hardware optimizations like those described in the AMD Vitis tutorials can significantly improve the efficiency of spectral operations in deep learning models.\n- **Adaptive Basis Selection**: Learning a polynomial basis from the space of all possible orthonormal bases, as discussed in \"Graph Neural Networks with Learnable and Optimal Polynomial Bases,\" can be adapted for language models to enhance their spectral processing capabilities[Analysis Note: Guo & Wei, 2023].\n\nBy integrating these concepts, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage the strengths of spectral methods and hierarchical normalization.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search reveals several important aspects relevant to the SpectralHierRMSNorm proposal:\n\na) Spectral Methods in Neural Networks:\n- Spectral filtering has been successfully applied in graph neural networks\n- There are established methods for efficient spectral operations\n- Stability and computational considerations are well-documented\n\nb) Hierarchical and Adaptive Processing:\n- Multi-scale processing with adaptive weights is an active research area\n- Hierarchical structures can improve model efficiency and expressiveness\n- Dynamic adaptation based on input complexity shows promise\n\n2. Useful References with Excerpts:\n\nFrom Specformer (Bo et al., 2023):\n\"effectively encodes the set of all eigenvalues and performs self-attention in the spectral domain, leading to a learnable set-to-set spectral filter\"\n- Relevant for spectral filtering implementation\n\nFrom \"How Powerful are Spectral Graph Neural Networks\" (Wang & Zhang, 2022):\n\"spectral GNNs without nonlinearity can produce arbitrary graph signals\"\n- Important for understanding spectral method capabilities\n\nFrom \"Graph Neural Networks with Learnable and Optimal Polynomial Bases\" (Guo & Wei, 2023):\n\"learns a polynomial basis from the space of all possible orthonormal bases\"\n- Relevant for adaptive basis selection\n\n3. Key Insights:\n\na) Spectral Processing:\n- Spectral methods can be computationally efficient when properly implemented\n- Orthogonal bases provide stability advantages\n- Dynamic adaptation of spectral filters is feasible\n\nb) Implementation Considerations:\n- Need to carefully balance computational overhead with model expressiveness\n- Memory efficiency requires specialized data structures\n- Stability can be maintained through proper normalization\n\nc) Research Gaps:\n- Limited work on spectral methods in language model normalization\n- Need more information on FFT optimization in deep learning\n- Integration with hierarchical structures needs further investigation\n\n4. Future Search Plan:\n\nRound 2 Search Needs:\n- Specific examples of FFT optimization in deep learning\n- Memory-efficient implementations of spectral operations\n- Hierarchical normalization techniques in language models\n\n5. References:\n\n[1] Bo, D., Shi, C., Wang, L., & Liao, R. (2023). Specformer: Spectral Graph Neural Networks Meet Transformers. International Conference on Learning Representations.\n\n[2] Wang, X., & Zhang, M. (2022). How Powerful are Spectral Graph Neural Networks. International Conference on Machine Learning.\n\n[3] Guo, Y., & Wei, Z. (2023). Graph Neural Networks with Learnable and Optimal Polynomial Bases. International Conference on Machine Learning.\n\n[4] Balcilar, M., et al. (2021). Analyzing the Expressive Power of Graph Neural Networks in a Spectral Perspective. International Conference on Learning Representations.\n\n[5] Defferrard, M., Bresson, X., & Vandergheynst, P. (2016). Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "hierarchical normalization language models stability",
            "detail": "Find papers discussing:\n1. Hierarchical normalization techniques in language models and transformers\n2. Stability analysis of spectral methods in deep neural networks\n3. Memory-efficient implementations of multi-scale processing\n4. Adaptive normalization techniques for varying sequence lengths\n5. Integration of spectral methods with layer normalization",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical normalization techniques in language models and transformers\n2. Stability analysis of spectral methods in deep neural networks\n3. Memory-efficient implementations of multi-scale processing\n4. Adaptive normalization techniques for varying sequence lengths\n5. Integration of spectral methods with layer normalization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.97)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.97)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Position Coupling: Leveraging Task Structure for Improved Length Generalization of Transformers (Avg. Score: 0.93)\n\n*Hanseul Cho, Jaeyoung Cha, Pranjal Awasthi, Srinadh Bhojanapalli, Anupam Gupta, Chulhee Yun*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is proved that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it.\n\n**Abstract:** Even for simple arithmetic tasks like integer addition, it is challenging for Transformers to generalize to longer sequences than those encountered during training. To tackle this problem, we propose position coupling, a simple yet effective method that directly embeds the structure of the tasks into the positional encoding of a (decoder-only) Transformer. Taking a departure from the vanilla absolute position mechanism assigning unique position IDs to each of the tokens, we assign the same position IDs to two or more\"relevant\"tokens; for integer addition tasks, we regard digits of the same significance as in the same position. On the empirical side, we show that with the proposed position coupling, a small (1-layer) Transformer trained on 1 to 30-digit additions can generalize up to 200-digit additions (6.67x of the trained length). On the theoretical side, we prove that a 1-layer Transformer with coupled positions can solve the addition task involving exponentially many digits, whereas any 1-layer Transformer without positional information cannot entirely solve it. We also demonstrate that position coupling can be applied to other algorithmic tasks such as addition with multiple summands, Nx2 multiplication, copy/reverse, and a two-dimensional task.\n\n##### *Relevant Chunk: No. 13/67 (Score: 0.93)*\n\n```\narXiv preprint arXiv:2310.00726, 2023. 5.1, 8\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. 3, E\n\nSrinadh Bhojanapalli, Chulhee Yun, Ankit Singh Rawat, Sashank Reddi, and Sanjiv Kumar. Low-rank bottleneck in multi-head attention models. In International conference on machine learning, pages 864-873. PMLR, 2020. 4.1\n\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1-113, 2023. 1\n\nYann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pages 933-941. PMLR, 2017. C\n\nGregoire Deletang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness, and Pedro A Ortega. Neural networks and the chomsky hierarchy. In The Eleventh International Conference on Learning Representations, 2023. URL https: //openreview.net/forum?id=WbxHAzkeQcn. 1\nShaoxiong Duan and Yining Shi. From interpolation to extrapolation: Complete length generalization for arithmetic transformers. arXiv preprint arXiv:2310.11984, 2023. 6.2\nDan Friedman, Alexander Wettig, and Danqi Chen. Learning transformer programs. Advances in Neural Information Processing Systems, 36, 2023. 11\n\nJonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N Dauphin. Convolutional sequence to sequence learning. In International conference on machine learning, pages 1243-1252. PMLR, 2017. 2.3\n\nGemini, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models.\n```\n\n#### 3. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections (Avg. Score: 0.89)\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 50  (*Influential: 4*)\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.89)*\n\n```\nGovernment. ## References\n\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [2] T. S. Chihara. An introduction to orthogonal polynomials. Dover Books on Mathematics. Dover Publications, 2011. ISBN 9780486479293. [3] Jared Quincy Davis, Albert Gu, Tri Dao, Krzysztof Choromanski, Christopher R\u00e9, Percy Liang, and Chelsea Finn. Catformer: Designing stable transformers via sensitivity analysis. In The International Conference on Machine Learning (ICML), 2021. [4] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pages 249-256. JMLR Workshop and Conference Proceedings, 2010. [5] Albert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Hippo: Recurrent memory with optimal polynomial projections. In Advances in Neural Information Processing Systems (NeurIPS), 2020. [6] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with the structured learnable linear state space layer. In Advances in Neural Information Processing Systems (NeurIPS), 2021. [7] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [8] Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. arXiv preprint arXiv:2206.11893, 2022. [9] Ankit Gupta. Diagonal state spaces are as effective as structured state spaces.\n```\n\n#### 4. Normalized Attention Without Probability Cage (Avg. Score: 0.84)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.84)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 5. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.71)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 14/27 (Score: 0.71)*\n\n```\nHinton. 2016. Layer normalization. CoRR, $\\mathrm{abs} / 1607.06450$. David Bamman and Noah A. Smith. 2013. New alignment methods for discriminative book summarization. CoRR, abs/1305.1319. Iz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. CoRR, abs/2004.05150. Sid Black, Stella Biderman, Eric Hallahan, et al. 2022. Gpt-neox-20b: An open-source autoregressive language model. CoRR, abs/2204.06745. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. volume 33, pages 1877-1901. Ilias Chalkidis, Emmanouil Fergadiotis, Prodromos Malakasiotis, and Ion Androutsopoulos. 2019. Largescale multi-label text classification on EU legislation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 63146322, Florence, Italy. Association for Computational Linguistics. Krzysztof Marcin Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Quincy Davis, Afroz Mohiuddin, Lukasz Kaiser, David Benjamin Belanger, Lucy J.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical normalization language models stability\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Strategies for Training Large Vocabulary Neural Language Models\n\n*From Search Query: hierarchical normalization language models stability*\n\n*Wenlin Chen, David Grangier, Michael Auli*\n\n**TL;DR:** A systematic comparison of strategies to represent and train large vocabularies, includingsoftmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization, and extends selfnormalization to be a proper estimator of likelihood and introduce an efficient variant of softmax.\n\n**Abstract:** Training neural network language models over large vocabularies is still computationally very costly compared to count-based models such as Kneser-Ney. At the same time, neural language models are gaining popularity for many applications such as speech recognition and machine translation whose success depends on scalability. We present a systematic comparison of strategies to represent and train large vocabularies, including softmax, hierarchical softmax, target sampling, noise contrastive estimation and self normalization. We further extend self normalization to be a proper estimator of likelihood and introduce an efficient variant of softmax. We evaluate each method on three popular benchmarks, examining performance on rare words, the speed/accuracy trade-off and complementarity to Kneser-Ney.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2015\n\n**Citations:** 137  (*Influential: 15*)\n\n#### 2. Finding Replicable Human Evaluations via Stable Ranking Probability\n\n*From Search Query: hierarchical normalization language models stability*\n\n*Parker Riley, Daniel Deutsch, George F. Foster, Viresh Ratnakar, Ali Dabirmoghaddam, Markus Freitag*\n\n**TL;DR:** This paper investigates the optimal configurations for item allocation to raters, number of ratings per item, and score normalization, and collects and releases the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two language pairs.\n\n**Abstract:** Reliable human evaluation is critical to the development of successful natural language generation models, but achieving it is notoriously difficult. Stability is a crucial requirement when ranking systems by quality: consistent ranking of systems across repeated evaluations is not just desirable, but essential. Without it, there is no reliable foundation for hill-climbing or product launch decisions. In this paper, we use machine translation and its state-of-the-art human evaluation framework, MQM, as a case study to understand how to set up reliable human evaluations that yield stable conclusions. We investigate the optimal configurations for item allocation to raters, number of ratings per item, and score normalization. Our study on two language pairs provides concrete recommendations for designing replicable human evaluation studies. We also collect and release the largest publicly available dataset of multi-segment translations rated by multiple professional translators, consisting of nearly 140,000 segment annotations across two language pairs.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. Revisiting Over-smoothing in BERT from the Perspective of Graph\n\n*From Search Query: hierarchical normalization language models stability*\n\n*Han Shi, Jiahui Gao, Hang Xu, Xiaodan Liang, Zhenguo Li, Lingpeng Kong, Stephen M. S. Lee, J. Kwok*\n\n**TL;DR:** This work makes the attempt to analyze the over-smoothing problem of Transformer-based models from the perspective of graph, where such problem was first discovered and explored and finds that layer normalization plays a key role.\n\n**Abstract:** Recently over-smoothing phenomenon of Transformer-based models is observed in both vision and language fields. However, no existing work has delved deeper to further investigate the main cause of this phenomenon. In this work, we make the attempt to analyze the over-smoothing problem from the perspective of graph, where such problem was first discovered and explored. Intuitively, the self-attention matrix can be seen as a normalized adjacent matrix of a corresponding graph. Based on the above connection, we provide some theoretical analysis and find that layer normalization plays a key role in the over-smoothing issue of Transformer-based models. Specifically, if the standard deviation of layer normalization is sufficiently large, the output of Transformer stacks will converge to a specific low-rank subspace and result in over-smoothing. To alleviate the over-smoothing problem, we consider hierarchical fusion strategies, which combine the representations from different layers adaptively to make the output more diverse. Extensive experiment results on various data sets illustrate the effect of our fusion method.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 57  (*Influential: 8*)\n\n#### 4. Stabilizing Transformer Training by Preventing Attention Entropy Collapse\n\n*From Search Query: hierarchical normalization language models stability*\n\n*Shuangfei Zhai, T. Likhomanenko, Etai Littwin, Dan Busbridge, Jason Ramapuram, Yizhe Zhang, Jiatao Gu, J. Susskind*\n\n**TL;DR:** This work investigates the training dynamics of Transformers by examining the evolution of the attention layers, and shows that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training a Vision Transformer without warmup, weight decay, layer normalization or adaptive optimizers.\n\n**Abstract:** Training stability is of great importance to Transformers. In this work, we investigate the training dynamics of Transformers by examining the evolution of the attention layers. In particular, we track the attention entropy for each attention head during the course of training, which is a proxy for model sharpness. We identify a common pattern across different architectures and tasks, where low attention entropy is accompanied by high training instability, which can take the form of oscillating loss or divergence. We denote the pathologically low attention entropy, corresponding to highly concentrated attention scores, as $\\textit{entropy collapse}$. As a remedy, we propose $\\sigma$Reparam, a simple and efficient solution where we reparametrize all linear layers with spectral normalization and an additional learned scalar. We demonstrate that $\\sigma$Reparam successfully prevents entropy collapse in the attention layers, promoting more stable training. Additionally, we prove a tight lower bound of the attention entropy, which decreases exponentially fast with the spectral norm of the attention logits, providing additional motivation for our approach. We conduct experiments with $\\sigma$Reparam on image classification, image self-supervised learning, machine translation, speech recognition, and language modeling tasks. We show that $\\sigma$Reparam provides stability and robustness with respect to the choice of hyperparameters, going so far as enabling training (a) a Vision Transformer {to competitive performance} without warmup, weight decay, layer normalization or adaptive optimizers; (b) deep architectures in machine translation and (c) speech recognition to competitive performance without warmup and adaptive optimizers. Code is available at \\url{https://github.com/apple/ml-sigma-reparam}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 41  (*Influential: 7*)\n\n#### 5. Spherical Topic Models\n\n*From Search Query: hierarchical normalization language models stability*\n\n*J. Reisinger, Austin Waters, Bryan Silverthorn, R. Mooney*\n\n**TL;DR:** The Spherical Admixture Model is introduced, a Bayesian topic model for arbitrary l2 normalized data that maintains the same hierarchical structure as Latent Dirichlet Allocation, but models documents as points on a high-dimensional spherical manifold, allowing a natural likelihood parameterization in terms of cosine distance.\n\n**Abstract:** We introduce the Spherical Admixture Model (SAM), a Bayesian topic model for arbitrary l2 normalized data. SAM maintains the same hierarchical structure as Latent Dirichlet Allocation (LDA), but models documents as points on a high-dimensional spherical manifold, allowing a natural likelihood parameterization in terms of cosine distance. Furthermore, SAM can model word absence/presence at the document level, and unlike previous models can assign explicit negative weight to topic terms. Performance is evaluated empirically, both through human ratings of topic quality and through diverse classification tasks from natural language processing and computer vision. In these experiments, SAM consistently outperforms existing models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2010\n\n**Citations:** 103  (*Influential: 13*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical normalization techniques, stability analysis, memory-efficient implementations, and adaptive normalization, here are some key points and references that can be useful:\n\n## Hierarchical Normalization Techniques in Language Models\n\n- Hierarchical normalization can be achieved by structuring the model into multiple stages, each handling different scales or contexts. For example, the **Swin Transformer** architecture uses a hierarchical design to capture multi-scale features effectively. This approach can be adapted to language models by segmenting the model into multiple stages, each processing different levels of contextual information.\n\n## Stability Analysis of Spectral Methods in Deep Neural Networks\n\n- Stability in spectral methods is crucial, and techniques like the 'perturb-then-diagonalize' (PTD) methodology can ensure backward stability. This is important for maintaining the stability of spectral operations within the model. Proper initialization and normalization are critical to avoid instability in these methods.\n\n## Memory-Efficient Implementations of Multi-Scale Processing\n\n- **Hierarchical Transformers** can be designed to process input sequences in a multi-scale manner, which can be memory-efficient. For instance, the hierarchical transformer proposed for ECG signal analysis segments the model into multiple stages, each handling different feature scales. This approach can be applied to language models to reduce memory usage while capturing complex contextual relationships.\n\n## Adaptive Normalization Techniques for Varying Sequence Lengths\n\n- **Layer Normalization** is a common technique used in transformers, but adaptive normalization can be more effective for varying sequence lengths. Techniques such as **Instance Normalization** or **Group Normalization** can be adapted to handle sequences of different lengths by normalizing the activations based on the instance or group rather than the entire batch. This can help in maintaining consistency across different sequence lengths.\n\n## Integration of Spectral Methods with Layer Normalization\n\n- Integrating spectral methods with layer normalization involves ensuring that the spectral operations do not disrupt the normalization process. This can be achieved by applying spectral methods within the normalization layers or by using techniques that maintain the stability of the normalization process. For example, using Fourier transforms within the model, as in the **FNet** architecture, can be combined with layer normalization to ensure stable and efficient processing of sequences.\n\n### Additional Insights\n\n- **Pre-training and Fine-tuning**: Pre-training on large datasets and fine-tuning for specific tasks can help in adapting hierarchical normalization and spectral methods to language models. This approach has been successful in models like BERT and T5, which can be extended to incorporate hierarchical and spectral techniques.\n\n- **Attention Mechanisms**: The self-attention mechanism in transformers can be modified to incorporate hierarchical and multi-scale processing. This can involve using different attention heads for different scales or contexts, enhancing the model's ability to capture complex relationships.\n\nBy integrating these techniques, researchers can develop more robust, efficient, and scalable autoregressive language models that leverage hierarchical normalization, stability in spectral methods, and memory-efficient multi-scale processing.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about spectral methods and FFT optimization in deep learning, particularly:\n\na) FFT Implementation Challenges:\n- FlashFFTConv research shows FFT operations have poor hardware utilization\n- Matrix decomposition techniques can improve FFT efficiency\n- Memory hierarchy and I/O between layers are critical bottlenecks\n\nb) Spectral Methods in Neural Networks:\n- HiPPO framework provides theoretical foundations for optimal polynomial projections\n- Spectral methods can be made computationally efficient with proper implementation\n- Stability considerations are crucial for spectral operations\n\n2. Useful References with Excerpts:\n\nFrom FlashFFTConv (Fu et al., 2023):\n\"We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy.\"\n- Critical for understanding FFT optimization challenges\n\nFrom HiPPO (Gu et al., 2020):\n\"This formal framework yields a new memory update mechanism that scales through time to remember all history, avoiding priors on the timescale.\"\n- Relevant for understanding polynomial projections and memory management\n\nFrom Robustifying State-space Models (Yu et al., 2023):\n\"We introduce a generic, backward-stable 'perturb-then-diagonalize' (PTD) methodology\"\n- Important for stability considerations\n\n3. Key Insights:\n\na) Implementation Efficiency:\n- Matrix decomposition can improve FFT hardware utilization\n- Memory hierarchy optimization is crucial\n- Kernel fusion can reduce I/O overhead\n\nb) Stability Considerations:\n- Backward stability is essential for spectral methods\n- Proper initialization and normalization are critical\n- Diagonalization challenges need careful handling\n\nc) Memory Management:\n- Hierarchical memory access patterns affect performance\n- Efficient state representation is crucial\n- Trade-offs between memory usage and computational cost\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Hierarchical normalization techniques specifically in language models\n2. Stability analysis of spectral methods in deep learning\n3. Memory-efficient implementations of multi-scale processing\n\n5. References:\n\n[1] Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[2] Gu, A., Dao, T., Ermon, S., Rudra, A., & R\u00e9, C. (2020). HiPPO: Recurrent Memory with Optimal Polynomial Projections. Neural Information Processing Systems.\n\n[3] Yu, A., Nigmetov, A., Morozov, D., Mahoney, M. W., & Erichson, N. B. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. arXiv preprint.\n\n[4] Lee-Thorp, J., Ainslie, J., Eckstein, I., & Onta\u00f1\u00f3n, S. (2021). Fnet: Mixing tokens with fourier transforms. North American Chapter of the Association for Computational Linguistics."
        }
    ],
    "rating": 4.1,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.775041,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.846936,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalRMSNorm",
    "proposal": "## Title: AdaptiveSpectralHRMSNorm: Enhanced Hierarchical Normalization with Adaptive Spectral Filtering and State Compression\n\n### 1. Motivation\n\nCurrent hierarchical normalization approaches face several challenges:\n- Limited ability to handle varying sequence lengths efficiently\n- High memory overhead for long sequences\n- Suboptimal balance between stability and adaptivity\n- Inefficient processing of multi-scale features\n\n### 2. Problem Analysis\n\n#### Current Limitations:\n1. Fixed processing patterns regardless of input complexity\n2. Inefficient memory utilization for state representation\n3. Limited robustness to varying input conditions\n4. High computational overhead for spectral operations\n\n#### Opportunities for Improvement:\n1. Integration of adaptive spectral filtering\n2. Dynamic state compression based on input complexity\n3. Efficient hardware-aware implementation\n4. Enhanced stability through careful normalization design\n\n### 3. Core Idea and Philosophy\n\nAdaptiveSpectralHRMSNorm enhances hierarchical normalization by:\n1. Using spectral state space models for robust feature processing\n2. Implementing adaptive state compression for memory efficiency\n3. Employing dynamic spectral filtering based on input complexity\n4. Maintaining stability through careful parameter management\n\n### 4. Design Plan\n\n#### Mathematical Formulation:\n\n1. Adaptive Spectral Filtering:\n```python\ndef adaptive_spectral_filter(x, complexity):\n    # Transform to frequency domain\n    x_freq = torch.fft.rfft(x, dim=-1)\n    \n    # Compute adaptive filter based on input complexity\n    filter_weights = self.compute_filter_weights(complexity)\n    \n    # Apply adaptive filtering\n    x_filtered = x_freq * filter_weights\n    return torch.fft.irfft(x_filtered, dim=-1)\n```\n\n2. Dynamic State Compression:\n```python\ndef compress_state(state, target_dim):\n    # Project state to lower dimension using SVD\n    U, S, V = torch.svd(state)\n    compressed = torch.matmul(U[:, :target_dim], \n                            torch.diag(S[:target_dim]))\n    return compressed\n```\n\n3. Scale-Adaptive Normalization:\n```python\ndef scale_norm(x, scale):\n    # Compute scale-specific statistics\n    rms = torch.sqrt(torch.mean(x.pow(2), dim=-1, keepdim=True) + self.eps)\n    \n    # Apply selective scaling\n    gamma = self.compute_adaptive_weight(x, scale)\n    return (x / rms) * gamma\n```\n\n#### Implementation:\n\n```python\nclass AdaptiveSpectralHRMSNorm(nn.Module):\n    def __init__(self, embed_dim, scales=[1, 2, 4], eps=1e-5):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.scales = scales\n        self.eps = eps\n        \n        # Spectral filtering parameters\n        self.filter_net = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 2),\n            nn.ReLU(),\n            nn.Linear(embed_dim // 2, embed_dim // 2 + 1)\n        )\n        \n        # State compression parameters\n        self.compression_net = nn.Linear(embed_dim, embed_dim)\n        \n        # Scale-specific parameters\n        self.scale_weights = nn.Parameter(torch.ones(len(scales)))\n        self.scale_biases = nn.Parameter(torch.zeros(len(scales)))\n\n    def forward(self, x):\n        # Process at different scales\n        scale_outputs = []\n        for s in self.scales:\n            # Compute input complexity\n            complexity = self.compute_complexity(x)\n            \n            # Apply spectral filtering\n            x_s = self.adaptive_spectral_filter(x, complexity)\n            \n            # Compress state if needed\n            if s > 1:\n                target_dim = self.compute_target_dim(complexity)\n                x_s = self.compress_state(x_s, target_dim)\n            \n            # Scale-specific normalization\n            x_s = self.scale_norm(x_s, s)\n            \n            scale_outputs.append(x_s)\n        \n        # Combine scales with learned weights\n        weights = F.softmax(self.scale_weights, dim=0)\n        output = sum(w * out for w, out in zip(weights, scale_outputs))\n        \n        return output\n```\n\n### 5. Implementation Guidelines\n\n1. Initialization:\n- Initialize spectral filters with identity mapping\n- Use careful weight initialization for stability\n- Set initial scale weights to equal values\n\n2. Training Considerations:\n- Implement gradient checkpointing for memory efficiency\n- Use adaptive learning rates for different components\n- Monitor scale weights for potential collapse\n\n3. Optimization:\n- Fuse operations where possible\n- Implement efficient parallel processing\n- Use hardware-aware memory management\n\n### 6. Theoretical Analysis\n\n#### Complexity Analysis:\n- Time Complexity: O(n log n) per scale\n- Memory Complexity: O(n * c) where c is compression ratio\n- Additional Parameters: O(d) for dimension d\n\n#### Stability Analysis:\n1. Gradient Flow:\n- Spectral filtering provides smooth gradient paths\n- Adaptive compression prevents information loss\n- Scale integration maintains stable information flow\n\n2. Memory Efficiency:\n- Dynamic compression reduces memory requirements\n- Efficient state management through selective updates\n- Hardware-aware implementation minimizes overhead\n\n### 7. Expected Benefits\n\n1. Improved Feature Processing:\n- Better handling of varying sequence lengths\n- Enhanced multi-scale feature representation\n- More efficient information flow between scales\n\n2. Computational Efficiency:\n- Reduced memory requirements through compression\n- Efficient parallel processing capabilities\n- Hardware-optimized implementation\n\n3. Enhanced Stability:\n- Better gradient propagation\n- More stable scale integration\n- Improved convergence properties\n\n### 8. References\n\n[1] Agarwal, N., et al. (2023). \"Spectral State Space Models.\" arXiv preprint.\n\n[2] Yu, A., et al. (2023). \"Robustifying State-space Models for Long Sequences via Approximate Diagonalization.\" arXiv preprint.\n\n[3] Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv preprint.\n\n[4] Li, W., et al. (2024). \"Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model.\" arXiv preprint.\n\n### 9. Conclusion\n\nAdaptiveSpectralHRMSNorm represents a significant advancement in hierarchical normalization by combining adaptive spectral filtering with dynamic state compression. The design achieves better feature representation and stability while maintaining computational efficiency through careful hardware-aware implementation. The integration of spectral state space models and adaptive compression enables more effective handling of long sequences and improved information flow between hierarchical levels.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "AdaptiveSpectralHRMSNorm enhances hierarchical RMS normalization by incorporating adaptive spectral filtering and selective state compression. The design leverages spectral state space models for robust feature processing while using dynamic compression to optimize memory usage. This approach enables efficient handling of long sequences while maintaining model stability and performance.",
    "ideation": null,
    "modelname": "AdaptiveSpectralHierTTT",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of FFT optimization strategies\n- Include concrete examples of memory hierarchy management\n- Add benchmarking guidelines for different components\n\n2. Stability Analysis:\n- Develop more detailed guidelines for preventing attention entropy collapse\n- Include analysis of spectral stability under different conditions\n- Add recommendations for initialization and training\n\n3. Memory Optimization:\n- Elaborate on memory access patterns\n- Include failure mode analysis\n- Add guidelines for very long sequence handling\n\n4. Hardware Optimization:\n- Provide more specific hardware-aware implementation details\n- Include cache optimization strategies\n- Add guidelines for different hardware architectures\n\n5. Empirical Validation:\n- Define specific metrics for evaluating efficiency gains\n- Include guidelines for comparing with baseline models\n- Add ablation study recommendations",
    "user_input": ""
}