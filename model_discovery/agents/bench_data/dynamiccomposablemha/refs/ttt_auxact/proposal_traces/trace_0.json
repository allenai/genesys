{
    "variantname": "TTTLinear-AuxAct",
    "review": "The TTTLinear-AuxAct proposal presents an innovative approach to enhancing the TTTLinear GAU through auxiliary activations and butterfly factorization. After comprehensive analysis of the proposal and related research, here is the detailed review:\n\nSTRENGTHS:\n\n1. Theoretical Foundation\n- The proposal builds on solid theoretical foundations, combining insights from memory-efficient training and structured matrix operations\n- The dual-path activation scheme is well-motivated by recent research showing the effectiveness of auxiliary activations\n- The butterfly factorization approach is supported by proven results in parameter reduction and computational efficiency\n\n2. Memory Efficiency\n- The auxiliary activation mechanism shows promise for significant memory reduction during training\n- The structured matrix parameterization through butterfly factorization offers substantial parameter efficiency\n- The design addresses a critical bottleneck in transformer architectures: memory usage during training\n\n3. Implementation Design\n- The code structure is clear and well-organized\n- The implementation provides detailed considerations for both training and inference phases\n- The backward pass optimization is thoughtfully designed\n\n4. Scalability\n- The butterfly factorization enables sub-quadratic computational complexity\n- The memory optimization strategy scales well with sequence length\n- The design maintains the core benefits of TTTLinear while improving efficiency\n\nCONCERNS:\n\n1. Complexity Trade-offs\n- The dual-path design introduces additional complexity that may impact training stability\n- The butterfly factorization requires careful initialization and tuning\n- The interaction between auxiliary activations and test-time adaptation needs more detailed analysis\n\n2. Performance Guarantees\n- The proposal lacks specific quantitative estimates of memory reduction\n- The impact on model quality during test-time adaptation needs more thorough analysis\n- The trade-off between structure and expressiveness requires more detailed discussion\n\n3. Hardware Considerations\n- Memory access patterns in butterfly operations may need optimization for different hardware\n- The implementation complexity might offset some efficiency gains on certain architectures\n- More detail needed on hardware-specific optimizations\n\n4. Stability Analysis\n- The interaction between auxiliary activations and test-time updates needs more rigorous analysis\n- The stability of the dual-path mechanism during long-sequence processing requires validation\n- The impact of structured matrices on gradient flow needs more consideration",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient matrix parameterization neural networks",
            "detail": "Find specific techniques for efficient matrix parameterization in neural networks, focusing on methods that could reduce memory usage while maintaining expressiveness. Include approaches for structured matrices and low-rank approximations.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques for efficient matrix parameterization in neural networks, focusing on methods that could reduce memory usage while maintaining expressiveness. Include approaches for structured matrices and low-rank approximations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 8/35 (Score: 0.99)*\n\n```\nFigure 3 also includes a baseline row factoring a matrix of appropriately scaled i.i.d. Gaussian entries, to indicate typical errors for factoring an unstructured matrix. ### 4.2 Neural Network Compression\n\nMany structured matrix approaches have been proposed to replace fully-connected (FC) layers of neural networks, to speed up training and inference, and to reduce the memory consumption. These structured matrices are cleverly designed by combining commonly used fast transforms. For example, Fastfood [24] and Deep Fried Convnets [47] compose the fast Hadamard transform and fast Fourier transforms, and Sindhwani et al. 40] use Toeplitz-like matrices that can be written as a sequence of 2 or 4 FFTs. However, the design choice for these light-weight replacement layers is restricted by the set of known and implementable transforms. On the first benchmark task of compressing a single hidden layer model, the real version of BPBP has better classification accuracy than a fully-connected layer on all datasets tested, and uses more than 56X fewer parameters (Table 1); the complex version performs even better with a slight parameter increase. The previous best methods fail to achieve this on the more challenging CIFAR-10 dataset at the same parameter budget [42]. We further demonstrate that this layer is effective as a lightweight addition to a larger-scale ResNet architecture. Fully-connected Previous work showed that structured matrix approaches based on the low displacement rank framework, including Toeplitz-like 40, LDR-SD and LDR-TD matrices 42, compare very favorably to other compression approaches. Following previous experimental settings [5, 40, 42, we compare our proposed classes to several baselines using dense structured matrices to compress the hidden layer of a single hidden layer neural network. Competing methods include simple low-rank factorizations [9], circulant matrices (equivalent to 1-dimensional convolutions) [6], the adaptive Fastfood transform 47, and low displacement rank methods 40, 42 which implicitly define a structured matrix through a displacement equation and admit specialized fast divide-and-conquer algorithms 8.\n```\n\n#### 2. Scatterbrain: Unifying Sparse and Low-rank Attention Approximation (Avg. Score: 0.97)\n\n*Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 93  (*Influential: 11*)\n\n**TL;DR:** Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, Scatterbrain is proposed, a novel way to unify sparse and low-rank attention for accurate and efficient approximation and is unbiased with provably low error.\n\n**Abstract:** Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.\n\n##### *Relevant Chunk: No. 37/51 (Score: 0.97)*\n\n```\narXiv preprint arXiv:2101.11986, 2021. [70] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. [71] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. arXiv preprint arXiv:2107.02192, 2021. ## Appendix\n\nTable of Contents\nA Extended Related Work ..... 17\nA. 1 Robust PCA ..... 17\nA. 2 Efficient Transformers ..... 17\nA. 3 Locality Sensitive Hashing for Efficient Neural Network Training ..... 18\nA. 4 Structured Matrices for Efficient Machine Learning Models ..... 18\nB Motivating Observations: Low-rank and Sparse Structures of Attention Matrices .....\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.89)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 5/86 (Score: 0.93)*\n\n```\nSeveral recent and concurrent works such as RetNet (Y. Sun et al. 2023) and GateLoop (Katsch 2023) strengthen this to more general forms of $L$ (Section 10). In this work, our formulation of structured masked attention will strongly generalize these ideas. ### 2.3 Structured Matrices\n\nGeneral matrices $M \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{T})}$ require $\\mathrm{T}^{2}$ parameters to represent and $O\\left(\\mathrm{~T}^{2}\\right)$ time to perform basic operations such as matrix-vector multiplication. Structured matrices are those that\n(i) can be represented in subquadratic (ideally linear) parameters through a compressed representation, and\n(ii) have fast algorithms (most importantly matrix multiplication) by operating directly on this compressed representation. Perhaps the most canonical families of structured matrices are sparse and low-rank matrices. However, there exist many other families, such as Toeplitz, Cauchy, Vandermonde, and butterfly matrices, which have all been used in machine learning for efficient models (Dao, Gu, et al.\n```\n\n##### *Relevant Chunk: No. 7/86 (Score: 0.86)*\n\n```\n2022; Thomas et al. 2018). Structured matrices are a powerful abstraction for efficient representations and algorithms. In this work, we will show that SSMs are equivalent to another class of structured matrices that have not previously been used in deep learning, and use this connection to derive efficient methods and algorithms. ### 2.4 Overview: Structured State Space Duality\n\nWhile this paper develops a much richer framework of connections between SSMs, attention, and structured matrices, we provide a brief summary of the main method, which is actually quite self-contained and simple algorithmically. Recurrent (Linear) Form. The state space dual (SSD) layer can be defined as a special case of the selective SSM (2). The standard computation of an SSM as a recurrence (or parallel scan) can be applied, which has linear complexity in sequence length. Compared to the version used in Mamba, SSD has two minor differences:\n\n- The structure on $A$ is further simplified from diagonal to scalar times identity structure. Each $A_{t}$ can also be identified with just a scalar in this case. - We use a larger head dimension $P$, compared to $P=1$ used in Mamba. Typically $P=\\{64,128\\}$ is chosen which is similar to conventions for modern Transformers. Compared to the original selective SSM, these changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements. In particular, our new algorithms will allow the use of matrix multiplication units on modern accelerators. Dual (Quadratic) Form. The dual form of SSD is a quadratic computation closely related to attention, defined as\n\n$$\n\\left(L \\circ Q K^{\\top}\\right) \\cdot V \\quad L_{i j}= \\begin{cases}a_{i} \\times \\cdots \\times a_{j+1} & i \\geq j \\\\ 0 & i<j\\end{cases}\n$$\n\nwhere $a_{i}$ are input-dependent scalars bounded in $[0,1]$. Compared to standard softmax attention, there are two main differences\n\n- The softmax is dropped. - The attention matrix is multiplied elementwise-wise by an additional mask matrix $L$. Both of these changes can be viewed as addressing problems in vanilla attention. For example, the softmax has been recently observed to cause problems in attention scores, such as the \"attention sink\" phenomenon (Darcet et al. 2024; Xiao et al. 2024). More importantly, the mask matrix $L$ can be viewed as replacing the heuristic positional embeddings of Transformers with a different data-dependent positional mask that controls how much information is transfered across time. More broadly, this form is an instance of our structured masked attention generalization of linear attention, defined in Section 4. Matrix Form and SSD Algorithm. The various forms of SSD are connected through a unified matrix representation, by showing that SSMs have a matrix transformation form $Y=M X$ for a matrix $M_{\\theta} \\in \\mathbb{R}^{(T, T)}$ that depends on $\\theta=(A, B, C)$. In particular, the dual form of SSD is equivalent to naive (quadratic-time) multiplication by the matrix $M$, and the recurrent form is a particular efficient (linear-time) algorithm that leverages the structure in $M$. Going beyond these, any algorithm for multiplication by $M$ can be applied. Our proposed hardware-efficient SSD algorithm (Section 6) is a new structured matrix multiplication method that involves block decompositions of $M$, which obtains better efficiency tradeoffs than either the pure linear or quadratic forms. It is relatively simple and easy-to-implement compared to general selective SSMs (Gu and Dao 2023); Listing 1 provides a complete implementation in a few lines of code.\n```\n\n#### 4. Linformer: Self-Attention with Linear Complexity (Avg. Score: 0.77)\n\n*Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, Hao Ma*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 1299  (*Influential: 140*)\n\n**TL;DR:** This paper demonstrates that the self-attention mechanism of the Transformer can be approximated by a low-rank matrix, and proposes a new self-Attention mechanism, which reduces the overall self-ATTention complexity from $O(n^2)$ to $O (n)$ in both time and space.\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n##### *Relevant Chunk: No. 8/18 (Score: 0.77)*\n\n```\n4171-4186, 2019. Carl Eckart and Gale Young. The approximation of one matrix by another of lower rank. Psychometrika, 1(3):211-218, 1936. Angela Fan, Pierre Stock, Benjamin Graham, Edouard Grave, Remi Gribonval, Herve Jegou, and Armand Joulin. Training with quantization noise for extreme fixed-point compression. arXiv preprint arXiv:2004.07320, 2020. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015. Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V Le, Yonghui Wu, et al. Gpipe: Efficient training of giant neural networks using pipeline parallelism.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient matrix parameterization neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Changwoo Lee, Hun-Seok Kim*\n\n**TL;DR:** This paper proposes a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent based on the Gaussian-Dirichlet kernel, and defines a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters.\n\n**Abstract:** This paper investigates efficient deep neural networks (DNNs) to replace dense unstructured weight matrices with structured ones that possess desired properties. The challenge arises because the optimal weight matrix structure in popular neural network models is obscure in most cases and may vary from layer to layer even in the same network. Prior structured matrices proposed for efficient DNNs were mostly hand-crafted without a generalized framework to systematically learn them. To address this issue, we propose a generalized and differentiable framework to learn efficient structures of weight matrices by gradient descent. We first define a new class of structured matrices that covers a wide range of structured matrices in the literature by adjusting the structural parameters. Then, the frequency-domain differentiable parameterization scheme based on the Gaussian-Dirichlet kernel is adopted to learn the structural parameters by proximal gradient descent. On the image and language tasks, our method learns efficient DNNs with structured matrices, achieving lower complexity and/or higher performance than prior approaches that employ low-rank, block-sparse, or block-low-rank matrices.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Jiong Zhang, Qi Lei, I. Dhillon*\n\n**TL;DR:** An efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training and empirically solves the vanishing gradient issue to a large extent.\n\n**Abstract:** Vanishing and exploding gradients are two of the main obstacles in training deep neural networks, especially in capturing long range dependencies in recurrent neural networks~(RNNs). In this paper, we present an efficient parametrization of the transition matrix of an RNN that allows us to stabilize the gradients that arise in its training. Specifically, we parameterize the transition matrix by its singular value decomposition(SVD), which allows us to explicitly track and control its singular values. We attain efficiency by using tools that are common in numerical linear algebra, namely Householder reflectors for representing the orthogonal matrices that arise in the SVD. By explicitly controlling the singular values, our proposed Spectral-RNN method allows us to easily solve the exploding gradient problem and we observe that it empirically solves the vanishing gradient issue to a large extent. We note that the SVD parameterization can be used for any rectangular weight matrix, hence it can be easily extended to any deep neural network, such as a multi-layer perceptron. Theoretically, we demonstrate that our parameterization does not lose any expressive power, and show how it controls generalization of RNN for the classification task. %, and show how it potentially makes the optimization process easier. Our extensive experimental results also demonstrate that the proposed framework converges faster, and has good generalization, especially in capturing long range dependencies, as shown on the synthetic addition and copy tasks, as well as on MNIST and Penn Tree Bank data sets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 101  (*Influential: 15*)\n\n#### 3. Learning Compressed Transforms with Low Displacement Rank\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Anna T. Thomas, Albert Gu, Tri Dao, A. Rudra, C. R\u00e9*\n\n**TL;DR:** A rich class of LDR matrices with more general displacement operators is introduced, and explicitly learn over both the operators and the low-rank component, which exceeds the accuracy of existing compression approaches and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.\n\n**Abstract:** The low displacement rank (LDR) framework for structured matrices represents a matrix through two displacement operators and a low-rank residual. Existing use of LDR matrices in deep learning has applied fixed displacement operators encoding forms of shift invariance akin to convolutions. We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component. This class generalizes several previous constructions while preserving compression and efficient computation. We prove bounds on the VC dimension of multi-layer neural networks with structured weight matrices and show empirically that our compact parameterization can reduce the sample complexity of learning. When replacing weight layers in fully-connected, convolutional, and recurrent neural networks for image classification and language modeling tasks, our new classes exceed the accuracy of existing compression approaches, and on some tasks even outperform general unstructured layers while using more than 20X fewer parameters.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 39  (*Influential: 6*)\n\n#### 4. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Steffen Schotth\u00f6fer, Emanuele Zangrando, J. Kusch, Gianluca Ceruti, Francesco Tudisco*\n\n**TL;DR:** A novel algorithm to find efficient low-rank subnetworks that are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced.\n\n**Abstract:** Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to find efficient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efficiency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 32  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Efficient and Scalable Bayesian Neural Nets with Rank-1 Factors\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Dustin Tran, Yi-An Ma, Ghassen Jerfel, Michael W. Dusenberry, Balaji Lakshminarayanan, Yeming Wen, Jasper Snoek, Katherine Heller*\n\n**Abstract:** Bayesian neural networks (BNNs) demonstrate promising success in improving the robustness and uncertainty quantification of modern deep learning. However, they generally struggle with underfitting at scale and parameter efficiency. On the other hand, deep ensembles have emerged as alternatives for uncertainty quantification that, while outperforming BNNs on certain problems, also suffer from efficiency issues. It remains unclear how to combine the strengths of these two approaches and remediate their common issues. To tackle this challenge, we propose a rank-1 parameterization of BNNs, where each weight matrix involves only a distribution on a rank-1 subspace. We also revisit the use of mixture approximate posteriors to capture multiple modes, where unlike typical mixtures, this approach admits a significantly smaller memory increase (e.g., only a 0.4% increase for a ResNet-50 mixture of size 10). We perform a systematic empirical study on the choices of prior, variational posterior, and methods to improve training. For ResNet-50 on ImageNet, Wide ResNet 28-10 on CIFAR-10/100, and an RNN on MIMIC-III, rank-1 BNNs achieve state-of-the-art performance across log-likelihood, accuracy, and calibration on the test sets and out-of-distribution variants.\n\n**Proceeding:** icml-2020-1\n\n**Published:** 2020-05-14\n\n\n\n#### 2. Hyperbolic Representation Learning for Fast and Efficient Neural Question Answering\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Luu Anh Tuan, Yi Tay, Siu Cheung Hui*\n\n**Abstract:** The dominant neural architectures in question answer retrieval are based on\nrecurrent or convolutional encoders configured with complex word matching\nlayers. Given that recent architectural innovations are mostly new word\ninteraction layers or attention-based matching mechanisms, it seems to be a\nwell-established fact that these components are mandatory for good performance.\nUnfortunately, the memory and computation cost incurred by these complex\nmechanisms are undesirable for practical applications. As such, this paper\ntackles the question of whether it is possible to achieve competitive\nperformance with simple neural architectures. We propose a simple but novel\ndeep learning architecture for fast and efficient question-answer ranking and\nretrieval. More specifically, our proposed model, \\textsc{HyperQA}, is a\nparameter efficient neural network that outperforms other parameter intensive\nmodels such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple\nQA benchmarks. The novelty behind \\textsc{HyperQA} is a pairwise ranking\nobjective that models the relationship between question and answer embeddings\nin Hyperbolic space instead of Euclidean space. This empowers our model with a\nself-organizing ability and enables automatic discovery of latent hierarchies\nwhile learning embeddings of questions and answers. Our model requires no\nfeature engineering, no similarity matrix matching, no complicated attention\nmechanisms nor over-parameterized layers and yet outperforms and remains\ncompetitive to many models that have these functionalities on multiple\nbenchmarks.\n\n**Published:** 2017-07-25\n\n\n\n#### 3. Kaleidoscope: An Efficient, Learnable Representation For All Structured Linear Maps\n\n*From Search Query: efficient matrix parameterization neural networks*\n\n*Christopher R\u00e9, Atri Rudra, Megan Leszczynski, Amit Blonder, Matthew Eichhorn, Albert Gu, Nimit S. Sohoni, Tri Dao*\n\n**Abstract:** Modern neural network architectures use structured linear transformations, such as low-rank matrices, sparse matrices, permutations, and the Fourier transform, to improve inference speed and reduce memory usage compared to general linear maps. However, choosing which of the myriad structured transformations to use (and its associated parameterization) is a laborious task that requires trading off speed, space, and accuracy. We consider a different approach: we introduce a family of matrices called kaleidoscope matrices (K-matrices) that provably capture any structured matrix with near-optimal space (parameter) and time (arithmetic operation) complexity. We empirically validate that K-matrices can be automatically learned within end-to-end pipelines to replace hand-crafted procedures, in order to improve model quality. For example, replacing channel shuffles in ShuffleNet improves classification accuracy on ImageNet by up to 5%. K-matrices can also simplify hand-engineered pipelines -- we replace filter bank feature computation in speech data preprocessing with a learnable kaleidoscope layer, resulting in only 0.4% loss in accuracy on the TIMIT speech recognition task. In addition, K-matrices can capture latent structure in models: for a challenging permuted image classification task, a K-matrix based representation of permutations is able to learn the right latent structure and improves accuracy of a downstream convolutional model by over 9%. We provide a practically efficient implementation of our approach, and use K-matrices in a Transformer network to attain 36% faster end-to-end inference speed on a language translation task.\n\n**Conference:** kaleidoscope-an-efficient-learnable\n\n**Published:** 2020-12-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design with a focus on efficient matrix parameterization, reducing memory usage, and maintaining expressiveness, here are some specific techniques and approaches that can be considered:\n\n## Low-Rank Matrix Approximation\nLow-rank matrix approximation is a method that can significantly reduce the memory usage and computational complexity of matrix operations in neural networks. This technique involves approximating a high-dimensional matrix with a product of two lower-dimensional matrices. For example, the study on \"Efficient Training via Low-Rank Matrix Approximation\" discusses integrating randomized algorithm-based low-rank matrix approximation to compute model parameters efficiently. This approach can be applied to the weight matrices in the neural network to reduce memory usage while preserving the model's expressiveness.\n\n## Structured Matrices\nUsing structured matrices, such as those with specific sparsity patterns or hierarchical structures, can also enhance efficiency. For instance, the concept of \"sequence alignment\" mentioned in the Hydra paper can be extended to use structured matrices that align sequences efficiently. This can increase the flexibility and performance of matrix mixers and other components within the autoregressive model[Analysis Note - Round 1].\n\n## Element-Wise Operations\nAdapting element-wise operations, as seen in the Attention Free Transformer (AFT) layers, can replace some of the more expensive matrix operations. This can help in reducing memory I/O overhead and computational costs. By combining the query, key, and value with learned position biases in an element-wise fashion, the model can maintain expressiveness while being more efficient[Analysis Note - Round 1].\n\n## Neural Tangent Kernel (NTK) Parametrization\nThe NTK parametrization provides insights into the behavior of wide neural networks and can be used to optimize the parameterization of matrices. By ensuring that the parameters are scaled appropriately as the widths of the layers grow, the NTK parametrization can help in maintaining the expressiveness of the model while optimizing its training dynamics. This approach can be particularly useful in overparametrized models, where it helps in understanding and optimizing the generalization properties.\n\n## Bidirectional Processing\nImplementing bidirectional processing, similar to Hydra's approach, can enhance the expressiveness of the autoregressive model without doubling the computation. This involves using generalized matrix mixers that can handle bidirectional information efficiently, which can be crucial for maintaining the GAU (Gated Attention Unit) structure while introducing improvements[Analysis Note - Round 1].\n\n## Efficient Hidden State Updates\nThe update mechanism in the TTT layers can be made more efficient by using matrix parameterization techniques. For example, incorporating data-driven priors via pretraining, as suggested in \"Never Train from Scratch,\" can help in initializing the matrices more effectively. Additionally, using self-supervised learning to update the hidden states, as in the TTT layers, can be optimized further by leveraging efficient matrix operations and structured matrices[Analysis Note - Round 1].\n\nBy combining these techniques, the researchers can develop an autoregressive language model that is more efficient in terms of memory usage, computational complexity, and training dynamics, while maintaining or even enhancing its expressiveness and performance.\n",
            "analysis": "Analysis Note - Round 1\n\n1. Summary of Analysis:\nThe seed design \"Learning to (Learn at Test Time)\" introduces TTT layers that use self-supervised learning to update hidden states even during test time. While innovative, there are several areas that could be improved:\n\na) Efficiency Concerns:\n- TTT-MLP faces memory I/O challenges\n- Current implementation may have redundant computations in the dual form\n- Opportunity to optimize the update mechanism\n\nb) Architectural Limitations:\n- Uses conventional RMSNorm and SwiGLU components\n- Linear complexity but could potentially be more expressive\n- May not fully leverage bidirectional information\n\n2. Useful References with Excerpts:\n\nFrom Hydra Paper:\n\"The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning... We identify a key axis of matrix parameterizations termed sequence alignment, which increases the flexibility and performance of matrix mixers\"\n\nKey insight: Matrix parameterization and sequence alignment could be used to enhance TTT's expressiveness.\n\nFrom AFT Paper:\n\"In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion.\"\n\nKey insight: Element-wise operations could potentially replace some of the more expensive matrix operations in TTT.\n\nFrom \"Never Train from Scratch\":\n\"...incorporation of data-driven priors via pretraining is essential for reliable performance estimation\"\n\nKey insight: The initialization and training strategy are as important as the architecture itself.\n\n3. Key Insights and Analysis:\n\na) Potential Improvements:\n- The TTTLinear unit could be enhanced with bidirectional processing similar to Hydra's approach\n- AFT's element-wise operations could be adapted to reduce memory usage\n- The update mechanism could be made more efficient using matrix parameterization techniques\n\nb) Design Opportunities:\n- Introduce a more efficient parameterization for the hidden state updates\n- Create a hybrid approach combining TTT's learning capability with AFT's efficiency\n- Implement bidirectional processing without doubling computation\n\nc) Technical Considerations:\n- Need to maintain the GAU structure while introducing improvements\n- Must ensure backward compatibility with existing components\n- Should focus on memory efficiency while preserving expressiveness\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for efficient matrix parameterization\n2. Methods for reducing memory I/O overhead\n3. Successful approaches to bidirectional processing in autoregressive models\n\n5. References:\n\n[1] Hwang, S., Lahoti, A., Dao, T., & Gu, A. (2024). Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers. arXiv:2401.xxxxx.\n\n[2] Zhai, S., Talbott, W. A., Srivastava, N., Huang, C., Goh, H., Zhang, R., & Susskind, J. (2021). An Attention Free Transformer. arXiv:2105.xxxxx.\n\n[3] Amos, I., Berant, J., & Gupta, A. (2023). Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors. arXiv:2310.xxxxx.\n\n[4] Yen, H., Gao, T., & Chen, D. (2024). Long-Context Language Modeling with Parallel Context Encoding. arXiv:2401.xxxxx."
        },
        {
            "ready": false,
            "query": "butterfly factorization neural network implementation",
            "detail": "Find specific implementations and applications of butterfly factorizations in neural networks, focusing on practical details of integration with existing architectures and performance implications.",
            "search_ret": "\n---\n## Found 2 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and applications of butterfly factorizations in neural networks, focusing on practical details of integration with existing architectures and performance implications.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 2 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.94)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 14/53 (Score: 0.94)*\n\n```\nIn Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2978-2988, 2019. [15] Tri Dao, Albert Gu, Matthew Eichhorn, Atri Rudra, and Christopher R\u00e9. Learning fast algorithms for linear transforms using butterfly factorizations. In International Conference on Machine Learning (ICML), 2019. [16] Tri Dao, Nimit Sohoni, Albert Gu, Matthew Eichhorn, Amit Blonder, Megan Leszczynski, Atri Rudra, and Christopher R\u00e9. Kaleidoscope: An efficient, learnable representation for all structured linear maps. In International Conference on Learning Representations (ICLR), 2020. [17] Tri Dao, Beidi Chen, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher R\u00e9. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations (ICLR), 2022. [18] Tri Dao, Beidi Chen, Nimit Sohoni, Arjun Desai, Michael Poli, Jessica Grogan, Alexander Liu, Aniruddh Rao, Atri Rudra, and Christopher R\u00e9. Monarch: Expressive structured matrices for efficient and accurate training. In International Conference on Machine Learning (ICML), 2022. [19] Giannis Daras, Nikita Kitaev, Augustus Odena, and Alexandros G Dimakis. Smyrf-efficient attention using asymmetric clustering.\n```\n\n#### 2. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.91)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 2/35 (Score: 0.99)*\n\n```\nAll of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of matrices with fast matrix-vector multiplication as factoring into products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the $O(N \\log N)$ Cooley-Tukey FFT algorithm to machine precision, for dimensions $N$ up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4 X faster inference speed and 40 X fewer parameters. ## 1 Introduction\n\nStructured linear transformations, such as the discrete Fourier transform (DFT), discrete cosine transform (DCT), and Hadamard transform, are a workhorse of machine learning, with applications ranging from data preprocessing, feature generation, and kernel approximation, to image and language modeling (convolutions). To date, these transformations rely on carefully designed algorithms, such as the famous fast Fourier transform (FFT) algorithm, and on specialized implementations (e.g., FFTW and cuFFT). Moreover, each specific transform requires hand-crafted implementations for every platform (e.g., Tensorflow and PyTorch lack the fast Hadamard transform), and it can be difficult to know when they are useful. Ideally, these barriers would be addressed by automatically learning the most effective transform for a given task and dataset, along with an efficient implementation of it. Such a method should be capable of recovering a range of fast transforms with high accuracy and realistic sizes given limited prior knowledge. It is also preferably composed of differentiable primitives and basic operations common to linear algebra/machine learning libraries, that allow it to run on any platform and be integrated into modern ML frameworks such as PyTorch/Tensorflow. More fundamentally, this problem ties into the foundational question of understanding the minimal prior knowledge needed to learn high-speed systems, in the spirit of modern trends toward relaxing manually imposed structure (i.e., AutoML). Recent progress in this vein of learning computational primitives includes addition/multiplication gates [43], the Strassen $2 \\times 2$ matrix multiplication algorithm [44, and PDE solvers [19]. We propose a method that addresses this problem for a class of important transforms that includes the\naforementioned examples. A key challenge lies in defining or parameterizing the space of transforms and corresponding fast algorithms, which requires using a minimal amount of prior knowledge that captures important and interesting transforms while remaining learnable and efficient. Egner \\& P\u00fcschel [13, 14] previously posed this question and a novel combinatorial approach, but their solution only addresses a limited set of transforms (primarily DFT) and only on limited problem sizes. In particular, these approaches search through an exponentially large discrete space using a symbolic form of the matrix [13, 14] and recover the solution only up to dimensions $8 \\times 8$. We instead draw two key lessons from the work of De Sa et al. 8, who characterize matrices with efficient matrix-vector multiplication algorithms as being factorizable into products of sparse matrices ${ }^{1}$ Thus, the task of learning algorithms can be reduced to finding appropriate sparse matrix product representations of the transforms. They further show that divide-and-conquer schemes lead to fast multiplication algorithms for a surprisingly general set of structured matrices. Motivated by the broad applicability of this recursive structure, we propose a particular factorization using sequences of special block diagonal matrices, called butterfly matrices. Specific instances of butterfly structure have been used before - for example as a random orthogonal preconditioner [36] or in matrix approximation [28 - but we use a relaxed representation that captures a larger class of structures and can learn from data. These form a class of structured matrices with $O(N)$ parameters and automatic fast multiplication in $O(N \\log N)$ operations. We empirically validate our method in two ways. First, we consider a specification of a transform (e.g., $N$ input-output pairs) and attempt to factorize it. We successfully recover a fast algorithm up to machine precision for several important transforms such as the DFT, Hadamard, DCT, and convolution for realistic sizes (dimensions up to $N=1024$ ), while standard sparse and low-rank baselines cannot (Section 4.1). Beyond recovering famous transforms, we additionally incorporate this method in end-to-end ML pipelines to learn fast and compressible latent transformations (Section 4.2). On the benchmark single hidden layer network, this parameterization exceeds the classification accuracy of a baseline fully connected layer on several datasets - such as by 3.9 points on CIFAR- 10 while using 40X fewer parameters-which is to our knowledge the first time a structured model has outperformed the unconstrained model for this task on a realistic dataset 42. We also find that the addition of a lightweight butterfly layer improves the accuracy of a modern ResNet architecture by 0.43 points. Finally, our method is simple with an easily implementable fast algorithm. We compare the training and inference speed of our implementation to specialized implementations of discrete transforms (Section 4.3). Our generic representation comes within 3-5X of implementations for specific transforms such as the DFT and DCT, while still being capable of learning a rich class of more general transforms. ## 2 Related Work\n\nFast transforms are crucial and ubiquitous in the machine learning pipelines, from data preprocessing, feature generation, and dimensionality reduction to compressing models. For example, the DFT and DCT form the basis of the mel-frequency cepstral coefficients (MFCCs), a standard feature representation for speech recognition [21]. State-of-the-art kernel approximation methods leverage circulant matrices (i.e., convolution) 49] and the DFT and Hadamard transform [24, 50] for fast projection. Structured matrices, which are matrix representations of fast transforms, play a crucial role in designing fast neural network layers with few parameters 40, 10. Given their importance, there have been significant efforts in finding more and more general classes of fast transforms. Traditional classes of structured matrices such as the Toeplitz, Hankel, Vandermonde, and Cauchy matrices are ubiquitous in engineering and signal processing [35], and more recently have found use in deep learning. These were generalized under the seminal notion of low displacement rank (LDR) introduced by Kailath et al. [22], and were later unified under a single class of displacement structure (the confluent Cauchy-like matrices) introduced by Olshevsky \\& Shokrollahi 34 to solve the Nevanlinna-Pick interpolation problem. Another class of fast transforms that directly generalize the DFT and DCT are based on orthogonal polynomials [7], which find usage in areas from differential equations to optics. Both orthogonal polynomial transforms [12], and all of the previously introduced matrices with displacement rank structure, were further\n\n[^0]significantly generalized under a single class by De Sa et al. 8. Notably, almost all of the structured matrix classes mentioned here exhibit a form of recursive structure in their construction and superfast algorithms. Since the product of sparse matrices immediately has a fast multiplication algorithm, the problem of sparse matrix factorization has been tackled in many settings. Sparse PCA 51] and dictionary learning [29] factor a matrix into two components, one of which is sparse. Sparse matrix factorization with more than two factors has also been considered, for example in the setting where the true matrix is the product of random sparse matrices 33, or in the context of learning multi-layer sparse approximations 25, 26. Our approach differs from these in that we focus on the recursive structure of the transforms - not just the sparsity of their factors-leading to sparse and structured transforms, and avoiding the discreteness problem inherent to learning sparsity. Since most distinct transforms typically require significant work both to design fast algorithms and to efficiently implement them on different platforms, there have been attempts to automatically learn these fast algorithms. The field of algebraic signal processing [39] uses methods from representation theory of groups and algebras to automatically generate fast algorithms from the symbolic form of the transform matrix. However, these methods require search over a combinatorially-large discrete space, limiting their approaches to small matrices of size up to $8 \\times 8$ [14, 45]. Attempts to learn general algorithms such as matching 31, sorting [16], and traveling salesman [2] using differentiable architectures face a similar challenge of having to effectively explore a large discrete space. Thus, they only work for problems of size at most 100. By contrast, our approach simplifies the discreteness of the problem into learning a simpler set of permutations, allowing us to recover fast algorithms for realistic dimensions. Independently, there has been growing interest in compressed deep learning models, motivated by the goal of adapting them to resource-constrained environments. A common approach for learning compressed models involves replacing the unconstrained weight matrices with a class of structured matrices and learning directly on the parametrization of that class. The most effective methods use matrix classes that are explicitly related to Fourier transforms 40, or employ highly specialized and complicated recursive algorithms 42]. As our method also implicitly defines a highly compressible subclass of matrices with linear parameter count and efficient multiplication, it can be used as a drop-in replacement for matrices in such end-to-end ML models. ## 3 Recovering Fast Transforms\n\nWe now set up and describe our approach. We first reiterate the connection between fast algorithms and sparse matrix factorization, and briefly outline a quintessential divide-and-conquer algorithm (the FFT) as motivation. We then elaborate the details of our method for learning particular recursive algorithms, including a core permutation-learning step that enables it to capture a wider range of structures. We also discuss the expressive power of these matrices, including which transforms they capture perfectly, and define a hierarchy of matrix classes built on butterflies that can theoretically capture richer recursive structures. ### 3.1 Preliminaries\n\nSparse factorizations One method of constructing matrices with obvious fast matrix-vector multiplication is as a product of sparse matrices, so that multiplication by an arbitrary vector will have cost proportional to the total number of nonzeros of the matrices in the product. Surprisingly, the converse is also true. The notion of sparse product width (SPW) [8, which roughly corresponds to the total sparsity of a factorization of a matrix, turns out to be equivalent to the length of the shortest linear straight-line program describing a matrix (up to a constant). Hence, it is an optimal descriptor of the algorithmic complexity of matrix-vector multiplication on these types of models [3. Given the general correspondence between sparse factorization and fast algorithms, we consider specific types of discrete transforms and their recursive factorizations. This is a prototype for our parameterization of fast recursive algorithms in Section 3.2\n\nCase study: DFT The Discrete Fourier Transform (DFT) transforms a complex input vector $x=$ $\\left[x_{0}, \\ldots, x_{N-1}\\right]$ into a complex output vector $X=\\left[X_{0}, \\ldots, X_{N-1}\\right]$ by expressing the input in the basis of the complex exponentials:\n\n$$\nX_{k}=\\sum_{n=0}^{N-1} x_{n} e^{-\\frac{2 \\pi i}{N} k n}, \\quad k=0, \\ldots, N-1, N=2^{m}\n$$\n\nLet $\\omega_{N}:=e^{2 \\pi i / N}$ denote a primitive $N$-th root of unity. The DFT can be expressed as matrix multiplication by the DFT matrix $F_{N} \\in \\mathbb{C}^{N \\times N}$, where $\\left(F_{N}\\right)_{k n}=\\omega_{N}^{-k n}$. The DFT of size $N$ can be reduced to two DFTs of size $N / 2$ on the even indices and the odd indices:\n\n$$\nF_{N} x=\\left[\\begin{array}{l}\nF_{N / 2} x_{\\mathrm{even}}+\\Omega_{N / 2} F_{N / 2} x_{\\text {odd }} \\\\\nF_{N / 2} x_{\\text {even }}-\\Omega_{N / 2} F_{N / 2} x_{\\text {odd }}\n\\end{array}\\right]\n$$\n\nwhere $x_{\\text {even }}=\\left[x_{0}, x_{2}, \\ldots, x_{N-2}\\right], x_{\\text {odd }}=\\left[x_{1}, x_{3}, \\ldots, x_{N-1}\\right]$, and $\\Omega_{N / 2}$ is the diagonal matrix with entries $1, \\omega_{N}^{-1}, \\ldots, \\omega_{N}^{-(N / 2-1)}$. This recursive structure yields the efficient recursive Cooley-Tukey Fast Fourier Transform (FFT) algorithm. This computation can be written as a matrix factorization\n\n$$\nF_{N}=\\left[\\begin{array}{cc}\nI_{N / 2} & \\Omega_{N / 2} \\\\\nI_{N / 2} & -\\Omega_{N / 2}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nF_{N / 2} & 0 \\\\\n0 & F_{N / 2}\n\\end{array}\\right]\\left[\\begin{array}{c}\n\\text { Sort the even } \\\\\n\\text { and odd indices }\n\\end{array}\\right]\n$$\n\nwhere $I_{N / 2}$ is the identity matrix, and the last factor is the permutation matrix $P_{N}$ that separates the even and odd indices (e.g., mapping $[0,1,2,3]$ to $[0,2,1,3]$ ) (see Figure 2]. Unrolling the recursion, we obtain:\n\n$$\n\\begin{aligned}\nF_{N} & =B_{N}\\left[\\begin{array}{cc}\nF_{N / 2} & 0 \\\\\n0 & F_{N / 2}\n\\end{array}\\right] P_{N} \\\\\n& =B_{N}\\left[\\begin{array}{cc}\nB_{N / 2} & 0 \\\\\n0 & B_{N / 2}\n\\end{array}\\right]\\left[\\begin{array}{cccc}\nF_{N / 4} & 0 & 0 & 0 \\\\\n0 & F_{N / 4} & 0 & 0 \\\\\n0 & 0 & F_{N / 4} & 0 \\\\\n0 & 0 & 0 & F_{N / 4}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nP_{N / 2} & 0 \\\\\n0 & P_{N / 2}\n\\end{array}\\right] P_{N} \\\\\n& =\\cdots \\\\\n& =\\left(B_{N} \\ldots\\left[\\begin{array}{ccc}\nB_{2} & \\ldots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\ldots & B_{2}\n\\end{array}\\right]\\right)\\left(\\left[\\begin{array}{ccc}\nP_{2} & \\ldots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\ldots & P_{2}\n\\end{array}\\right] \\ldots P_{N}\\right)\n\\end{aligned}\n$$\n\nThe product of all the $B_{N / 2^{k}}$ matrices on the left is called a butterfly matrix, and each factor $B_{N / 2^{k}}$ is a $2 \\times 2$ block matrix of diagonal matrices called a butterfly factor. Figure 1 illustrates the sparsity pattern of the structured butterfly factors. One can also combine the product of permutation matrices on the right to obtain a single permutation called the bit-reversal permutation, which sorts the indices by the reverse of their binary representation (e.g. $[0, \\ldots, 7] \\rightarrow[0,4,2,6,1,5,3,7]$ ). Other transforms have similar recursive structure but differ in the entries of $B_{N / 2^{k}}$, and in the permutation. For example, the DCT involves separating the even and the odd indices, and then reversing the second half (e.g., $[0,1,2,3] \\rightarrow[0,2,1,3] \\rightarrow[0,2,3,1]$ ). Appendix A provides some examples of how important transforms, such as the DFT, DCT, Hadamard, and convolutions, can factor as similar products of sparse matrices. ### 3.2 Recovering Fast Transform Algorithms\n\nMany previous works attempt to compress generic matrices by sparsifying them. We note that allowing for products of matrices with a total sparsity budget is strictly more expressive than a single matrix with that sparsity, while retaining the same compression and computation complexity. Therefore one can hope to recover all fast algorithms by learning over the set of matrix products with a total sparsity budget. However, this is infeasible to learn due to the discreteness of the sparsity constraint (Section 122). We instead use a class of matrices built as products of specific factors that captures the recursive nature of many fast algorithms. ![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-05.jpg?height=201&width=825&top_left_y=249&top_left_x=647)\n\nFigure 1: Butterfly matrix for $N=16$. From left to right: single copy of $B_{16}$, blocks of $B_{8}$, blocks of $B_{4}$, blocks of $B_{2}$. A butterfly parametrization Let $x=\\left[x_{0}, \\ldots, x_{N-1}\\right]$ be an input vector ${ }^{2}$ Let $\\mathcal{T}_{N}$ be a linear transform of size $N$ with matrix representation $T_{N} \\in \\mathbb{F}^{N \\times N}$, where $\\mathbb{F} \\in\\{\\mathbb{R}, \\mathbb{C}\\}$. A general recursive structure is to separate the input vector into two halves by some permutation, apply the transform on each half, and combine the result in a linear manner by scaling by an diagonal matrix and adding the results. Written as a matrix factorization:\n\n$$\nT_{N}=\\left[\\begin{array}{ll}\nD_{1} & D_{2} \\\\\nD_{3} & D_{4}\n\\end{array}\\right]\\left[\\begin{array}{cc}\nT_{N / 2} & 0_{N / 2 \\times N / 2} \\\\\n0_{N / 2 \\times N / 2} & T_{N / 2}\n\\end{array}\\right] P_{N}\n$$\n\nwhere $P_{N}$ is some permutation matrix and $D_{1}, \\ldots, D_{4} \\in \\mathbb{F}^{N / 2}$ are diagonal matrices. Inspired by the factors of the FFT, we call the matrix $\\left[\\begin{array}{ll}D_{1} & D_{2} \\\\ D_{3} & D_{4}\\end{array}\\right]$ a butterfly factor, denoted by $B_{N}$. Unrolling the recursion as in equation (1) gives the factorization $T_{N}=B^{(N)} P^{(N)}$, where $B^{(N)}$ is a butterfly matrix and $P^{(N)}$ is a permutation that can be written as the product of $\\log _{2}(N)$ simpler block permutations. We also consider composing this module, hence learn either\n\n$$\nT_{N}=B^{(N)} P^{(N)} \\quad T_{N}=B_{2}^{(N)} P_{2}^{(N)} B_{1}^{(N)} P_{1}^{(N)}\n$$\n\nwhich we term the BP and the BPBP parametrization respectively. One dimensional convolutions (i.e. circulant matrices) are notably captured by BPBP, since they can be computed via an FFT, a component-wise product, then an inverse FFT (see Appendix A). In the case of the FFT, as in Section 3.1 the entries of the butterfly factors are called twiddle factors, and the combined permutation $P^{(N)}$ is called the bit-reversal permutation. Learning a recursive permutation The butterfly blocks in the BP or BPBP parametrization have a fixed sparsity pattern and their parameters can be directly optimized. However, the transforms we are interested in capturing frequently require different permutations as part of the \"divide\" step, which form a set of discrete objects that we must consider. We will restrict to learning over permutations that have a simple structure often encountered in these algorithms: we assume that the distribution factors into $\\log _{2} N$ steps following the $\\log _{2} N$ recursive layers. At each step in the recursion, the permutation $P_{N / 2^{k}}$ is allowed to either keep the first half and second half intact or separate the even and the odd indices (e.g., $[0,1,2,3] \\rightarrow[0,2,1,3]$ ). Then, it can choose to reverse the first half (e.g., $[0,1] \\rightarrow[1,0]$ ) and can choose to reverse the second half (e.g., $[2,3] \\rightarrow[3,2]$ ). Thus at each step, there are 3 binary choices and hence 8 possible permutations. These are illustrated in Figure 2, where $P_{N}^{a}$ denotes the permutation matrix on $N$ elements that separates the even and odd elements, $P_{N}^{b}$ denotes the permutation matrix that reverses the first half, and $P_{N}^{c}$ denotes the permutation matrix that reverses the second half. Instead of searching over $8^{\\log _{2} N}$ discrete permutations, we parameterize the permutation $P^{(N)}$ as a categorical distribution of these $8^{\\log _{2} N}$ permutations. The permutation $P_{N / 2^{k}}$ at step $k$ is thus chosen as a convex combination of the 8 possible choices:\n\n$$\nP_{N / 2^{k}}=p_{c b a} P_{N / 2^{k}}^{c} P_{N / 2^{k}}^{b} P_{N / 2^{k}}^{a}+p_{c b} P_{N / 2^{k}}^{c} P_{N / 2^{k}}^{b}+\\ldots\n$$\n\nThis can be learned by representing this probability distribution $\\left\\{p_{c b a}, p_{c b}, \\ldots\\right\\}$ for example via logits and the softmax. [^1]![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-06.jpg?height=320&width=833&top_left_y=247&top_left_x=644)\n\nFigure 2: Three binary choices for constructing the permutation used at every step of the recursive process. One of 8 possible permutations can be constructed by multiplying a subset of these matrices in the presented order. We make the further simplification that the probabilities $p_{c b a}$ factor into the three components; conceptually, that the choices of choosing $P_{N / 2^{k}}^{c}, P_{N / 2^{k}}^{b}, P_{N / 2^{k}}^{a}$ to be part of the product are independent of each other. This results in the representation\n\n$$\nP_{N / 2^{k}}=\\prod_{s=c, b, a}\\left(p_{s} P_{N / 2^{k}}^{s}+\\left(1-p_{s}\\right) I\\right)\n$$\n\nThus we learn the permutation $P_{N / 2^{k}}$ via equation 3y optimizing over 3 logits $\\ell_{a}, \\ell_{b}, \\ell_{c}$ and setting $p_{s}=\\sigma\\left(\\ell_{s}\\right)$, where $\\sigma$ is the sigmoid function. To encourage the distribution over permutations to be peaked, one can add entropy regularization [15] or semantic loss [46.\n```\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.93)*\n\n```\nHowever, we found that these tricks are not necessary. For example, the learned transforms in Section 4.1 typically put weight at least 0.99 on a permutation. Initialization As the BP or BPBP construction is a product of many matrices, proper initialization is crucial to avoid exponential blowup in the size of the entries or condition numbers (i.e., the exploding/vanishing gradient problem [37]). We aim to initialize each butterfly factor to be close to unitary or orthogonal, so that the magnitude of the inputs and outputs to the transform are preserved. This is easy since each of the factors $B_{N}, \\ldots, B_{2}$ has exactly two nonzeros in each row and column; for example in the real case, initializing each entry of $B_{k}$ as $\\mathcal{N}(0,1 / 2)$ guarantees $\\mathbb{E} B_{k}^{*} B_{k}=I_{N}$. Comparison to related methods Some previous works have examined similar butterfly matrices in numerical algebra or machine learning [36, 20, 32, mainly motivated by trying to parametrize cheap orthogonal matrices. Our parametrization, motivated by the goal of learning recursive transforms, differs in several ways from all previous works: 1 . We explicitly model and learn a permutation matrix $P$. 2. Our relaxation does not enforce the matrix to be orthogonal. 3. Our butterfly factors are ordered so that closer elements interact first (Figure 1), whereas some works (e.g. [32]) reverse the order. 4. Every work has a different weight-tying scheme; ours ties the blocks in each butterfly factor, leading to fewer parameters and a tighter recursive interpretation than for example [20]. Our main baseline for deep learning experiments is Thomas et al. 42, who define a special matrix class with a complicated recursive algorithm. While our BP method and theirs share some overlap (e.g., they both capture circulant matrices), they have a distinct parametrization, and the exact relation between the BP hierarchy and their LDR-SD or LDR-TD classes is unknown. From a practical standpoint, BP is significantly faster and simpler to implement than their methods. ### 3.3 Expressivity and the butterfly hierarchy\n\nThe butterfly matrix $B$ has a total of $4 N$ learnable parameters (the butterfly factors $B_{N}, B_{N / 2}, \\ldots, B_{2}$ have $2 N, N, \\ldots, 4$ entries respectively). The overall permutation $P$ has $3 \\log _{2} N$ learnable parameters; we can also tie the logits of the $\\log _{2} N$ probabilistic permutations-reflecting the fact that for some algorithms the reduction from size $N$ to $N / 2$ is self-similar to the reduction from size $N / 2^{k}$ to $N / 2^{k+1}$ reducing this to just 3 parameters. We can define a natural hierarchy of matrix classes built on the BP primitive. This hierarchy covers a spectrum ranging from extremely structured matrices with a linear number of parameters, to the entire space of square matrices. Definition 1. For any dimension $N$, let $(\\mathrm{BP})_{r}^{k}(k, r \\in \\mathbb{N})$ denote the classes of matrices that can be expressed as\n\n$$\nS\\left(\\prod_{i=1}^{k} B_{i} P_{i}\\right) S^{T}\n$$\n\nwhere each $B_{i} P_{i} \\in \\mathbb{F}^{r N \\times r N}$ is a BP module as in equation (2), and $S \\in \\mathbb{F}^{N \\times r N}=\\left[\\begin{array}{llll}I_{N} & 0 & \\ldots & 0\\end{array}\\right]$ (that is, $S$ and $S^{T}$ select the upper left $N \\times N$ entries of the BP product matrix). The subscript $r$ is understood to be 1 if omitted. Note that the BP and BPBP classes are equivalent to (BP) ${ }^{1}$ and (BP) ${ }^{2}$ respectively. We remark that $B$ and $P$ are both capable of being the identity, and thus $(\\mathrm{BP})^{k} \\subseteq(\\mathrm{BP})^{k+1}$. The BP hierarchy is expressive enough to theoretically represent many important transforms with low depth, as well as all matrices with linear depth:\n\nProposition 1. $(B P)^{1}$ captures the fast Fourier transform, the fast Hadamard transform, and their inverses exactly. $(B P)^{2}$ captures the DCT, DST, and convolution exactly. All $N \\times N$ matrices are contained in $(B P)_{2}^{4 N+10}$. Proposition 1 is shown in Appendix B. We suggest some additional conjectures about the expressiveness of the BP hierarchy in Appendix D. Even though the BP parameterization is expressive, it still retains the learnability characteristic of compressed parameterizations. In fact, neural networks comprising layers of BP and BPBP matrices still have VC dimension that is almost linear in the number of parameters (Appendix B), similar to networks with fully-connected layers [1, 17] and LDR 42, which implies a corresponding sample complexity bound. ## 4 Empirical Evaluation\n\nWe evaluate the proposed approach to verify that our butterfly parameterization can both recover fast transforms and be integrated as an effective component in ML pipelines ${ }^{3}$, In Section 4.1, we confirm that it automatically learns the fast algorithms for many discrete transforms commonly used in signal processing and machine learning. Section 4.2 further shows that it can be a useful component to increase the performance of deep learning models while ensuring fast multiplication and few parameters by design. ### 4.1 Discrete Transforms\n\nBelow we list several important classes of structured matrices. Some of them are directly captured by our parametrization and we expect that they can be recovered close to perfectly, thus providing a $O(N \\log N)$ algorithm that closely approximates the naive $O\\left(N^{2}\\right)$ matrix multiplication. Others are not perfectly captured by the BPBP class but still have recursive structure; for these, we expect that our method reconstructs them better than standard matrix compression methods (sparse, low-rank, and combinations) can. Transforms We describe the matrices we evaluate on and their applications; a standard reference is Proakis 38.\n```\n\n##### *Relevant Chunk: No. 9/35 (Score: 0.89)*\n\n```\nOur implementation is built on top of the publicly available implementation of Thomas et al. 42 with the same hyperparameters, and we report their numbers for the competing baseline methods directly. We test on the three main datasets from Thomas et al. 42]: two challenging variants of MNIST-one with randomly rotated images and random background, the other with correlated background noise-and the standard CIFAR-10 dataset. Table 1: Test accuracy when replacing the hidden layer with structured classes. For the BPBP methods, the permutations $P$ have been fixed to the bit-reversal permutation. The butterfly parameterization achieves higher accuracy than the unstructured layer on all datasets. | Method | MNIST-bg-rot | MNIST-noise | CIFAR-10 | Compression factor |\n| :--- | :--- | :--- | :--- | :--- |\n| Unstructured | 44.08 | 65.15 | 46.03 | 1 |\n| BPBP (complex, fixed permutation) | $\\mathbf{4 6 . 2 6}$ | 77.00 | $\\mathbf{4 9 . 9 3}$ | 39.4 |\n| BPBP (real, fixed permutation) | 46.16 | 75.00 | 48.69 | 56.9 |\n| LDR-TD 42] | 45.81 | $\\mathbf{7 8 . 4 5}$ | 45.33 | 56.9 |\n| Toeplitz-like [40] | 42.67 | 75.75 | 41.78 | 56.9 |\n| Fastfood 47] | 38.13 | 63.55 | 39.64 | 78.7 |\n| Circulant 6] | 34.46 | 65.35 | 34.28 | 93.0 |\n| Low-rank [9] | 35.67 | 52.25 | 32.28 | 56.9 |\n\nTable 1 reports results for variants of our butterfly parametrization, compared to the unstructured matrix baseline and other structured matrix approaches. Notably, the butterfly methods achieve higher classification accuracy than the fully-connected layer on all datasets and are highly competitive with the other approaches. We note that improvements over unconstrained matrices can arise from lower generalization error due to fewer parameters (relating to VC bounds, Proposition 2), or better inductive bias encoded by the structured class. For example, convolutions are important in image tasks due to encoding shift equivariance, and Thomas et al. 42 hypothesize that their structured classes improve over FC layers through imposing approximate equivariance to more general transformations. Since our BP parametrization can represent arbitrary convolutions, it can encode these important priors. ResNet In addition to the standard single hidden layer benchmarks, we test the effect of using butterfly layers in a standard ResNet18 [18] implementation on the CIFAR-10 dataset. This architecture is normally fully convolutional, ending with a FC layer of dimensions $512 \\times 10$ before the softmax. However, we experiment with adding an additional FC or structured layer right before this final FC layer. Table 2 shows that the ResNet18 architecture can benefit from an additional fully connected layer, and using a BPBP layer instead improves performance even more while adding a negligible ( $0.07 \\%$ increase) number of parameters to the original model. Table 2: Classification accuracy for the ResNet18 architecture with different layers inserted before the final FC/softmax layer. | Last layer | None | FC | BPBP |\n| :--- | :--- | :--- | :--- |\n| Accuracy | $93.58 \\pm 0.15$ | $93.89 \\pm 0.19$ | $\\mathbf{9 4 . 0 1} \\pm 0.09$ |\n\n### 4.3 Training and Inference Speed Comparison\n\nBy design, the BP parameterization yields a fast algorithm of complexity $O(N \\log N)$, no matter which transform it learns. Moreover, given the parameters of the BP model, it is easy to implement this fast algorithm (this can be done in 5 lines of Python, and our code provides a function to do this automatically). The BP parameterization captures many common transforms (Section 4.1), and its implementation makes no transform-specific optimizations. Nevertheless, our simple implementation is surprisingly competitive with hand-tuned kernels both for training and for inference (after the parameters of the BP model are learned and we wish to evaluate $B P x$ for new input $x$ ). In Figure 4, we compare the speed of the BP fast multiplication against specialized implementation of common transforms such as the FFT, DCT, and DST (all have complexity $O(N \\log N)$ ), using dense matrix-vector multiply (GEMV, complexity $O\\left(N^{2}\\right)$ ) as a baseline. For training with realistic input sizes $N=1024$ and batch size 256 on GPU, the training time (forward and\nbackward) of butterfly matrix is $15 \\%$ faster than dense matrix multiply (GEMM from cuBLAS) and within $40 \\%$ of FFT (from cuFFT). For inference on CPU, the BP fast multiplication can be one or two orders of magnitude faster than GEMV, is within a factor of 5 of the FFT, and is within a factor of 3 of the DCT and the DST, across a range of input sizes. The GEMM/GEMV and the FFT are two of the most heavily tuned numerical routines. ![](https://cdn.mathpix.com/cropped/2024_09_12_bdb1f06984456725f946g-11.jpg?height=732&width=1612&top_left_y=490&top_left_x=243)\n\nFigure 4: Speedup of FFT and Butterfly against dense matrix-matrix multiply (GEMM) for training, and FFT, DCT, DST, and BP against dense matrix-vector multiply (GEMV) for inference. Butterfly's performance is constant with respect to any of the possible transforms it can learn, in contrast to the highly tuned implementations for specific transforms. ## 5 Conclusion\n\nWe address the problem of automatically learning fast algorithms for a class of important linear transforms, through a parameterization of recursive algorithms via butterfly factorizations. We validate our method by learning transforms including the DFT, DCT, Hadamard transform, and convolutions up to machine precision and dimension $N=1024$. Finally, we show that the same method yields consistent performance improvements and substantial compression and speed increases as a component of end-to-end ML models. ## Acknowledgments\n\nWe thank Maximilian Lam for his help with early experiments. We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M) and FA86501827865 (SDH), NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity) and CCF1563078 (Volume to Velocity), ONR under No. N000141712266 (Unifying Weak Supervision), the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, Google, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, and American Family Insurance, Google Cloud, Swiss Re, and members of the Stanford DAWN project: Intel, Microsoft, Teradata, Facebook, Google, Ant Financial, NEC, SAP, VMWare, and Infosys.\n```\n\n##### *Relevant Chunk: No. 20/35 (Score: 0.82)*\n\n```\nIn Dasgupta, S. and McAllester, D. (eds.), Proceedings of the 30th International Conference on Machine Learning, volume 28 of Proceedings of Machine Learning Research, pp. 244-252, Atlanta, Georgia, USA, 17-19 Jun 2013. PMLR. URL http://proceedings.mlr.press/v28/le13.html. [25] Le Magoarou, L. and Gribonval, R. Chasing butterflies: In search of efficient dictionaries. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 3287-3291, April 2015. doi: $10.1109 /$ ICASSP.2015.7178579. [26] Le Magoarou, L. and Gribonval, R. Flexible multilayer sparse approximations of matrices and applications. IEEE Journal of Selected Topics in Signal Processing, 10(4):688-700, June 2016. ISSN 1932-4553. doi: 10.1109/JSTSP.2016.2543461. [27] Li, L., Jamieson, K., DeSalvo, G., Rostamizadeh, A., and Talwalkar, A. Hyperband: A novel bandit-based approach to hyperparameter optimization. The Journal of Machine Learning Research, 18(1):6765-6816, 2017 . [28] Li, Y., Yang, H., Martin, E. R., Ho, K. L., and Ying, L. Butterfly factorization. Multiscale Modeling G8 Simulation, 13(2):714-732, 2015. [29] Mairal, J., Ponce, J., Sapiro, G., Zisserman, A., and Bach, F. R. Supervised dictionary learning. In Koller, D., Schuurmans, D., Bengio, Y., and Bottou, L. (eds.), Advances in Neural Information Processing Systems 21, pp. 1033-1040. Curran Associates, Inc., 2009. URL http://papers.nips.cc/ paper/3448-supervised-dictionary-learning.pdf\n[30] Makhoul, J. A fast cosine transform in one and two dimensions. IEEE Transactions on Acoustics, Speech, and Signal Processing, 28(1):27-34, February 1980. ISSN 0096-3518. doi: 10.1109/TASSP.1980.1163351. [31] Mena, G., Belanger, D., Linderman, S., and Snoek, J. Learning latent permutations with GumbelSinkhorn networks.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: butterfly factorization neural network implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. On a Neural Implementation of Brenier's Polar Factorization\n\n*From Search Query: butterfly factorization neural network implementation*\n\n*Nina Vesseron, Marco Cuturi*\n\n**TL;DR:** A practical implementation of Brenier's polar factorization theorem is proposed, and possible applications to non-convex optimization problems, as well as sampling of densities that are not log-concave are illustrated.\n\n**Abstract:** In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization\n\n*From Search Query: butterfly factorization neural network implementation*\n\n*Eldad Meller, Alexander Finkelstein, Uri Almog, Mark Grobman*\n\n**TL;DR:** This paper exploits an oft-overlooked degree of freedom in most networks for a given layer, individual output channels can be scaled by any factor provided that the corresponding weights of the next layer are inversely scaled.\n\n**Abstract:** Quantization of neural networks has become common practice, driven by the need for efficient implementations of deep neural networks on embedded devices. In this paper, we exploit an oft-overlooked degree of freedom in most networks for a given layer, individual output channels can be scaled by any factor provided that the corresponding weights of the next layer are inversely scaled. Therefore, a given network has many factorizations which change the weights of the network without changing its function. We present a conceptually simple and easy to implement method that uses this property and show that proper factorizations significantly decrease the degradation caused by quantization. We show improvement on a wide variety of networks and achieve state-of-the-art degradation results for MobileNets. While our focus is on quantization, this type of factorization is applicable to other domains such as network-pruning, neural nets regularization and network interpretability.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications\n\n*From Search Query: butterfly factorization neural network implementation*\n\n*Yong-Deok Kim, Eunhyeok Park, S. Yoo, Taelim Choi, Lu Yang, Dongjun Shin*\n\n**TL;DR:** A simple and effective scheme to compress the entire CNN, called one-shot whole network compression, which addresses the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by the proposed scheme.\n\n**Abstract:** Although the latest high-end smartphone has powerful CPU and GPU, running deeper convolutional neural networks (CNNs) for complex tasks such as ImageNet classification on mobile devices is challenging. To deploy deep CNNs on mobile devices, we present a simple and effective scheme to compress the entire CNN, which we call one-shot whole network compression. The proposed scheme consists of three steps: (1) rank selection with variational Bayesian matrix factorization, (2) Tucker decomposition on kernel tensor, and (3) fine-tuning to recover accumulated loss of accuracy, and each step can be easily implemented using publicly available tools. We demonstrate the effectiveness of the proposed scheme by testing the performance of various compressed CNNs (AlexNet, VGGS, GoogLeNet, and VGG-16) on the smartphone. Significant reductions in model size, runtime, and energy consumption are obtained, at the cost of small loss in accuracy. In addition, we address the important implementation level issue on 1?1 convolution, which is a key operation of inception module of GoogLeNet as well as CNNs compressed by our proposed scheme.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2015\n\n**Citations:** 856  (*Influential: 92*)\n\n#### 4. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\n\n*From Search Query: butterfly factorization neural network implementation*\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 92  (*Influential: 13*)\n\n#### 5. Collaborative Filtering with Stacked Denoising AutoEncoders and Sparse Inputs\n\n*From Search Query: butterfly factorization neural network implementation*\n\n*Florian Strub, J\u00e9r\u00e9mie Mary*\n\n**TL;DR:** This paper introduces a neural network architecture which computes a non-linear matrix factorization from sparse rating inputs and provides an implementation of the algorithm as a reusable plugin for Torch, a popular neural network framework.\n\n**Abstract:** Neural networks have not been widely studied in Collaborative Filtering. For instance, no paper using neural networks was published during the Net-flix Prize apart from Salakhutdinov et al's work on Restricted Boltzmann Machine (RBM) [14]. While deep learning has tremendous success in image and speech recognition, sparse inputs received less attention and remains a challenging problem for neural networks. Nonetheless, sparse inputs are critical for collaborative filtering. In this paper, we introduce a neural network architecture which computes a non-linear matrix factorization from sparse rating inputs. We show experimentally on the movieLens and jester dataset that our method performs as well as the best collaborative filtering algorithms. We provide an implementation of the algorithm as a reusable plugin for Torch [4], a popular neural network framework.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2015\n\n**Citations:** 158  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using butterfly factorizations, here are some specific implementations and applications that could be beneficial:\n\n## Butterfly Factorization in Neural Networks\n\n### Efficient Matrix Parameterization\nButterfly factorizations can be used to reparameterize dense fully-connected layers in neural networks, which is particularly useful for autoregressive models that involve complex matrix operations.\n\n- **Butterfly Matrices and Kronecker-Sparse Matrices**: These matrices can be expressed as products of sparse factors with specific sparsity patterns, offering sub-quadratic matrix multiplication complexity. This approach can accelerate both training and inference phases by reducing the number of parameters and improving computational efficiency.\n\n### Practical Implementation Details\n\n- **Integration with Existing Architectures**: Butterfly factorizations can be integrated into the linear layers of transformers or other autoregressive models. For instance, the weight matrices of linear layers and the projection matrices for keys, queries, and values in multi-head attention modules can be reparameterized using Kronecker-sparse matrices. This can maintain or even improve the accuracy of the model while reducing the computational cost.\n\n### Performance Implications\n\n- **Speed and Memory Efficiency**: Using butterfly factorizations can significantly reduce the inference time compared to standard dense matrix implementations. This is because the structured sparsity of butterfly matrices allows for faster matrix multiplication algorithms, such as those exploiting the Discrete Fourier Transform (DFT) and Hadamard Transform.\n\n### Example Implementations\n\n- **Batched Matrix Multiplication**: Implementations like `bmm` and `bsr` can be used for batched matrix multiplication with Kronecker-sparse matrices. These implementations leverage batched GEMM routines and block-sparse libraries to optimize memory transfers and computation, making them suitable for large-scale neural network models.\n\n## Applications in Autoregressive Models\n\n- **TTT (Tensor Train) Design**: Butterfly factorizations can enhance the TTT design by providing an efficient parameterization for the TTT update mechanism. This can improve the computational efficiency of the model without compromising its expressive power.\n\n- **Memory-Efficient Attention Mechanisms**: By using butterfly factorizations in attention mechanisms, autoregressive models can achieve better memory efficiency. This is crucial for large models where memory constraints are significant.\n\n## Future Integration Points\n\n- **Hidden State Updates**: Butterfly factorizations can be applied to the hidden state updates in autoregressive models, potentially using K-matrices for automatic learning of optimal structured transformations. This could further improve the efficiency and expressiveness of the model.\n\nIn summary, incorporating butterfly factorizations into autoregressive language models can lead to significant improvements in computational efficiency, memory usage, and potentially even the model's accuracy, making it a promising approach for enhancing state-of-the-art models.\n",
            "analysis": "Analysis Note - Round 2\n\n1. Summary of Analysis:\nFrom the search results, several promising techniques for efficient matrix parameterization have emerged that could enhance the TTT design:\n\na) Structured Matrix Approaches:\n- Butterfly factorizations for fast linear transforms\n- Low displacement rank (LDR) matrices\n- Kaleidoscope matrices (K-matrices) for capturing structured transformations\n- Rank-1 parameterizations for efficient Bayesian neural networks\n\nb) Efficiency Techniques:\n- Block decompositions for hardware-efficient implementations\n- Differentiable learning of matrix structures\n- SVD parameterization for gradient stability\n- Hyperbolic space representations for parameter efficiency\n\n2. Useful References with Excerpts:\n\nFrom \"Learning Fast Algorithms for Linear Transforms\":\n\"Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms.\"\n\nKey insight: Butterfly factorizations could provide an efficient parameterization for the TTT update mechanism.\n\nFrom \"Learning Compressed Transforms with Low Displacement Rank\":\n\"We introduce a rich class of LDR matrices with more general displacement operators, and explicitly learn over both the operators and the low-rank component.\"\n\nKey insight: LDR matrices could offer a more flexible and learnable structure for the TTT hidden states.\n\nFrom \"Transformers are SSMs\":\n\"The state space dual (SSD) layer can be defined as a special case of the selective SSM... These changes can be viewed as slightly decreasing the expressive power in return for significant training efficiency improvements.\"\n\nKey insight: Trading some expressiveness for efficiency through structured parameterization can be beneficial.\n\n3. Key Insights and Analysis:\n\na) Matrix Structure Opportunities:\n- Butterfly factorizations could provide efficient computation for TTT updates\n- LDR matrices offer a learnable structure that maintains expressiveness\n- K-matrices could automatically learn optimal structured transformations\n\nb) Implementation Considerations:\n- Block decompositions can improve hardware efficiency\n- SVD parameterization could help stabilize training\n- Hyperbolic representations might reduce parameter count\n\nc) Design Integration Points:\n- TTTLinear unit could be enhanced with butterfly or LDR structure\n- Hidden state updates could use K-matrices for automatic structure learning\n- Memory efficiency could be improved through rank-1 parameterizations\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementations of butterfly factorizations in neural networks\n2. Memory-efficient attention mechanisms that could complement TTT\n3. Hardware-efficient matrix multiplication algorithms for structured matrices\n\n5. References:\n\n[1] Dao, T., Gu, A., Eichhorn, M., Rudra, A., & R\u00e9, C. (2019). Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations. International Conference on Machine Learning.\n\n[2] Thomas, A. T., Gu, A., Dao, T., Rudra, A., & R\u00e9, C. (2018). Learning Compressed Transforms with Low Displacement Rank. Neural Information Processing Systems.\n\n[3] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv.org.\n\n[4] Lee, C., & Kim, H. S. (2023). Differentiable Learning of Generalized Structured Matrices for Efficient Deep Neural Networks. International Conference on Learning Representations.\n\n[5] Zhang, J., Lei, Q., & Dhillon, I. (2018). Stabilizing Gradients for Deep Neural Networks via Efficient SVD Parameterization. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "memory efficient neural network implementation IO patterns",
            "detail": "Find specific techniques and approaches for implementing memory-efficient neural network operations, focusing on IO patterns, tiling strategies, and cache optimization methods.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques and approaches for implementing memory-efficient neural network operations, focusing on IO patterns, tiling strategies, and cache optimization methods.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Attention as an RNN (Avg. Score: 0.96)\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama Ahmed, Y. Bengio, Greg Mori*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Aaren is introduced, an attention-based module that can not only be trained in parallel but also be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs).\n\n**Abstract:** The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.\n\n##### *Relevant Chunk: No. 21/34 (Score: 0.96)*\n\n```\nO. (2023). Memory efficient neural processes via constant memory attention block.\n```\n\n#### 2. Large Memory Layers with Product Keys (Avg. Score: 0.73)\n\n*Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, H. J\u00e9gou*\n\n**Published in:** Neural Information Processing Systems (2019)\t**Cited by** 113  (*Influential: 17*)\n\n**TL;DR:** A structured memory which can be easily integrated into a neural network and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead is introduced.\n\n**Abstract:** This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.\n\n##### *Relevant Chunk: No. 4/21 (Score: 0.73)*\n\n```\nThe output is the sparse weighted sum over the memories associated with the selected keys. For a large number of keys $|\\mathcal{K}|$, the key selection procedure becomes too expensive in practice. Our product key method is exact and makes this search process very fast. Figure 11, we define keys as the concatenation of two sub-keys, in the spirit of product quantization [21]. As shown in more details in Figure 2, this structure implicitly defines a very large set of keys, each being associated with a value memory slot. The set of value vectors introduces the bulk of the parameters, as it scales quadratically with the number of sub-keys. Despite the large number of memory slots, finding the exact closest keys to the input is very efficient, typically requiring $\\mathcal{O}(\\sqrt{|\\mathcal{K}|})$ vector comparisons, where $|\\mathcal{K}|$ is the total number of memory slots. All the memory parameters are trainable, yet only a handful of memory slots are updated for each input at training time. Sparsity of key selection and parameter updates make both training and inference very efficient. Our layer allows us to tackle problems where current architectures underfit given the vast amount of available data, or when they are too slow to work in practice. We thus focus on the language modeling task, integrating our memory within the popular transformer architecture [44]. This choice is motivated by the success of BERT [11] and GPT-2 [36], which demonstrated that increasing the capacity of large models directly translates to large improvements in language modeling, which in turn translates to better performance in both language understanding tasks [11, 46] and text generation [36]. Overall, our paper makes the following contributions:\n\n- We introduce a new layer that provides a large capacity to a neural network for only a slight computational overhead both at train and test time. - Our fast indexing strategy offers exact nearest neighbor search by construction, and avoids the pitfall of relying on an indexing structure that needs to be re-learned during training. - We demonstrate our method within a large state-of-the-art transformer, composed of 24 layers of dimension 1600. Our method with 1 memory and 12 layers outperforms a 24 layer transformer while being twice faster at inference time. We show that adding more memory layers to transformers of various complexities provides systematic and significant improvements on our target task. ## 2 Related work\n\nDifferent approaches have been proposed to increase the capacity of neural networks without increasing too much the computational complexity. For instance, conditional computation models aim at routing inputs into very large neural networks such that only a subset of connections and/or layers are used to process each input. Different methods have been developed like large mixture of experts [40], gating techniques [3, 12, 6] or even reinforcement learning-based approaches [10]. Another line of research is the development of memory augmented neural networks. For instance, memory-based neural layers [47, 42] are an efficient way to represent variable length inputs for complex problems such as question answering [48]. Such memories can also operate in feature space and have various reading and writing mechanisms [23, 17]. Unfortunately, these approaches scale linearly with the size of the memory which is prohibitive for very large memories. Neural cache models [15] suffer from the same scaling issues, which are circumvented by adopting approximate lookup techniques at test time [14]. Discretization techniques have been intensively studied for compressing network weights [8, 38] and/or activations [7, 38] or to accelerate inference. For instance, Gerald et al. [13] propose to map an input to a low-dimensional binary code, each code being associated with one category, thus reducing the complexity of inference by avoiding the use of a final large linear layer. Another model is proposed in [45], where the authors develop a fast locality-sensitive hashing technique to approximate the dot product between large matrices and vectors in neural networks. However, exploiting binary codes or approximate techniques at training time raises several challenges in terms of optimization, because approximate indexes are not accurate in high-dimensional spaces. In our paper, we borrow some ideas from product quantization (PQ) [21]. This is an approximate search technique that maps database vectors into compact codes. However, our goal is different: we do not build an approximate index, but rather we exploit the idea to represent a large set of key vectors by a drastically smaller number of vectors, that we update by regular back-propagation. As discussed later, the selection of the closest keys is exact and inherits from the fast neighbor search of PQ. Our model is also related to sparsity models which have been mainly studied in the unsupervised learning setting [34, 24]. For instance, the k-sparse autoencoder [30] only keeps the k largest values in the latent representation of an auto-encoder, similar to our memory layer but without the product keys component. In winner take all autoencoders [31], sparsity is induced by using mini-batch statistics, while in the sparse access memory [37] reports some speed-up by both thresholding the memory to a sparse subset, and by using efficient data structures for content-based read operations.\n```\n\n#### 3. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.54)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.54)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n#### 4. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.32)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.32)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 5. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.23)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.23)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: memory efficient neural network implementation IO patterns\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Memory Efficient Optimizers with 4-bit States\n\n*From Search Query: memory efficient neural network implementation IO patterns*\n\n*Bingrui Li, Jianfei Chen, Jun Zhu*\n\n**TL;DR:** This work pushes optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments, and identifies a zero point problem of quantizing the second moment, and solves this problem with a linear quantizer that excludes the zero point.\n\n**Abstract:** Optimizer states are a major source of memory consumption for training neural networks, limiting the maximum trainable model within given memory budget. Compressing the optimizer states from 32-bit floating points to lower bitwidth is promising to reduce the training memory footprint, while the current lowest achievable bitwidth is 8-bit. In this work, we push optimizer states bitwidth down to 4-bit through a detailed empirical analysis of first and second moments. Specifically, we find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization. We further identify a zero point problem of quantizing the second moment, and solve this problem with a linear quantizer that excludes the zero point. Our 4-bit optimizers are evaluated on a wide variety of benchmarks including natural language understanding, machine translation, image classification, and instruction tuning. On all the tasks our optimizers can achieve comparable accuracy with their full-precision counterparts, while enjoying better memory efficiency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 2. MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning\n\n*From Search Query: memory efficient neural network implementation IO patterns*\n\n*Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, Song Han*\n\n**TL;DR:** This study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.\n\n**Abstract:** Tiny deep learning on microcontroller units (MCUs) is challenging due to the limited memory size. We find that the memory bottleneck is due to the imbalanced memory distribution in convolutional neural network (CNN) designs: the first several blocks have an order of magnitude larger memory usage than the rest of the network. To alleviate this issue, we propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. However, naive implementation brings overlapping patches and computation overhead. We further propose network redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. Manually redistributing the receptive field is difficult. We automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the peak memory usage of existing networks by 4-8x. Co-designed with neural networks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves>90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2 also unblocks object detection on tiny devices, achieving 16.9% higher mAP on Pascal VOC compared to the state-of-the-art result. Our study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 123  (*Influential: 17*)\n\n#### 3. Backprop with Approximate Activations for Memory-efficient Network Training\n\n*From Search Query: memory efficient neural network implementation IO patterns*\n\n*Ayan Chakrabarti, Benjamin Moseley*\n\n**TL;DR:** A new implementation for back-propagation is proposed that significantly reduces memory usage, by enabling the use of approximations with negligible computational cost and minimal effect on training performance.\n\n**Abstract:** Training convolutional neural network models is memory intensive since back-propagation requires storing activations of all intermediate layers. This presents a practical concern when seeking to deploy very deep architectures in production, especially when models need to be frequently re-trained on updated datasets. In this paper, we propose a new implementation for back-propagation that significantly reduces memory usage, by enabling the use of approximations with negligible computational cost and minimal effect on training performance. The algorithm reuses common buffers to temporarily store full activations and compute the forward pass exactly. It also stores approximate per-layer copies of activations, at significant memory savings, that are used in the backward pass. Compared to simply approximating activations within standard back-propagation, our method limits accumulation of errors across layers. This allows the use of much lower-precision approximations without affecting training accuracy. Experiments on CIFAR-10, CIFAR-100, and ImageNet show that our method yields performance close to exact training, while storing activations compactly with as low as 4-bit precision.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 34  (*Influential: 12*)\n\n#### 4. Learning with Auxiliary Activation for Memory-Efficient Training\n\n*From Search Query: memory efficient neural network implementation IO patterns*\n\n*Sunghyeon Woo, Dongsuk Jeon*\n\n**TL;DR:** This work proposes a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation, and combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation.\n\n**Abstract:** While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks\n\n*From Search Query: memory efficient neural network implementation IO patterns*\n\n*Jongseok Park, Kyungmin Bin, Gibum Park, Sangtae Ha, Kyunghan Lee*\n\n**TL;DR:** ASEN is a novel parallel computation solution for DNNs that allows fine-grained dynamic execution of DNNs, which removes the operator barriers and expresses DNNs in dataflow graphs of fine-grained tiles to expose the parallel computation opportunities across operators, and exploits these opportunities by dynamically locating and scheduling them in runtime.\n\n**Abstract:** Modern Deep Neural Network (DNN) frameworks use tensor operators as the main building blocks of DNNs. However, we observe that operator-based construction of DNNs incurs significant drawbacks in parallelism in the form of synchronization barriers . Synchronization barriers of operators confine the scope of parallel computation to each operator and obscure the rich parallel computation opportunities that exist across operators. To this end, we present ASPEN, a novel parallel computation solution for DNNs that allows fine-grained dynamic execution of DNNs , which (1) removes the operator barriers and expresses DNNs in dataflow graphs of fine-grained tiles to expose the parallel computation opportunities across operators, and (2) exploits these opportunities by dynamically locating and scheduling them in runtime. This novel approach of ASPEN enables opportunistic parallelism , a new class of parallelism for DNNs that is unavailable in the existing operator-based approaches. ASPEN also achieves high resource utilization and memory reuse by letting each resource asynchronously traverse depthwise in the DNN graph to its full computing potential. We provide challenges and solutions to our approach and show that our proof-of-concept implementation of ASPEN on CPU shows exceptional performance, outperforming state-of-the-art inference systems of TorchScript and TVM by up to 3.2 \u00d7 and 4.3 \u00d7 , respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on memory-efficient neural network operations, IO patterns, tiling strategies, and cache optimization, here are some specific techniques and approaches that can be beneficial:\n\n## Memory-Efficient Techniques\n\n### Compressed Activations\nThe CompAct method introduced in the paper \"CompAct: Compressed Activations for Memory-Efficient LLM Training\" is highly relevant. This technique involves storing low-rank, compressed activations during the forward pass, which significantly reduces the required memory for training large language models (LLMs). By compressing activations, gradients, and optimizer states, CompAct achieves substantial memory savings (25-30% for pretraining and 50% for fine-tuning) without compromising training throughput and performance.\n\n### Bottleneck Layers\nUsing bottleneck layers, as discussed in \"Reducing Data Bottlenecks in Distributed, Heterogeneous Neural Networks,\" can significantly reduce data transfer volume between layers. These layers compress and encode information into smaller feature maps, reducing memory bandwidth requirements and enhancing computational efficiency. This approach is particularly useful in resource-constrained environments and can be applied to autoregressive models to minimize data movement and optimize memory usage.\n\n## IO Patterns and Tiling Strategies\n\n### FlashAttention\nThe FlashAttention mechanism is optimized for IO complexity, requiring fewer High-Bandwidth Memory (HBM) accesses compared to standard attention mechanisms. This is achieved through IO-aware implementation and tiling strategies that optimize memory access patterns. Such strategies can be adapted for autoregressive models to improve their memory efficiency and reduce the number of memory accesses [Analysis Note - Round 3].\n\n### Batched Matrix Multiplication\nOptimizing batched matrix multiplication using block-sparse routines and tiling strategies can enhance the efficiency of matrix operations in neural networks. This approach ensures better cache utilization and reduces the computational overhead associated with large matrix operations, which is crucial for autoregressive models that involve complex matrix multiplications[Analysis Note - Round 3].\n\n## Cache Optimization Methods\n\n### Structured Sparsity\nImplementing structured sparsity patterns in neural network operations can significantly improve cache efficiency. Techniques like butterfly factorizations, which involve products of sparse factors with specific sparsity patterns, can reduce parameter counts and improve inference speed while maintaining or even improving accuracy. This structured sparsity helps in optimizing memory access patterns and cache utilization[Analysis Note - Round 3].\n\n### Hardware-Software Co-Design\nThe hardware-software co-design methodology discussed in \"Reducing Data Bottlenecks in Distributed, Heterogeneous Neural Networks\" emphasizes the importance of mapping algorithmic bottleneck layers to on-chip regions to reduce inter-chip and inter-core data movement. This approach can be applied to optimize cache usage and minimize data transfer volume, leading to more efficient neural network operations.\n\n## Additional Considerations\n\n### Integration with Existing Architectures\nIntegrating memory-efficient techniques with existing architectures, such as reparameterizing linear layers to incorporate butterfly factorizations or using compressed activations, can enhance the overall efficiency of autoregressive models. This integration should balance parameter efficiency with computational overhead to ensure optimal performance[Analysis Note - Round 3].\n\nBy combining these techniques\u2014compressed activations, bottleneck layers, optimized IO patterns through tiling strategies, and cache optimization using structured sparsity and hardware-software co-design\u2014the researchers can significantly improve the memory efficiency, computational performance, and scalability of their autoregressive language models.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\nFrom the search results, I've identified several key technical approaches for implementing butterfly factorizations efficiently in neural networks, particularly focusing on their application to autoregressive models:\n\na) Implementation Strategies:\n- Butterfly matrices can be implemented as products of sparse factors with specific sparsity patterns\n- Integration with existing architectures through reparameterization of linear layers\n- Batched matrix multiplication optimizations using bmm and bsr implementations\n- Memory-efficient attention mechanisms using structured sparsity\n\nb) Performance Characteristics:\n- Sub-quadratic complexity for matrix operations\n- Significant parameter reduction (up to 40x fewer parameters shown in some cases)\n- Improved inference speed (4x faster reported in benchmarks)\n- Better memory efficiency through structured sparsity\n\n2. Useful References with Excerpts:\n\nFrom \"Learning Fast Algorithms for Linear Transforms\":\n\"Our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations... exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\"\n\nKey insight: Butterfly factorizations can actually improve accuracy while reducing parameters.\n\nFrom \"FlashAttention\":\n\"We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes.\"\n\nKey insight: Memory access patterns are crucial for performance optimization.\n\n3. Key Insights and Analysis:\n\na) Technical Integration Points:\n- The TTTLinear unit could be enhanced with butterfly factorization for the weight matrices\n- Memory access patterns can be optimized using tiling strategies similar to FlashAttention\n- Batched operations can be implemented efficiently using block-sparse routines\n\nb) Performance Optimization Opportunities:\n- IO-aware implementation for better hardware efficiency\n- Structured sparsity patterns that maintain hardware efficiency\n- Tiled computations for better cache utilization\n\nc) Design Considerations:\n- Need to balance parameter efficiency with computational overhead\n- Memory access patterns crucial for real-world performance\n- Trade-off between structure complexity and implementation efficiency\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for memory-efficient implementation of butterfly factorizations\n2. Methods for combining butterfly factorizations with attention mechanisms\n3. Approaches for optimizing IO patterns in neural network operations\n\n5. References:\n\n[1] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[2] Dao, T., Gu, A., Eichhorn, M., Rudra, A., & R\u00e9, C. (2019). Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations. International Conference on Machine Learning.\n\n[3] Kim, Y. D., Park, E., Yoo, S., Choi, T., Yang, L., & Shin, D. (2015). Compression of Deep Convolutional Neural Networks for Fast and Low Power Mobile Applications. International Conference on Learning Representations.\n\n[4] Meller, E., Finkelstein, A., Almog, U., & Grobman, M. (2019). Same, Same But Different: Recovering Neural Network Quantization Error Through Weight Factorization. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "auxiliary activation neural network implementation",
            "detail": "Find specific techniques and approaches for implementing auxiliary activations in neural networks, focusing on integration with existing architectures and performance implications.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques and approaches for implementing auxiliary activations in neural networks, focusing on integration with existing architectures and performance implications.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.13)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.13)*\n\n```\nOfir Press and Lior Wolf. 2016. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf. Jack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. 2018. Fast parametric learning with activation memorization. arXiv preprint arXiv:1803.10049. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pages 10434-10443. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Noam Shazeer, Joris Pelemans, and Ciprian Chelba. 2014. Skip-gram language modeling using sparse non-negative matrix probability estimation. arXiv preprint arXiv:1412.1454. Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. 2018. Learning longer-term dependencies in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008. Tian Wang and Kyunghyun Cho. 2015. Largercontext language modelling. arXiv preprint arXiv:1511.03729. Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and Lawrence Carin. 2017. Topic compositional neural language model. arXiv preprint arXiv:1712.09783. Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint arXiv:1410.3916. Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. 2016. On multiplicative integration with recurrent neural networks. In Advances in neural information processing systems, pages 2856-2864. Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. 2017. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.04)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.04)*\n\n```\n# Sparse Modular Activation for Efficient Sequence Modeling \n\nLiliang Ren ${ }^{1 *}$ Yang Liu ${ }^{2} \\quad$ Shuohang Wang ${ }^{2} \\quad$ Yichong Xu ${ }^{\\dagger}$<br>Chenguang Zhu ${ }^{2}$ Chengxiang Zhai ${ }^{1}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign $\\quad{ }^{2}$ Microsoft<br>\\{liliang3, czhai\\}@illinois.edu<br>\\{yaliu10, shuowa, chezhu\\}@microsoft.com<br>xuyc11@gmail.com\n\n\n#### Abstract\n\nRecent hybrid models combining Linear State Space Models (SSMs) with selfattention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to suboptimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat. ## 1 Introduction\n\nRecent advance on efficient sequence modeling with State Space Models (SSMs) [GGR21; GDE ${ }^{+}$20; GGGR22; GB22; SWL23] has shown impressive performance for a wide range of tasks across modalities, such as text classification, image recognition and speech recognition. SSMs, as firstorder linear models, defined by a set of input, output, and state variables connected by first-order differential equations, can efficiently capture the recurrent structure in sequential data with carefully designed state matrices and the application of convolutional parallelism [GGR21]. However, they still significantly underperform the self-attention [BCB14; $\\mathrm{VSP}^{+}$17] based model in both language modeling and machine translation [VPSP23] tasks. A recent work [FDS ${ }^{+}$23] reveals that this is due to its deficiency of modeling the second-order pairwise comparisons between the input tokens, and shows that the augmentation of an additional shifted SSM layer can improve SSM's associative recalling\n\n[^0]ability. Furthermore, better quality-efficiency trade-off can be achieved by directly introducing extra self-attention modules to form a hybrid model (e.g. MEGA [MZK ${ }^{+}$23] and Hybrid H3 [FDS ${ }^{+}$23]) that utilizes both the first and the second order inductive biases, i.e., SSM and self-attention. However, the current hybrid models apply the attention modules statically and uniformly to each of the input token regardless the property of the task itself. This can lead to sub-optimal quality-efficiency trade-offs since not all input tokens require second-order modeling and this computation need can vary substantially depending on both its context and the task difficulty. In this paper, we aim to answer the following research questions for efficiently combining attention with SSMs:\n\n- RQ1: Can neural networks learn to activate their attention modules on demand to achieve better quality-efficiency trade-off? - RQ2: How much extra attention is needed for the SSMs on a task-by-task basis? To answer these questions, we develop a new general mechanism, Sparse Modular Activation (SMA), that allows a neural network to sparsely and dynamically activate its sub-modules for each of the input token in a fully differentiable manner. Specifically, we assume a neural model can be composed of multiple heterogeneous sub-modules. For the input sequence, a latent configurator sparsely maps tokens to multiple compressed sequences corresponding to sub-modules. Each sub-module is then only applied on its mapped shorter sequence. Compared with activating all sub-modules on the whole input, Sparse Modular Activation can reduce computation and memory consumption for both the training and inference stages. Notably, SMA is proved to have a full coverage of the combinatorial search space of module activation, which is further explained in Section 3.2. Efficient learning of dynamic sparsity is notoriously difficult under the constraint of the current parallel hardware [LQC ${ }^{+} 22$; GZYE20; XM22]. To enable the practical efficiency gains from our module-level sparsity, we provide a simple yet efficient parallel implementation of SMA without any custom fused GPU kernels. Specifically, when compressing a batch of sequences in SMA, our implementation conducts both token selection and the sequence re-padding simultaneously using a single scatter operation that is widely optimized and present in modern deep learning frameworks. To address RQ1, we apply SMA to construct a novel neural architecture, SeqBoat, that sparsely activate a Gated Attention Unit (GAU) [HDLL22] based on the state representation learned from an SSM. Both the GAU and the SSM representations are then aggregated through simple addition and activation to form a layer-level representation. Multiple same-sized SeqBoat layers are stacked sequentially to form a full neural model. Inspired by the working memory mechanism [AS68] used in human cognition, we further restrict the GAU to only apply local attention on the compressed sequence, which allows our model to have linear sequence inference complexity but theoretically infinite attention span. We conduct comprehensive experiments to show that SeqBoat has significantly better qualityefficiency trade-off than state-of-the-art hybrid models on a wide range of tasks, including Long Range Arena (LRA) [TDA ${ }^{+}$20], speech classification [War18] and language modeling [Hut06]. On the competitive LRA benchmark, SeqBoat achieves 1.96 higher average accuracy than MEGA-chunk $\\left[\\mathrm{MZK}^{+} 23\\right]$, the previous best hybrid model, with a $10.4 \\times$ training speed up and a $95 \\%$ memory reduction compared to the Transformer [VSP ${ }^{+}$17] on the Text task with 4,096 input length. Thanks to the intrinsic modular sparsity brought by SMA, SeqBoat directly reveals the amount of attention needed for each data sample of each task through its sparse activation patterns of GAU, addressing RQ2. We demonstrate that our working memory mechanism provides substantially better computation-accuracy trade-off than chunking based models, and analyze the relationship between the working memory size and the effective attention span on various long sequence modeling tasks. ## 2 Background\n\nTo motivate and clarify our proposed techniques, we first present a mathematical formulation of our Sparse Modular Activation mechanism and show how it encompasses and generalizes previous attempts that aimed for module-level dynamic sparsity. A dedicated section for detailed comparisons between our approach with the related works is also included in Appendix F. We begin by reviewing how the standard sequence modeling is formalized to establish the common ground for our discussion. ### 2.1 Time-Invariant Sequence Modeling\n\nGiven a discrete sequence, $\\mathbf{x}=\\left\\{x_{1}, \\ldots, x_{n}\\right\\} \\in \\mathbb{R}^{n}$, consisting of $n$ tokens, a time-invariant sequence model $P_{\\theta}$ is optimized to maximize the likelihood of the observed sequences by factorizing them as follows:\n\n$$\n\\max _{\\theta} P_{\\theta}(\\mathbf{x})=\\prod_{t=1}^{n} P\\left(x_{t} \\mid \\mathbf{x}_{<t}, \\theta\\right)\n$$\n\nwhere $\\mathbf{x}_{<t}=\\left\\{x_{1}, \\ldots, x_{t-1}\\right\\}$ is the sequence history at time step $t$, and the parameter $\\theta$ is independent of the time step $t$. This formulation implies that the full model parameters $\\theta$ and the full history $\\mathbf{x}_{<t}$ are both essential for the conditional prediction of each token $x_{t}$. However, one potential issue is as the prediction difficulty of each token may differ depending on the context and the position, this static model $P_{\\theta}$ can lead to sub-optimal accuracy-efficiency trade-off by wasting computation on either unimportant context $\\left[\\mathrm{SJP}^{+} 21\\right]$ or easy-to-predict tokens [Gra16]. ## 3 Learning Sparse Modular Activation\n\nTo cover a larger search space that may contain more efficient sequence models, we propose to formulate sequence modeling as a problem of finding an optimal time-variant model that can dynamically activate a subset of modules from a pre-defined function space for each time step. ### 3.1 Time-Variant Sequence Modeling\n\nFormally, a time-variant sequence model is defined on a compact function space $\\mathcal{F}: \\mathcal{X}_{t}^{c} \\mapsto[0,1]^{n \\times V}$, where $V$ is the size of the vocabulary and $\\mathcal{X}_{t}^{c}=\\left\\{\\mathbf{x}_{t}^{c}: \\mathbf{x}_{t}^{c} \\subseteq \\mathbf{x}_{<t} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{n}\\right\\}$, contains all possible sub-sequences of the sequence history $\\mathbf{x}_{<t}$. Then for each of the token prediction at the time step $t$, the model learns to apply a function $f_{t} \\in \\mathcal{F}$ with the parameters $\\theta_{t}$ that maximizes the sequence probability, i.e.,\n\n$$\n\\max _{f_{t}, \\theta_{t}, \\mathbf{x}_{t}^{c}} P_{\\mathcal{F}}(\\mathbf{x})=\\prod_{t=1}^{n} P_{f_{t}}\\left(x_{t} \\mid \\mathbf{x}_{t}^{c}, \\theta_{t}\\right) \\quad \\text { s.t. } \\quad \\mathbf{x}_{t}^{c} \\subseteq \\mathbf{x}_{<t}\n$$\n\nThis formulation generalizes the previous works in pursuing a dynamic and sparse model for sequence modeling, where the connections are further explained in Appendix F. In this work, we assume the function space $\\mathcal{F}$ is chain-structured, i.e., $\\mathcal{F}=\\mathcal{H} \\circ \\mathcal{L}_{N} \\circ \\cdots \\circ \\mathcal{L}_{1} \\circ \\mathcal{E}$, where $\\mathcal{H}: \\mathbb{R}^{n \\times d_{m}} \\mapsto[0,1]^{n \\times V}$ is the classification function, $\\mathcal{E}: \\mathbb{R}^{n} \\mapsto \\mathbb{R}^{n \\times d_{m}}$ is the embedding function, $N$ is the number of intermediate layers, $d_{m}$ is the model size and $\\mathcal{L}: \\mathbb{R}^{n \\times d_{m}} \\mapsto \\mathbb{R}^{n \\times d_{m}}$ is the function space of the intermediate mappings. We further assume that $\\mathcal{L}$ is the spanning set of a finite number of the function $f_{i}^{l}$ with its parameters $\\theta_{i}^{l}$, i.e., $\\mathcal{L}=\\operatorname{span}\\left\\{f_{1}^{l}, \\ldots, f_{M}^{l}\\right\\}$, where $M$ is the number of pre-defined functions. These assumptions justify the design of our Sparse Modular Activation mechanism, which is further explained in the following section. ### 3.2 Sparse Modular Activation\n\nSparse Modular Activation (SMA) introduces a latent configurator at each time step $t$ and each layer of a neural sequence model.\n```\n\n#### 3. Towards mental time travel: a hierarchical memory for reinforcement learning agents (Avg. Score: 0.01)\n\n*Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, Felix Hill*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** Hierarchical Chunk Attention Memory improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures), and is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Abstract:** Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore\"mentally time-travel\"-- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n##### *Relevant Chunk: No. 23/47 (Score: 0.01)*\n\n```\nComputing in Science \\& Engineering, 9 (3):90-95, 2007. doi: $10.1109 / \\mathrm{MCSE} .2007 .55$. [23] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with unsupervised auxiliary tasks. In International Conference on Learning Representations, 2016. [24] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention. arXiv preprint arXiv:2103.03206, 2021. [25] Nan Rosemary Ke, Anirudh Goyal, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, and Yoshua Bengio. Sparse attentive backtracking: Temporal credit assignment through reminding. In Advance in Neural Information Processing Systems, 2018. [26] Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. Generalization through memorization: Nearest neighbor language models. In International Conference on Learning Representations, 2020. [27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [28] Nikita Kitaev, \u0141ukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer.\n```\n\n#### 4. Time-aware Large Kernel Convolutions (Avg. Score: 0.01)\n\n*Vasileios Lioutas, Yuhong Guo*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 28  (*Influential: 0*)\n\n**TL;DR:** Time-aware Large Kernel (TaLK) Convolutions is introduced, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix.\n\n**Abstract:** To date, most state-of-the-art sequence modeling architectures use attention to build generative models for language based tasks. Some of these models use all the available sequence tokens to generate an attention distribution which results in time complexity of $O(n^2)$. Alternatively, they utilize depthwise convolutions with softmax normalized kernels of size $k$ acting as a limited-window self-attention, resulting in time complexity of $O(k{\\cdot}n)$. In this paper, we introduce Time-aware Large Kernel (TaLK) Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of $O(n)$, effectively making the sequence encoding process linear to the number of tokens. We evaluate the proposed method on large-scale standard machine translation, abstractive summarization and language modeling datasets and show that TaLK Convolutions constitute an efficient improvement over other attention/convolution based approaches.\n\n##### *Relevant Chunk: No. 28/31 (Score: 0.01)*\n\n```\nRamachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions, 2017. URL https: / / arxiv . org/abs/1710.05941. Sachan, D. S., Zaheer, M., and Salakhutdinov, R. Revisiting lstm networks for semi-supervised text classification via mixed objective function. In $A A A I, 2019$. Sennrich, R., Haddow, B., and Birch, A. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2016. Shaw, P., Uszkoreit, J., and Vaswani, A. Self-attention with relative position representations. Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018. Shen, T., Zhou, T., Long, G., Jiang, J., and Zhang, C. Bi-directional block self-attention for fast and memoryefficient sequence modeling. In International Conference on Learning Representations, 2018. Springenberg, J. T., Dosovitskiy, A., Brox, T., and Riedmiller, M. Striving for simplicity: The all convolutional net. International Conference on Learning Representations, 2015. Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. Dropout: A simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 2014. Sundermeyer, M., Schl\u00fcter, R., and Ney, H. Lstm neural networks for language modeling. In INTERSPEECH, 2012. Sutskever, I., Vinyals, O., and Le, Q. V. Sequence to sequence learning with neural networks. In Proceedings of the 27th International Conference on Neural Information Processing Systems, 2014. Szegedy, C., Ioffe, S., Vanhoucke, V., and Alemi, A. Inception-v4, inception-resnet and the impact of residual connections on learning.\n```\n\n#### 5. Normalized Attention Without Probability Cage (Avg. Score: 0.00)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 3/28 (Score: 0.00)*\n\n```\nSimilar to layer normalization [1], we introduce trainable gain and bias parameters $g$ and $b$ initialized to 1 and 0 , respectively. However, while [1] introduce gain and bias vectors, we only introduce scalar parameters and broadcast these over the sequence/vector length, as we want the architecture to be independent of the sequence length $N$. Note that while no convex combination can represent the logical XOR, a normalized weighting can - see Appendix Afor the corresponding proof. No Online Logit Normalization (NON): To investigate whether a dynamic normalization of the attention logits is necessary, we also train a model where we use the logits $l_{m}^{i, j}$ directly as attention weights, i.e., $\\boldsymbol{o}_{m}^{i}=\\operatorname{GELU}\\left(\\frac{1}{\\sqrt{N}} \\sum_{j} l_{m}^{i, j} \\cdot \\boldsymbol{v}_{m}^{j}\\right)$. We also replaced the layer normalization after the attention weighting here through a simple scaling factor $\\frac{1}{\\sqrt{N}}$. Note that this also yields an in expectation constant contribution of context at initialization, independent of sequence length. However, the model can easily deviate from it during training. Simple Summation of Embeddings (sum): From a theoretical perspective summation is sufficient for general function approximation [42, 38, 30]. Therefore, we investigate to simply replace attention through a sum-reduce-broadcast operation. Max Pooling over Sequence Dimension (max): Similar to sum pooling, we can replace the attention sub-module through a simple max-reduce-broadcast operation over the sequence dimension. Note that max pooling over the sequence is a powerful operation, as the resulting embedding has a direct link to up to $d$ different tokens. If not varied in a corresponding experiment, we default architecture hyperparameters to $L=2$ Transformer-layers (consisting of an attention sub-module and feed forward sub-module each), $M=4$ heads to calculate the logits (if applicable), $d=128$ as model dimension and train on a total of 3200 batches of 32 example sequences each, using the Adam optimizer [15]. The hidden dimension of the feed forward sub-modules is $4 \\cdot d$ for the models $B E R T, M T E, N A P$ and $N O N$. For the models sum and max we increase the feed forward hidden dimension to approximately match the parameter counts of the other models. ## 5 Experiments and Results\n\nOur goal with this work is to provide an insight into the variety of performance implications that the architecture choices entail. We aim to provide these insights independent of any particular downstream application, as these architectures can be applied to a variety of tasks - from NLP to\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_d96e688fdee0c910d08bg-05.jpg?height=289&width=1367&top_left_y=252&top_left_x=375)\n\nFigure 2: Left: Pseudo code for case distinction task data. The case distinction points 64 and 50 are chosen arbitrarily. Middle/Right: Task setup for outputs across all tokens (middle, cf. Section 5.1) and outputs from the first token (right, cf. Section 5.2). Green boxes represent the trainable network layers (shared across tokens) while red boxes represent the pooling across tokens, the focus of this work. The targets of the displayed examples would be $[0,0,0,1]$ and $[1,0,0,0]$, respectively. graph neural networks to reinforcement learning agents. We therefore focus on carefully crafted synthetic tasks that (1) are general enough in that we can expect the insights to generalize to a large set of downstream tasks and (2) let us modify key aspects that are hidden in real world data sets, such as a bias towards a certain sub-task. The focus on synthetic tasks also allows us to get a better grasp on the learning dynamics - the focus of this work - as we can train thousands of models in diverse hyperparameter combinations. To limit the influence of confounding variables, we generate new data points for every batch.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: auxiliary activation neural network implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Learning with Auxiliary Activation for Memory-Efficient Training\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Sunghyeon Woo, Dongsuk Jeon*\n\n**TL;DR:** This work proposes a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation, and combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation.\n\n**Abstract:** While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. On a Neural Implementation of Brenier's Polar Factorization\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Nina Vesseron, Marco Cuturi*\n\n**TL;DR:** A practical implementation of Brenier's polar factorization theorem is proposed, and possible applications to non-convex optimization problems, as well as sampling of densities that are not log-concave are illustrated.\n\n**Abstract:** In 1991, Brenier proved a theorem that generalizes the polar decomposition for square matrices -- factored as PSD $\\times$ unitary -- to any vector field $F:\\mathbb{R}^d\\rightarrow \\mathbb{R}^d$. The theorem, known as the polar factorization theorem, states that any field $F$ can be recovered as the composition of the gradient of a convex function $u$ with a measure-preserving map $M$, namely $F=\\nabla u \\circ M$. We propose a practical implementation of this far-reaching theoretical result, and explore possible uses within machine learning. The theorem is closely related to optimal transport (OT) theory, and we borrow from recent advances in the field of neural optimal transport to parameterize the potential $u$ as an input convex neural network. The map $M$ can be either evaluated pointwise using $u^*$, the convex conjugate of $u$, through the identity $M=\\nabla u^* \\circ F$, or learned as an auxiliary network. Because $M$ is, in general, not injective, we consider the additional task of estimating the ill-posed inverse map that can approximate the pre-image measure $M^{-1}$ using a stochastic generator. We illustrate possible applications of Brenier's polar factorization to non-convex optimization problems, as well as sampling of densities that are not log-concave.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. EXACT: Scalable Graph Neural Networks Training via Extreme Activation Compression\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Zirui Liu, Kaixiong Zhou, Fan Yang, Li Li, Rui Chen, Xia Hu*\n\n**TL;DR:** An optimized GPU implementation which supports training GNNs with compressed activations for Pytorch Geometric and Pytorch, and a memory-ef\ufb01cient framework called EXACT, which for the first time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations.\n\n**Abstract:** Training Graph Neural Networks (GNNs) on large graphs is a fundamental challenge due to the high memory usage, which is mainly occupied by activations (e.g., node embeddings). Previous works usually focus on reducing the number of nodes retained in memory. In parallel, unlike what has been developed for other types of neural networks, training with compressed activation maps is less explored for GNNs. This extension is notoriously dif\ufb01cult to implement due to the lack of necessary tools in common graph learning packages. To un-leash the potential of this direction, we provide an optimized GPU implementation which supports training GNNs with compressed activations. Based on the implementation, we propose a memory-ef\ufb01cient framework called \u201cEXACT\u201d, which for the \ufb01rst time demonstrates the potential and evaluates the feasibility of training GNNs with compressed activations. We systematically analyze the trade-off among the memory saving, time overhead, and accuracy drop. In practice, EXACT can reduce the memory footprint of activations by up to 32 \u00d7 with 0 . 2 - 0 . 5% accuracy drop and 10 - 25% time overhead across different models and datasets. We implement EXACT as an extension for Pytorch Geometric and Pytorch. In practice, for Pytorch Geometric, EXACT can trim down the hardware requirement of training a three-layer full-batch GraphSAGE on ogbn-products from a 48GB GPU to a 12GB GPU. The code is available at https://github.com/warai-0toko\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 55  (*Influential: 7*)\n\n#### 4. Globally Optimal Training of Neural Networks with Threshold Activation Functions\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Tolga Ergen, Halil Ibrahim Gulluk, Jonathan Lacotte, Mert Pilanci*\n\n**TL;DR:** It is shown that regularized deep threshold network training problems can be equivalently formulated as a standard convex optimization problem, which parallels the LASSO method, provided that the last hidden layer width exceeds a certain threshold.\n\n**Abstract:** Threshold activation functions are highly preferable in neural networks due to their efficiency in hardware implementations. Moreover, their mode of operation is more interpretable and resembles that of biological neurons. However, traditional gradient based algorithms such as Gradient Descent cannot be used to train the parameters of neural networks with threshold activations since the activation function has zero gradient except at a single non-differentiable point. To this end, we study weight decay regularized training problems of deep neural networks with threshold activations. We first show that regularized deep threshold network training problems can be equivalently formulated as a standard convex optimization problem, which parallels the LASSO method, provided that the last hidden layer width exceeds a certain threshold. We also derive a simplified convex optimization formulation when the dataset can be shattered at a certain layer of the network. We corroborate our theoretical results with various numerical experiments.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 5. GACT: Activation Compressed Training for Generic Network Architectures\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael W. Mahoney, Alvin Cheung*\n\n**TL;DR:** GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge is presented, and the convergence of GACT is proved by analyzing a linearized version of ACT's approximate gradient.\n\n**Abstract:** Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT's approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 20  (*Influential: 3*)\n\n### 2 related papers from Papers with Code\n\n#### 1. FLAMO: An Open-Source Library for Frequency-Domain Differentiable Audio Processing\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Vesa V\u00e4lim\u00e4ki, Sebastian J. Schlecht, Karolina Prawda, Gian Marco De Bortoli, Gloria Dal Santo*\n\n**Abstract:** We present FLAMO, a Frequency-sampling Library for Audio-Module Optimization designed to implement and optimize differentiable linear time-invariant audio systems. The library is open-source and built on the frequency-sampling filter design method, allowing for the creation of differentiable modules that can be used stand-alone or within the computation graph of neural networks, simplifying the development of differentiable audio systems. It includes predefined filtering modules and auxiliary classes for constructing, training, and logging the optimized systems, all accessible through an intuitive interface. Practical application of these modules is demonstrated through two case studies: the optimization of an artificial reverberator and an active acoustics system for improved response smoothness.\n\n**Published:** 2024-09-13\n\n\n\n#### 2. A Tandem Learning Rule for Effective Training and Rapid Inference of Deep Spiking Neural Networks\n\n*From Search Query: auxiliary activation neural network implementation*\n\n*Haizhou Li, Guoqi Li, Malu Zhang, Jibin Wu, Yansong Chua, Kay Chen Tan*\n\n**Abstract:** Spiking neural networks (SNNs) represent the most prominent biologically inspired computing model for neuromorphic computing (NC) architectures. However, due to the non-differentiable nature of spiking neuronal functions, the standard error back-propagation algorithm is not directly applicable to SNNs. In this work, we propose a tandem learning framework, that consists of an SNN and an Artificial Neural Network (ANN) coupled through weight sharing. The ANN is an auxiliary structure that facilitates the error back-propagation for the training of the SNN at the spike-train level. To this end, we consider the spike count as the discrete neural representation in the SNN, and design ANN neuronal activation function that can effectively approximate the spike count of the coupled SNN. The proposed tandem learning rule demonstrates competitive pattern recognition and regression capabilities on both the conventional frame-based and event-based vision datasets, with at least an order of magnitude reduced inference time and total synaptic operations over other state-of-the-art SNN implementations. Therefore, the proposed tandem learning rule offers a novel solution to training efficient, low latency, and high accuracy deep SNNs with low computing resources.\n\n**Published:** 2019-07-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using auxiliary activations, here are some specific techniques and approaches that can be integrated with existing architectures, along with their performance implications:\n\n## Integration of Auxiliary Activations\n\n### Forward and Backward Propagation\nThe concept of using auxiliary activations during both forward and backward propagation can significantly reduce memory requirements. As mentioned in the analysis, the algorithm can combine auxiliary activation with output activation during forward propagation, and use only the auxiliary activation during backward propagation. This approach reduces the amount of data that needs to be temporarily stored, thus improving memory efficiency.\n\n### Memory-Efficient Backpropagation\nAuxiliary activations can be used to optimize the backpropagation process. By using auxiliary activations instead of actual input activations during backpropagation, the model can reduce the memory footprint. This is particularly beneficial in autoregressive models where sequential data processing can lead to high memory usage.\n\n## Implementation Considerations\n\n### Balancing Parameter Efficiency and Computational Overhead\nWhen integrating auxiliary activations, it is crucial to balance parameter efficiency with computational overhead. This involves careful design to ensure that the use of auxiliary activations does not significantly increase computational costs while reducing memory usage. Techniques such as compressed activations and structured sparsity patterns can help in achieving this balance.\n\n### Hardware-Aware Design\nImplementing auxiliary activations should be done with hardware-aware design principles in mind. This includes optimizing memory access patterns and leveraging patch-based processing to reduce peak memory usage. For example, patch-by-patch inference scheduling can significantly cut down the peak memory, which is beneficial for autoregressive models.\n\n### Integration with Existing Architectures\nTo integrate auxiliary activations with existing autoregressive architectures, one can follow these steps:\n- Modify the forward propagation to include auxiliary activations.\n- Use only auxiliary activations during backward propagation to reduce temporary storage needs.\n- Ensure that the auxiliary activation functions are chosen such that they do not compromise the expressiveness of the model.\n\n## Performance Implications\n\n### Memory Efficiency\nThe use of auxiliary activations can lead to significant memory savings, which is critical for large autoregressive models. By reducing the amount of data that needs to be stored during backpropagation, these models can be trained more efficiently on hardware with limited memory resources.\n\n### Computational Efficiency\nWhile auxiliary activations can reduce memory usage, they should be implemented in a way that does not introduce substantial computational overhead. Efficient implementation strategies, such as leveraging block-wise operations and careful quantization, can help maintain computational efficiency.\n\n### Expressiveness and Robustness\nThe choice of auxiliary activation functions is crucial to ensure that the model's expressiveness and robustness are not compromised. The functions should be designed to capture the necessary patterns in the data without introducing additional complexity that could hinder the model's performance.\n\nBy incorporating these techniques, researchers can improve the design of autoregressive language models, enhancing their memory efficiency, computational efficiency, and overall performance.\n",
            "analysis": "Analysis Note - Round 4\n\n1. Summary of Analysis:\nAfter analyzing three rounds of search results, several promising directions have emerged for improving the TTT design:\n\na) Memory Optimization Approaches:\n- Compressed activations and bottleneck layers for reducing memory footprint\n- IO-aware implementation strategies from FlashAttention\n- Efficient matrix parameterization using butterfly factorizations\n- Memory-efficient backpropagation techniques\n\nb) Implementation Considerations:\n- Need to balance parameter efficiency with computational overhead\n- Importance of hardware-aware design for memory access patterns\n- Trade-offs between expressiveness and efficiency\n- Integration challenges with existing architectures\n\n2. Useful References with Excerpts:\n\nFrom \"Memory Efficient Optimizers\":\n\"We find that moments have complicated outlier patterns, that current block-wise quantization cannot accurately approximate. We use a smaller block size and propose to utilize both row-wise and column-wise information for better quantization.\"\n\nKey insight: Block-wise operations need careful consideration of quantization and memory patterns.\n\nFrom \"MCUNetV2\":\n\"We find that the memory bottleneck is due to the imbalanced memory distribution... patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory.\"\n\nKey insight: Patch-based processing can significantly reduce peak memory usage.\n\nFrom \"Learning with Auxiliary Activation\":\n\"The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored.\"\n\nKey insight: Auxiliary activations can help reduce memory requirements while maintaining performance.\n\n3. Key Insights and Analysis:\n\na) Memory Efficiency Opportunities:\n- Use of compressed activations for reducing memory footprint\n- Implementation of patch-based processing for better memory distribution\n- Integration of auxiliary activations for efficient backpropagation\n- Structured sparsity patterns for improved cache utilization\n\nb) Technical Integration Points:\n- Block-wise operations with careful quantization strategies\n- Patch-based processing for TTTLinear unit\n- Memory-efficient backpropagation using auxiliary activations\n- IO-aware implementation patterns from FlashAttention\n\nc) Design Considerations:\n- Balance between compression and computational overhead\n- Trade-off between patch size and processing efficiency\n- Integration of auxiliary activations with TTT architecture\n- Hardware-aware implementation strategies\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for integrating auxiliary activations with TTT\n2. Methods for efficient patch-based processing in autoregressive models\n3. Approaches for combining compressed activations with TTT architecture\n\n5. References:\n\n[1] Li, B., Chen, J., & Zhu, J. (2023). Memory Efficient Optimizers with 4-bit States. Neural Information Processing Systems.\n\n[2] Lin, J., Chen, W. M., Cai, H., Gan, C., & Han, S. (2021). MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning. Neural Information Processing Systems.\n\n[3] Woo, S., & Jeon, D. (2023). Learning with Auxiliary Activation for Memory-Efficient Training. International Conference on Learning Representations.\n\n[4] Park, J., Bin, K., Park, G., Ha, S., & Lee, K. (2023). ASPEN: Breaking Operator Barriers for Efficient Parallelization of Deep Neural Networks. Neural Information Processing Systems.\n\n[5] Chakrabarti, A., & Moseley, B. (2019). Backprop with Approximate Activations for Memory-efficient Network Training. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "transformer memory optimization matrix factorization",
            "detail": "Search for papers discussing:\n1. Memory optimization techniques in transformer architectures\n2. Matrix factorization methods for efficient computation\n3. Hardware-efficient implementations of structured operations\n4. Memory-compute trade-offs in language models\n5. Stability analysis of different initialization strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory optimization techniques in transformer architectures\n2. Matrix factorization methods for efficient computation\n3. Hardware-efficient implementations of structured operations\n4. Memory-compute trade-offs in language models\n5. Stability analysis of different initialization strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.62)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.62)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.47)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.47)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 3. Large Memory Layers with Product Keys (Avg. Score: 0.07)\n\n*Guillaume Lample, Alexandre Sablayrolles, Marc'Aurelio Ranzato, Ludovic Denoyer, H. J\u00e9gou*\n\n**Published in:** Neural Information Processing Systems (2019)\t**Cited by** 113  (*Influential: 17*)\n\n**TL;DR:** A structured memory which can be easily integrated into a neural network and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead is introduced.\n\n**Abstract:** This paper introduces a structured memory which can be easily integrated into a neural network. The memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a state-of-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes.\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.07)*\n\n```\nThe memory is very large by design and significantly increases the capacity of the architecture, by up to a billion parameters with a negligible computational overhead. Its design and access pattern is based on product keys, which enable fast and exact nearest neighbor search. The ability to increase the number of parameters while keeping the same computational budget lets the overall system strike a better trade-off between prediction accuracy and computation efficiency both at training and test time. This memory layer allows us to tackle very large scale language modeling tasks. In our experiments we consider a dataset with up to 30 billion words, and we plug our memory layer in a stateof-the-art transformer-based architecture. In particular, we found that a memory augmented model with only 12 layers outperforms a baseline transformer model with 24 layers, while being twice faster at inference time. We release our code for reproducibility purposes ${ }^{3}$\n\n\n## 1 Introduction\n\nNeural networks are commonly employed to address many complex tasks such as machine translation [43], image classification [27] or speech recognition [16]. As more and more data becomes available for training, these networks are increasingly larger [19]. For instance, recent models both in vision [29] and in natural language processing [20, 36, 28] have more than a billion parameters. The higher-capacity enables better modeling of data like natural text or images, and it also improves generalization [41, 33]. Unfortunately, increasing capacity has led to a dramatic increase of computational complexity, both at training and inference time [20]. There is a growing interest in developing architectures with reasonable computational complexity. Recently, there has been some efforts to develop high capacity architectures that operate on a limited computational budget [40, 18]. This is well illustrated by the \"On-device Visual Intelligence Challenge\" [5], which specifically focuses on the complexity/accuracy trade-off for image classification.\n```\n\n#### 4. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.05)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.05)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 5. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.05)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 23/41 (Score: 0.05)*\n\n```\nhutter1. net, 2012. Kao, S.-C., Subramanian, S., Agrawal, G., Yazdanbakhsh, A., and Krishna, T. FLAT: An Optimized Dataflow for Mitigating Attention Bottlenecks. In ASPLOS, 2023. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In ICML, 2020. Kim, S., Mangalam, K., Malik, J., Mahoney, M. W., Gholami, A., and Keutzer, K. Big Little Transformer Decoder. arXiv preprint arXiv:2302.07863, 2023. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J., Zhang, H., and Stoica, I. Efficient Memory Management for Large Language Model Serving with PagedAttention. In SOSP, 2023.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: transformer memory optimization matrix factorization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**TL;DR:** The low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter, is introduced and achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models.\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 3*)\n\n#### 2. Language model compression with weighted low-rank factorization\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Yen-Chang Hsu, Ting Hua, Sung-En Chang, Qiang Lou, Yilin Shen, Hongxia Jin*\n\n**TL;DR:** The Fisher-Weighted SVD method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training.\n\n**Abstract:** Factorizing a large matrix into small matrices is a popular strategy for model compression. Singular value decomposition (SVD) plays a vital role in this compression strategy, approximating a learned matrix with fewer parameters. However, SVD minimizes the squared error toward reconstructing the original matrix without gauging the importance of the parameters, potentially giving a larger reconstruction error for those who affect the task accuracy more. In other words, the optimization objective of SVD is not aligned with the trained model's task accuracy. We analyze this previously unexplored problem, make observations, and address it by introducing Fisher information to weigh the importance of parameters affecting the model prediction. This idea leads to our method: Fisher-Weighted SVD (FWSVD). Although the factorized matrices from our approach do not result in smaller reconstruction errors, we find that our resulting task accuracy is much closer to the original model's performance. We perform analysis with the transformer-based language models, showing our weighted SVD largely alleviates the mismatched optimization objectives and can maintain model performance with a higher compression rate. Our method can directly compress a task-specific model while achieving better performance than other compact model strategies requiring expensive model pre-training. Moreover, the evaluation of compressing an already compact model shows our method can further reduce 9% to 30% parameters with an insignificant impact on task accuracy.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 65  (*Influential: 14*)\n\n#### 3. Block Low-Rank Preconditioner with Shared Basis for Stochastic Optimization\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Jui-Nan Yen, Sai Surya Duvvuri, Inderjit S. Dhillon, Cho-Jui Hsieh*\n\n**TL;DR:** This work proposes approximating the diagonal blocks of the second moment matrix by low-rank matrices and enforcing the same basis for the blocks within each layer within each layer, and designs an algorithm to maintain this shared-basis block low-rank approximation during training.\n\n**Abstract:** Adaptive methods with non-diagonal preconditioning have shown state-of-the-art results on various tasks. However, their computational complexity and memory requirement make it challenging to scale these methods to modern neural network architectures. To address this challenge, some previous works have adopted block-diagonal preconditioners. However, the memory cost of storing the block-diagonal matrix remains substantial, leading to the use of smaller block sizes that ultimately leads to suboptimal performance. To reduce the time and memory complexity without sacri\ufb01cing performance, we propose approximating the diagonal blocks of the second moment matrix by low-rank matrices and enforcing the same basis for the blocks within each layer. We provide theoretical justi\ufb01cation for such basis sharing and design an algorithm to ef\ufb01ciently maintain this shared-basis block low-rank approximation during training. Our results on a deep autoencoder and a Transformer benchmark demonstrate that the proposed method outperforms \ufb01rst-order methods with slightly more time and memory usage, while also achieving competitive or superior performance compared to other second-order methods with less time and memory usage.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Hongyu Ren, H. Dai, Zihang Dai, Mengjiao Yang, J. Leskovec, D. Schuurmans, Bo Dai*\n\n**TL;DR:** Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 69  (*Influential: 7*)\n\n#### 5. Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, D. Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, V. Chaudhary, Shuai Xu, Xia Hu*\n\n**TL;DR:** This work proposes a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient in a stochastic manner.\n\n**Abstract:** With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, neural networks are usually trained using stochastic gradient descent. We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance. Following this motivation, we propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones. By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7$\\times$ peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size. Under the same hardware, WTA-CRS enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n### 2 related papers from Papers with Code\n\n#### 1. How to Retrain Recommender System? A Sequential Meta-Learning Method\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Fuli Feng, Chenxu Wang, Yan Li, Yang Zhang, Meng Wang, Yongdong Zhang, Xiangnan He*\n\n**Abstract:** Practical recommender systems need be periodically retrained to refresh the model with new interaction data. To pursue high model fidelity, it is usually desirable to retrain the model on both historical and new data, since it can account for both long-term and short-term user preference. However, a full model retraining could be very time-consuming and memory-costly, especially when the scale of historical data is large. In this work, we study the model retraining mechanism for recommender systems, a topic of high practical values but has been relatively little explored in the research community. Our first belief is that retraining the model on historical data is unnecessary, since the model has been trained on it before. Nevertheless, normal training on new data only may easily cause overfitting and forgetting issues, since the new data is of a smaller scale and contains fewer information on long-term user preference. To address this dilemma, we propose a new training method, aiming to abandon the historical data during retraining through learning to transfer the past training experience. Specifically, we design a neural network-based transfer component, which transforms the old model to a new model that is tailored for future recommendations. To learn the transfer component well, we optimize the \"future performance\" -- i.e., the recommendation accuracy evaluated in the next time period. Our Sequential Meta-Learning(SML) method offers a general training paradigm that is applicable to any differentiable model. We demonstrate SML on matrix factorization and conduct experiments on two real-world datasets. Empirical results show that SML not only achieves significant speed-up, but also outperforms the full model retraining in recommendation accuracy, validating the effectiveness of our proposals. We release our codes at: https://github.com/zyang1580/SML.\n\n**Published:** 2020-05-27\n\n\n\n#### 2. Sparse Factorization of Large Square Matrices\n\n*From Search Query: transformer memory optimization matrix factorization*\n\n*Zhirong Yang, Lei Cheng, Tong Yu, Ruslan Khalitov*\n\n**Abstract:** Square matrices appear in many machine learning problems and models. Optimization over a large square matrix is expensive in memory and in time. Therefore an economic approximation is needed. Conventional approximation approaches factorize the square matrix into a number matrices of much lower ranks. However, the low-rank constraint is a performance bottleneck if the approximated matrix is intrinsically high-rank or close to full rank. In this paper, we propose to approximate a large square matrix with a product of sparse full-rank matrices. In the approximation, our method needs only $N(\\log N)^2$ non-zero numbers for an $N\\times N$ full matrix. We present both non-parametric and parametric ways to find the factorization. In the former, we learn the factorizing matrices directly, and in the latter, we train neural networks to map input data to the non-zero matrix entries. The sparse factorization method is tested for a variety of synthetic and real-world square matrices. The experimental results demonstrate that our method gives a better approximation when the approximated matrix is sparse and high-rank. Based on this finding, we use our parametric method as a scalable attention architecture that performs strongly in learning tasks for long sequential data and defeats Transformer and its several variants.\n\n**Published:** 2021-09-16\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on memory optimization, matrix factorization, hardware efficiency, and stability analysis, here are some key findings and references from the provided sources:\n\n## Memory Optimization Techniques in Transformer Architectures\n\n- **Neural Attention Memory Models (NAMMs):** This approach introduces a learned network for memory management that improves both performance and efficiency of transformers. NAMMs evolve on top of pre-trained transformers to provide different latent contexts, focusing on the most relevant information for individual layers and attention heads. This method reduces the model's input contexts significantly while improving performance across multiple long-context benchmarks.\n\n- **Memory-Efficient Bidirectional Transformers:** These models utilize parallel layer execution, omission of biases in QKV projections and LayerNorms, and sharding techniques to distribute model parameters across multiple compute devices. These modifications enhance performance while minimizing memory usage, particularly beneficial for handling long sequences.\n\n## Matrix Factorization Methods for Efficient Computation\n\n- **Structured Sparsity and Butterfly Factorization:** While not explicitly mentioned in the provided sources, these techniques are generally known to reduce parameter counts significantly. However, the concept of structured operations is relevant. For instance, the use of memristor-based accelerators for matrix-matrix multiplication in transformers shows promising efficiency gains through approximate analog computation and specific mapping algorithms.\n\n## Hardware-Efficient Implementations of Structured Operations\n\n- **Memristor-Based Transformers:** This approach utilizes memristor crossbars to accelerate self-attention operations in transformers. The design involves mapping weight matrices into memristor conductance matrices and using RRAM-based compare and select logics for softmax calculations. This method achieves a 10\u00d7 acceleration compared to digital counterparts and significantly reduces power consumption and latency.\n\n## Memory-Compute Trade-offs in Language Models\n\n- **NAMMs and Context Reduction:** By learning to manage memory effectively, NAMMs can reduce the input contexts of transformers to a fraction of their original sizes while improving performance. This trade-off between memory and compute resources is crucial for scaling language models efficiently.\n\n- **Memory-Efficient Bidirectional Transformers:** These models balance performance and memory usage through techniques like parallel layer execution and sharding. This balance is essential for handling large models and long sequences efficiently.\n\n## Stability Analysis of Different Initialization Strategies\n\n- While the provided sources do not delve deeply into stability analysis of initialization strategies specifically, the importance of careful initialization is highlighted in the context of memory-efficient and hardware-friendly operations. For example, the stability of training processes in memory-efficient bidirectional transformers is improved through recent normalization techniques proposed by Gilmer et al. (2023).\n\nIn summary, the key insights for improving autoregressive language model design include:\n\n- **Using NAMMs** to optimize memory management and reduce context sizes while improving performance.\n- **Employing memory-efficient bidirectional transformers** with parallel layer execution and sharding techniques.\n- **Utilizing memristor-based accelerators** for efficient matrix operations.\n- **Balancing memory and compute resources** through careful design and optimization.\n- **Ensuring training stability** through appropriate normalization techniques and careful initialization strategies.\n\nThese approaches can help in achieving better performance, efficiency, and scalability in autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about memory-efficient training techniques, test-time adaptation, and structured matrix operations that are relevant to the TTTLinear-AuxAct proposal. Key findings include:\n\na) Memory Efficiency:\n- Auxiliary activation methods show promise for reducing memory requirements\n- Structured sparsity and butterfly factorization offer significant parameter reduction\n- Memory access patterns are crucial for performance optimization\n\nb) Test-Time Adaptation:\n- Recent advances in TTA show the importance of stable adaptation mechanisms\n- Memory-efficient TTA methods are emerging as an important research direction\n- Balancing adaptation capability with memory constraints is crucial\n\nc) Implementation Considerations:\n- Hardware-friendly structured operations are essential\n- Careful initialization is critical for performance\n- Memory I/O remains a key bottleneck\n\n2. Useful References with Excerpts:\n\nFrom \"MECTA: Memory-Economic Continual Test-Time Model Adaptation\":\n\"The major memory overhead comes from the intermediate cache for back-propagation, which scales by the batch size, channel, and layer number.\"\n\nFrom \"FlashFFTConv\":\n\"A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in O(N logN) time in sequence length N but has poor hardware utilization.\"\n\nFrom \"Towards Stable Test-Time Adaptation\":\n\"TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts\"\n\n3. Key Insights:\n\na) Memory Optimization:\n- Intermediate activation storage is a major bottleneck\n- Structured operations can reduce parameter count significantly\n- Memory access patterns crucial for performance\n\nb) Adaptation Stability:\n- Need for robust adaptation mechanisms\n- Importance of initialization strategies\n- Balance between adaptation capability and efficiency\n\nc) Implementation Requirements:\n- Hardware-friendly structured operations essential\n- Memory I/O optimization critical\n- Careful consideration of batch size impacts\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific memory reduction techniques in transformer architectures\n2. Hardware efficiency of different matrix factorization methods\n3. Stability analysis of test-time adaptation methods\n\n5. References:\n\n[1] Hong, J., Lyu, L., Zhou, J., & Spranger, M. (2023). MECTA: Memory-Economic Continual Test-Time Adaptation. International Conference on Learning Representations.\n\n[2] Fu, D.Y., Kumbong, H., Nguyen, E.N.D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[3] Niu, S., Wu, J., Zhang, Y., Wen, Z., Chen, Y., Zhao, P., & Tan, M. (2023). Towards Stable Test-Time Adaptation in Dynamic Wild World. International Conference on Learning Representations.\n\n[4] Wang, Z., Mehta, S.V., P\u00f3czos, B., & Carbonell, J. (2020). Efficient Meta Lifelong-Learning with Limited Memory. Conference on Empirical Methods in Natural Language Processing.\n\n[5] Zhang, Y.F., Wang, X., Jin, K., Yuan, K., Zhang, Z., Wang, L., Jin, R., & Tan, T.P. (2023). AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation. International Conference on Machine Learning."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "auxiliary activation memory efficient training",
            "detail": "Search for papers discussing:\n1. Memory-efficient training techniques for neural networks\n2. Auxiliary activation methods and their impact on model performance\n3. Applications of butterfly factorization in language models\n4. Dual-path architectures for efficient training\n5. Test-time adaptation mechanisms\nFocus on implementations, theoretical analysis, and empirical results.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory-efficient training techniques for neural networks\n2. Auxiliary activation methods and their impact on model performance\n3. Applications of butterfly factorization in language models\n4. Dual-path architectures for efficient training\n5. Test-time adaptation mechanisms\nFocus on implementations, theoretical analysis, and empirical results.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention (Avg. Score: 0.53)\n\n*Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, G. Fung, Yin Li, Vikas Singh*\n\n**Published in:** AAAI Conference on Artificial Intelligence (2021)\t**Cited by** 375  (*Influential: 62*)\n\n**TL;DR:** This work proposes Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length and performs favorably relative to other efficient self-attention methods.\n\n**Abstract:** Transformers have emerged as a powerful tool for a broad range of natural language processing tasks. A key component that drives the impressive performance of Transformers is the self-attention mechanism that encodes the influence or dependence of other tokens on each specific token. While beneficial, the quadratic complexity of self-attention on the input sequence length has limited its application to longer sequences - a topic being actively studied in the community. To address this limitation, we propose Nystr\u00f6mformer - a model that exhibits favorable scalability as a function of sequence length. Our idea is based on adapting the Nystr\u00f6m method to approximate standard self-attention with O(n) complexity. The scalability of Nystr\u00f6mformer enables application to longer sequences with thousands of tokens. We perform evaluations on multiple downstream tasks on the GLUE benchmark and IMDB reviews with standard sequence length, and find that our Nystr\u00f6mformer performs comparably, or in a few cases, even slightly better, than standard self-attention. On longer sequence tasks in the Long Range Arena (LRA) benchmark, Nystr\u00f6mformer performs favorably relative to other efficient self-attention methods. Our code is available at https://github.com/mlpen/Nystromformer.\n\n##### *Relevant Chunk: No. 25/36 (Score: 0.53)*\n\n```\narXiv preprint arXiv:1905.12346. Frostig, R.; Johnson, M. J.; and Leary, C. 2018. Compiling machine learning programs via high-level tracing. Systems for Machine Learning . Howard, J.; and Ruder, S. 2018. Universal Language Model Finetuning for Text Classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (ACL), 328-339. Huang, Y.; Cheng, Y.; Bapna, A.; Firat, O.; Chen, D.; Chen, M.; Lee, H.; Ngiam, J.; Le, Q. V.; Wu, Y.; et al. 2019. Gpipe: Efficient training of giant neural networks using pipeline parallelism. In Advances in Neural Information Processing Systems (NeurIPS), $103-112$. Katharopoulos, A.; Vyas, A.; Pappas, N.; and Fleuret, F. 2020. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In Proceedings of the International Conference on Machine Learning (ICML). Kitaev, N.; Kaiser, L.; and Levskaya, A. 2019. Reformer: The Efficient Transformer. In International Conference on Learning Representations (ICLR). Krizhevsky, A.; Hinton, G.; et al. 2009. Learning multiple layers of features from tiny images. Technical Report TR-2009, University of Toronto . Kumar, S.; Mohri, M.; and Talwalkar, A. 2009. Ensemble Nystr\u00f6m method. In Advances in Neural Information Processing Systems (NeurIPS), 1060-1068. Lample, G.; Sablayrolles, A.; Ranzato, M.; Denoyer, L.; and J\u00e9gou, H. 2019. Large memory layers with product keys. In Advances in Neural Information Processing Systems (NeurIPS), 8548-8559. Lan, Z.; Chen, M.; Goodman, S.; Gimpel, K.; Sharma, P.; and Soricut, R. 2020. ALBERT: A lite BERT for self-supervised learning of language representations. In International Conference on Learning Representations (ICLR). Lee, J.; Lee, Y.; Kim, J.; Kosiorek, A.; Choi, S.; and Teh, Y. W. 2019. Set transformer: A framework for attention-based permutation-invariant neural networks.\n```\n\n#### 2. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.41)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.41)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 3. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.23)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 39/72 (Score: 0.23)*\n\n```\narXiv preprint arXiv:2311.05908, 2023. [68] Markus N Rabe and Charles Staats. Self-attention does not need o $\\left(n^{2}\\right)$ memory. arXiv preprint $\\operatorname{arXiv:2112.05682,2021.}$\n[69] Hanhwi Jang, Joonsung Kim, Jae-Eon Jo, Jaewon Lee, and Jangwoo Kim. Mnnfast: A fast and scalable system architecture for memory-augmented neural networks. In 2019 ACM/IEEE 46 th Annual International Symposium on Computer Architecture (ISCA), pages 250-263, 2019. [70] Hao Liu and Pieter Abbeel. Blockwise parallel transformer for long context large models. arXiv preprint arXiv:2305.19370, 2023. [71] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc Le. Transformer quality in linear time. In International Conference on Machine Learning, pages 9099-9117. PMLR, 2022. [72] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models.\n```\n\n#### 4. A Faster and Better Large Language Model with Improved TransNormer (Avg. Score: 0.16)\n\n*Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, Y. Qiao, Yiran Zhong*\n\n**Published in:**  (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** TransNormerLLM is presented, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency and develops a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length.\n\n**Abstract:** We present TransNormerLLM, the first linear attention-based Large Language Model (LLM) that outperforms conventional softmax attention-based models in terms of both accuracy and efficiency. TransNormerLLM evolves from the previous linear attention architecture TransNormer by making advanced modifications that include positional embedding, linear attention acceleration, gating mechanisms, tensor normalization, and inference acceleration and stabilization. Specifically, we use LRPE together with an exponential decay to avoid attention dilution issues while allowing the model to retain global interactions between tokens. Additionally, we propose Lightning Attention, a cutting-edge technique that accelerates linear attention by more than twice in runtime and reduces memory usage by a remarkable four times. To further enhance the performance of TransNormer, we leverage a gating mechanism for smooth training and a new tensor normalization scheme to accelerate the model, resulting in an impressive acceleration of over $20\\%$. Furthermore, we develop a robust inference algorithm that ensures numerical stability and consistent inference speed, regardless of the sequence length, showcasing superior efficiency during both training and inference stages. We also implement an efficient model parallel schema for TransNormerLLM, enabling seamless deployment on large-scale clusters and facilitating expansion to even more extensive models, i.e., LLMs with 175B parameters. We validate our model design through a series of ablations and train models with sizes of 385M, 1B, and 7B on our self-collected corpus. Benchmark results demonstrate that our models not only match the performance of state-of-the-art LLMs with Transformer but are also significantly faster. Code is released at: https://github.com/OpenNLPLab/TransnormerLLM.\n\n##### *Relevant Chunk: No. 19/32 (Score: 0.16)*\n\n```\narXiv preprint arXiv:2202.10447, 2022. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models, 2023. Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja Vooturi, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, et al. A study of bfloat16 for deep learning training. arXiv preprint arXiv:1905.12322, 2019. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Guolin Ke, Di He, and Tie-Yan Liu. Rethinking positional encoding in language pre-training. In International Conference on Learning Representations, 2021. URL https://openreview. net/forum?id=09-528y2Fgf\n\nDiederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization, 2017. Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Ves Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension, 2019. Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese, 2023. Zexiang Liu, Dong Li, Kaiyue Lu, Zhen Qin, Weixuan Sun, Jiacheng Xu, and Yiran Zhong. Neural architecture search on efficient transformers and beyond. arXiv preprint arXiv:2207.13955, 2022. Paulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia, Boris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision training.\n```\n\n#### 5. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.14)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.14)*\n\n```\nOfir Press and Lior Wolf. 2016. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf. Jack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. 2018. Fast parametric learning with activation memorization. arXiv preprint arXiv:1803.10049. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pages 10434-10443. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Noam Shazeer, Joris Pelemans, and Ciprian Chelba. 2014. Skip-gram language modeling using sparse non-negative matrix probability estimation. arXiv preprint arXiv:1412.1454. Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. 2018. Learning longer-term dependencies in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008. Tian Wang and Kyunghyun Cho. 2015. Largercontext language modelling. arXiv preprint arXiv:1511.03729. Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and Lawrence Carin. 2017. Topic compositional neural language model. arXiv preprint arXiv:1712.09783. Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint arXiv:1410.3916. Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. 2016. On multiplicative integration with recurrent neural networks. In Advances in neural information processing systems, pages 2856-2864. Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. 2017. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: auxiliary activation memory efficient training\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Learning with Auxiliary Activation for Memory-Efficient Training\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Sunghyeon Woo, Dongsuk Jeon*\n\n**TL;DR:** This work proposes a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation, and combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation.\n\n**Abstract:** While deep learning has achieved great success in various fields, a large amount of memory is necessary to train deep neural networks, which hinders the development of massive state-of-the-art models. The reason is the conventional learning rule, backpropagation, should temporarily store input activations of all the layers in the network. To overcome this, recent studies suggested various memory-efficient implementations of backpropagation. However, those approaches incur computational overhead due to the recomputation of activations, slowing down neural network training. In this work, we propose a new learning rule which significantly reduces memory requirements while closely matching the performance of backpropagation. The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored. We mathematically show that our learning rule can reliably train the networks if the auxiliary activation satisfies certain conditions. Based on this observation, we suggest candidates of auxiliary activation that satisfy those conditions. Experimental results confirm that the proposed learning rule achieves competitive performance compared to backpropagation in various models such as ResNet, Transformer, BERT, ViT, and MLP-Mixer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Changhyeon Lee, Seulki Lee*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. DIVISION: Memory Efficient Training via Dual Activation Precision\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Guanchu Wang, Zirui Liu, Zhimeng Jiang, Ninghao Liu, Nannan Zou, Xia Hu*\n\n**TL;DR:** DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision during the training, which can significantly reduce the memory cost without negatively affecting the precision of backward propagation.\n\n**Abstract:** Activation compressed training provides a solution towards reducing the memory cost of training deep neural networks~(DNNs). However, state-of-the-art work combines a search of quantization bit-width with the training, which makes the procedure complicated and less transparent. To this end, we propose a simple and effective method to compress DNN training. Our method is motivated by an instructive observation: DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training. This indicates the HFC of activation maps is highly redundant and compressible during DNN training, which inspires our proposed Dual Activation Precision (DIVISION). During the training, DIVISION preserves the high-precision copy of LFC and compresses the HFC into a light-weight copy with low numerical precision. This can significantly reduce the memory cost without negatively affecting the precision of backward propagation such that DIVISION maintains competitive model accuracy. Experiment results show DIVISION has better comprehensive performance than state-of-the-art methods, including over 10x compression of activation maps and competitive training throughput, without loss of model accuracy.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Sunghyeon Woo, Sunwoo Lee, Dongsuk Jeon*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Baohao Liao, Shaomu Tan, C. Monz*\n\n**TL;DR:** This paper proposes memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training.\n\n**Abstract:** Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n### 2 related papers from Papers with Code\n\n#### 1. Low-Rank Quantization-Aware Training for LLMs\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Markus Nagel, Riccardo Del Chiaro, Yelysei Bondarenko*\n\n**Abstract:** Large language models (LLMs) are omnipresent, however their practical deployment is challenging due to their ever increasing computational and memory demands. Quantization is one of the most effective ways to make them more compute and memory efficient. Quantization-aware training (QAT) methods, generally produce the best quantized performance, however it comes at the cost of potentially long training time and excessive memory usage, making it impractical when applying for LLMs. Inspired by parameter-efficient fine-tuning (PEFT) and low-rank adaptation (LoRA) literature, we propose LR-QAT -- a lightweight and memory-efficient QAT algorithm for LLMs. LR-QAT employs several components to save memory without sacrificing predictive performance: (a) low-rank auxiliary weights that are aware of the quantization grid; (b) a downcasting operator using fixed-point or double-packed integers and (c) checkpointing. Unlike most related work, our method (i) is inference-efficient, leading to no additional overhead compared to traditional PTQ; (ii) can be seen as a general extended pretraining framework, meaning that the resulting model can still be utilized for any downstream task afterwards; (iii) can be applied across a wide range of quantization settings, such as different choices quantization granularity, activation quantization, and seamlessly combined with many PTQ techniques. We apply LR-QAT to LLaMA-1/2/3 and Mistral model families and validate its effectiveness on several downstream tasks. Our method outperforms common post-training quantization (PTQ) approaches and reaches the same model performance as full-model QAT at the fraction of its memory usage. Specifically, we can train a 7B LLM on a single consumer grade GPU with 24GB of memory. Our source code is available at https://github.com/qualcomm-ai-research/LR-QAT\n\n**Published:** 2024-06-10\n\n\n\n#### 2. Automatic Recall Machines: Internal Replay, Continual Learning and the Brain\n\n*From Search Query: auxiliary activation memory efficient training*\n\n*Xu Ji, Tinne Tuytelaars, Joao Henriques, Andrea Vedaldi*\n\n**Abstract:** Replay in neural networks involves training on sequential data with memorized samples, which counteracts forgetting of previous behavior caused by non-stationarity. We present a method where these auxiliary samples are generated on the fly, given only the model that is being trained for the assessed objective, without extraneous buffers or generator networks. Instead the implicit memory of learned samples within the assessed model itself is exploited. Furthermore, whereas existing work focuses on reinforcing the full seen data distribution, we show that optimizing for not forgetting calls for the generation of samples that are specialized to each real training batch, which is more efficient and scalable. We consider high-level parallels with the brain, notably the use of a single model for inference and recall, the dependency of recalled samples on the current environment batch, top-down modulation of activations and learning, abstract recall, and the dependency between the degree to which a task is learned and the degree to which it is recalled. These characteristics emerge naturally from the method without being controlled for.\n\n**Published:** 2020-06-22\n\n\n\n\n\n---\n## Web search results\n\n To address the researcher's goals and questions, here are some key findings and references related to memory-efficient training techniques, auxiliary activation methods, and other relevant aspects:\n\n## Memory-Efficient Training Techniques\n\n### Batch Processing and Memory Optimization\n- The use of batch processing is a common technique to manage memory constraints during neural network training. This involves processing data in chunks rather than all at once, which can significantly reduce memory usage. This method is discussed in the context of training neural networks with limited memory.\n\n### Efficient Optimizer States\n- The GaLore algorithm and its adaptation, Natural GaLore, are designed to reduce the memory footprint during the training of large language models (LLMs). These methods exploit the low-rank structure of gradients to approximate optimizer states, leading to significant memory savings without compromising model performance.\n\n### Distributed and Parallel Training\n- Techniques such as Distributed Data-Parallel (DDP), model parallelism, pipeline parallelism, and tensor parallelism are used to distribute the training process across multiple devices, reducing the memory requirements on individual devices. These methods are crucial for training large models efficiently.\n\n## Auxiliary Activation Methods\n\n### Novel Dual-Path Activation Scheme\n- While the provided sources do not specifically discuss a \"novel dual-path activation scheme,\" the concept of auxiliary activation mechanisms can be inferred from the context of efficient training methods. For instance, the use of efficient activation functions and compression techniques can be part of auxiliary activation methods. Researching existing activation compression techniques and their impact on model expressiveness would be essential.\n\n### Impact on Model Performance\n- The impact of auxiliary activation mechanisms on model performance would typically involve analyzing how these mechanisms affect the model's expressiveness and training efficiency. This could involve theoretical analysis on how these mechanisms reduce memory usage during backpropagation and empirical results comparing their performance to traditional activation methods.\n\n## Applications of Butterfly Factorization\n\n### Structured Matrix Parameterization\n- Butterfly factorization is not explicitly mentioned in the provided sources, but it is a technique used in structured matrix parameterization. This method can provide computational efficiency gains by reducing the number of parameters needed to represent matrices. Researching papers on efficient matrix multiplication methods and sparse computation techniques would be relevant to understanding its application in language models.\n\n## Dual-Path Architectures for Efficient Training\n\n### Dual-Path Design\n- The concept of dual-path architectures is part of the researcher's proposal, but it is not directly discussed in the provided sources. However, the idea of using parallel and distributed training techniques can be seen as a form of dual-path design, where different parts of the model or data are processed in parallel to enhance efficiency.\n\n## Test-Time Adaptation Mechanisms\n\n### Impact on Adaptation Capabilities\n- Test-time adaptation mechanisms, such as those involving dual-path architectures, would need to be evaluated for their impact on model stability and adaptation capabilities. This involves examining how the model adapts to new data during test time and ensuring that the adaptation does not compromise the model's performance or stability.\n\n### Key References\n\n- **Natural GaLore**: This method reduces memory usage by approximating optimizer states, which is crucial for memory-efficient training of LLMs.\n- **Distributed and Parallel Training**: These techniques are essential for managing the memory footprint and computational demands of large models.\n- **Data Augmentation and Efficiency Metrics**: While not directly related to auxiliary activation, data augmentation techniques and efficient metrics can help in understanding how to optimize model training and performance.\n\nTo further address the researcher's questions and goals, here are some steps and areas to focus on:\n\n### Phase 1: Memory Efficiency Techniques\n- Research existing auxiliary activation methods and their impact on model performance.\n- Analyze memory-efficient training approaches such as Natural GaLore and distributed training techniques.\n- Study activation compression techniques and their effects on model expressiveness.\n\n### Phase 2: Structured Matrices\n- Investigate the theoretical guarantees and empirical results of butterfly factorization in language models.\n- Research efficient matrix multiplication methods and sparse computation techniques.\n\n### Phase 3: Test-Time Adaptation\n- Examine the impact of dual-path architectures on test-time adaptation capabilities.\n- Research similar dual-path architectures and analyze the stability of test-time updates.\n\nBy focusing on these areas, the researcher can establish a comprehensive understanding of how to improve the autoregressive language model design with respect to memory efficiency, auxiliary activation methods, and test-time adaptation.\n",
            "analysis": "Initial Analysis of TTTLinear-AuxAct Proposal:\n\n1. Key Components to Investigate:\n   a) Auxiliary Activation Mechanism\n   - Novel dual-path activation scheme\n   - Memory efficiency claims during backpropagation\n   - Impact on model expressiveness\n   \n   b) Butterfly Factorization Integration\n   - Structured matrix parameterization\n   - Computational efficiency gains\n   - Trade-offs with model capacity\n   \n   c) Memory Optimization Strategy\n   - Memory footprint reduction techniques\n   - I/O bottleneck mitigation\n   - Training stability\n\n2. Critical Questions:\n   - How does the auxiliary activation compare to existing memory-efficient training methods?\n   - What are the theoretical guarantees for butterfly factorization in this context?\n   - How does the dual-path design affect test-time adaptation capabilities?\n   - What is the actual memory-computation trade-off?\n\n3. Investigation Plan:\n   a) Phase 1: Memory Efficiency Techniques\n   - Research existing auxiliary activation methods\n   - Analyze memory-efficient training approaches\n   - Study activation compression techniques\n   \n   b) Phase 2: Structured Matrices\n   - Investigate butterfly factorization in language models\n   - Research efficient matrix multiplication methods\n   - Study sparse computation techniques\n   \n   c) Phase 3: Test-Time Adaptation\n   - Examine impact on adaptation capabilities\n   - Research similar dual-path architectures\n   - Analyze stability of test-time updates\n\n4. Initial Insights:\n   - The proposal combines memory efficiency with test-time adaptation in a novel way\n   - The dual-path design could offer interesting training-inference trade-offs\n   - Butterfly factorization might provide significant computational benefits\n   - Memory optimization strategy seems theoretically sound\n\n5. Potential Concerns:\n   - Implementation complexity might offset efficiency gains\n   - Additional hyperparameters could complicate training\n   - Need to verify memory reduction claims\n   - Impact on model quality needs careful evaluation\n\nLet's start with searching for memory-efficient training techniques and auxiliary activation methods to establish a baseline for comparison."
        },
        {
            "ready": false,
            "query": "butterfly factorization transformer structured matrices",
            "detail": "Search for papers discussing:\n1. Applications of butterfly factorization in transformer models\n2. Structured matrix operations in language models\n3. Memory-computation trade-offs in transformer architectures\n4. Empirical results of structured matrices in practice\n5. Hardware efficiency of different matrix factorization methods",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Applications of butterfly factorization in transformer models\n2. Structured matrix operations in language models\n3. Memory-computation trade-offs in transformer architectures\n4. Empirical results of structured matrices in practice\n5. Hardware efficiency of different matrix factorization methods\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.47)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 35/35 (Score: 0.76)*\n\n```\nWith the exception of learning rate, hyperparameters such as batch size 50, validation set comprising $15 \\%$ of training data, and fixed momentum at 0.9 are fixed as reported in Appendix F. 1 of their paper. For the BP methods, the learning rate was tested for the values $\\{0.005,0.01,0.02,0.05,0.1,0.2\\}$; parameters outside this range were found to be ineffective. For each method, Table 1 reports the test accuracy of the model with the highest validation accuracy. ## C. 3 Resnet\n\nWe build on top of the standard ResNet18 model from PyTorch. ${ }^{6}$ The model is modified for CIFAR-10 by reducing the kernel size and stride for the initial convolution to 3 and 1 respectively, and removing the first max pool layer. Weight decay of $\\lambda=0.0002$ was used. The learning rate was initialized in $\\{0.1,0.2\\}$,\n\n[^4]and decayed by $\\{0.1,0.2\\}$ every 25 epochs for 100 epochs total. For each method, Table 2 reports the mean and standard deviation of the test accuracies for the hyperparameters with the highest average validation accuracy. ## C. 4 Speed Comparison\n\nIn Section 4.3, we benchmark the speed of training and inference of butterfly factorizations. For training, we compare our CUDA implementation of the fast algorithm for butterfly matrices with dense matrix-matrix multiply (GEMM from cuBLAS) and FFT (from cuFFT). The batch size is 256, and we measure the total time of the forward and backward pass. The experiment is run on a Tesla P100 GPU with 16 GB of memory. For inference, we compare our simple Python implementation of the fast algorithm for the BP parameterization, against dense matrix-vector multiplication (GEMV), FFT, DCT, and DST. Our BP parameterization here refers to the product of a butterfly matrix $B^{(N)}$ and a fixed permutation $P^{(N)}$ (say, learned from data). We use the standard dense matrix-vector multiplication implementation in Numpy (BLAS binding), the FFT implementation from Numpy and the DCT and DST implementation from Scipy (FFTPACK binding). We compare their speed in single-threaded mode, running on a server Intel Xeon CPU E5-2690 v4 at 2.60GHz. Results are shown in Figure 4\n\n## D BP Hierarchy\n\nIn Definition 1. we defined the notion of a BP hierarchy, which we believes captures a natural class of matrices. To this point, we offer the following observations, the latter left as a conjecture, about the expressiveness of this hierarchy, supplementing the inclusion results of Proposition 1. Proposition 3. For every fixed $c \\geq 1$, there is a sufficiently large $N$ such that there is an $N \\times N$ matrix $M_{N}$ that is in $(B P)^{c+1}$ but not in $(B P)^{c}$. Proof. Given $c$, fix $N$ such that $N$ is even and such that $c<\\frac{N}{8 \\log _{2} N}$. For sake of contradiction, assume that every $N \\times N$ matrix in (BP) ${ }^{c+1}$ is also in $(\\mathrm{BP})^{c}$. Let $A$ be an arbitrary $\\frac{N}{2} \\times \\frac{N}{2}$ matrix. Then, from Proposition 1, $A$ is in $(\\mathrm{BP})_{2}^{2 N+10}$. Therefore, from Definition 1, there is some $N \\times N$ matrix $M \\in(\\mathrm{BP})^{2 N+10}$ such that the upper-left $\\frac{N}{2} \\times \\frac{N}{2}$ entries are $A$. From our assumption, we can replace the first $c+1$ BP factors in $M$ with $c$ (possibly different) BP factors. We can repeat this process until we are left with $c$ (BP) factors, so $M$ in $(\\mathrm{BP})^{c}$. This representation for $M$ has $c \\cdot 2 N \\log _{2} N$ parameters, which must be less than $\\frac{N}{8 \\log _{2} N} \\cdot 2 N \\log _{2} N=\\frac{N^{2}}{4}$ based on how we fixed $N$ above. However, $A$ (and therefore $M$ ) has $\\frac{N^{2}}{4}$ arbitrary entries, contradicting that it can be represented with fewer than $\\frac{N^{2}}{4}$ parameters. Hence, there must be some $N \\times N$ matrix in $(\\mathrm{BP})^{c+1}$ that is not in $(\\mathrm{BP})^{c}$. Conjecture 1. Let $M$ be an $N \\times N$ matrix such that for any $x \\in \\mathcal{F}^{N}, M x$ can be computed with an arithmetic circuit of size $N$ poly $\\log (N)$ and depth poly $\\log (N)$. Then, $M$ is in $(B P)_{O(1)}^{\\mathrm{poly} \\log N}$. We believe that we can prove an approximation of the above using known approximations of the Jacobi transform by the DCT (up to some scaling) 41. It is known that such transforms have an arithmetic circuit of the kind mentioned in the conjecture above $[12$. [^0]:    ${ }^{1}$ This characterization was equivalently known in the language of arithmetic circuits [3]. [^1]:    ${ }^{2}$ For simplicity, we assume that $N$ is a power of 2 . Otherwise, the input can be padded with zeros. [^2]:    ${ }^{3}$ Code to reproduce experiments and plots is available at https://github.com/HazyResearch/butterfly\n\n[^3]:    ${ }^{4}$ Although there is an extra addition, this can also be written as a sparse product of 3 matrices by adding auxiliary identity blocks. [^4]:    ${ }^{5}$ Available at https://github.com/HazyResearch/structured-nets\n    ${ }^{6}$ Available at https://github.com/pytorch/vision/blob/master/torchvision/models/resnet.py\n\n\n```\n\n##### *Relevant Chunk: No. 3/35 (Score: 0.19)*\n\n```\nHowever, we found that these tricks are not necessary. For example, the learned transforms in Section 4.1 typically put weight at least 0.99 on a permutation. Initialization As the BP or BPBP construction is a product of many matrices, proper initialization is crucial to avoid exponential blowup in the size of the entries or condition numbers (i.e., the exploding/vanishing gradient problem [37]). We aim to initialize each butterfly factor to be close to unitary or orthogonal, so that the magnitude of the inputs and outputs to the transform are preserved. This is easy since each of the factors $B_{N}, \\ldots, B_{2}$ has exactly two nonzeros in each row and column; for example in the real case, initializing each entry of $B_{k}$ as $\\mathcal{N}(0,1 / 2)$ guarantees $\\mathbb{E} B_{k}^{*} B_{k}=I_{N}$. Comparison to related methods Some previous works have examined similar butterfly matrices in numerical algebra or machine learning [36, 20, 32, mainly motivated by trying to parametrize cheap orthogonal matrices. Our parametrization, motivated by the goal of learning recursive transforms, differs in several ways from all previous works: 1 . We explicitly model and learn a permutation matrix $P$. 2. Our relaxation does not enforce the matrix to be orthogonal. 3. Our butterfly factors are ordered so that closer elements interact first (Figure 1), whereas some works (e.g. [32]) reverse the order. 4. Every work has a different weight-tying scheme; ours ties the blocks in each butterfly factor, leading to fewer parameters and a tighter recursive interpretation than for example [20]. Our main baseline for deep learning experiments is Thomas et al. 42, who define a special matrix class with a complicated recursive algorithm. While our BP method and theirs share some overlap (e.g., they both capture circulant matrices), they have a distinct parametrization, and the exact relation between the BP hierarchy and their LDR-SD or LDR-TD classes is unknown. From a practical standpoint, BP is significantly faster and simpler to implement than their methods. ### 3.3 Expressivity and the butterfly hierarchy\n\nThe butterfly matrix $B$ has a total of $4 N$ learnable parameters (the butterfly factors $B_{N}, B_{N / 2}, \\ldots, B_{2}$ have $2 N, N, \\ldots, 4$ entries respectively). The overall permutation $P$ has $3 \\log _{2} N$ learnable parameters; we can also tie the logits of the $\\log _{2} N$ probabilistic permutations-reflecting the fact that for some algorithms the reduction from size $N$ to $N / 2$ is self-similar to the reduction from size $N / 2^{k}$ to $N / 2^{k+1}$ reducing this to just 3 parameters. We can define a natural hierarchy of matrix classes built on the BP primitive. This hierarchy covers a spectrum ranging from extremely structured matrices with a linear number of parameters, to the entire space of square matrices. Definition 1. For any dimension $N$, let $(\\mathrm{BP})_{r}^{k}(k, r \\in \\mathbb{N})$ denote the classes of matrices that can be expressed as\n\n$$\nS\\left(\\prod_{i=1}^{k} B_{i} P_{i}\\right) S^{T}\n$$\n\nwhere each $B_{i} P_{i} \\in \\mathbb{F}^{r N \\times r N}$ is a BP module as in equation (2), and $S \\in \\mathbb{F}^{N \\times r N}=\\left[\\begin{array}{llll}I_{N} & 0 & \\ldots & 0\\end{array}\\right]$ (that is, $S$ and $S^{T}$ select the upper left $N \\times N$ entries of the BP product matrix). The subscript $r$ is understood to be 1 if omitted. Note that the BP and BPBP classes are equivalent to (BP) ${ }^{1}$ and (BP) ${ }^{2}$ respectively. We remark that $B$ and $P$ are both capable of being the identity, and thus $(\\mathrm{BP})^{k} \\subseteq(\\mathrm{BP})^{k+1}$. The BP hierarchy is expressive enough to theoretically represent many important transforms with low depth, as well as all matrices with linear depth:\n\nProposition 1. $(B P)^{1}$ captures the fast Fourier transform, the fast Hadamard transform, and their inverses exactly. $(B P)^{2}$ captures the DCT, DST, and convolution exactly. All $N \\times N$ matrices are contained in $(B P)_{2}^{4 N+10}$. Proposition 1 is shown in Appendix B. We suggest some additional conjectures about the expressiveness of the BP hierarchy in Appendix D. Even though the BP parameterization is expressive, it still retains the learnability characteristic of compressed parameterizations. In fact, neural networks comprising layers of BP and BPBP matrices still have VC dimension that is almost linear in the number of parameters (Appendix B), similar to networks with fully-connected layers [1, 17] and LDR 42, which implies a corresponding sample complexity bound. ## 4 Empirical Evaluation\n\nWe evaluate the proposed approach to verify that our butterfly parameterization can both recover fast transforms and be integrated as an effective component in ML pipelines ${ }^{3}$, In Section 4.1, we confirm that it automatically learns the fast algorithms for many discrete transforms commonly used in signal processing and machine learning. Section 4.2 further shows that it can be a useful component to increase the performance of deep learning models while ensuring fast multiplication and few parameters by design. ### 4.1 Discrete Transforms\n\nBelow we list several important classes of structured matrices. Some of them are directly captured by our parametrization and we expect that they can be recovered close to perfectly, thus providing a $O(N \\log N)$ algorithm that closely approximates the naive $O\\left(N^{2}\\right)$ matrix multiplication. Others are not perfectly captured by the BPBP class but still have recursive structure; for these, we expect that our method reconstructs them better than standard matrix compression methods (sparse, low-rank, and combinations) can. Transforms We describe the matrices we evaluate on and their applications; a standard reference is Proakis 38.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.33)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 22/46 (Score: 0.33)*\n\n```\nIn International Conference on Machine Learning, pages 5547-5569. PMLR, 2022 . [34] Yuli Eidelman and Israel Gohberg. On a new class of structured matrices. Integral Equations and Operator Theory, 34(3):293-324, 1999. [35] Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E Papka, Rick Stevens, Laura Florescu, Sumti Jairath, William Liu, Tejas Nama, and Arvind Sujeeth. Accelerating scientific applications with sambanova reconfigurable dataflow architecture. Computing in Science 8 Engineering, 23(2):114-119, 2021 . [36] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. Multi-head state space model for speech recognition. arXiv preprint arXiv:2305.12498, 2023. [37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022 . [38] Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 2021. [39] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [40] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.\n```\n\n#### 3. Scatterbrain: Unifying Sparse and Low-rank Attention Approximation (Avg. Score: 0.31)\n\n*Beidi Chen, Tri Dao, Eric Winsor, Zhao Song, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 93  (*Influential: 11*)\n\n**TL;DR:** Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, Scatterbrain is proposed, a novel way to unify sparse and low-rank attention for accurate and efficient approximation and is unbiased with provably low error.\n\n**Abstract:** Recent advances in efficient Transformers have exploited either the sparsity or low-rank properties of attention matrices to reduce the computational and memory bottlenecks of modeling long sequences. However, it is still challenging to balance the trade-off between model quality and efficiency to perform a one-size-fits-all approximation for different tasks. To better understand this trade-off, we observe that sparse and low-rank approximations excel in different regimes, determined by the softmax temperature in attention, and sparse + low-rank can outperform each individually. Inspired by the classical robust-PCA algorithm for sparse and low-rank decomposition, we propose Scatterbrain, a novel way to unify sparse (via locality sensitive hashing) and low-rank (via kernel feature map) attention for accurate and efficient approximation. The estimation is unbiased with provably low error. We empirically show that Scatterbrain can achieve 2.1x lower error than baselines when serving as a drop-in replacement in BigGAN image generation and pre-trained T2T-ViT. On a pre-trained T2T Vision transformer, even without fine-tuning, Scatterbrain can reduce 98% of attention memory at the cost of only 1% drop in accuracy. We demonstrate Scatterbrain for end-to-end training with up to 4 points better perplexity and 5 points better average accuracy than sparse or low-rank efficient transformers on language modeling and long-range-arena tasks.\n\n##### *Relevant Chunk: No. 37/51 (Score: 0.31)*\n\n```\narXiv preprint arXiv:2101.11986, 2021. [70] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. [71] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. arXiv preprint arXiv:2107.02192, 2021. ## Appendix\n\nTable of Contents\nA Extended Related Work ..... 17\nA. 1 Robust PCA ..... 17\nA. 2 Efficient Transformers ..... 17\nA. 3 Locality Sensitive Hashing for Efficient Neural Network Training ..... 18\nA. 4 Structured Matrices for Efficient Machine Learning Models ..... 18\nB Motivating Observations: Low-rank and Sparse Structures of Attention Matrices .....\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.17)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 47/86 (Score: 0.17)*\n\n```\nIn: arXiv preprint arXiv:2402.19427 (2024). [30] Christopher De Sa, Albert Gu, Rohan Puttagunta, Christopher R\u00e9, and Atri Rudra. \"A Two-Pronged Progress in Structured Dense Matrix Vector Multiplication\". In: Proceedings of the Twenty-Ninth Annual ACM-SIAM Symposium on Discrete Algorithms. SIAM. 2018, pp. 1060-1079. [31] Hantian Ding, Zijian Wang, Giovanni Paolini, Varun Kumar, Anoop Deoras, Dan Roth, and Stefano Soatto. \"Fewer truncations improve language modeling\". In: arXiv preprint arXiv:2404.10830 (2024). [32] Yuli Eidelman and Israel Gohberg. \"On a new class of structured matrices\". In: Integral Equations and Operator Theory 34.3 (1999), pp. 293-324. [33] Dan Fu, Simran Arora, Jessica Grogan, Isys Johnson, Evan Sabri Eyuboglu, Armin Thomas, Benjamin Spector, Michael Poli, Atri Rudra, and Christopher R\u00e9. \"Monarch mixer: A simple sub-quadratic gemm-based architecture\".\n```\n\n\n\n---\n## Found 4 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: butterfly factorization transformer structured matrices\n\nConsidering refining your search by improving the query keywords input.\n\n### 3 related papers from Semantic Scholar\n\n#### 1. Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization\n\n*From Search Query: butterfly factorization transformer structured matrices*\n\n*Weiyang Liu, Zeju Qiu, Yao Feng, Yuliang Xiu, Yuxuan Xue, Longhui Yu, Haiwen Feng, Zhen Liu, Juyeon Heo, Songyou Peng, Yandong Wen, Michael J. Black, Adrian Weller, Bernhard Sch\u00f6lkopf*\n\n**TL;DR:** Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, an efficient orthogonal parameterization using butterfly structures is proposed, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT).\n\n**Abstract:** Large foundation models are becoming ubiquitous, but training them from scratch is prohibitively expensive. Thus, efficiently adapting these powerful models to downstream tasks is increasingly important. In this paper, we study a principled finetuning paradigm -- Orthogonal Finetuning (OFT) -- for downstream task adaptation. Despite demonstrating good generalizability, OFT still uses a fairly large number of trainable parameters due to the high dimensionality of orthogonal matrices. To address this, we start by examining OFT from an information transmission perspective, and then identify a few key desiderata that enable better parameter-efficiency. Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures. We apply this parameterization to OFT, creating a novel parameter-efficient finetuning method, called Orthogonal Butterfly (BOFT). By subsuming OFT as a special case, BOFT introduces a generalized orthogonal finetuning framework. Finally, we conduct an extensive empirical study of adapting large vision transformers, large language models, and text-to-image diffusion models to various downstream tasks in vision and language.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 7*)\n\n#### 2. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\n\n*From Search Query: butterfly factorization transformer structured matrices*\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 92  (*Influential: 13*)\n\n#### 3. Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models\n\n*From Search Query: butterfly factorization transformer structured matrices*\n\n*Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, A. Rudra, C. R\u00e9*\n\n**TL;DR:** This work uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers and empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs.\n\n**Abstract:** Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 67  (*Influential: 14*)\n\n### 1 related papers from Papers with Code\n\n#### 1. Efficient Identification of Butterfly Sparse Matrix Factorizations\n\n*From Search Query: butterfly factorization transformer structured matrices*\n\n*R\u00e9mi Gribonval, Elisa Riccietti, L\u00e9on Zheng*\n\n**Abstract:** Fast transforms correspond to factorizations of the form $\\mathbf{Z} = \\mathbf{X}^{(1)} \\ldots \\mathbf{X}^{(J)}$, where each factor $ \\mathbf{X}^{(\\ell)}$ is sparse and possibly structured. This paper investigates essential uniqueness of such factorizations, i.e., uniqueness up to unavoidable scaling ambiguities. Our main contribution is to prove that any $N \\times N$ matrix having the so-called butterfly structure admits an essentially unique factorization into $J$ butterfly factors (where $N = 2^{J}$), and that the factors can be recovered by a hierarchical factorization method, which consists in recursively factorizing the considered matrix into two factors. This hierarchical identifiability property relies on a simple identifiability condition in the two-layer and fixed-support setting. This approach contrasts with existing ones that fit the product of butterfly factors to a given matrix via gradient descent. The proposed method can be applied in particular to retrieve the factorization of the Hadamard or the discrete Fourier transform matrices of size $N=2^J$. Computing such factorizations costs $\\mathcal{O}(N^{2})$, which is of the order of dense matrix-vector multiplication, while the obtained factorizations enable fast $\\mathcal{O}(N \\log N)$ matrix-vector multiplications and have the potential to be applied to compress deep neural networks.\n\n**Published:** 2021-10-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the use of butterfly factorization, structured matrix operations, and memory-computation trade-offs, here are some relevant findings and suggestions:\n\n## Applications of Butterfly Factorization in Transformer Models\nButterfly factorization, which involves expressing matrices as products of sparse factors with specific sparsity patterns, can be highly efficient for matrix operations. While there is no direct paper explicitly discussing its application in transformer models, the concept can be inferred from the general efficiency gains of such matrices.\n\n- **Butterfly Matrices and Kronecker-Sparse Matrices**: These structured matrices offer sub-quadratic theoretical matrix multiplication complexity, which could be beneficial for the dense matrix operations in transformer models. The work on Kronecker-sparse matrices highlights their potential in reducing computational complexity, which could be adapted for transformer architectures.\n\n## Structured Matrix Operations in Language Models\nStructured matrix operations can significantly improve the efficiency of transformer models.\n\n- **Low-Rank Matrix Adaptations**: Techniques like LoRA (Low-Rank Adaptation) and its variants (e.g., VBLoRA, XLora) in the PEFT library utilize low-rank matrix adaptations to efficiently fine-tune pre-trained models. These methods can be seen as a form of structured matrix operation, where the adaptation matrices are constrained to be low-rank, reducing the number of parameters and computational overhead.\n\n## Memory-Computation Trade-offs in Transformer Architectures\nBalancing memory and computation is crucial for efficient transformer models.\n\n- **Memory-Efficient Training Techniques**: Methods like using auxiliary activations, as discussed in the analysis, can reduce the memory footprint during training. Additionally, techniques such as MoH (Mixture-of-Head attention) can select a subset of attention heads, reducing computational resources without sacrificing performance.\n- **Quantization and Factorization**: Approaches like FLATQUANT, which employ learnable affine transformations and Kronecker decomposition, can enhance the flatness of weights and activations, allowing for efficient quantization and reducing memory footprint. Similarly, methods like LoLDU, which use Lower-Diag-Upper (LDU) decomposition, can optimize low-rank matrices efficiently.\n\n## Empirical Results of Structured Matrices in Practice\nEmpirical results on structured matrices are crucial for understanding their practical benefits.\n\n- **Efficiency Gains**: While direct empirical results on butterfly factorization in language models are scarce, the efficiency gains from using structured matrices like low-rank adaptations have been demonstrated. For example, VBLoRA and XLora show significant reductions in parameters and computational overhead while maintaining performance.\n- **Quantization Results**: FLATQUANT has shown less than 1% accuracy drop for 4-bit weight and activation quantization on large language models, highlighting the practical efficiency of structured matrix operations in quantization.\n\n## Hardware Efficiency of Different Matrix Factorization Methods\nThe hardware efficiency of these methods is critical for real-world deployment.\n\n- **Butterfly Matrices**: Theoretical efficiency of butterfly matrices suggests they could be highly beneficial for hardware implementation due to their sub-quadratic complexity. However, practical implementation details would need to be explored further.\n- **Low-Rank Adaptations**: The PEFT library's methods, such as LoRA and VBLoRA, are designed to be efficient in both memory and computation, making them suitable for deployment on various hardware platforms, including resource-constrained environments.\n\nIn summary, while direct applications of butterfly factorization in transformer models are not yet extensively documented, the principles of structured matrices and low-rank adaptations offer promising avenues for improving the efficiency and scalability of autoregressive language models. Further research into adapting butterfly matrices and other structured matrix techniques could provide significant gains in memory-computation trade-offs and hardware efficiency.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search has revealed several important aspects related to the TTTLinear-AuxAct proposal, particularly around memory-efficient training techniques and auxiliary activation methods. However, more targeted searches are needed for comprehensive evaluation.\n\nKey Findings:\n\n1. Memory-Efficient Training:\n- Woo & Jeon (2023) proposed a learning rule that \"combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation\" - highly relevant to our proposal\n- DIVISION (Wang et al., 2022) demonstrates that \"DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps\" - supports our dual-path design rationale\n\n2. Auxiliary Activation Methods:\n- Recent work shows promise in using auxiliary activations for memory reduction without significant performance loss\n- Multiple approaches exist for balancing memory efficiency and model performance\n\n3. Structured Matrices:\n- Need more information on butterfly factorization applications in language models\n- Limited data on efficiency gains in practice\n\nUseful References with Key Excerpts:\n\n1. From \"Learning with Auxiliary Activation for Memory-Efficient Training\":\n\"The algorithm combines auxiliary activation with output activation during forward propagation, while only auxiliary activation is used during backward propagation instead of actual input activation to reduce the amount of data to be temporarily stored.\"\n\n2. From \"DIVISION: Memory Efficient Training\":\n\"DNN backward propagation mainly utilizes the low-frequency component (LFC) of the activation maps, while the majority of memory is for caching the high-frequency component (HFC) during the training.\"\n\nKey Insights for Review:\n\n1. Memory Efficiency:\n- Auxiliary activations show promise for reducing memory footprint\n- Dual-path approaches are supported by recent research\n- Need to investigate specific memory reduction claims\n\n2. Model Performance:\n- Evidence suggests minimal performance impact when properly implemented\n- Need more data on test-time adaptation capabilities\n\n3. Implementation Considerations:\n- Balance between memory savings and computational overhead\n- Importance of proper initialization and training stability\n\nFuture Search Plan:\n\n1. Next Search Focus:\n- Butterfly factorization in language models\n- Test-time adaptation mechanisms\n- Specific memory-computation trade-offs in transformer architectures\n\n2. Areas Needing More Data:\n- Empirical results on structured matrices\n- Long-sequence performance metrics\n- Hardware efficiency considerations\n\nReferences:\n\n[1] Woo, S., & Jeon, D. (2023). Learning with Auxiliary Activation for Memory-Efficient Training. International Conference on Learning Representations.\n\n[2] Wang, G., Liu, Z., Jiang, Z., Liu, N., Zou, N., & Hu, X. (2022). DIVISION: Memory Efficient Training via Dual Activation Precision. International Conference on Machine Learning.\n\n[3] Lee, C., & Lee, S. (2023). Softmax Output Approximation for Activation Memory-Efficient Training of Attention-based Networks. Neural Information Processing Systems.\n\n[4] Woo, S., Lee, S., & Jeon, D. (2024). ALAM: Averaged Low-Precision Activation for Memory-Efficient Training of Transformer Models. International Conference on Learning Representations.\n\n[5] Liao, B., Tan, S., & Monz, C. (2023). Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "test time adaptation memory efficient training",
            "detail": "Search for papers discussing:\n1. Test-time adaptation mechanisms in neural networks\n2. Memory-efficient training techniques for autoregressive models\n3. Convergence analysis of structured matrix operations\n4. Hardware implementation considerations for butterfly operations\n5. Impact of structured sparsity on model quality",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Test-time adaptation mechanisms in neural networks\n2. Memory-efficient training techniques for autoregressive models\n3. Convergence analysis of structured matrix operations\n4. Hardware implementation considerations for butterfly operations\n5. Impact of structured sparsity on model quality\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.10)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 22/46 (Score: 0.10)*\n\n```\nIn International Conference on Machine Learning, pages 5547-5569. PMLR, 2022 . [34] Yuli Eidelman and Israel Gohberg. On a new class of structured matrices. Integral Equations and Operator Theory, 34(3):293-324, 1999. [35] Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E Papka, Rick Stevens, Laura Florescu, Sumti Jairath, William Liu, Tejas Nama, and Arvind Sujeeth. Accelerating scientific applications with sambanova reconfigurable dataflow architecture. Computing in Science 8 Engineering, 23(2):114-119, 2021 . [36] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. Multi-head state space model for speech recognition. arXiv preprint arXiv:2305.12498, 2023. [37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022 . [38] Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 2021. [39] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [40] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.02)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.02)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. Efficiently Modeling Long Sequences with Structured State Spaces (Avg. Score: 0.00)\n\n*Albert Gu, Karan Goel, Christopher R'e*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 744  (*Influential: 119*)\n\n**TL;DR:** The Structured State Space sequence model (S4) is proposed based on a new parameterization for the SSM, and it is shown that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths.\n\n**Abstract:** A central goal of sequence modeling is designing a single principled model that can address sequence data across a range of modalities and tasks, particularly on long-range dependencies. Although conventional models including RNNs, CNNs, and Transformers have specialized variants for capturing long dependencies, they still struggle to scale to very long sequences of $10000$ or more steps. A promising recent approach proposed modeling sequences by simulating the fundamental state space model (SSM) \\( x'(t) = Ax(t) + Bu(t), y(t) = Cx(t) + Du(t) \\), and showed that for appropriate choices of the state matrix \\( A \\), this system could handle long-range dependencies mathematically and empirically. However, this method has prohibitive computation and memory requirements, rendering it infeasible as a general sequence modeling solution. We propose the Structured State Space sequence model (S4) based on a new parameterization for the SSM, and show that it can be computed much more efficiently than prior approaches while preserving their theoretical strengths. Our technique involves conditioning \\( A \\) with a low-rank correction, allowing it to be diagonalized stably and reducing the SSM to the well-studied computation of a Cauchy kernel. S4 achieves strong empirical results across a diverse range of established benchmarks, including (i) 91\\% accuracy on sequential CIFAR-10 with no data augmentation or auxiliary losses, on par with a larger 2-D ResNet, (ii) substantially closing the gap to Transformers on image and language modeling tasks, while performing generation $60\\times$ faster (iii) SoTA on every task from the Long Range Arena benchmark, including solving the challenging Path-X task of length 16k that all prior work fails on, while being as efficient as all competitors.\n\n##### *Relevant Chunk: No. 26/45 (Score: 0.00)*\n\n```\narXiv preprint arXiv:1609.03499, 2016. [29] Victor Pan. Structured matrices and polynomials: unified superfast algorithms. Springer Science \\& Business Media, 2001. [30] Victor Pan. Fast approximate computations with cauchy matrices and polynomials. Mathematics of Computation, 86(308):2799-2826, 2017. [31] Victor Y Pan. Transformations of matrix structures work again. Linear Algebra and Its Applications, $465: 107-138,2015$. [32] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318, 2013. [33] Jack Rae, Chris Dyer, Peter Dayan, and Timothy Lillicrap. Fast parametric learning with activation memorization. The International Conference on Machine Learning (ICML), 2018. [34] Prajit Ramachandran, Tom Le Paine, Pooya Khorrami, Mohammad Babaeizadeh, Shiyu Chang, Yang Zhang, Mark A Hasegawa-Johnson, Roy H Campbell, and Thomas S Huang. Fast generation for convolutional autoregressive models. arXiv preprint arXiv:1704.06001, 2017. [35] David W Romero, Anna Kuzina, Erik J Bekkers, Jakub M Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data. arXiv preprint arXiv:2102.02611, 2021. [36] David W Romero, Robert-Jan Bruintjes, Jakub M Tomczak, Erik J Bekkers, Mark Hoogendoorn, and Jan C van Gemert. Flexconv: Continuous kernel convolutions with differentiable kernel sizes. In The International Conference on Learning Representations (ICLR), 2022. [37] Yulia Rubanova, Tian Qi Chen, and David K Duvenaud. Latent ordinary differential equations for irregularly-sampled time series. In Advances in Neural Information Processing Systems, pages 5321-5331, 2019 . [38] T Konstantin Rusch and Siddhartha Mishra. Unicornn: A recurrent model for learning very long time dependencies. The International Conference on Machine Learning (ICML), 2021. [39] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik P Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications.\n```\n\n#### 4. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.00)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 33/51 (Score: 0.00)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. [22] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In In Uncertainty in Artificial Intelligence, pages 148-155. Morgan Kaufmann, 1998. [23] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [24] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [25] Xinyang Geng. EasyLM: A Simple And Scalable Training Framework for Large Language Models. https://github.com/young-geng/EasyLM, mar 2023. https://github.com/ young-geng/EasyLM. [26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\u00e0, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020. [29] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.00)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.00)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: test time adaptation memory efficient training\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Efficient Meta Lifelong-Learning with Limited Memory\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Zirui Wang, Sanket Vaibhav Mehta, B. P\u00f3czos, J. Carbonell*\n\n**TL;DR:** This paper identifies three common principles of lifelong learning methods and proposes an efficient meta-lifelong framework that combines them in a synergistic fashion and alleviates both catastrophic forgetting and negative transfer at the same time.\n\n**Abstract:** Current natural language processing models work well on a single task, yet they often fail to continuously learn new tasks without forgetting previous ones as they are re-trained throughout their lifetime, a challenge known as lifelong learning. State-of-the-art lifelong language learning methods store past examples in episodic memory and replay them at both training and inference time. However, as we show later in our experiments, there are three significant impediments: (1) needing unrealistically large memory module to achieve good performance, (2) suffering from negative transfer, (3) requiring multiple local adaptation steps for each test example that significantly slows down the inference speed. In this paper, we identify three common principles of lifelong learning methods and propose an efficient meta-lifelong framework that combines them in a synergistic fashion. To achieve sample efficiency, our method trains the model in a manner that it learns a better initialization for local adaptation. Extensive experiments on text classification and question answering benchmarks demonstrate the effectiveness of our framework by achieving state-of-the-art performance using merely 1% memory size and narrowing the gap with multi-task learning. We further show that our method alleviates both catastrophic forgetting and negative transfer at the same time.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 66  (*Influential: 13*)\n\n#### 2. Towards Stable Test-Time Adaptation in Dynamic Wild World\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Shuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Z. Wen, Yaofo Chen, P. Zhao, Mingkui Tan*\n\n**TL;DR:** This paper proposes a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples.\n\n**Abstract:** Test-time adaptation (TTA) has shown to be effective at tackling distribution shifts between training and testing data by adapting a given model on test samples. However, the online model updating of TTA may be unstable and this is often a key obstacle preventing existing TTA methods from being deployed in the real world. Specifically, TTA may fail to improve or even harm the model performance when test data have: 1) mixed distribution shifts, 2) small batch sizes, and 3) online imbalanced label distribution shifts, which are quite common in practice. In this paper, we investigate the unstable reasons and find that the batch norm layer is a crucial factor hindering TTA stability. Conversely, TTA can perform more stably with batch-agnostic norm layers, \\ie, group or layer norm. However, we observe that TTA with group and layer norms does not always succeed and still suffers many failure cases. By digging into the failure cases, we find that certain noisy test samples with large gradients may disturb the model adaption and result in collapsed trivial solutions, \\ie, assigning the same class label for all samples. To address the above collapse issue, we propose a sharpness-aware and reliable entropy minimization method, called SAR, for further stabilizing TTA from two aspects: 1) remove partial noisy samples with large gradients, 2) encourage model weights to go to a flat minimum so that the model is robust to the remaining noisy samples. Promising results demonstrate that SAR performs more stably over prior methods and is computationally efficient under the above wild test scenarios.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 179  (*Influential: 56*)\n\n#### 3. AdaNPC: Exploring Non-Parametric Classifier for Test-Time Adaptation\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Yi-Fan Zhang, Xue Wang, Kexin Jin, Kun Yuan, Zhang Zhang, Liang Wang, Rong Jin, Tien-Ping Tan*\n\n**TL;DR:** This work adopts Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC), and builds a memory that contains the feature and label pairs from training domains that can be gradually changed from the training distribution towards the test distribution with very little extra computation cost.\n\n**Abstract:** Many recent machine learning tasks focus to develop models that can generalize to unseen distributions. Domain generalization (DG) has become one of the key topics in various fields. Several literatures show that DG can be arbitrarily hard without exploiting target domain information. To address this issue, test-time adaptive (TTA) methods are proposed. Existing TTA methods require offline target data or extra sophisticated optimization procedures during the inference stage. In this work, we adopt Non-Parametric Classifier to perform the test-time Adaptation (AdaNPC). In particular, we construct a memory that contains the feature and label pairs from training domains. During inference, given a test instance, AdaNPC first recalls K closed samples from the memory to vote for the prediction, and then the test feature and predicted label are added to the memory. In this way, the sample distribution in the memory can be gradually changed from the training distribution towards the test distribution with very little extra computation cost. We theoretically justify the rationality behind the proposed method. Besides, we test our model on extensive numerical experiments. AdaNPC significantly outperforms competitive baselines on various DG benchmarks. In particular, when the adaptation target is a series of domains, the adaptation accuracy of AdaNPC is 50% higher than advanced TTA methods. The code is available at https://github.com/yfzhang114/AdaNPC.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 28  (*Influential: 2*)\n\n#### 4. Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Zeshuai Deng, Zhuokun Chen, Shuaicheng Niu, Thomas H. Li, Bohan Zhuang, Mingkui Tan*\n\n**TL;DR:** An efficient test-time adaptation framework for SR is presented, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types, and designs a second-order degradation scheme to construct paired data based on the degradation type of the test image, predicted by a pre-trained degradation classifier.\n\n**Abstract:** Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, e.g., Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (e.g., blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 5. MECTA: Memory-Economic Continual Test-Time Model Adaptation\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Junyuan Hong, Lingjuan Lyu, Jiayu Zhou, Michael Spranger*\n\n**Abstract:** Continual Test-time Adaptation (CTA) is a promising art to secure accuracy gains in continually-changing environments. The state-of-the-art adaptations improve out-of-distribution model accuracy via computation-efficient online test-time gradient descents but meanwhile cost about times of memory versus the inference, even if only a small portion of parameters are updated. Such high memory consumption of CTA substantially impedes wide applications of advanced CTA on memory-constrained devices. In this paper, we provide a novel solution, dubbed MECTA, to drastically improve the memory efficiency of gradient-based CTA. Our profiling shows that the major memory overhead comes from the intermediate cache for back-propagation, which scales by the batch size, channel, and layer number. Therefore, we propose to reduce batch sizes, adopt an adaptive normalization layer to maintain stable and accurate predictions, and stop the back-propagation caching heuristically. On the other hand, we prune the networks to reduce the computation and memory overheads in optimization and recover the parameters afterward to avoid forgetting. The proposed MECTA is efficient and can be seamlessly plugged into state-of-the-art CTA algorithms at negligible overhead on computation and memory. On three datasets, CIFAR10, CIFAR100, and ImageNet, MECTA improves the accuracy by at least 6% with constrained memory and significantly reduces the memory costs of ResNet50 on ImageNet by at least 70% with comparable accuracy. Our codes can be accessed at https://github.com/SonyAI/MECTA .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 24  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Three things everyone should know about Vision Transformers\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Herv\u00e9 J\u00e9gou, Jakob Verbeek, Alaaeldin El-Nouby, Matthieu Cord, Hugo Touvron*\n\n**Abstract:** After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.\n\n**Published:** 2022-03-18\n\n\n\n#### 2. Glance and Focus: a Dynamic Approach to Reducing Spatial Redundancy in Image Classification\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Gao Huang, Le Yang, Shiji Song, Rui Huang, Kangchen Lv, Yulin Wang*\n\n**Abstract:** The accuracy of deep convolutional neural networks (CNNs) generally improves when fueled with high resolution images. However, this often comes at a high computational cost and high memory footprint. Inspired by the fact that not all regions in an image are task-relevant, we propose a novel framework that performs efficient image classification by processing a sequence of relatively small inputs, which are strategically selected from the original image with reinforcement learning. Such a dynamic decision process naturally facilitates adaptive inference at test time, i.e., it can be terminated once the model is sufficiently confident about its prediction and thus avoids further redundant computation. Notably, our framework is general and flexible as it is compatible with most of the state-of-the-art light-weighted CNNs (such as MobileNets, EfficientNets and RegNets), which can be conveniently deployed as the backbone feature extractor. Experiments on ImageNet show that our method consistently improves the computational efficiency of a wide variety of deep models. For example, it further reduces the average latency of the highly efficient MobileNet-V3 on an iPhone XS Max by 20% without sacrificing accuracy. Code and pre-trained models are available at https://github.com/blackfeather-wang/GFNet-Pytorch.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-11\n\n\n\n#### 3. LoRA-GA: Low-Rank Adaptation with Gradient Approximation\n\n*From Search Query: test time adaptation memory efficient training*\n\n*Jian Li, Linxi Yu, Shaowen Wang*\n\n**Abstract:** Fine-tuning large-scale pretrained models is prohibitively expensive in terms of computational and memory costs. LoRA, as one of the most popular Parameter-Efficient Fine-Tuning (PEFT) methods, offers a cost-effective alternative by fine-tuning an auxiliary low-rank model that has significantly fewer parameters. Although LoRA reduces the computational and memory requirements significantly at each iteration, extensive empirical evidence indicates that it converges at a considerably slower rate compared to full fine-tuning, ultimately leading to increased overall compute and often worse test performance. In our paper, we perform an in-depth investigation of the initialization method of LoRA and show that careful initialization (without any change of the architecture and the training algorithm) can significantly enhance both efficiency and performance. In particular, we introduce a novel initialization method, LoRA-GA (Low Rank Adaptation with Gradient Approximation), which aligns the gradients of low-rank matrix product with those of full fine-tuning at the first step. Our extensive experiments demonstrate that LoRA-GA achieves a convergence rate comparable to that of full fine-tuning (hence being significantly faster than vanilla LoRA as well as various recent improvements) while simultaneously attaining comparable or even better performance. For example, on the subset of the GLUE dataset with T5-Base, LoRA-GA outperforms LoRA by 5.69% on average. On larger models such as Llama 2-7B, LoRA-GA shows performance improvements of 0.34, 11.52%, and 5.05% on MT-bench, GSM8K, and Human-eval, respectively. Additionally, we observe up to 2-4 times convergence speed improvement compared to vanilla LoRA, validating its effectiveness in accelerating convergence and enhancing model performance. Code is available at https://github.com/Outsider565/LoRA-GA.\n\n**Published:** 2024-07-06\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on test-time adaptation, memory-efficient training, and other relevant aspects, here are some key findings and suggestions based on the provided keywords and analysis:\n\n## Test-Time Adaptation Mechanisms in Neural Networks\n\n- Test-Time Adaptation (TTA) is crucial for adapting pre-trained models to new domains during the testing phase. Papers like \"Prompt-Based Memory Bank for Continual Test-Time Domain Adaptation\" discuss using prompt-based methods to adapt models efficiently. This approach involves using a memory bank to store and update prompts that help the model adapt to new domains without extensive retraining.\n- Another relevant work is \"Domain-Conditioned Transformer for Fully Test-time Adaptation,\" which highlights the importance of adjusting self-attention profiles in transformer networks to adapt to new domains, thereby mitigating performance degradation.\n\n## Memory-Efficient Training Techniques for Autoregressive Models\n\n- Structured sparsity and butterfly factorization are promising techniques for reducing memory footprint and improving computational efficiency. The \"Pixelated Butterfly\" method, for instance, uses a fixed sparsity pattern to sparsify network layers, resulting in faster training times and reduced memory usage without compromising accuracy in the analysis note.\n- Auxiliary activations and structured sparsity can also help in reducing the memory footprint. These techniques are discussed in the context of vision transformers but can be adapted for autoregressive language models by carefully balancing compression and performance.\n\n## Convergence Analysis of Structured Matrix Operations\n\n- While the provided sources do not directly address the convergence analysis of structured matrix operations, the benefits of butterfly factorization, such as sub-quadratic computational complexity and significant parameter reduction, suggest that these structures can lead to more efficient and stable matrix operations. This is supported by works like \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\" and \"Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization\" in the analysis note.\n\n## Hardware Implementation Considerations for Butterfly Operations\n\n- Hardware optimization is crucial for leveraging the benefits of butterfly factorization. The structured sparsity inherent in these methods can be hardware-friendly, but careful consideration of memory access patterns and initialization is necessary to avoid gradient issues and optimize performance. This is a key insight from the analysis note and relevant papers.\n\n## Impact of Structured Sparsity on Model Quality\n\n- Structured sparsity, as seen in methods like \"Pixelated Butterfly,\" can significantly reduce the memory footprint and improve training speed without compromising model accuracy. However, there is a trade-off between structure and expressiveness that needs to be balanced to maintain model quality. This balance is critical to ensure that the sparsity does not overly restrict the model's ability to learn complex patterns.\n\n### Additional Considerations\n\n- **Test-Time Adaptation in Structured Matrices**: Integrating test-time adaptation mechanisms with structured matrices like butterfly factorization could enhance the adaptability of autoregressive models. This involves adjusting the structured matrices to fit new domains efficiently during testing.\n- **Memory-Efficient Training for Autoregressive Models**: Techniques such as auxiliary activations and structured sparsity can be specifically tailored for autoregressive models. This might involve sparsifying the attention mechanisms or using efficient parameterization methods like those proposed in \"Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization\" in the analysis note.\n\nBy focusing on these aspects, researchers can develop more efficient, adaptable, and scalable autoregressive language models that leverage the benefits of structured sparsity, butterfly factorization, and test-time adaptation.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about structured matrices, particularly butterfly factorization and memory-efficient training techniques. The findings suggest that while butterfly factorization shows promise for efficient computation, its direct application in transformer architectures requires careful consideration of implementation details and trade-offs.\n\n2. Useful References with Excerpts:\n\nFrom \"Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations\":\n\"Our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations [...] with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\"\n\nFrom \"Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization\":\n\"Inspired by how the Cooley-Tukey fast Fourier transform algorithm enables efficient information transmission, we propose an efficient orthogonal parameterization using butterfly structures.\"\n\nFrom \"Pixelated Butterfly\":\n\"Our method uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers [...] trains up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.\"\n\n3. Key Insights:\n\na) Butterfly Factorization Benefits:\n- Sub-quadratic computational complexity\n- Potential for significant parameter reduction\n- Hardware-friendly structured sparsity\n- Proven success in vision transformers\n\nb) Implementation Considerations:\n- Need for careful initialization to avoid gradient issues\n- Trade-off between structure and expressiveness\n- Hardware optimization requirements\n- Memory access patterns impact\n\nc) Memory Efficiency:\n- Auxiliary activations show promise\n- Structured sparsity can reduce memory footprint\n- Need to balance compression vs. performance\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Test-time adaptation mechanisms in structured matrices\n2. Memory-efficient training techniques specific to autoregressive models\n3. Hardware implementation details for butterfly operations\n4. Impact on model quality and convergence\n\n5. References:\n\n[1] Dao, T., Gu, A., Eichhorn, M., Rudra, A., & R\u00e9, C. (2019). Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations. International Conference on Machine Learning.\n\n[2] Liu, W., et al. (2023). Parameter-Efficient Orthogonal Finetuning via Butterfly Factorization. International Conference on Learning Representations.\n\n[3] Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., & R\u00e9, C. (2021). Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models. International Conference on Learning Representations.\n\n[4] Fu, D.Y., Kumbong, H., Nguyen, E.N.D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv.org."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.6001350000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.530997,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "TTTLinear",
    "proposal": "## TTTLinear-AuxAct: Memory-Efficient Test-Time Training with Auxiliary Activations\n\n### 1. Title\nTTTLinear-AuxAct: Memory-Efficient Test-Time Training through Auxiliary Activations and Structured Matrices\n\n### 2. Motivation\nThe original TTTLinear design, while innovative in its approach to test-time adaptation, faces challenges in memory efficiency and computational overhead. The use of full matrix operations and the need to store intermediate activations during training creates bottlenecks, particularly for long sequences. Additionally, the current implementation may not fully leverage the potential for structured computations that could improve both memory and computational efficiency.\n\n### 3. Problem Analysis\nKey limitations of the current TTTLinear design include:\n- High memory requirements for storing intermediate activations\n- Computational overhead from full matrix operations\n- Limited exploitation of potential structure in weight matrices\n- Memory I/O bottlenecks during training\n\n### 4. Core Idea and Philosophy\nThe core idea is to enhance TTTLinear with two key innovations:\n1. A dual-path activation scheme using auxiliary activations for memory-efficient training\n2. Structured matrix parameterization through butterfly factorization for efficient computation\n\nThis approach maintains the test-time adaptation capabilities while significantly reducing memory requirements and computational overhead.\n\n### 5. Design Plan\n\n#### 5.1 Modifications to TTTLinear\n1. Auxiliary Activation Integration:\n```python\ndef forward(self, X, training=True):\n    # Main activation path\n    main_act = self.compute_main_activation(X)\n    \n    if training:\n        # Auxiliary activation path\n        aux_act = self.compute_auxiliary_activation(X)\n        # Combine during forward pass\n        return main_act + self.aux_gate * aux_act\n    return main_act\n```\n\n2. Butterfly Matrix Parameterization:\n```python\nclass ButterflyLinear(nn.Module):\n    def __init__(self, in_dim, out_dim, factors=2):\n        super().__init__()\n        self.factors = factors\n        self.butterflies = nn.ModuleList([\n            nn.Linear(in_dim // (2**i), in_dim // (2**(i+1)))\n            for i in range(factors)\n        ])\n    \n    def forward(self, x):\n        for butterfly in self.butterflies:\n            x = butterfly(x)\n        return x\n```\n\n3. Memory-Efficient Update Rule:\n```python\ndef update_hidden_state(self, hidden_state, grad, aux_act):\n    # Use auxiliary activation for gradient computation\n    with torch.no_grad():\n        grad_proj = self.project_gradient(grad, aux_act)\n        return hidden_state - self.lr * grad_proj\n```\n\n#### 5.2 Mathematical Formulation\n\nThe auxiliary activation scheme is defined as:\n\n$h_{aux}(x) = \\sigma(W_{aux}x + b_{aux})$\n\nwhere $\\sigma$ is a memory-efficient activation function.\n\nThe butterfly factorization decomposes weight matrices as:\n\n$W = \\prod_{i=1}^k B_i$\n\nwhere each $B_i$ is a sparse structured matrix.\n\nThe combined forward pass becomes:\n\n$y = h_{main}(x) + \\alpha h_{aux}(x)$\n\nwhere $\\alpha$ is a learnable gating parameter.\n\n### 6. Implementation Guidelines\n\n1. Initialization:\n```python\nclass TTTLinearAuxAct(nn.Module):\n    def __init__(self, dim, hidden_dim):\n        super().__init__()\n        self.main_path = ButterflyLinear(dim, hidden_dim)\n        self.aux_path = ButterflyLinear(dim, hidden_dim)\n        self.aux_gate = nn.Parameter(torch.zeros(1))\n        self.register_buffer('aux_acts', None)\n```\n\n2. Forward Pass Implementation:\n```python\ndef forward(self, x, store_aux=True):\n    main_out = self.main_path(x)\n    if self.training:\n        aux_out = self.aux_path(x)\n        if store_aux:\n            self.aux_acts = aux_out.detach()\n        return main_out + torch.sigmoid(self.aux_gate) * aux_out\n    return main_out\n```\n\n3. Memory-Efficient Backward Pass:\n```python\ndef backward_hook(self, grad):\n    if self.aux_acts is not None:\n        grad = grad * self.compute_aux_scaling(self.aux_acts)\n    return grad\n```\n\n### 7. Research Summary\n\nKey search findings that influenced the design:\n- Auxiliary activations can reduce memory requirements by up to 75%\n- Butterfly factorizations provide efficient matrix operations\n- Structured sparsity patterns improve hardware efficiency\n- Memory access patterns crucial for performance\n\n### 8. Evolution of Design\n\nThe design evolved through several stages:\n1. Initial focus on auxiliary activations\n2. Integration of butterfly factorization\n3. Addition of memory-efficient backward pass\n4. Optimization of memory access patterns\n\n### 9. Theoretical Analysis\n\nBenefits:\n- Reduced memory footprint through auxiliary activations\n- Improved computational efficiency via structured matrices\n- Maintained model expressiveness through dual-path design\n- Better hardware utilization through optimized memory patterns\n\nTrade-offs:\n- Slight increase in implementation complexity\n- Additional hyperparameters to tune\n- Need for careful initialization of auxiliary components\n\n### 10. Conclusion\n\nTTTLinear-AuxAct provides a significant advancement in memory-efficient test-time training while maintaining the core benefits of the original TTTLinear design. The combination of auxiliary activations and structured matrices offers a practical solution to the memory and computational challenges faced in large-scale language modeling.\n\n### 11. References\n\n1. Woo, S., & Jeon, D. (2023). Learning with Auxiliary Activation for Memory-Efficient Training. ICLR 2023.\n2. Dao, T., et al. (2019). Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations. ICML 2019.\n3. Liu, X., et al. (2022). GACT: Activation Compressed Training for Generic Network Architectures. ICML 2022.\n4. Lin, J., et al. (2021). MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning. NeurIPS 2021.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "We propose TTTLinear-AuxAct, an enhanced version of TTTLinear that incorporates auxiliary activations and butterfly factorization for improved memory efficiency and computational performance. The design introduces a novel dual-path activation scheme where auxiliary activations are used during backpropagation while maintaining the model's expressiveness through structured matrix parameterization. This approach significantly reduces memory requirements during training while preserving the model's ability to learn test-time adaptations.",
    "ideation": null,
    "modelname": "TTT-AuxAct",
    "suggestions": "1. Implementation Clarity\n- Provide more detailed analysis of memory-computation trade-offs\n- Include specific initialization strategies for butterfly matrices\n- Add quantitative estimates of expected memory reduction\n\n2. Theoretical Analysis\n- Strengthen the analysis of test-time adaptation stability\n- Include theoretical bounds on approximation error from structured matrices\n- Provide more detailed analysis of gradient flow through the dual-path structure\n\n3. Hardware Optimization\n- Add specific considerations for different hardware architectures\n- Include memory access pattern optimization strategies\n- Provide guidelines for hardware-specific parameter tuning\n\n4. Experimental Validation\n- Suggest specific benchmarks for evaluating memory efficiency\n- Outline experiments to validate test-time adaptation stability\n- Propose metrics for measuring the impact on model quality\n\n5. Architecture Integration\n- Clarify integration with existing transformer architectures\n- Provide guidelines for scaling to different model sizes\n- Include recommendations for hyperparameter tuning",
    "user_input": ""
}