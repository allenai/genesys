{
    "variantname": "AdaptiveHybridMHA",
    "review": "The AdaptiveHybridMHA proposal presents an innovative approach to enhancing Multi-Head Attention by combining selective state space models (SSMs) with traditional attention through an adaptive gating system. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.5/5):\n+ Well-structured presentation with clear objectives\n+ Detailed mathematical formulations and implementation guidelines\n+ Comprehensive pseudo-code implementation\n+ Clear explanation of architectural components\n- Some implementation details about state compression could be more explicit\n\nINNOVATION (4.2/5):\n+ Novel integration of SSM, attention, and test-time training\n+ Unique approach to bidirectional state processing\n+ Thoughtful incorporation of adaptive gating\n- Some concepts build on existing work (e.g., Mamba, Hydra)\n- Integration strategy shares similarities with existing approaches\n\nFEASIBILITY (3.8/5):\n+ Clear implementation path with detailed pseudo-code\n+ Practical considerations for hardware efficiency\n+ Reasonable memory requirements\nConcerns:\n- Complex integration of multiple advanced techniques\n- Potential challenges in training stability\n- Need for careful parameter tuning\n\nSCALABILITY (4.0/5):\n+ Linear complexity through selective computation\n+ Efficient memory scaling through state compression\n+ Hardware-friendly implementation strategy\nConcerns:\n- Memory requirements for very long sequences\n- Potential bottlenecks in state management\n- Trade-offs between compression and model quality\n\nACCURACY AND ROBUSTNESS (4.2/5):\n+ Enhanced expressiveness through hybrid processing\n+ Improved handling of both local and global dependencies\n+ Balance between efficiency and model quality\nConcerns:\n- Impact of compression on model quality needs validation\n- Stability under different input distributions requires testing\n- Trade-offs between efficiency and accuracy need empirical validation\n\nEFFICIENCY (4.3/5):\n+ Linear computational complexity\n+ Reduced memory usage through selective activation\n+ Hardware-aware design considerations\nLimitations:\n- Additional computation from test-time training\n- Memory overhead from state management\n- Implementation complexity may affect practical efficiency\n\nSTRENGTHS:\n1. Novel integration of multiple advanced techniques\n2. Clear and detailed implementation guidelines\n3. Strong theoretical foundation\n4. Potential for significant efficiency improvements\n5. Hardware-aware design considerations\n\nCONCERNS:\n1. Complex integration of multiple components\n2. Training stability challenges\n3. Memory scaling with sequence length\n4. Need for careful parameter tuning\n5. Impact on model quality requires validation",
    "search_stack": [
        {
            "ready": false,
            "query": "test-time training language models, bidirectional autoregressive attention",
            "detail": "Provide detailed information about:\n1. Implementation approaches for test-time training in language models\n2. Methods for combining causal and non-causal processing in autoregressive models\n3. Techniques for efficient state tracking and compression in language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide detailed information about:\n1. Implementation approaches for test-time training in language models\n2. Methods for combining causal and non-causal processing in autoregressive models\n3. Techniques for efficient state tracking and compression in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.70)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 33/51 (Score: 0.70)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. [22] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In In Uncertainty in Artificial Intelligence, pages 148-155. Morgan Kaufmann, 1998. [23] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [24] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [25] Xinyang Geng. EasyLM: A Simple And Scalable Training Framework for Large Language Models. https://github.com/young-geng/EasyLM, mar 2023. https://github.com/ young-geng/EasyLM. [26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\u00e0, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020. [29] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.19)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 14/21 (Score: 0.19)*\n\n```\nAdvances in neural information processing systems, 33: 1474-1487, 2020. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. Hendrycks, D. and Gimpel, K. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415, 2016. Hua, W., Dai, Z., Liu, H., and Le, Q. V. Transformer quality in linear time, 2022. Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 4700-4708, 2017. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. Scaling laws for neural language models, 2020 . Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. Lei, T. When attention meets fast recurrence: Training language models with reduced compute. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 7633-7648, 2021. Lin, X. V., Mihaylov, T., Artetxe, M., Wang, T., Chen, S., Simig, D., Ott, M., Goyal, N., Bhosale, S., Du, J., Pasunuru, R., Shleifer, S., Koura, P. S., Chaudhary, V., O'Horo, B., Wang, J., Zettlemoyer, L., Kozareva, Z., Diab, M. T., Stoyanov, V., and Li, X. Few-shot learning with multilingual language models. $\\operatorname{CoRR}$, abs/2112.10668, 2021. URL https: / arxiv.org/ $\\mathrm{abs} / 2112.10668$. Loshchilov, I. and Hutter, F. Decoupled weight decay regularization, 2019. Mehta, H., Gupta, A., Cutkosky, A., and Neyshabur, B. Long range language modeling via gated state spaces, 2022. Merity, S., Xiong, C., Bradbury, J., and Socher, R.\n```\n\n#### 3. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.12)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 17/29 (Score: 0.12)*\n\n```\nArXiv, abs/2405.21060, 2024. URL https://api semanticscholar.org/CorpusID:270199762\n\nSoham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando de Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models. ArXiv, abs/2402.19427, 2024. URL https://api semanticscholar.org/CorpusID:268091246\n\nDaniel Y. Fu, Tri Dao, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The Pile: An 800 gb dataset of diverse text for language modeling. arXiv preprint\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_31ffb4eba11c70dd7265g-11.jpg?height=45&width=359&top_left_y=2501&top_left_x=403)\n\nAlex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. ArXiv, abs/1410.5401, 2014. URL https://api.semanticscholar.org/CorpusID: 15299054\n\nAlbert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\n```\n\n#### 4. MambaByte: Token-free Selective State Space Model (Avg. Score: 0.12)\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 18  (*Influential: 1*)\n\n**TL;DR:** This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling.\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.\n\n##### *Relevant Chunk: No. 8/40 (Score: 0.12)*\n\n```\narXiv preprint arXiv:2402.11131, 2024. Guy E Blelloch. Prefix Sums and Their Applications. (CMU-CS-90-190), nov 1990. URL https://www.cs.cmu.edu/ guyb/papers/Ble93.pdf. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are Few-Shot Learners. Advances in neural information processing systems, 33:1877-1901, 2020. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating Large Language Model Decoding with Speculative Sampling, 2023a. Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462, 2023b. Dokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, and Noah Constant. Bridging the gap for tokenizer-free language models. arXiv preprint arXiv:1908.10322, 2019. Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation. Transactions of the Association for Computational Linguistics, 10:73-91, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. Advances in neural information processing systems, 33:4271-4282, 2020. Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models.\n```\n\n#### 5. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.11)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 16/41 (Score: 0.11)*\n\n```\n3. Downstream Tasks on LLaMA-2-7B. We analysis six downstream tasks: BBH, PIQA, MMLU, COPA, ARCC, and AGNews. Using standard evaluation settings, MMLU was tested with 5 shots, BBH with 3 shots, and the remaining tasks with zero shots. As shown in Tab. 7, our augmented linear attention not only reduces perplexity but also improves accuracy across all tasks. Specifically, with models like FLASH, our method achieved an average accuracy improvement of $3.53 \\%$. In addition, we extend our methods to three more linear attention methods, with summarized results in Appendix H. Generation Speedups by Integrating LAs with Speculative Decoding. We benchmark the speedups of our compatible LAs with speculative decoding. As shown in Tab. 8, we test the LLaMA-7B/13B models which are adapted into a chat model format, similar to LongLora (Chen et al., 2023c). Following Medusa (Cai et al., 2023b), we train Medusa heads for speculative decoding. Speed tests for the 7B and 13B models are conducted on a single A100-80GB GPU, we observe that our revised LAs are compatible with speculative decoding and approximately doubled the speed. Table 9. Comparison of our method with the integration of FLASH (Hua et al., 2022) and Medusa (Cai et al., 2023b). | Methods | Total Latency | Attention | FFNs | Others |\n| :--- | :---: | :---: | :---: | :---: |\n| FLASH + Medusa | 137.2 ms | 119.7 ms | 8.2 ms | 9.3 ms |\n| Ours Aug. LA | $49.7 \\mathrm{~ms}(\\mathbf{- 6 4 \\%})$ | 32.2 ms | 8.2 ms | 9.3 ms |\n\n### 5.3. Ablation Study\n\nComparison with Direct Integration. To verify the effectiveness of our causal and compatible augmentation techniques, we compare them with the direct integration of previous linear attention FLASH (Hua et al., 2022) and the speculative decoding method Medusa (Cai et al., 2023b). As shown in Tab. 9, our method applied to LLaMA-2-7B models on A100 GPUs for a single batch of speculative decoding ( 64 speculated tokens and 42 sequence candidates), achieves a $64 \\%$ reduction in total latency compared to the direct integration, while also reducing QKV memory requirements by $75 \\%$ from 0.4 GB to 0.1 GB . Our techniques outperform direct integration because standard implementations, even with linear attention like FLASH and speculative decoding like Medusa, face two key limitations without our augmentations: (1) slow sequencebased decoding and (2) lack of optimizations such as shared cumulative sum (cumsum) and key-value (KV) states for batch processing. Conventional strategies for compatible KV caching rely on sequence-based decoding, assigning distinct KV caches to each speculated sequence candidate, as shown in Fig. 7. This results in unnecessary computational effort and memory inefficiency since candidates with identical prefixes are processed separately. In contrast, our method addresses these issues by ensuring identical prefixes are computed only once, mitigating these issues with timedependent causal and compatible augmentation in linear attention and speculative decoding. Our LA Speedups. We benchmarked the training speed of FLASH using both the original attention and our augmented LAs, with a batch size of 1 , on a single A100-40G GPU. Our results show that the augmented LAs significantly improve training speed. For sequence lengths of 4 K and 8 K , they are $1.52 \\times$ and $2.94 \\times$ faster, respectively. FLASH with augmented LAs takes 1.05 seconds and 1.95 seconds per training step for 4 K and 8 K sequences, compared to 1.60 seconds and 5.74 seconds with the original attention. The group size in FLASH was consistently set to 256. Extend to Longer Sequence. We fine-tuned LLaMA-27 B to extend its sequence length from 4 K to 8 K using our augmented LAs, following LongLora (Chen et al., 2023c) setting on the RedPajama dataset. For a fair comparison, we used only the local attention in LongLora, maintaining a block size of 256 . Our augmented LAs reduced perplexity from 15.29 to 13.86 , demonstrating their effectiveness in handling longer sequences. ## 6. Conclusion\n\nThis paper presents the first empirical analysis of linearized autoregressive LLMs, revealing significant limitations of existing linear attention methods in effectively handling masked attention and integration with speculative decoding. To address these challenges, we introduced an approach that combines effective local augmentation with seamless compatibility for speculative decoding. Our experiments across a range of LLMs consistently demonstrate that our method achieves substantial performance gains. Notably, we achieve up to a 6.67 perplexity reduction and up to $2 \\times$ speedups in generation compared to existing linear attention methods. Our work paves the way for more efficient training and deployment of powerful autoregressive LLMs, especially for long-sequence applications. ## Acknowledgements\n\nThis work is supported by the National Science Foundation (NSF) EPCN program (Award number: 1934767) and the CoCoSys, one of the seven centers in JUMP 2.0, a Semiconductor Research Corporation (SRC) program sponsored by DARPA. We extend our gratitude towards Arthur Szlam, Marc'aurelio Ranzato, and Cliff Young for reviewing the paper and providing insightful feedback. We also thank the extended team at Google DeepMind, who enabled and supported this research direction. ## Impact Statement\n\nEfficient LLM Training and Serving Goal. The recent advancements in Large Language Models (LLMs), exemplified by OpenAI's GPT- 3 with its 175 billion parameters, have underscored the significant data and computational power required for such technologies. Training models of this scale incur substantial costs, both financially and environmentally. For instance, the cost necessary to train GPT-3 could exceed 4 million equivalent GPU hours (Brown et al., 2020), and the carbon footprint of training a single Transformer model might rival the lifetime emissions of five average American cars (Strubell et al., 2019). Addressing the challenges of efficient training and serving of LLMs is therefore not only a technical imperative but also an environmental and ethical necessity. Societal Consequences. The success of this project in enabling more efficient training and serving of LLMs will have far-reaching implications, especially in processing long sequences commonly encountered in document handling. Our\nefforts are set to substantially influence various societal and economic sectors. The enhanced efficiency of LLMs promises transformative changes in diverse applications ranging from document summarization and question answering to personal digital assistants, security, and augmented reality. The development and exploration of linearized LLMs mark a pivotal progress in rendering these models both more accessible and environmentally sustainable. ## References\n\nAgrawal, A., Kedia, N., Panwar, A., Mohan, J., Kwatra, N., Gulavani, B. S., Tumanov, A., and Ramjee, R. Taming Throughput-Latency Tradeoff in LLM Inference with Sarathi-Serve. arXiv preprint arXiv:2403.02310, 2024. Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: A Family of Highly Capable Multimodal Models. arXiv preprint arXiv:2312.11805, 2023. Arar, M., Shamir, A., and Bermano, A. H. Learned Queries for Efficient Local Attention. In CVPR, 2022. Bae, S., Ko, J., Song, H., and Yun, S.-Y. Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: test-time training language models, bidirectional autoregressive attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models\n\n*From Search Query: test-time training language models*\n\n*Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, T. Goldstein, Anima Anandkumar, Chaowei Xiao*\n\n**TL;DR:** Test-time prompt tuning (TPT) is proposed, a method that can learn adaptive prompts on the fly with a single test sample and performs on par with the state-of-the-art approaches that use additional training data.\n\n**Abstract:** Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. For image classification, TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPT performs on par with the state-of-the-art approaches that use additional training data. Project page: https://azshue.github.io/TPT.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 202  (*Influential: 44*)\n\n#### 2. Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models\n\n*From Search Query: test-time training language models*\n\n*Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yezhou Yang*\n\n**TL;DR:** This work proposes TTA with feedback to rectify the model output and prevent the model from becoming blindly confident, and builds different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs.\n\n**Abstract:** One fascinating aspect of pre-trained vision-language models~(VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation~(TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 3. ImageBART: Bidirectional Context with Multinomial Diffusion for Autoregressive Image Synthesis\n\n*From Search Query: bidirectional autoregressive attention*\n\n*Patrick Esser, Robin Rombach, A. Blattmann, B. Ommer*\n\n**TL;DR:** The resulting autoregressive ImageBART model can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training and can take unrestricted, user-provided masks into account to perform local image editing.\n\n**Abstract:** Autoregressive models and their sequential factorization of the data likelihood have recently demonstrated great potential for image representation and synthesis. Nevertheless, they incorporate image context in a linear 1D order by attending only to previously synthesized image patches above or to the left. Not only is this unidirectional, sequential bias of attention unnatural for images as it disregards large parts of a scene until synthesis is almost complete. It also processes the entire image on a single scale, thus ignoring more global contextual information up to the gist of the entire scene. As a remedy we incorporate a coarse-to-fine hierarchy of context by combining the autoregressive formulation with a multinomial diffusion process: Whereas a multistage diffusion process successively removes information to coarsen an image, we train a (short) Markov chain to invert this process. In each stage, the resulting autoregressive ImageBART model progressively incorporates context from previous stages in a coarse-to-fine manner. Experiments show greatly improved image modification capabilities over autoregressive models while also providing high-fidelity image generation, both of which are enabled through efficient training in a compressed latent space. Specifically, our approach can take unrestricted, user-provided masks into account to perform local image editing. Thus, in contrast to pure autoregressive models, it can solve free-form image inpainting and, in the case of conditional models, local, text-guided image modification without requiring mask-specific training.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 136  (*Influential: 17*)\n\n#### 4. Long-Short Transformer: Efficient Transformers for Language and Vision\n\n*From Search Query: bidirectional autoregressive attention*\n\n*Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, T. Goldstein, Anima Anandkumar, Bryan Catanzaro*\n\n**TL;DR:** This paper proposes Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks, and proposes a dual normalization strategy to account for the scale mismatch between the two attention mechanisms.\n\n**Abstract:** Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 118  (*Influential: 14*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Test-Time Training on Nearest Neighbors for Large Language Models\n\n*From Search Query: test-time training language models*\n\n*Yu Sun, Moritz Hardt*\n\n**Abstract:** Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.\n\n**Published:** 2023-05-29\n\n\n\n#### 2. Selecting Informative Contexts Improves Language Model Finetuning\n\n*From Search Query: test-time training language models*\n\n*Nicole Beckage, Richard Antonello, Javier Turek, Alexander Huth*\n\n**Abstract:** Language model fine-tuning is essential for modern natural language processing, but is computationally expensive and time-consuming. Further, the effectiveness of fine-tuning is limited by the inclusion of training examples that negatively affect performance. Here we present a general fine-tuning method that we call information gain filtration for improving the overall training efficiency and final performance of language model fine-tuning. We define the information gain of an example as the improvement on a test metric after training on that example. A secondary learner is then trained to approximate this quantity. During fine-tuning, this learner selects informative examples and skips uninformative ones. We show that our method has consistent improvement across datasets, fine-tuning tasks, and language model architectures. For example, we achieve a median perplexity of 54.0 on a books dataset compared to 57.3 for standard fine-tuning. We present statistical evidence that offers insight into the improvements of our method over standard fine-tuning. The generality of our method leads us to propose a new paradigm for language model fine-tuning -- we encourage researchers to release pretrained secondary learners on common corpora to promote efficient and effective fine-tuning, thereby improving the performance and reducing the overall energy footprint of language model fine-tuning.\n\n**Published:** 2020-05-01\n\n\n\n#### 3. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: bidirectional autoregressive attention*\n\n*Bo Dai, Dale Schuurmans, Jure Leskovec, Mengjiao Yang, Zihang Dai, Hanjun Dai, Hongyu Ren*\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-12\n\n\n\n#### 4. UniLMv2: Pseudo-Masked Language Models for Unified Language Model Pre-Training\n\n*From Search Query: bidirectional autoregressive attention*\n\n*Hsiao-Wuen Hon, Ming Zhou, Yu Wang, Nan Yang, Furu Wei, Xiaodong Liu, Jianfeng Gao, Hangbo Bao, Wenhui Wang, Songhao Piao, Li Dong*\n\n**Abstract:** We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.\n\n**Published:** 2020-02-28\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on test-time training, combining causal and non-causal processing, and efficient state tracking and compression, here are some key insights and methods derived from the provided sources and analysis:\n\n## Implementation Approaches for Test-Time Training in Language Models\n\n### Active Fine-Tuning\nThe concept of active fine-tuning at test time, as described in the paper \"Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs,\" introduces a data selection algorithm called SIFT. This approach optimizes the selection of data to reduce uncertainty about the model's response given a prompt, combining ideas from retrieval and active learning. This method can be particularly useful for test-time training, as it ensures that the model is fine-tuned on the most informative data, thereby improving its performance and adaptability.\n\n### Continuous Training of Hidden States\nThe idea of making the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning, allows for continuous training of the hidden state even on test sequences. This approach, mentioned in \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States,\" can be integrated into autoregressive models to enhance their ability to adapt and improve state tracking during test time[Analysis Note].\n\n## Methods for Combining Causal and Non-Causal Processing in Autoregressive Models\n\n### Bidirectional State Space Models\nThe Hydra model, which is a bidirectional extension of state space models, demonstrates superior performance on non-causal tasks. This model can be parameterized as a quasiseparable matrix mixer, allowing it to leverage both causal and non-causal information. Integrating such bidirectional processing into autoregressive models could enhance their performance, especially on tasks that benefit from considering both past and future context[Analysis Note].\n\n### Hybrid Approaches\nCombining the benefits of causal (autoregressive) and non-causal (bidirectional) processing can be achieved through hybrid models. For example, using selective SSMs (State Space Models) that can be viewed as attention-driven models, similar to the self-attention layers in transformers, can help in designing models that leverage both types of information. This hybrid approach can maintain the autoregressive properties necessary for generation while incorporating the advantages of bidirectional processing[Analysis Note].\n\n## Techniques for Efficient State Tracking and Compression in Language Models\n\n### Time-Aware Mechanisms\nTime-aware large kernel convolutions (TaLK Convolutions) offer an adaptive convolution operation that learns to predict the size of a summation kernel, resulting in a time complexity of O(n). This method can be integrated with attention mechanisms to enhance the efficiency of sequence handling and state tracking in autoregressive models[Analysis Note].\n\n### Efficient Attention Mechanisms\nLinear attention variants, such as those mentioned in the context of hierarchical structures and adaptive/dynamic approaches, can improve the efficiency of state tracking. These mechanisms can better handle varying sequence lengths and long-range dependencies, which are crucial for maintaining efficient and scalable models[Analysis Note].\n\n### State Compression and Management\nUsing techniques like feedback attention as working memory, as described in \"TransformerFAM: Feedback attention is working memory,\" can help in managing and compressing states efficiently. This approach allows the model to selectively retain and update relevant information, reducing the computational overhead associated with large state sizes[Analysis Note].\n\n## Additional Insights\n\n### Test-Time Compute Scaling\nOptimally scaling test-time computation, as discussed in the paper \"Scaling LLM Test-Time Compute Optimally,\" can significantly improve the performance of language models. This involves adaptively allocating test-time compute based on the difficulty of the prompt, which can lead to more efficient and effective use of computational resources.\n\n### Dual Prototype Evolving\nThe Dual Prototype Evolving (DPE) approach for test-time adaptation in vision-language models, while not directly applicable to text-only models, highlights the importance of accumulating task-specific knowledge during test time. This concept can be generalized to other modalities, emphasizing the need for models to adapt and learn from the test data efficiently.\n\nBy integrating these approaches\u2014test-time training, bidirectional processing, time-aware mechanisms, efficient attention, and state compression\u2014the researcher can develop a more robust, efficient, and scalable autoregressive language model that outperforms existing state-of-the-art models.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe seed design shows a GPT2-style architecture with three main components:\n- Multi-head attention (MHA) with rotary positional embeddings\n- Gated MLP\n- RMS normalization\n\nRecent research reveals several key trends and opportunities for improvement:\n\na) State Tracking & Memory:\n- Test-time training (TTT) approaches show promise for improving state tracking\n- Hydra demonstrates benefits of bidirectional processing\n- Time-aware mechanisms can enhance sequence modeling\n\nb) Efficiency & Scalability:\n- Linear attention variants offer improved efficiency\n- Adaptive/dynamic approaches can better handle varying sequence lengths\n- Hierarchical structures show promise for long-range dependencies\n\nc) Innovation Opportunities:\n- Combining TTT with efficient attention mechanisms\n- Incorporating bidirectional capabilities while maintaining autoregressive properties\n- Enhancing state tracking through temporal awareness\n\n2. Useful References with Excerpts\n\nFrom \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\":\n\"The key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\"\n\nFrom \"Hydra: Bidirectional State Space Models\":\n\"We propose a natural bidirectional extension of the Mamba model (Hydra), parameterized as a quasiseparable matrix mixer, which demonstrates superior performance over other sequence models including Transformers on non-causal tasks.\"\n\nFrom \"Time-aware Large Kernel Convolutions\":\n\"TaLK Convolutions, a novel adaptive convolution operation that learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n).\"\n\nFrom \"The Hidden Attention of Mamba Models\":\n\"Selective SSMs can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers.\"\n\n3. Key Insights & Detailed Analysis\n\na) Test-Time Training Integration:\n- TTT approaches allow models to continuously adapt their hidden states\n- This could be particularly valuable for improving the quality of state tracking in autoregressive models\n- Challenge: Maintaining efficiency while incorporating TTT mechanisms\n\nb) Bidirectional Processing:\n- Hydra shows that bidirectional processing can significantly improve model performance\n- Challenge: Incorporating bidirectional elements while maintaining autoregressive properties\n- Opportunity: Design hybrid approaches that leverage both causal and non-causal information\n\nc) Temporal Awareness:\n- TaLK Convolutions demonstrate the value of adaptive, time-aware processing\n- This could be integrated with attention mechanisms for more efficient sequence handling\n- Opportunity: Combine temporal awareness with state tracking improvements\n\nd) Attention Mechanisms:\n- Hidden attention in SSMs suggests new ways to think about attention design\n- Opportunity: Develop hybrid approaches that combine benefits of different attention types\n- Challenge: Maintaining efficiency while enhancing expressiveness\n\n4. Future Search Plan\n\nNext search rounds should focus on:\n1. Specific implementations of test-time training in language models\n2. Techniques for efficient bidirectional processing in autoregressive models\n3. Methods for combining different types of attention mechanisms\n4. State compression and management approaches\n\n5. References\n\n[1] Sun, Y., et al. (2024). Learning to (Learn at Test Time): RNNs with Expressive Hidden States. arXiv preprint.\n\n[2] Hwang, S., Lahoti, A., Dao, T., & Gu, A. (2024). Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers. arXiv preprint.\n\n[3] Lioutas, V., & Guo, Y. (2020). Time-aware Large Kernel Convolutions. International Conference on Machine Learning.\n\n[4] Ali, A., Zimerman, I., & Wolf, L. (2024). The Hidden Attention of Mamba Models. arXiv preprint.\n\n[5] Hwang, D., Wang, W., Huo, Z., Sim, K., & Mengibar, P. M. (2024). TransformerFAM: Feedback attention is working memory. arXiv preprint."
        },
        {
            "ready": false,
            "query": "state compression language models, efficient memory management transformers",
            "detail": "Provide detailed information about:\n1. State compression techniques in language models\n2. Memory management strategies in transformer architectures\n3. Efficient implementations of hybrid attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide detailed information about:\n1. State compression techniques in language models\n2. Memory management strategies in transformer architectures\n3. Efficient implementations of hybrid attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.87)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.87)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.85)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.85)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 4. Recurrent Memory Transformer (Avg. Score: 0.66)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 0.66)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 5. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.66)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.66)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: state compression language models, efficient memory management transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Compression of Generative Pre-trained Language Models via Quantization\n\n*From Search Query: state compression language models*\n\n*Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong*\n\n**TL;DR:** This paper compress generative PLMs by quantization with comparable performance with the full-precision models, and proposes a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules.\n\n**Abstract:** The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 88  (*Influential: 6*)\n\n#### 2. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: state compression language models*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n#### 3. LLMLingua: Compressing Prompts for Accelerated Inference of Large Language Models\n\n*From Search Query: state compression language models*\n\n*Huiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, Lili Qiu*\n\n**TL;DR:** A coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models.\n\n**Abstract:** Large language models (LLMs) have been applied in various applications due to their astonishing capabilities. With advancements in technologies such as chain-of-thought (CoT) prompting and in-context learning (ICL), the prompts fed to LLMs are becoming increasingly lengthy, even exceeding tens of thousands of tokens. To accelerate model inference and reduce cost, this paper presents LLMLingua, a coarse-to-fine prompt compression method that involves a budget controller to maintain semantic integrity under high compression ratios, a token-level iterative compression algorithm to better model the interdependence between compressed contents, and an instruction tuning based method for distribution alignment between language models. We conduct experiments and analysis over four datasets from different scenarios, i.e., GSM8K, BBH, ShareGPT, and Arxiv-March23; showing that the proposed approach yields state-of-the-art performance and allows for up to 20x compression with little performance loss. Our code is available at https://aka.ms/LLMLingua.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 62  (*Influential: 6*)\n\n#### 4. Memory Efficient Continual Learning with Transformers\n\n*From Search Query: efficient memory management transformers*\n\n*B. Ermi\u015f, Giovanni Zappella, Martin Wistuba, Aditya Rawal, C. Archambeau*\n\n**TL;DR:** This paper devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters that is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks.\n\n**Abstract:** In many real-world scenarios, data to train machine learning models becomes available over time. Unfortunately, these models struggle to continually learn new concepts without forgetting what has been learnt in the past. This phenomenon is known as catastrophic forgetting and it is difficult to prevent due to practical constraints. For instance, the amount of data that can be stored or the computational resources that can be used might be limited. Moreover, applications increasingly rely on large pre-trained neural networks, such as pre-trained Transformers, since the resources or data might not be available in sufficiently large quantities to practitioners to train the model from scratch. In this paper, we devise a method to incrementally train a model on a sequence of tasks using pre-trained Transformers and extending them with Adapters. Different than the existing approaches, our method is able to scale to a large number of tasks without significant overhead and allows sharing information across tasks. On both image and text classification tasks, we empirically demonstrate that our method maintains a good predictive performance without retraining the model or increasing the number of model parameters over time. The resulting model is also significantly faster at inference time compared to Adapter-based state-of-the-art methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 36  (*Influential: 1*)\n\n#### 5. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n*From Search Query: efficient memory management transformers*\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 1366  (*Influential: 136*)\n\n#### 6. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: efficient memory management transformers*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n### 4 related papers from Papers with Code\n\n#### 1. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n\n*From Search Query: state compression language models*\n\n*Carole-Jean Wu, Xing Liu, Bilge Acun, Chunxing Yin*\n\n**Abstract:** The memory capacity of embedding tables in deep learning recommendation models (DLRMs) is increasing dramatically from tens of GBs to TBs across the industry. Given the fast growth in DLRMs, novel solutions are urgently needed, in order to enable fast and efficient DLRM innovations. At the same time, this must be done without having to exponentially increase infrastructure capacity demands. In this paper, we demonstrate the promising potential of Tensor Train decomposition for DLRMs (TT-Rec), an important yet under-investigated context. We design and implement optimized kernels (TT-EmbeddingBag) to evaluate the proposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT implementation. The performance of TT-Rec is further optimized with the batched matrix multiplication and caching strategies for embedding vector lookup operations. In addition, we present mathematically and empirically the effect of weight initialization distribution on DLRM accuracy and propose to initialize the tensor cores of TT-Rec following the sampled Gaussian distribution. We evaluate TT-Rec across three important design space dimensions -- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM with Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112 times model size compression, for Kaggle and Terabyte, respectively. This impressive model size reduction can come with no accuracy nor training time overhead as compared to the uncompressed baseline.\n\n**Published:** 2021-01-25\n\n\n\n#### 2. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing\n\n*From Search Query: state compression language models*\n\n*Quoc V. Le, Zihang Dai, Yiming Yang, Guokun Lai*\n\n**Abstract:** With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at https://github.com/laiguokun/Funnel-Transformer.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-05\n\n\n\n#### 3. FedML Parrot: A Scalable Federated Learning System via Heterogeneity-aware Scheduling on Sequential and Hierarchical Training\n\n*From Search Query: efficient memory management transformers*\n\n*Chaoyang He, Salman Avestimehr, Alex Qiaozhong Liang, Yuxin Wang, Yonggang Zhang, Shaohuai Shi, Sunwoo Lee, Ryan Yide Ran, Xiaowen Chu, Zhenheng Tang*\n\n**Abstract:** Federated Learning (FL) enables collaborations among clients for train machine learning models while protecting their data privacy. Existing FL simulation platforms that are designed from the perspectives of traditional distributed training, suffer from laborious code migration between simulation and production, low efficiency, low GPU utility, low scalability with high hardware requirements and difficulty of simulating stateful clients. In this work, we firstly demystify the challenges and bottlenecks of simulating FL, and design a new FL system named as FedML \\texttt{Parrot}. It improves the training efficiency, remarkably relaxes the requirements on the hardware, and supports efficient large-scale FL experiments with stateful clients by: (1) sequential training clients on devices; (2) decomposing original aggregation into local and global aggregation on devices and server respectively; (3) scheduling tasks to mitigate straggler problems and enhance computing utility; (4) distributed client state manager to support various FL algorithms. Besides, built upon our generic APIs and communication interfaces, users can seamlessly transform the simulation into the real-world deployment without modifying codes. We evaluate \\texttt{Parrot} through extensive experiments for training diverse models on various FL datasets to demonstrate that \\texttt{Parrot} can achieve simulating over 1000 clients (stateful or stateless) with flexible GPU devices setting ($4 \\sim 32$) and high GPU utility, 1.2 $\\sim$ 4 times faster than FedScale, and 10 $\\sim$ 100 times memory saving than FedML. And we verify that \\texttt{Parrot} works well with homogeneous and heterogeneous devices in three different clusters. Two FL algorithms with stateful clients and four algorithms with stateless clients are simulated to verify the wide adaptability of \\texttt{Parrot} to different algorithms.\n\n**Published:** 2023-03-03\n\n\n\n#### 4. Metall: A Persistent Memory Allocator For Data-Centric Analytics\n\n*From Search Query: efficient memory management transformers*\n\n*Anonymous*\n\n**Abstract:** Data analytics applications transform raw input data into analytics-specific data structures before performing analytics. Unfortunately, such data ingestion step is often more expensive than analytics. In addition, various types of NVRAM devices are already used in many HPC systems today. Such devices will be useful for storing and reusing data structures beyond a single process life cycle. We developed Metall, a persistent memory allocator built on top of the memory-mapped file mechanism. Metall enables applications to transparently allocate custom C++ data structures into various types of persistent memories. Metall incorporates a concise and high-performance memory management algorithm inspired by Supermalloc and the rich C++ interface developed by Boost.Interprocess library. On a dynamic graph construction workload, Metall achieved up to 11.7x and 48.3x performance improvements over Boost.Interprocess and memkind (PMEM kind), respectively. We also demonstrate Metall's high adaptability by integrating Metall into a graph processing framework, GraphBLAS Template Library. This study's outcomes indicate that Metall will be a strong tool for accelerating future large-scale data analytics by allowing applications to leverage persistent memory efficiently.\n\n**Published:** 2021-08-10\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on state compression techniques, memory management strategies in transformer architectures, and efficient implementations of hybrid attention mechanisms, here are some key points and techniques from the provided sources and additional insights:\n\n## State Compression Techniques in Language Models\n\n### Post-Training Compression Using Pseudo-Random Generators\n- The SeedLM method is a novel post-training compression technique that uses seeds of pseudo-random generators to encode and compress model weights. This approach achieves nearly identical accuracy with 4-bit compression and shows significant speed-up over FP16 baselines, especially for large models like Llama 2 and Llama 3.\n\n### Layer Pruning and Replacement\n- Techniques like ShortGPT and LLM-Streamline involve pruning redundant layers based on their impact on hidden states. ShortGPT uses a Block Influence (BI) metric to identify and remove less important layers, while LLM-Streamline uses cosine similarity to assess layer importance and replaces pruned layers with lightweight networks to mitigate performance loss.\n\n### Quantization\n- Post-training quantization is another method to decrease the space requirement by lowering the precision of model parameters. This can be done by truncating numbers to a given number of bits or using different quantization codebooks per layer. Quantization can be combined with other techniques for further reduction in parameters and computation.\n\n## Memory Management Strategies in Transformer Architectures\n\n### Efficient Layer Pruning\n- Methods like LLM-Streamline and ShortGPT focus on identifying and removing redundant layers, which helps in reducing the memory footprint of the model. These techniques ensure that the model's performance is not significantly compromised while achieving substantial memory savings.\n\n### Quantization and Precision Management\n- Using lower precision (e.g., float16 or even lower) for model parameters can significantly reduce memory requirements. This can be further optimized by applying different precisions to different parameters, with higher precision for critical weights (\"outlier weights\").\n\n### Feedback Attention Mechanisms\n- Techniques such as TransformerFAM, which incorporates feedback attention as a form of working memory, can help in managing the state and memory efficiently. This approach can enhance the model's ability to handle long-range dependencies and reduce the need for excessive memory (referenced in the analysis note).\n\n## Efficient Implementations of Hybrid Attention Mechanisms\n\n### Linear Attention Variants\n- Linear attention mechanisms can provide efficiency gains over traditional self-attention. These variants reduce the computational complexity from \\(O(n^2)\\) to \\(O(n)\\), making them more suitable for large-scale models[Analysis Note].\n\n### Bidirectional Processing\n- Models like Hydra, which use bidirectional state space models, can combine causal and non-causal processing. This approach uses generalized matrix mixers and can offer superior performance on non-causal tasks while maintaining autoregressive properties[Analysis Note].\n\n### Selective State Space Models\n- Selective state space models (SSMs) can be viewed as attention-driven models and offer new perspectives on attention design. These models enable empirical and theoretical comparisons with self-attention layers in transformers, potentially leading to more efficient attention mechanisms[Analysis Note].\n\n### Hybrid Causal/Non-Causal Models\n- Combining causal and non-causal processing, as seen in Hydra, can provide a balanced approach that leverages the strengths of both. This involves parameterizing the model as a quasiseparable matrix mixer, which is efficient and maintains autoregressive properties[Analysis Note].\n\n## Additional Insights\n\n### Time-Aware Mechanisms\n- Time-aware large kernel convolutions (TaLK Convolutions) can learn to predict the size of a summation kernel, leading to linear time complexity. This can be integrated into transformer architectures to enhance efficiency[Analysis Note].\n\n### Test-Time Training and Adaptation\n- Test-time training on nearest neighbors can significantly improve model performance with minimal computational overhead. Combining this with efficient attention mechanisms can further optimize the model's adaptability and efficiency[Analysis Note].\n\nBy integrating these techniques\u2014state compression through layer pruning and quantization, efficient memory management via feedback attention and precision management, and hybrid attention mechanisms that combine causal and non-causal processing\u2014the researcher can develop an autoregressive language model that is highly efficient, scalable, and performs well on various tasks.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe initial research reveals several promising directions for improving the seed GPT2 design:\n\na) Test-Time Adaptation & State Management:\n- Test-time training (TTT) shows potential for improving model adaptability\n- Bidirectional processing can enhance model capabilities while maintaining autoregressive properties\n- Time-aware mechanisms offer efficiency improvements\n\nb) Attention & Processing Mechanisms:\n- Linear attention variants provide efficiency gains\n- Hybrid approaches combining causal and non-causal processing show promise\n- Selective state space models offer new perspectives on attention\n\nc) Memory & State Optimization:\n- Feedback attention mechanisms for working memory\n- State compression techniques for efficiency\n- Adaptive computation for resource optimization\n\n2. Useful References with Excerpts\n\nFrom \"Test-Time Training on Nearest Neighbors\":\n\"For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance.\"\n\nFrom \"Hydra: Bidirectional State Space Models\":\n\"Hydra demonstrates superior performance over other sequence models including Transformers on non-causal tasks [...] parameterized as a quasiseparable matrix mixer.\"\n\nFrom \"Time-aware Large Kernel Convolutions\":\n\"TaLK Convolutions [...] learns to predict the size of a summation kernel instead of using a fixed-sized kernel matrix. This method yields a time complexity of O(n).\"\n\nFrom \"The Hidden Attention of Mamba Models\":\n\"Selective SSMs can be viewed as attention-driven models [...] enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers.\"\n\n3. Key Insights & Detailed Analysis\n\na) Test-Time Adaptation:\n- TTT can significantly improve model performance with minimal computational overhead\n- Neighbor-based fine-tuning offers a promising approach for context adaptation\n- Challenge: Balancing adaptation speed with computational efficiency\n\nb) Bidirectional Processing:\n- Hydra's success suggests value in combining causal and non-causal processing\n- Matrix mixer parameterization offers efficient implementation path\n- Challenge: Maintaining autoregressive properties while incorporating bidirectional elements\n\nc) Efficient State Management:\n- Time-aware mechanisms provide linear complexity solutions\n- Selective state space models offer new perspectives on attention design\n- Opportunity: Combining multiple efficiency techniques\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of state compression techniques\n2. Methods for combining TTT with efficient attention mechanisms\n3. Detailed architectures of hybrid causal/non-causal models\n4. Memory management strategies in large language models\n\n5. References\n\n[1] Sun, Y., & Hardt, M. (2023). Test-Time Training on Nearest Neighbors for Large Language Models. arXiv preprint.\n\n[2] Hwang, S., et al. (2024). Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers. arXiv preprint.\n\n[3] Lioutas, V., & Guo, Y. (2020). Time-aware Large Kernel Convolutions. International Conference on Machine Learning.\n\n[4] Ali, A., et al. (2024). The Hidden Attention of Mamba Models. arXiv preprint.\n\n[5] Hwang, D., et al. (2024). TransformerFAM: Feedback attention is working memory. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hybrid attention SSM architectures, compression expressiveness balance",
            "detail": "Provide detailed information about:\n1. Implementations of hybrid attention-SSM architectures\n2. Methods for balancing compression and model expressiveness\n3. Integration strategies for dense connections in attention mechanisms",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide detailed information about:\n1. Implementations of hybrid attention-SSM architectures\n2. Methods for balancing compression and model expressiveness\n3. Integration strategies for dense connections in attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.60)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.86)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.34)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.14)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.19)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.10)*\n\n```\nWhile state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5\\% accuracy improvement on public benchmarks. code is avalaible at: https://github.com/ WailordHe/DenseSSM. ## 1. Introduction\n\nSince the release of ChatGPT (OpenAI, 2023), large language models have entered a new epoch, showcasing outstanding abilities in language comprehension, dialogue, and logical reasoning. Over the past year, the industry has witnessed the emergence of numerous large language models, such as LLaMA (Touvron et al., 2023) and ChatGLM (Zeng et al., 2023). These large language models have given rise to a plethora of practical applications, including conversational bots, code assistants, and AI agents. The foundation of large language models lies in the Transformer network\n\n[^0]structure (Vaswani et al., 2017), primarily utilizing a multihead self-attention module for modeling relationships between tokens and a Feed-forward network for non-linear feature transformations. The scaling law (Kaplan et al., 2020) based on the Transformer structure has propelled the continuous development and expansion of large language models. In the Transformer network, multi-head self-attention (MHSA) plays a crucial role, but it comes with significant computational demands and memory requirements during inference. In terms of computational complexity, for an input sentence of length $N$, the calculation of selfattention has a complexity of $O\\left(N^{2}\\right)$ during training and inference. Regarding memory usage, previously encountered keys and values are stored, leading to a memory occupation of $O(N D)$. As a result, recent efforts on network architectures have focused on simplifying Transformer by reducing its computation and space complexity. This includes various approaches, notably convolutional language models (Poli et al., 2023), recurrent unit (Lei, 2021), long context models (Ding et al., 2023), and state space models (SSMs) (Gu et al., 2021; Gu \\& Dao, 2023). These new models have provided strong alternatives to Transformer for building efficient LLMs. SSMs propose modeling sequences by introducing an appropriate design of hidden states for handling long-range dependencies with both training parallelizability and inference efficiency. Starting from the continuous mapping system, SSMs are discretized to process discrete inputs in deep learning such as language sequence. The discretized SSMs can be computed in both linear recurrence and global convolution modes. Commonly, convolution mode is used during training to achieve parallel acceleration, while recurrence mode is used during autoregressive inference because it has lower computational complexity. The core distinction of SSMs from other neural networks, such as fully-connected neural networks, lies in the design of hidden states. Hidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters $A$, hidden states transfer the hidden information from the previous time\nsteps to the current time step, allowing for autoregressive prediction of the next token. Hidden states play a crucial role in SSMs, but have not received sufficient investigation in the past. Weights and hidden features in different layers contain information at various levels from fine-grained to coarsegrained (Gu et al., 2021). However, in previous versions of SSMs, hidden states only flowed within the current layer and could not transmit more information to deeper layers, thus failing to capture more hierarchical information. In this paper, we propose DenseSSM to facilitate a more comprehensive flow of hidden information between layers in state space models. We first analyze the hidden state degradation in conventional SSMs which will prevent hidden information flow from low levels to high levels. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information that is useful for the final output. The proposed method is applicable to different types of SSMs, such as RetNet (Sun et al., 2023) and Mamba (Gu \\& Dao, 2023). Our approach maintains the training parallelizability and inference efficiency of SSMs, while achieving a significant improvement with only a slight increase in the number of parameters. For instance, our DenseRetNet model outperforms traditional RetNet with up to 5\\% accuracy improvement on public benchmarks.\n```\n\n#### 3. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.06)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.06)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: hybrid attention SSM architectures, compression expressiveness balance\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Block-State Transformers\n\n*From Search Query: hybrid attention SSM architectures*\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention SSM architectures*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. CoAtNet: Marrying Convolution and Attention for All Data Sizes\n\n*From Search Query: hybrid attention SSM architectures*\n\n*Zihang Dai, Hanxiao Liu, Quoc V. Le, Mingxing Tan*\n\n**TL;DR:** This work presents CoAtNets, a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention and (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency.\n\n**Abstract:** Transformers have attracted increasing interests in computer vision, but they still fall behind state-of-the-art convolutional networks. In this work, we show that while Transformers tend to have larger model capacity, their generalization can be worse than convolutional networks due to the lack of the right inductive bias. To effectively combine the strengths from both architectures, we present CoAtNets(pronounced\"coat\"nets), a family of hybrid models built from two key insights: (1) depthwise Convolution and self-Attention can be naturally unified via simple relative attention; (2) vertically stacking convolution layers and attention layers in a principled way is surprisingly effective in improving generalization, capacity and efficiency. Experiments show that our CoAtNets achieve state-of-the-art performance under different resource constraints across various datasets: Without extra data, CoAtNet achieves 86.0% ImageNet top-1 accuracy; When pre-trained with 13M images from ImageNet-21K, our CoAtNet achieves 88.56% top-1 accuracy, matching ViT-huge pre-trained with 300M images from JFT-300M while using 23x less data; Notably, when we further scale up CoAtNet with JFT-3B, it achieves 90.88% top-1 accuracy on ImageNet, establishing a new state-of-the-art result.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 997  (*Influential: 105*)\n\n#### 4. LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\n\n*From Search Query: compression expressiveness balance*\n\n*Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo Zhao*\n\n**TL;DR:** LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix, which significantly outperforms existing compression methods.\n\n**Abstract:** Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 43  (*Influential: 4*)\n\n#### 5. RECOMBINER: Robust and Enhanced Compression with Bayesian Implicit Neural Representations\n\n*From Search Query: compression expressiveness balance*\n\n*Jiajun He, Gergely Flamich, Zongyu Guo, Jos\u00e9 Miguel Hern\u00e1ndez-Lobato*\n\n**TL;DR:** The proposed method, Robust and Enhanced COMBINER (RECOMBINER), achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates.\n\n**Abstract:** COMpression with Bayesian Implicit NEural Representations (COMBINER) is a recent data compression method that addresses a key inefficiency of previous Implicit Neural Representation (INR)-based approaches: it avoids quantization and enables direct optimization of the rate-distortion performance. However, COMBINER still has significant limitations: 1) it uses factorized priors and posterior approximations that lack flexibility; 2) it cannot effectively adapt to local deviations from global patterns in the data; and 3) its performance can be susceptible to modeling choices and the variational parameters' initializations. Our proposed method, Robust and Enhanced COMBINER (RECOMBINER), addresses these issues by 1) enriching the variational approximation while retaining a low computational cost via a linear reparameterization of the INR weights, 2) augmenting our INRs with learnable positional encodings that enable them to adapt to local details and 3) splitting high-resolution data into patches to increase robustness and utilizing expressive hierarchical priors to capture dependency across patches. We conduct extensive experiments across several data modalities, showcasing that RECOMBINER achieves competitive results with the best INR-based methods and even outperforms autoencoder-based codecs on low-resolution images at low bitrates. Our PyTorch implementation is available at https://github.com/cambridge-mlg/RECOMBINER/.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 6. Are GATs Out of Balance?\n\n*From Search Query: compression expressiveness balance*\n\n*Nimrah Mustafa, Aleksandar Bojchevski, R. Burkholz*\n\n**TL;DR:** A conservation law of GAT gradient flow dynamics is derived, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training, and an initialization scheme that balances the GAT network is devised.\n\n**Abstract:** While the expressive power and computational capabilities of graph neural networks (GNNs) have been theoretically studied, their optimization and learning dynamics, in general, remain largely unexplored. Our study undertakes the Graph Attention Network (GAT), a popular GNN architecture in which a node's neighborhood aggregation is weighted by parameterized attention coefficients. We derive a conservation law of GAT gradient flow dynamics, which explains why a high portion of parameters in GATs with standard initialization struggle to change during training. This effect is amplified in deeper GATs, which perform significantly worse than their shallow counterparts. To alleviate this problem, we devise an initialization scheme that balances the GAT network. Our approach i) allows more effective propagation of gradients and in turn enables trainability of deeper networks, and ii) attains a considerable speedup in training and convergence time in comparison to the standard initialization. Our main theorem serves as a stepping stone to studying the learning dynamics of positive homogeneous models with attention mechanisms.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: hybrid attention SSM architectures*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid attention SSM architectures*\n\n*ChengXiang Zhai, Chenguang Zhu, Yichong Xu, Shuohang Wang, Yang Liu, Liliang Ren*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.\n\n**Conference:** sparse-modular-activation-for-efficient\n\n**Published:** 2023-06-19\n\n\n\n#### 3. MicroExpNet: An Extremely Small and Fast Model For Expression Recognition From Face Images\n\n*From Search Query: compression expressiveness balance*\n\n*Emre Akba\u015f, Eren \u015eener, \u0130lke \u00c7u\u011fu*\n\n**Abstract:** This paper is aimed at creating extremely small and fast convolutional neural networks (CNN) for the problem of facial expression recognition (FER) from frontal face images. To this end, we employed the popular knowledge distillation (KD) method and identified two major shortcomings with its use: 1) a fine-grained grid search is needed for tuning the temperature hyperparameter and 2) to find the optimal size-accuracy balance, one needs to search for the final network size (or the compression rate). On the other hand, KD is proved to be useful for model compression for the FER problem, and we discovered that its effects gets more and more significant with the decreasing model size. In addition, we hypothesized that translation invariance achieved using max-pooling layers would not be useful for the FER problem as the expressions are sensitive to small, pixel-wise changes around the eye and the mouth. However, we have found an intriguing improvement on generalization when max-pooling is used. We conducted experiments on two widely-used FER datasets, CK+ and Oulu-CASIA. Our smallest model (MicroExpNet), obtained using knowledge distillation, is less than 1MB in size and works at 1851 frames per second on an Intel i7 CPU. Despite being less accurate than the state-of-the-art, MicroExpNet still provides significant insights for designing a microarchitecture for the FER problem.\n\n**Published:** 2017-11-19\n\n\n\n#### 4. The role of time integration in energy conservation in Smoothed Particle Hydrodynamics fluid dynamics simulations\n\n*From Search Query: compression expressiveness balance*\n\n*Anonymous*\n\n**Abstract:** The choice of a time integration scheme is a crucial aspect of any transient fluid simulation, and Smoothed-Particle Hydrodynamics (SPH) is no exception. The influence of the time integration scheme on energy balance is here addressed. To do so, explicit expressions allowing to compute the deviations from the energy balance, induced by the time integration scheme, are provided. These expressions, computed \\textit{a posteriori}, are valid for different integration methods. Such formulation is tested with the simulation of a two-dimensional non-viscous impact of two jets, with no artificial dissipation terms. To the best of our knowledge, this is the first stable simulation of a non-dissipative system with a weakly-compressible SPH method. A viscous case, the Taylor-Green vortex, has also been simulated. Results show that an implicit time integration scheme also behaves better in a viscous context.\n\n**Published:** 2022-10-22\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on hybrid attention-SSM architectures, balancing compression and expressiveness, and integrating dense connections, here are some key points and strategies derived from the provided sources:\n\n## Implementations of Hybrid Attention-SSM Architectures\n\n### Taipan Architecture\nThe Taipan architecture combines the efficiency of Mamba-2 (a variant of State Space Models) with Selective Attention Layers (SALs) to enhance long-range dependency handling. This hybrid approach balances the computational efficiency of Mamba-2 with the expressive power of SALs, allowing the model to excel in tasks requiring both speed and accurate information retrieval.\n\n### Zamba Architecture\nThe Zamba architecture, as seen in models like Zamba2-7B, integrates Mamba layers with shared transformer blocks. This hybrid model alternates between Mamba layers and shared attention layers, utilizing techniques like LoRA projection matrices to add depth-specialization without significant parameter increase. This architecture achieves low inference latency and rapid generation while maintaining a smaller memory footprint compared to pure transformer models.\n\n### E-Tamba Architecture\nE-Tamba is another hybrid model that combines the strengths of Transformers and Mamba. It interleaves attention and Mamba layers, leveraging the memory efficiency of Mamba and the long-context capabilities of Transformers. E-Tamba demonstrates significant memory savings and improved performance in long-context retrieval tasks compared to pure Transformer models.\n\n## Methods for Balancing Compression and Model Expressiveness\n\n### Mimetic Initialization\nMimetic initialization techniques for state space layers can improve the recall abilities of SSMs by mimicking the patterns seen in self-attention layers. This approach allows Mamba layers to quickly learn tasks like copying and associative recall, enhancing their expressiveness while maintaining efficiency.\n\n### LoRA Projection Matrices\nUsing LoRA (Low-Rank Adaptation) projection matrices in hybrid models like Zamba2-7B enables the network to specialize at each invocation of the shared transformer layer without a significant increase in the total parameter count. This technique balances expressiveness and compression by allowing depth-specialization with minimal additional parameters.\n\n### Dense Connections\nDense connections between layers, as seen in DenseMamba, enhance the retention of fine-grained information crucial for the final output. This approach maintains training parallelizability and inference efficiency while improving the model's expressiveness[Analysis Note].\n\n## Integration Strategies for Dense Connections in Attention Mechanisms\n\n### Selective Attention Layers (SALs)\nIn the Taipan architecture, SALs employ a lightweight gating network to dynamically determine which tokens should undergo softmax attention processing. This selective attention mechanism integrates dense connections by focusing on critical tokens while maintaining overall efficiency.\n\n### Shared Attention Blocks with Rotary Position Embeddings\nIn the Zamba architecture, shared attention blocks are used in conjunction with rotary position embeddings. This integration improves performance by better maintaining information across depth and slightly enhancing performance over single shared blocks.\n\n### Layer Importance Analysis and Transplantation\nE-Tamba's approach involves a layer importance analysis and transplantation method, which allows for the fine-tuning of Transformer-Mamba hybrid models. This method integrates the strengths of both architectures by strategically replacing or combining layers to achieve a balance between memory efficiency and expressiveness.\n\n## Additional Insights\n\n### Memory Optimization\nTechniques like lossless compression of the KV cache (LoMA) and dense connections between layers can significantly optimize memory usage without compromising performance. Hybrid memory architectures that balance local and global information processing also show promise[Analysis Note].\n\n### State Management\nSelective state space models and content-based parameter adaptation in SSMs can address the weaknesses of traditional attention mechanisms, particularly in handling discrete modalities and long-range dependencies[Analysis Note].\n\nBy integrating these strategies, researchers can develop autoregressive language models that balance compression and expressiveness, efficiently handle long-range dependencies, and optimize memory usage, ultimately leading to improved overall performance.\n",
            "analysis": "Analysis Note: Memory and State Management in LM Blocks\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for improving LM block design:\n\na) Memory Management:\n- Lossless compression of KV cache shows potential for reducing memory demands\n- Dense hidden connections can enhance information flow between layers\n- Feedback attention mechanisms enable working memory capabilities\n\nb) State Space Models:\n- Selective SSMs offer efficient alternatives to traditional attention\n- DenseSSM demonstrates benefits of enhanced hidden state connections\n- Hybrid approaches combining SSMs with attention show promise\n\nc) Efficiency Techniques:\n- Time-aware mechanisms provide linear complexity solutions\n- Blockwise parallel processing enables longer sequence handling\n- Compression techniques can maintain performance while reducing resource usage\n\n2. Useful References with Excerpts\n\nFrom \"LoMA: Lossless Compressed Memory Attention\":\n\"We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\"\n\nFrom \"DenseMamba\":\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom \"Mamba: Linear-Time Sequence Modeling\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\n\nFrom \"Recurrent Memory Transformer\":\n\"Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence.\"\n\n3. Key Insights & Detailed Analysis\n\na) Memory Optimization:\n- Lossless compression of KV cache offers significant efficiency gains without performance loss\n- Dense connections between layers improve information retention\n- Hybrid memory architectures can balance local and global information processing\n\nb) State Management:\n- Selective state space models provide efficient alternatives to attention\n- Content-based parameter adaptation improves discrete modality handling\n- Recurrent memory mechanisms enable better long-range dependency modeling\n\nc) Architecture Integration:\n- Combining multiple efficiency techniques requires careful balance\n- Memory compression can be integrated with selective state tracking\n- Dense connections complement existing attention mechanisms\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of hybrid attention-SSM architectures\n2. Techniques for balancing compression and model expressiveness\n3. Methods for efficient integration of dense connections with attention\n4. Impact of different compression strategies on model performance\n\n5. References\n\n[1] Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv preprint.\n\n[2] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[4] Bulatov, A., et al. (2022). Recurrent Memory Transformer. Neural Information Processing Systems.\n\n[5] Liu, H., & Abbeel, P. (2023). Blockwise Parallel Transformer for Large Context Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "selective activation hybrid models, efficient state compression architectures",
            "detail": "Provide detailed information about:\n1. Implementation approaches for selective activation in hybrid models\n2. Methods for efficient state compression in mixed architectures\n3. Integration strategies for combining SSMs with attention mechanisms",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide detailed information about:\n1. Implementation approaches for selective activation in hybrid models\n2. Methods for efficient state compression in mixed architectures\n3. Integration strategies for combining SSMs with attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.99)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 1.00)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 1/32 (Score: 1.00)*\n\n```\n# Sparse Modular Activation for Efficient Sequence Modeling \n\nLiliang Ren ${ }^{1 *}$ Yang Liu ${ }^{2} \\quad$ Shuohang Wang ${ }^{2} \\quad$ Yichong Xu ${ }^{\\dagger}$<br>Chenguang Zhu ${ }^{2}$ Chengxiang Zhai ${ }^{1}$<br>${ }^{1}$ University of Illinois at Urbana-Champaign $\\quad{ }^{2}$ Microsoft<br>\\{liliang3, czhai\\}@illinois.edu<br>\\{yaliu10, shuowa, chezhu\\}@microsoft.com<br>xuyc11@gmail.com\n\n\n#### Abstract\n\nRecent hybrid models combining Linear State Space Models (SSMs) with selfattention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to suboptimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat. ## 1 Introduction\n\nRecent advance on efficient sequence modeling with State Space Models (SSMs) [GGR21; GDE ${ }^{+}$20; GGGR22; GB22; SWL23] has shown impressive performance for a wide range of tasks across modalities, such as text classification, image recognition and speech recognition. SSMs, as firstorder linear models, defined by a set of input, output, and state variables connected by first-order differential equations, can efficiently capture the recurrent structure in sequential data with carefully designed state matrices and the application of convolutional parallelism [GGR21]. However, they still significantly underperform the self-attention [BCB14; $\\mathrm{VSP}^{+}$17] based model in both language modeling and machine translation [VPSP23] tasks. A recent work [FDS ${ }^{+}$23] reveals that this is due to its deficiency of modeling the second-order pairwise comparisons between the input tokens, and shows that the augmentation of an additional shifted SSM layer can improve SSM's associative recalling\n\n[^0]ability. Furthermore, better quality-efficiency trade-off can be achieved by directly introducing extra self-attention modules to form a hybrid model (e.g. MEGA [MZK ${ }^{+}$23] and Hybrid H3 [FDS ${ }^{+}$23]) that utilizes both the first and the second order inductive biases, i.e., SSM and self-attention. However, the current hybrid models apply the attention modules statically and uniformly to each of the input token regardless the property of the task itself. This can lead to sub-optimal quality-efficiency trade-offs since not all input tokens require second-order modeling and this computation need can vary substantially depending on both its context and the task difficulty. In this paper, we aim to answer the following research questions for efficiently combining attention with SSMs:\n\n- RQ1: Can neural networks learn to activate their attention modules on demand to achieve better quality-efficiency trade-off? - RQ2: How much extra attention is needed for the SSMs on a task-by-task basis? To answer these questions, we develop a new general mechanism, Sparse Modular Activation (SMA), that allows a neural network to sparsely and dynamically activate its sub-modules for each of the input token in a fully differentiable manner. Specifically, we assume a neural model can be composed of multiple heterogeneous sub-modules. For the input sequence, a latent configurator sparsely maps tokens to multiple compressed sequences corresponding to sub-modules. Each sub-module is then only applied on its mapped shorter sequence. Compared with activating all sub-modules on the whole input, Sparse Modular Activation can reduce computation and memory consumption for both the training and inference stages. Notably, SMA is proved to have a full coverage of the combinatorial search space of module activation, which is further explained in Section 3.2. Efficient learning of dynamic sparsity is notoriously difficult under the constraint of the current parallel hardware [LQC ${ }^{+} 22$; GZYE20; XM22]. To enable the practical efficiency gains from our module-level sparsity, we provide a simple yet efficient parallel implementation of SMA without any custom fused GPU kernels. Specifically, when compressing a batch of sequences in SMA, our implementation conducts both token selection and the sequence re-padding simultaneously using a single scatter operation that is widely optimized and present in modern deep learning frameworks. To address RQ1, we apply SMA to construct a novel neural architecture, SeqBoat, that sparsely activate a Gated Attention Unit (GAU) [HDLL22] based on the state representation learned from an SSM. Both the GAU and the SSM representations are then aggregated through simple addition and activation to form a layer-level representation. Multiple same-sized SeqBoat layers are stacked sequentially to form a full neural model. Inspired by the working memory mechanism [AS68] used in human cognition, we further restrict the GAU to only apply local attention on the compressed sequence, which allows our model to have linear sequence inference complexity but theoretically infinite attention span. We conduct comprehensive experiments to show that SeqBoat has significantly better qualityefficiency trade-off than state-of-the-art hybrid models on a wide range of tasks, including Long Range Arena (LRA) [TDA ${ }^{+}$20], speech classification [War18] and language modeling [Hut06]. On the competitive LRA benchmark, SeqBoat achieves 1.96 higher average accuracy than MEGA-chunk $\\left[\\mathrm{MZK}^{+} 23\\right]$, the previous best hybrid model, with a $10.4 \\times$ training speed up and a $95 \\%$ memory reduction compared to the Transformer [VSP ${ }^{+}$17] on the Text task with 4,096 input length. Thanks to the intrinsic modular sparsity brought by SMA, SeqBoat directly reveals the amount of attention needed for each data sample of each task through its sparse activation patterns of GAU, addressing RQ2. We demonstrate that our working memory mechanism provides substantially better computation-accuracy trade-off than chunking based models, and analyze the relationship between the working memory size and the effective attention span on various long sequence modeling tasks. ## 2 Background\n\nTo motivate and clarify our proposed techniques, we first present a mathematical formulation of our Sparse Modular Activation mechanism and show how it encompasses and generalizes previous attempts that aimed for module-level dynamic sparsity. A dedicated section for detailed comparisons between our approach with the related works is also included in Appendix F. We begin by reviewing how the standard sequence modeling is formalized to establish the common ground for our discussion. ### 2.1 Time-Invariant Sequence Modeling\n\nGiven a discrete sequence, $\\mathbf{x}=\\left\\{x_{1}, \\ldots, x_{n}\\right\\} \\in \\mathbb{R}^{n}$, consisting of $n$ tokens, a time-invariant sequence model $P_{\\theta}$ is optimized to maximize the likelihood of the observed sequences by factorizing them as follows:\n\n$$\n\\max _{\\theta} P_{\\theta}(\\mathbf{x})=\\prod_{t=1}^{n} P\\left(x_{t} \\mid \\mathbf{x}_{<t}, \\theta\\right)\n$$\n\nwhere $\\mathbf{x}_{<t}=\\left\\{x_{1}, \\ldots, x_{t-1}\\right\\}$ is the sequence history at time step $t$, and the parameter $\\theta$ is independent of the time step $t$. This formulation implies that the full model parameters $\\theta$ and the full history $\\mathbf{x}_{<t}$ are both essential for the conditional prediction of each token $x_{t}$. However, one potential issue is as the prediction difficulty of each token may differ depending on the context and the position, this static model $P_{\\theta}$ can lead to sub-optimal accuracy-efficiency trade-off by wasting computation on either unimportant context $\\left[\\mathrm{SJP}^{+} 21\\right]$ or easy-to-predict tokens [Gra16]. ## 3 Learning Sparse Modular Activation\n\nTo cover a larger search space that may contain more efficient sequence models, we propose to formulate sequence modeling as a problem of finding an optimal time-variant model that can dynamically activate a subset of modules from a pre-defined function space for each time step. ### 3.1 Time-Variant Sequence Modeling\n\nFormally, a time-variant sequence model is defined on a compact function space $\\mathcal{F}: \\mathcal{X}_{t}^{c} \\mapsto[0,1]^{n \\times V}$, where $V$ is the size of the vocabulary and $\\mathcal{X}_{t}^{c}=\\left\\{\\mathbf{x}_{t}^{c}: \\mathbf{x}_{t}^{c} \\subseteq \\mathbf{x}_{<t} \\in \\mathcal{X} \\subseteq \\mathbb{R}^{n}\\right\\}$, contains all possible sub-sequences of the sequence history $\\mathbf{x}_{<t}$. Then for each of the token prediction at the time step $t$, the model learns to apply a function $f_{t} \\in \\mathcal{F}$ with the parameters $\\theta_{t}$ that maximizes the sequence probability, i.e.,\n\n$$\n\\max _{f_{t}, \\theta_{t}, \\mathbf{x}_{t}^{c}} P_{\\mathcal{F}}(\\mathbf{x})=\\prod_{t=1}^{n} P_{f_{t}}\\left(x_{t} \\mid \\mathbf{x}_{t}^{c}, \\theta_{t}\\right) \\quad \\text { s.t. } \\quad \\mathbf{x}_{t}^{c} \\subseteq \\mathbf{x}_{<t}\n$$\n\nThis formulation generalizes the previous works in pursuing a dynamic and sparse model for sequence modeling, where the connections are further explained in Appendix F. In this work, we assume the function space $\\mathcal{F}$ is chain-structured, i.e., $\\mathcal{F}=\\mathcal{H} \\circ \\mathcal{L}_{N} \\circ \\cdots \\circ \\mathcal{L}_{1} \\circ \\mathcal{E}$, where $\\mathcal{H}: \\mathbb{R}^{n \\times d_{m}} \\mapsto[0,1]^{n \\times V}$ is the classification function, $\\mathcal{E}: \\mathbb{R}^{n} \\mapsto \\mathbb{R}^{n \\times d_{m}}$ is the embedding function, $N$ is the number of intermediate layers, $d_{m}$ is the model size and $\\mathcal{L}: \\mathbb{R}^{n \\times d_{m}} \\mapsto \\mathbb{R}^{n \\times d_{m}}$ is the function space of the intermediate mappings. We further assume that $\\mathcal{L}$ is the spanning set of a finite number of the function $f_{i}^{l}$ with its parameters $\\theta_{i}^{l}$, i.e., $\\mathcal{L}=\\operatorname{span}\\left\\{f_{1}^{l}, \\ldots, f_{M}^{l}\\right\\}$, where $M$ is the number of pre-defined functions. These assumptions justify the design of our Sparse Modular Activation mechanism, which is further explained in the following section. ### 3.2 Sparse Modular Activation\n\nSparse Modular Activation (SMA) introduces a latent configurator at each time step $t$ and each layer of a neural sequence model.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 22/86 (Score: 0.99)*\n\n```\nY. Fu, et al. 2023; De et al. 2024; Glorioso et al. 2024; Lieber et al. 2024) suggests that a hybrid architecture with both SSM layers and attention layers could improve the model quality over that of a Transformer, or a pure SSM (e.g., Mamba) model, especially for in-context learning. We explore the different ways that SSD layers can be combined with attention and MLP to understand the benefits of each. Empirically we find that having around $10 \\%$ of the total number of layers being attention performs best. Combining SSD layers, attention layers, and MLP also works better than either pure Transformer++ or Mamba-2. SSD and Attention We find that SSD and attention layers are complementary: by themselves (e.g. in the Mamba-2 architecture vs. Transformer++) their performance (measured by perplexity) is nearly the same, but a mixture of SSD and attention layers outperforms the pure Mamba-2 or Transformer++ architecture. We show some results (Table 2) for the 350 M model ( 48 layers) trained to 7B tokens on the Pile with the GPT- 2 tokenizer (same number of parameters, same hyperparameters, same training and validation set). Adding in just a few attention layers already yields notable improvement and strikes the best balance between quality and efficiency. We hypothesize that the SSM layers function well as a general sequence-to-sequence mapping, and attention layers act as a retrieval mechanism to quickly refer to previous tokens in the sequence instead of forcing the model to compress all the context to its memory (SSM states). Table 2: (Combining SSD and Attention Blocks.) Perplexity of a 350 M model with 48 layers, with different number of attention layers. Having around a $10 \\%$ ratio of attention layers performs best. | Num. AtTn Blocks | 0 (Mamba-2) | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 9 | 11 | 15 | 24 | Transformer++ |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Perplexity $\\downarrow$ | 8.60 | 8.38 | 8.32 | 8.29 | 8.29 | 8.28 | $\\mathbf{8 . 2 6}$ | 8.27 | 8.28 | 8.30 | 8.34 | 8.50 | 8.68 |\n\nHybrid Models with SSD, MLP, and Attention We compare different ways that SSD can be combined with the (gated) MLP and attention layers, and evaluate at the 2.7B scale (64 layers), trained to 300B tokens on the Pile (same number of parameters, same hyperparameters, same training and validation set, same data order):\n\n1. Transformer++: 32 attention layers and 32 gated MLP, interleaving. 2. Mamba-2: 64 SSD layers. 3. Mamba-2-MLP: 32 SSD and 32 gated MLP layers, interleaving. 4. Mamba-2-Attention: 58 SSD layers and 6 attention layers (at indices $9,18,27,36,45,56)^{6}$. 5. Mamba-2-MLP-Attention: 28 SSD layers and 4 attention layers, interleaving with 32 gated MLP layers. We report the validation perplexity on the Pile, as well as zero-shot evaluation, in Table 3. In general, the quality of Transformer++ and Mamba-2 models are around the same. We see that adding just 6 attention layers noticeably improves over the pure Mamba-2 model (and over Transformer++). Adding MLP layers reduces model quality, but can (i) speed up training and inference due to the simplicity and hardware-efficiency of the MLP layer (ii) be easier to up-cycle to MoE models by replacing MLP layers with mixture-of-experts. Table 3: (Zero-shot Evaluations.) Best results for each size in bold. We compare different ways SSD, MLP, and attention layers can be combined, evaluated at 2.7 B scale trained to 300 B tokens on the Pile. | MODEl | Token. | PILE <br> PPL $\\downarrow$ | LAMBADA <br> PPL $\\downarrow$ | LAMBADA <br> ACC $\\uparrow$ | HellaSwag <br> ACC $\\uparrow$ | PIQA <br> ACC $\\uparrow$ | Arc-E <br> ACC $\\uparrow$ | Arc-C <br> ACC $\\uparrow$ | WinoGrande <br> ACC $\\uparrow$ | OpenbookQA <br> ACC $\\uparrow$ | Average <br> ACC $\\uparrow$ |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n| Transformer++ | NeoX | 6.13 | 3.99 | $\\underline{70.3}$ | 66.4 | 75.2 | 67.7 | $\\underline{37.8}$ | 63.9 | 40.4 | 60.2 |\n| Mamba-2 | NeoX | 6.09 | 4.10 | 69.7 | 66.6 | 76.4 | 69.6 | 36.4 | 64.0 | 38.8 | 60.2 |\n| Mamba-2-MLP | NeoX | 6.13 | 4.18 | 69.3 | 65.0 | 76.4 | 68.1 | 37.0 | 63.1 | 38.2 | 59.6 |\n| Mamba-2-Attention | NeoX | 5.95 | 3.85 | 71.1 | 67.8 | $\\underline{75.8}$ | 69.9 | $\\underline{37.8}$ | 65.3 | 39.0 | 61.0 |\n| Mamba-2-MLP-Attention | NeoX | 6.00 | 3.95 | 70.0 | 66.6 | 75.4 | 70.6 | 38.6 | 64.6 | 39.2 | 60.7 |\n\n### 9.3 Speed Benchmarks\n\nWe benchmark the speed of the SSD algorithm against Mamba's scan implementation and FlashAttention-2 (Figure 10). SSD, thanks to its reformulation to use matrix multiplication as a subroutine, can exploit specialized matrix multiplication (matmul) units on GPUs, also known as tensor cores. As a result, it is 2-8\u00d7 faster than Mamba's fused associative scan, which does not leverage matmul units. Due to its linear scaling in sequence length, SSD is faster than FlashAttention-2 starting at sequence length $2 K$. However, we note that the Mamba-2 model as a whole might not be as efficient to train as Transformer at short sequence length (e.g. at $2 K$ ), since a Transformer with $L$ layers would have $\\frac{L}{2}$ MLP layers and $\\frac{L}{2}$ attention layers, while a Mamba- 2 model would have $L$ SSD layers for the same number of parameters. Generally the MLP layers are very hardware efficient since they consist of simple matrix multiplication and pointwise linearity. As shown in Section 9.2.3, one can also combine $\\frac{L}{2}$ SSD layers and $\\frac{L}{2}$ MLP layers to speed up training at short sequence length. [^4]Table 4: (Ablations: Mamba-2 block.) We ablate the major differences between the Mamba-2 and Mamba-1 neural network blocks (Figure 6, Section 7.1). Note that these components are independent of the inner sequence mixing layer; in these ablations, we use SSD for the inner SSM layer (differing from the S6 layer of Mamba-1). | Block | ABCX Projections | Extra Normalization | Parameters | Perplexity |\n| :--- | :--- | :--- | :--- | :--- |\n| Mamba-1 | Sequential | $\\boldsymbol{X}$ | 129.3 M | 11.76 |\n|  | Sequential | $\\boldsymbol{\\checkmark}$ | 129.3 M | 11.54 |\n|  | Parallel | $\\boldsymbol{X}$ | 126.5 M | 11.66 |\n| Mamba-2 | Parallel | $\\boldsymbol{\\checkmark}$ | 126.5 M | 11.49 |\n\n### 9.4 Architecture Ablations\n\n### 9.4.1 Block Design\n\nSection 7.1 introduces the Mamba-2 block, which has small modifications to the Mamba-1 block which are partly motivated by the connection to attention and also to improve the scalability of Mamba-2. Table 4 ablates these architecture changes to the block, which occur outside of the core SSM layer. The ablations validate that parallel projections to create $(A, B, C, X)$ saves parameters and performs slightly better than Mamba's sequential projections. More importantly, this modification is amenable to tensor parallelism at larger model sizes (Section 8). Additionally, the extra normalization layer also slightly improves performance. More importantly, preliminary experiments at larger scales observed that it also helps with training stability. ### 9.4.2 Head Structure\n\nSection 7.2 describes how the dimensions of the $B, C, X$ projections can be viewed as a hyperparameter analogous to notions of multi-head attention and multi-query attention. We also showed how the original Mamba architecture is analogous to multi-value attention (Proposition 7.2), which was a choice that naturally developed from the state-space model point of view and was not previously ablated. Table 5 ablates choices of the multi-head structure for the Mamba-2 architecture. Strikingly, we find a large difference between multi-value and multi-query or multi-key head patterns, despite seeming very similar. Note that this is not explained by the total state size, which is the same for all of them (equal to HPN or the product of the number of heads, head dimension, and state dimension). We also compare to multi-head patterns where the number of $C, B, X$ (analogous to $Q, K, V$ ) heads is equal. We compare against the standard multi-head pattern, as well as one with aggressive sharing where they all have only 1 head. Note that in the latter case, the model still has H different sequence mixers $M$, because each head still has a different $A$. When parameter matched, these multi-head patterns perform similarly to each other, in between the MVA and MQA/MKA patterns. ### 9.4.3 Attention Kernel Approximations\n\nSection 7.3 noted how SSD can be combined with ideas from the linear attention literature, such as various forms of kernel approximations. We ablate several variants of these suggested by previous works in Table 6. These include the cosFormer (Qin, Weixuan Sun, et al. 2022), Random Feature Attention H. Peng et al. 2021, and Positive Random Features (Performer) (Choromanski et al. 2021). We also ablate adding a normalization term, akin to the denominator of the softmax function in standard attention. We found that this introduced instabilities to most variants, but slightly improved performance for the ReLU activation function $\\psi$. Table 7 also tests more recent proposals to improve linear attention that involve expanding the feature dimension (Based (Arora, Eyuboglu, Zhang, et al. 2024) and ReBased (Aksenov et al. 2024)). These linear attention extensions aim to appropriate the $\\exp$ kernel with a quadratic approximation. ReBased also proposes to replace the QK activation function with a layer normalization; from an SSM-centric view we apply a normalization on top of $(B, C)$ before applying the SSM function. Table 5: (Ablations: Multi-head structure.) All models have state expansion factor $N=64$ and head size $P=64$ and are trained to Chinchilla scaling law token counts. The number of $A$ heads is always equal to the total heads H , i.e. each head has a separate input-dependent $A$ decay factor. (Top) 125M models, 2.5B tokens (Bottom) 360 M models, 7 B tokens\n\n| SSM Head Pattern | Attn. Analog | $A$ heads | $B$ heads | $C$ heads | $X$ heads | Layers | Params | Ppl. |\n| :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- | :--- |\n| Multi-input (MIS) | Multi-value (MVA) | 24 | 1 | 1 | 24 | 24 | 126.5 M | $\\mathbf{1 1 . 6 6}$ |\n| Multi-contract (MCS) | Multi-query (MQA) | 24 | 1 | 24 | 1 | 24 | 126.5 M | 12.62 |\n| Multi-expand (MES) | Multi-key (MKA) | 24 | 24 | 1 | 1 | 24 | 126.5 M | 12.59 |\n| Multi-head (MHS) | Multi-head (MHA) | 24 | 24 | 24 | 24 | 15 | 127.6 M | 12.06 |\n| Multi-state (MSS) | - | 24 | 1 | 1 | 1 | 36 | 129.6 M | 12.00 |\n| Multi-input (MIS) | Multi-value (MVA) | 32 | 1 | 1 | 32 | 48 | 361.8 M | 8.73 |\n| Multi-contract (MCS) | Multi-query (MQA) | 32 | 1 | 32 | 1 | 48 | 361.8 M | 9.33 |\n| Multi-expand (MES) | Multi-key (MKA) | 32 | 32 | 1 | 1 | 48 | 361.8 M | 9.36 |\n| Multi-head (MHS) | Multi-head (MHA) | 32 | 1 | 1 | 1 | 70 | 361.3 M | 9.01 |\n| Multi-state (MSS) | - | 32 | 32 | 32 | 32 | 29 | 357.3 M | 9.04 |\n\nTable 6: (Ablations: Kernel approximations.) We test various proposals for the kernel activation function $\\psi$, including linear attention variants aiming to approximate the exp kernel from standard softmax attention. | Kernel activation $\\varphi$ | Perplexity |\n| :--- | :--- |\n| none | 11.58 |\n| Swish | 11.66 |\n| Exp | 11.62 |\n| ReLU | 11.73 |\n| ReLU + normalization | 11.64 |\n| cosFormer | 11.97 |\n| Random Feature Attention | 11.57 |\n| Positive Random Features (Performer) | 12.21 |\n\nTable 7: (Ablations: Kernel approximations.) We test the (Re)Based methods for linear attention approximations, which involve expanded feature maps. (Top) 130 M models. (Top) 380 M models with $N=256$. | Kernel activation $\\varphi$ | Perplexity |\n| :--- | :--- |\n| Swish | 11.67 |\n| Swish + Taylor (Based) | 12.19 |\n| LayerNorm | 11.50 |\n| LayerNorm + Square (ReBased) | 11.84 |\n| Swish | 8.58 |\n| Swish + Taylor (Based) | 8.71 |\n| LayerNorm | 8.61 |\n| LayerNorm + Square (ReBased) | 8.63 |\n\nWe note that this technique has been independently proposed as the \"QK-Norm\" for softmax attention (Team 2024) and an \"internal normalization\" for Mamba (Lieber et al. 2024). Overall, Table 6 and Table 7 found that the kernel approximation methods we tried did not seem to improve over simple pointwise non-linear activation functions for $\\psi$. Thus our default settings for Mamba-2 used $\\psi(x)=$ Swish $(x)$ to follow Mamba-1, but we suggest that removing this activation entirely may be a simpler choice that we did not extensively test. We emphasize however that SSD and vanilla linear attention differ in the inclusion of the 1-semiseparable mask $L$, while the various linear attention methods in the literature were derived to approximate softmax attention without this term; thus, our negative results may be not unexpected. ## 10 Related Work and Discussion\n\nThe state space duality framework bridges connections between SSMs, structured matrices, and attention. We discuss in more depth the relations between SSD and these concepts more broadly. Using ideas from each of the viewpoints, we also suggest some directions that the SSD framework can be extended in future work. ### 10.1 State Space Models\n\nStructured state space models can be characterized along the axes\n(i) whether it is time-invariant or time-varying.\n```\n\n#### 4. MambaMixer: Efficient Selective State Space Models with Dual Token and Channel Selection (Avg. Score: 0.97)\n\n*Ali Behrouz, Michele Santacatterina, Ramin Zabih*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 12  (*Influential: 3*)\n\n**TL;DR:** The results show that while Transformers, cross-channel attention, and MLPs are sufficient for good performance in time series forecasting, neither is necessary.\n\n**Abstract:** Recent advances in deep learning have mainly relied on Transformers due to their data dependency and ability to learn at scale. The attention module in these architectures, however, exhibits quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models, with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the success of SSMs, we present MambaMixer, a new architecture with data-dependent weights that uses a dual selection mechanism across tokens and channels, called Selective Token and Channel Mixer. MambaMixer connects selective mixers using a weighted averaging mechanism, allowing layers to have direct access to early features. As a proof of concept, we design Vision MambaMixer (ViM2) and Time Series MambaMixer (TSM2) architectures based on the MambaMixer block and explore their performance in various vision and time series forecasting tasks. Our results underline the importance of selective mixing across both tokens and channels. In ImageNet classification, object detection, and semantic segmentation tasks, ViM2 achieves competitive performance with well-established vision models and outperforms SSM-based vision models. In time series forecasting, TSM2 achieves outstanding performance compared to state-of-the-art methods while demonstrating significantly improved computational cost. These results show that while Transformers, cross-channel attention, and MLPs are sufficient for good performance in time series forecasting, neither is necessary.\n\n##### *Relevant Chunk: No. 2/35 (Score: 0.97)*\n\n```\nThe attention module in these architectures, however, exhibit quadratic time and space in input size, limiting their scalability for long-sequence modeling. Despite recent attempts to design efficient and effective architecture backbone for multi-dimensional data, such as images and multivariate time series, existing models are either data independent, or fail to allow inter- and intra-dimension communication. Recently, State Space Models (SSMs), and more specifically Selective State Space Models (S6), with efficient hardware-aware implementation, have shown promising potential for long sequence modeling. Motivated by the recent success of SSMs, we present MambaMixer block, a new architecture with data dependent weights that uses a dual selection mechanism across tokens and channels-called Selective Token and Channel Mixer. MambaMixer further connects the sequential selective mixers using a weighted averaging mechanism, allowing layers to have direct access to different layers' input and output. As a proof of concept, we design Vision MambaMixer (ViM2) and Time Series MambaMixer (TSM2) architectures based on MambaMixer block and explore their performance in various vision and time series forecasting tasks. Our results underline the importance of selectively mixing across both tokens and channels. In ImageNet classification, object detection, and semantic segmentation tasks, ViM2 achieves competitive performance with well-established vision models, i.e., ViT, MLP-Mixer, ConvMixer, and outperforms SSM-based vision models, i.e., ViM and VMamba. In time series forecasting, TSM2, an attention and MLPfree architecture, achieves outstanding performance compared to state-of-the-art methods while demonstrating significantly improved computational cost. These results show that while Transformers, cross-channel attention, and cross-channel MLPs are sufficient for good performance in practice, neither is necessary. ## 1 Introduction\n\nIn recent years, Transformers (Vaswani et al., 2017) have been the pivotal backbone architecture behind deep learning's success, enabling a number of breakthrough advances in language modeling (Wolf et al., 2019), vision (Dosovitskiy et al., 2021), time series (Zhou et al., 2021), healthcare (Tang et al., 2023), and several other domains (Radford et al., 2023; Behrouz et al., 2023). The attention modules in Transformers are crucial for their data dependency and enable them to generalize to unseen data and tasks given the context as input. They, however, are difficult to scale efficiently to long sequences due to their quadratic time and space complexity. Breaking this quadratic computational cost is a key step towards new possibilities for deep learning such as long-range\ncontext learning (Gu \\& Dao, 2023), large object detection (Zhu et al., 2024), and long-range time series forecasting (Liu et al., 2021a). To alleviate this computational complexity bottleneck, several recent studies have focused on designing sub-quadratic sequence models motivated by a diverse range of objectives: i.e., MLP-Mixer (Tolstikhin et al., 2021) and ConvMixer (Trockman \\& Kolter, 2023) are motivated as simpler alternatives to attention modules, MonarchMixer (M2) (Fu et al., 2023a) tackles efficiency without losing quality by using sparse Monarch matrices, and efficient attentions (Xiao et al., 2024; Kacham et al., 2023; Ding et al., 2023; Chen et al., 2021) sparsify or approximate the full attention matrix. These methods, however, either (1) are based on data-independent parameters, (2) introduce a trade-off between expressivity and speed, underperforming relative to Transformers when are scalable and efficient, or (3) are actually slow in practice, due to low hardware utilization (Dao et al., 2022; Chen et al., 2021). Recently, structured State Space Models (SSMs) have emerged as a promising class of architectures for sequence modeling (Gu et al., 2022b; Fu et al., 2023b; Smith et al., 2023). SSMs can be seen as a combination of Recurrent Neural Networks (RNNs) and Convolutional Neural Networks (CNNs), making them very efficient in training (as a CNN) and inference (as an RNN) (Gu et al., 2020). Despite their efficiency, primary general SSM architectures, e.g., S4 (Gu et al., 2022b), S4D (Gu et al., 2022a), are based on data-independent parameters, limiting their effectiveness in compressing context into a smaller state (Gu \\& Dao, 2023). To alleviate this limitation, recently, Gu \\& Dao (2023) present Mamba, a selective SSMs (S6) that effectively selects relevant context by making the SSM weights time variant (i.e., data dependent). Mamba achieves on par performance with Transformer-based state-of-the-art methods in language modeling while having less parameters and scaling near-linearly in sequence length (Gu \\& Dao, 2023), addressing all the three abovementioned limitations. The success of Mamba motivates several studies to adapt its design to different domains and modalities, e.g., vision (Zhu et al., 2024; Liu et al., 2024), graphs (Behrouz \\& Hashemi, 2024), videos (Li et al., 2024), DNA modeling (Schiff et al., 2024), etc. Surprisingly, Mamba and its variants independently apply the S6 block to each channel, overlooking information flow across channels (also known as Channel Mixing). The lack of channel mixing in Mamba not only results in stability issues while scaling to large-sized networks (Patro \\& Agneeswaran, 2024), but it also cause missing the relationships among feature maps, limiting Mamba's ability to model global information in multi-dimensional data such as images and multivariate time series. Using additional channel mixing blocks for S6 blocks, however, might be challenging in large-scale networks as due to their recurrent nature, increasing the number of blocks for both token and channel mixing can damage the information flow, limiting their ability to use early features. In this paper, we present MambaMixer, an efficient selective state space models with dual selection across both channels and tokens. MambaMixer sequentially uses Selective Token Mixer and Selective Channel Mixer, each of which consists of a bidirectional S6 block (Gu \\& Dao, 2023), to efficiently select and mix (resp. filter) informative (resp. irrelevant) tokens and channels. Contrary to original Mamba block that uses simple skip connections between consecutive blocks, inspired by DenseNet (Huang et al., 2017) and DenseFormer (Pagliardini et al., 2024), MambaMixer block allows layers to have direct access to earlier features (i.e., inputs and outputs of earlier layers) and further enhances the information flow between selective channel and token mixer blocks as well as different layers using a weighted averaging mechanism, enabling MambaMixer-based models to use a large number of layers and making training more stable for large networks. As a proof of concept, we employ MambaMixer block to design Vision MambaMixer (ViM2) and Time Series MambaMixer (TSM2) for vision and time series forecasting tasks, respectively. ViM2 first tokenizes the images and uses Cross-Scan Module (CSM) (Liu et al., 2024) to vertically and horizontally scan images, determining the order of tokens. It then, uses a MambaMixer block to selectively mixes tokens and channels. While TSM2 shares very similar architecture with ViM2, Contrary to ViM2, it uses unidirectional S6 blocks for time dimension due to its causal nature and inspired by (Chen et al., 2023), it uses an additional MambaMixer block to selectively mix auxiliary information of time series (whenever is available) as well as a 2-dimensional normalization across both time and variate dimensions. We further explore the performance of proposed models in various vision and time series forecasting tasks. In ImageNet classification, object detection, and semantic segmentation tasks, ViM2 achieves competitive performance with well-established vision models, and outperforms SSM-based vision models. In time series forecasting, TSM2 achieves outstanding performance compared to state-of-the-art methods on various datasets with diverse domains. Table 1: Comparison of the architecture design of MambaMixer and existing models. | Backbone | Token Mixing |  | Channel Mixing |  | Complexity | Models |\n| :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | Data-dependent | Module | Data-dependent | Module |  |  |\n| MLP-Mixer | - | MLP | - | MLP | $O\\left(L^{2}\\right)$ | MLP-Mixer (Tolstikhin et al., 2021) <br> DynaMixer (Wang et al., 2022b) |\n| ConvMixer | - | Conv | - | Conv | $O(L)$ | ConvMixer (Trockman \\& Kolter, 2023) |\n| Convolution | - | ConvNet | - | MLP | $O(L)$ | AlexNet (Krizhevsky et al., 2012) <br> ResNet (He et al., 2016) |\n| Transformers | $\\checkmark$ <br> $\\checkmark$ | Attention <br> Attention + Conv | - | MLP <br> MLP | $O\\left(L^{2}\\right)$ | ViT (Dosovitskiy et al., 2021) <br> DeIT (Touvron et al., 2021) <br> SwinTransformer (Liu et al., 2021b) |\n| SSMs | - <br> $\\checkmark$ <br> $\\checkmark$ | SSM <br> Selective SSM <br> Selective SSM | - <br> - <br> $\\checkmark$ | Selective SSM | $O(L)$ | Hyena (Poli et al., 2023) <br> H3 (Fu et al., 2023b) <br> Mamba (Gu \\& Dao, 2023) <br> Vim (Zhu et al., 2024) <br> MambaMixer (Ours) |\n\nContributions. To summarize, our contributions are: (1) Presenting MambaMixer block, a new SSM-based architecture with dual selection that efficiently and effectively selects and mixes (resp.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: selective activation hybrid models, efficient state compression architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Hybrid Uncertainty Quantification for Selective Text Classification in Ambiguous Tasks\n\n*From Search Query: selective activation hybrid models*\n\n*Artem Vazhentsev, Gleb Kuzmin, Akim Tsvigun, A. Panchenko, Maxim Panov, M. Burtsev, Artem Shelmanov*\n\n**TL;DR:** This paper proposes a new uncertainty estimation method that combines epistemic and aleatoric UE methods and shows that by using this hybrid method, it can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks.\n\n**Abstract:** Many text classification tasks are inherently ambiguous, which results in automatic systems having a high risk of making mistakes, in spite of using advanced machine learning models. For example, toxicity detection in user-generated content is a subjective task, and notions of toxicity can be annotated according to a variety of definitions that can be in conflict with one another. Instead of relying solely on automatic solutions, moderation of the most difficult and ambiguous cases can be delegated to human workers. Potential mistakes in automated classification can be identified by using uncertainty estimation (UE) techniques. Although UE is a rapidly growing field within natural language processing, we find that state-of-the-art UE methods estimate only epistemic uncertainty and show poor performance, or under-perform trivial methods for ambiguous tasks such as toxicity detection. We argue that in order to create robust uncertainty estimation methods for ambiguous tasks it is necessary to account also for aleatoric uncertainty. In this paper, we propose a new uncertainty estimation method that combines epistemic and aleatoric UE methods. We show that by using our hybrid method, we can outperform state-of-the-art UE methods for toxicity detection and other ambiguous text classification tasks.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 0*)\n\n#### 2. Enhancing Computation Efficiency in Large Language Models through Weight and Activation Quantization\n\n*From Search Query: selective activation hybrid models*\n\n*Janghwan Lee, Minsoo Kim, Seungcheol Baek, Seok Joong Hwang, Wonyong Sung, Jungwook Choi*\n\n**TL;DR:** Two innovative techniques are presented: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks to enhance computational efficiency.\n\n**Abstract:** Large Language Models (LLMs) are proficient in natural language processing tasks, but their deployment is often restricted by extensive parameter sizes and computational demands. This paper focuses on post-training quantization (PTQ) in LLMs, specifically 4-bit weight and 8-bit activation (W4A8) quantization, to enhance computational efficiency -- a topic less explored compared to weight-only quantization. We present two innovative techniques: activation-quantization-aware scaling (AQAS) and sequence-length-aware calibration (SLAC) to enhance PTQ by considering the combined effects on weights and activations and aligning calibration sequence lengths to target tasks. Moreover, we introduce dINT, a hybrid data format combining integer and denormal representations, to address the underflow issue in W4A8 quantization, where small values are rounded to zero. Through rigorous evaluations of LLMs, including OPT and LLaMA, we demonstrate that our techniques significantly boost task accuracies to levels comparable with full-precision models. By developing arithmetic units compatible with dINT, we further confirm that our methods yield a 2$\\times$ hardware efficiency improvement compared to 8-bit integer MAC unit.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 3. Selective Demonstrations for Cross-domain Text-to-SQL\n\n*From Search Query: selective activation hybrid models*\n\n*Shuaichen Chang, Eric Fosler-Lussier*\n\n**TL;DR:** A demonstration selection framework ODIS is proposed which utilizes both out-of-domain examples and synthetically generated in- domain examples to construct demonstrations, and outperforms state- of-the-art approaches on two cross-domain text-to-SQL datasets.\n\n**Abstract:** Large language models (LLMs) with in-context learning have demonstrated impressive generalization capabilities in the cross-domain text-to-SQL task, without the use of in-domain annotations. However, incorporating in-domain demonstration examples has been found to greatly enhance LLMs' performance. In this paper, we delve into the key factors within in-domain examples that contribute to the improvement and explore whether we can harness these benefits without relying on in-domain annotations. Based on our findings, we propose a demonstration selection framework ODIS which utilizes both out-of-domain examples and synthetically generated in-domain examples to construct demonstrations. By retrieving demonstrations from hybrid sources, ODIS leverages the advantages of both, showcasing its effectiveness compared to baseline methods that rely on a single data source. Furthermore, ODIS outperforms state-of-the-art approaches on two cross-domain text-to-SQL datasets, with improvements of 1.1 and 11.8 points in execution accuracy, respectively.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 4. COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\n\n*From Search Query: efficient state compression architectures*\n\n*Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan*\n\n**TL;DR:** This paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.\n\n**Abstract:** Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\\times$ speedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the existing works.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 5. Learnable Embedding Space for Efficient Neural Architecture Compression\n\n*From Search Query: efficient state compression architectures*\n\n*Shengcao Cao, Xiaofang Wang, Kris M. Kitani*\n\n**TL;DR:** It is demonstrated that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training.\n\n**Abstract:** We propose a method to incrementally learn an embedding space over the domain of network architectures, to enable the careful selection of architectures for evaluation during compressed architecture search. Given a teacher network, we search for a compressed network architecture by using Bayesian Optimization (BO) with a kernel function defined over our proposed embedding space to select architectures for evaluation. We demonstrate that our search algorithm can significantly outperform various baseline methods, such as random search and reinforcement learning (Ashok et al., 2018). The compressed architectures found by our method are also better than the state-of-the-art manually-designed compact architecture ShuffleNet (Zhang et al., 2018). We also demonstrate that the learned embedding space can be transferred to new settings for architecture search, such as a larger teacher network or a teacher network in a different architecture family, without any training. Code is publicly available here: this https URL .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 43  (*Influential: 6*)\n\n#### 6. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: efficient state compression architectures*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n### 4 related papers from Papers with Code\n\n#### 1. SkipNet: Learning Dynamic Routing in Convolutional Networks\n\n*From Search Query: selective activation hybrid models*\n\n*Zi-Yi Dou, Joseph E. Gonzalez, Fisher Yu, Xin Wang, Trevor Darrell*\n\n**Abstract:** While deeper convolutional networks are needed to achieve maximum accuracy in\nvisual perception tasks, for many inputs shallower networks are sufficient. We\nexploit this observation by learning to skip convolutional layers on a\nper-input basis. We introduce SkipNet, a modified residual network, that uses a\ngating network to selectively skip convolutional blocks based on the\nactivations of the previous layer. We formulate the dynamic skipping problem in\nthe context of sequential decision making and propose a hybrid learning\nalgorithm that combines supervised learning and reinforcement learning to\naddress the challenges of non-differentiable skipping decisions. We show\nSkipNet reduces computation by 30-90% while preserving the accuracy of the\noriginal model on four benchmark datasets and outperforms the state-of-the-art\ndynamic networks and static compression methods. We also qualitatively evaluate\nthe gating policy to reveal a relationship between image scale and saliency and\nthe number of layers skipped.\n\n**Conference:** skipnet-learning-dynamic-routing-in-1\n\n**Published:** 2017-11-26\n\n\n\n#### 2. A2-LINK: Recognizing Disguised Faces via Active Learning and Adversarial Noise based Inter-Domain Knowledge\n\n*From Search Query: selective activation hybrid models*\n\n*Mayank Vatsa, Richa Singh, Anshuman Suri*\n\n**Abstract:** Face recognition in the unconstrained environment is an ongoing research challenge. Although several covariates of face recognition such as pose and low resolution have received significant attention\u201c, disguise\u201d is considered an onerous covariate of face recognition. One of the primary reasons for this is the scarcity of large and representative labeled databases, along with the lack of algorithms that work well for multiple covariates in such environments. In order to address the problem of face recognition in the presence of disguise, the paper proposes an active learning framework termed as A2-LINK. Starting with a face recognition machine-learning model, A2-LINK intelligently selects training samples from the target domain to be labeled and, using hybrid noises such as adversarial noise, fine-tunes a model that works well both in the presence and absence of disguise. Experimental results demonstrate the effectiveness and generalization of the proposed framework on the DFW and DFW2019 datasets with state-of-the-art deep learning featurization models such as LCSSE, ArcFace, and DenseNet.\n\n**Proceeding:** ieee-transactions-on-biometrics-behavior-and\n\n**Published:** 2020-06-01\n\n\n\n#### 3. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing\n\n*From Search Query: efficient state compression architectures*\n\n*Quoc V. Le, Zihang Dai, Yiming Yang, Guokun Lai*\n\n**Abstract:** With the success of language pretraining, it is highly desirable to develop more efficient architectures of good scalability that can exploit the abundant unlabeled data at a lower cost. To improve the efficiency, we examine the much-overlooked redundancy in maintaining a full-length token-level presentation, especially for tasks that only require a single-vector presentation of the sequence. With this intuition, we propose Funnel-Transformer which gradually compresses the sequence of hidden states to a shorter one and hence reduces the computation cost. More importantly, by re-investing the saved FLOPs from length reduction in constructing a deeper or wider model, we further improve the model capacity. In addition, to perform token-level predictions as required by common pretraining objectives, Funnel-Transformer is able to recover a deep representation for each token from the reduced hidden sequence via a decoder. Empirically, with comparable or fewer FLOPs, Funnel-Transformer outperforms the standard Transformer on a wide variety of sequence-level prediction tasks, including text classification, language understanding, and reading comprehension. The code and pretrained checkpoints are available at https://github.com/laiguokun/Funnel-Transformer.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-05\n\n\n\n#### 4. 1-bit Adam: Communication Efficient Large-Scale Training with Adam's Convergence Speed\n\n*From Search Query: efficient state compression architectures*\n\n*Yuxiong He, Ce Zhang, Ji Liu, Xiangru Lian, Conglong Li, Samyam Rajbhandari, Ammar Ahmad Awan, Shaoduo Gan, Hanlin Tang*\n\n**Abstract:** Scalable training of large models (like BERT and GPT-3) requires careful optimization rooted in model design, architecture, and system capabilities. From a system standpoint, communication has become a major bottleneck, especially on commodity systems with standard TCP interconnects that offer limited network bandwidth. Communication compression is an important technique to reduce training time on such systems. One of the most effective methods is error-compensated compression, which offers robust convergence speed even under 1-bit compression. However, state-of-the-art error compensation techniques only work with basic optimizers like SGD and momentum SGD, which are linearly dependent on the gradients. They do not work with non-linear gradient-based optimizers like Adam, which offer state-of-the-art convergence efficiency and accuracy for models like BERT. In this paper, we propose 1-bit Adam that reduces the communication volume by up to $5\\times$, offers much better scalability, and provides the same convergence speed as uncompressed Adam. Our key finding is that Adam's variance (non-linear term) becomes stable (after a warmup phase) and can be used as a fixed precondition for the rest of the training (compression phase). Experiments on up to 256 GPUs show that 1-bit Adam enables up to $3.3\\times$ higher throughput for BERT-Large pre-training and up to $2.9\\times$ higher throughput for SQuAD fine-tuning. In addition, we provide theoretical analysis for our proposed work.\n\n**Published:** 2021-02-04\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design using hybrid architectures and efficient state management, here are some detailed approaches and methods based on the provided analysis and references:\n\n## Implementation Approaches for Selective Activation in Hybrid Models\n\n### Sparse Modular Activation (SMA)\nSMA enables neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. This approach reduces computation and memory consumption at both training and inference stages. SMA involves training the model to selectively activate only the necessary sub-modules, which can be particularly useful in hybrid models combining State Space Models (SSMs) with attention mechanisms.\n\n### Open-RAG's MoE Architecture\nThe Open-RAG framework transforms a dense Large Language Model (LLM) into a parameter-efficient sparse Mixture of Experts (MoE) model. This model selectively activates the most suitable experts dynamically for each query, ensuring that only relevant parts of the model are engaged. This selective activation is learned through fine-tuning using tailored training data, which helps in handling complex reasoning tasks efficiently.\n\n### Block-State Transformers\nBlock-State Transformers propose a hybrid layer that combines an SSM sublayer for long-range contextualization and a Block Transformer sublayer for short-term representation of sequences. This architecture allows for selective activation of different components based on the input sequence, optimizing computational resources and improving performance on language modeling tasks [Analysis Note].\n\n## Methods for Efficient State Compression in Mixed Architectures\n\n### Selective State Compression\nModels like Samba and Mamba use selective state compression to preserve important information while reducing the computational overhead. Samba, for example, selectively compresses a given sequence into recurrent hidden states, maintaining the ability to precisely recall memories. This approach ensures that only critical information is retained, reducing memory requirements without compromising performance [Analysis Note].\n\n### Hierarchical State Representations\nHierarchical state representations involve organizing the model's state in a layered or hierarchical manner. This can help in efficient state compression by allowing the model to focus on different levels of abstraction and detail. For instance, higher levels might capture long-range dependencies, while lower levels handle short-term context, reducing the overall memory footprint[Analysis Note].\n\n### Lossless Compression Techniques\nTechniques like LoSparse, which use low-rank and sparse approximations, can be applied to compress the attention mechanisms and other components of the model. These methods aim to reduce memory requirements without losing significant information, thereby maintaining the model's performance while optimizing its efficiency [Analysis Note].\n\n## Integration Strategies for Combining SSMs with Attention Mechanisms\n\n### Layer-wise Mixing Strategies\nModels like Samba combine SSMs with attention mechanisms layer-wise. This involves alternating between SSM layers that handle long-range contextualization and attention layers that focus on short-term representations. This layer-wise mixing strategy improves efficiency by leveraging the strengths of both components and reduces the computational overhead by selectively activating the necessary layers [Analysis Note].\n\n### Block-wise Combination\nBlock-State Transformers demonstrate a block-wise combination of SSMs and Block Transformers. This hybrid layer internally combines an SSM sublayer for long-range contextualization and a Block Transformer sublayer for short-term representation. This approach ensures that the model can handle both long-range and short-term dependencies efficiently, improving its performance on language modeling tasks [Analysis Note].\n\n### Dynamic Adaptation Mechanisms\nDynamic adaptation mechanisms allow the model to adjust its components based on the input. For example, Mamba lets the SSM parameters be functions of the input addresses, addressing the weakness of content-based reasoning in traditional SSMs. This dynamic adaptation enhances the model's flexibility and ability to handle diverse input contexts efficiently [Analysis Note].\n\nBy integrating these approaches, researchers can develop hybrid autoregressive language models that are not only efficient in terms of computation and memory but also highly performant and scalable.\n",
            "analysis": "Analysis Note: Hybrid Architectures and Efficient State Management\n\n1. Summary of Analysis\n\nThe research reveals several promising directions for improving the seed GPT2 design:\n\na) Hybrid Architectures:\n- Combining SSMs with attention mechanisms shows strong potential\n- Block-State Transformers demonstrate benefits of hybrid approaches\n- Selective activation and sparse computation offer efficiency gains\n\nb) State Management:\n- Selective state compression improves efficiency\n- Dense connections enhance information flow\n- Dynamic adaptation mechanisms show promise\n\nc) Memory Optimization:\n- Lossless compression techniques for attention\n- Hierarchical state representations\n- Adaptive computation paths\n\n2. Useful References with Excerpts\n\nFrom \"Block-State Transformers\":\n\"We propose a hybrid layer that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences [...] outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\"\n\nFrom \"Samba\":\n\"A simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories.\"\n\nFrom \"Sparse Modular Activation\":\n\"SMA enables neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner [...] reduces computation and memory consumption at both training and inference stages.\"\n\nFrom \"Mamba\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\n\n3. Key Insights & Detailed Analysis\n\na) Hybrid Architecture Design:\n- Block-wise combination of SSMs and attention shows promise\n- Selective activation can reduce computational overhead\n- Layer-wise mixing strategies improve efficiency\n- Challenge: Balancing component interactions\n\nb) State Management Strategies:\n- Selective compression preserves important information\n- Dense connections improve information flow\n- Dynamic adaptation enhances model flexibility\n- Challenge: Maintaining stability with complex interactions\n\nc) Memory Optimization:\n- Lossless compression reduces memory requirements\n- Hierarchical representations improve efficiency\n- Adaptive computation reduces resource usage\n- Challenge: Balancing compression and model capacity\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of selective activation in hybrid models\n2. Methods for efficient state compression in mixed architectures\n3. Integration strategies for dense connections\n4. Impact of different compression techniques on model performance\n\n5. References\n\n[1] Fathi, M., et al. (2023). Block-State Transformers. Neural Information Processing Systems.\n\n[2] Chen, W., et al. (2024). Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling. arXiv preprint.\n\n[3] Zhai, C., et al. (2023). Sparse Modular Activation for Efficient Sequence Modeling. arXiv preprint.\n\n[4] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[5] Li, Y., et al. (2023). LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "adaptive gating mechanisms memory optimization bidirectional processing",
            "detail": "Search for papers discussing:\n1. Adaptive gating mechanisms in language models\n2. Memory-efficient bidirectional processing techniques\n3. Integration strategies for combining different architectural components\n4. Stability analysis of test-time adaptation\n5. Efficient state management in hybrid architectures\n\nFocus on:\n- Implementation details of gating mechanisms\n- Memory efficiency techniques\n- Stability analysis\n- Performance comparisons\n- Integration strategies",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Adaptive gating mechanisms in language models\n2. Memory-efficient bidirectional processing techniques\n3. Integration strategies for combining different architectural components\n4. Stability analysis of test-time adaptation\n5. Efficient state management in hybrid architectures\n\nFocus on:\n- Implementation details of gating mechanisms\n- Memory efficiency techniques\n- Stability analysis\n- Performance comparisons\n- Integration strategies\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.38)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.38)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 2. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.29)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 13/24 (Score: 0.29)*\n\n```\nArXiv, abs/2205.14135, 2022. URL https://api.semanticscholar.org/CorpusID:249151871. Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with gated convolutional networks. In International conference on machine learning, pp. 933-941. PMLR, 2017. Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. DeepSeek-AI and Damai Dai. Deepseek-v2: A strong, economical, and efficient mixture-ofexperts language model.\n```\n\n#### 3. RRWKV: Capturing Long-range Dependencies in RWKV (Avg. Score: 0.21)\n\n*Leilei Wang*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** The paper devises the Retrospected Receptance Weighted Key Value architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n\n**Abstract:** Owing to the impressive dot-product attention, the Transformers have been the dominant architectures in various natural language processing (NLP) tasks. Recently, the Receptance Weighted Key Value (RWKV) architecture follows a non-transformer architecture to eliminate the drawbacks of dot-product attention, where memory and computational complexity exhibits quadratic scaling with sequence length. Although RWKV has exploited a linearly tensor-product attention mechanism and achieved parallelized computations by deploying the time-sequential mode, it fails to capture long-range dependencies because of its limitation on looking back at previous information, compared with full information obtained by direct interactions in the standard transformer. Therefore, the paper devises the Retrospected Receptance Weighted Key Value (RRWKV) architecture via incorporating the retrospecting ability into the RWKV to effectively absorb information, which maintains memory and computational efficiency as well.\n\n##### *Relevant Chunk: No. 7/11 (Score: 0.21)*\n\n```\nCurran Associates Inc. ISBN 9781510860964. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS'20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In Jill Burstein, Christy Doran, and Thamar Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pages 4171-4186. Association for Computational Linguistics, 2019. doi $10.18653 / \\mathrm{v} 1 / \\mathrm{n} 19-1423$. URL https://doi.org/10.18653/v1/n19-1423. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models, 2020. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735-1780, nov 1997. ISSN 0899-7667. doi:10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735. Junyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling, 2014. Sepp Hochreiter. The vanishing gradient problem during learning recurrent neural nets and problem solutions.\n```\n\n#### 4. Recurrent Attention Networks for Long-text Modeling (Avg. Score: 0.17)\n\n*Xianming Li, Zongxi Li, Xiaotian Luo, Haoran Xie, Xing Lee, Yingbin Zhao, Fu Lee Wang, Qing Li*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** A novel long-document encoding model, Recurrent Attention Network (RAN), is proposed to enable the recurrent operation of self-attention and is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively.\n\n**Abstract:** Self-attention-based models have achieved remarkable progress in short-text mining. However, the quadratic computational complexities restrict their application in long text processing. Prior works have adopted the chunking strategy to divide long documents into chunks and stack a self-attention backbone with the recurrent structure to extract semantic representation. Such an approach disables parallelization of the attention mechanism, significantly increasing the training cost and raising hardware requirements. Revisiting the self-attention mechanism and the recurrent structure, this paper proposes a novel long-document encoding model, Recurrent Attention Network (RAN), to enable the recurrent operation of self-attention. Combining the advantages from both sides, the well-designed RAN is capable of extracting global semantics in both token-level and document-level representations, making it inherently compatible with both sequential and classification tasks, respectively. Furthermore, RAN is computationally scalable as it supports parallelization on long document processing. Extensive experiments demonstrate the long-text encoding ability of the proposed RAN model on both classification and sequential tasks, showing its potential for a wide range of applications.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.17)*\n\n```\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell, Quoc Viet Le, and Ruslan Salakhutdinov. 2019. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, pages 2978-2988. Association for Computational Linguistics. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171-4186. Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Nitesh Bharadwaj Gundavarapu, Alex M Lamb, Nan Rosemary Ke, and Yoshua Bengio. 2022. Temporal latent bottleneck: Synthesis of fast and slow processing mechanisms in sequence learning. Advances in Neural Information Processing Systems, 35:10505-10520. Ming Ding, Chang Zhou, Hongxia Yang, and Jie Tang. 2020. Cogltx: Applying BERT to long texts. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020. Li Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pretraining for natural language understanding and generation. In Annual Conference on Neural Information Processing Systems 2019, pages 13042-13054. Edouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving neural language models with a continuous cache. In 5th International Conference on Learning Representations, ICLR 2017.\n```\n\n#### 5. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.16)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.16)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 3 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive gating mechanisms memory optimization bidirectional processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 3 related papers from Semantic Scholar\n\n#### 1. Gated Word-Character Recurrent Language Model\n\n*From Search Query: adaptive gating mechanisms memory optimization bidirectional processing*\n\n*Yasumasa Onoe, Kyunghyun Cho*\n\n**TL;DR:** A recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs that outperforms word- level language models on several English corpora.\n\n**Abstract:** We introduce a recurrent neural network language model (RNN-LM) with long short-term memory (LSTM) units that utilizes both character-level and word-level inputs. Our model has a gate that adaptively finds the optimal mixture of the character-level and word-level inputs. The gate creates the final vector representation of a word by combining two distinct representations of the word. The character-level inputs are converted into vector representations of words using a bidirectional LSTM. The word-level inputs are projected into another high-dimensional space by a word lookup table. The final vector representations of words are used in the LSTM language model which predicts the next word given all the preceding words. Our model with the gating mechanism effectively utilizes the character-level inputs for rare and out-of-vocabulary words and outperforms word-level language models on several English corpora.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2016\n\n**Citations:** 105  (*Influential: 12*)\n\n#### 2. From Bayesian Sparsity to Gated Recurrent Nets\n\n*From Search Query: adaptive gating mechanisms memory optimization bidirectional processing*\n\n*Hao He, Bo Xin, Satoshi Ikehata, D. Wipf*\n\n**TL;DR:** The parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences are examined, leading to a novel sparse estimation system that can estimate optimal solutions efficiently in regimes where other algorithms fail.\n\n**Abstract:** The iterations of many first-order algorithms, when applied to minimizing common regularized regression functions, often resemble neural network layers with pre-specified weights. This observation has prompted the development of learning-based approaches that purport to replace these iterations with enhanced surrogates forged as DNN models from available training data. For example, important NP-hard sparse estimation problems have recently benefitted from this genre of upgrade, with simple feedforward or recurrent networks ousting proximal gradient-based iterations. Analogously, this paper demonstrates that more powerful Bayesian algorithms for promoting sparsity, which rely on complex multi-loop majorization-minimization techniques, mirror the structure of more sophisticated long short-term memory (LSTM) networks, or alternative gated feedback networks previously designed for sequence prediction. As part of this development, we examine the parallels between latent variable trajectories operating across multiple time-scales during optimization, and the activations within deep network structures designed to adaptively model such characteristic sequences. The resulting insights lead to a novel sparse estimation system that, when granted training data, can estimate optimal solutions efficiently in regimes where other algorithms fail, including practical direction-of-arrival (DOA) and 3D geometry recovery problems. The underlying principles we expose are also suggestive of a learning process for a richer class of multi-loop algorithms in other domains.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 36  (*Influential: 6*)\n\n#### 3. A Combinatorial Perspective on Transfer Learning\n\n*From Search Query: adaptive gating mechanisms memory optimization bidirectional processing*\n\n*Jianan Wang, Eren Sezener, D. Budden, Marcus Hutter, J. Veness*\n\n**TL;DR:** This work studies how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data, and provides a concrete instantiation of the Forget-Me-Not Process using a combination of these techniques.\n\n**Abstract:** Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both offline and online methods on standard continual learning benchmarks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 8  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the provided keywords and analysis, here are some key findings and suggestions from the available sources:\n\n## Adaptive Gating Mechanisms in Language Models\n\n- The paper on Adaptive Task-planning Mixture of Experts (AT-MoE) introduces a dynamic weighting allocation mechanism through an adaptive grouped routing module. This approach enhances the model's capability to handle complex tasks by optimizing module fusion based on task instructions. The gating mechanism here is crucial as it determines the weights assigned to the outputs of different experts, ensuring that the appropriate amount of influence is given to each expert\u2019s output based on the specific input characteristics.\n\n## Memory-Efficient Bidirectional Processing Techniques\n\n- The concept of using sparse activation in Mixture of Experts (MoE) architectures can be seen as a memory-efficient approach. Sparsely-gated MoE layers activate only a chosen subset of experts during each forward pass, reducing computational overhead and memory usage. This method computes a weighted sum of the outputs from only the top-K experts instead of aggregating outputs from all experts.\n\n- The \"Scissorhands\" approach mentioned in the analysis note highlights the importance of efficient state management. By selecting only pivotal tokens that significantly influence future generations, it achieves a 5X reduction in inference memory usage without compromising model quality. This method can be integrated into bidirectional processing to optimize memory usage[Analysis Note].\n\n## Integration Strategies for Combining Different Architectural Components\n\n- The AT-MoE architecture integrates task-specific experts trained via the LoRA approach with a layer-wise adaptive grouped routing module. This integration strategy ensures that each expert network has a distinct task domain attribute, enhancing the system\u2019s capability to handle sophisticated and diverse challenges more effectively.\n\n- Hybrid architectures, as discussed in \"Mechanic Design and Scaling,\" combine various computational primitives such as Transformer, convolutional, and recurrent layers. These architectures have been shown to outperform state-of-the-art models by leveraging the strengths of different layers. Careful integration strategies are essential to maximize performance benefits[Analysis Note].\n\n## Stability Analysis of Test-Time Adaptation\n\n- Test-time training on nearest neighbors, as described by Hardt and Sun, demonstrates the feasibility of test-time adaptation with minimal computational overhead. However, this approach requires careful design of adaptation mechanisms to ensure stability and significant performance improvements[Analysis Note].\n\n- Stability in hybrid architectures can be enhanced by ensuring that the adaptation mechanisms are well-integrated and do not introduce instability. For example, using parameter-efficient fine-tuning (PEFT) approaches, as in AT-MoE, can help maintain efficiency and stability during model training and inference.\n\n## Efficient State Management in Hybrid Architectures\n\n- Efficient state management is crucial in hybrid architectures. The \"Scissorhands\" approach provides insights into importance-based token selection, which can significantly reduce memory usage without compromising model quality. This method can be adapted for hybrid architectures to manage states efficiently[Analysis Note].\n\n- The use of dynamic adaptability in state-assistance designs, similar to the attention mechanism in transformers, can also improve state management. This involves making matrix parameters input-dependent to only keep event features, thus optimizing the use of the hidden state.\n\n### Implementation Details and Performance Comparisons\n\n- The AT-MoE architecture provides detailed implementation insights, including the use of a gating network and expert layers that are jointly trained to minimize the overall loss function. This approach ensures that the gating network learns to route each input to the most relevant expert layer(s), enhancing performance and interpretability.\n\n- Performance comparisons between different architectures, such as sparsely-gated MoE and dense MoE, highlight the trade-offs between performance and computational efficiency. Sparsely-gated MoE layers are more efficient but may require careful optimization to maintain performance.\n\nBy integrating these insights, researchers can develop more efficient, scalable, and performant autoregressive language models that leverage adaptive gating mechanisms, memory-efficient bidirectional processing, and effective integration strategies for hybrid architectures.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about test-time training, state compression, and hybrid architectures in language models. The proposal's core ideas align with current research trends but require deeper investigation in specific areas.\n\n2. Useful References with Key Excerpts:\n\na) Test-Time Training on Nearest Neighbors (Hardt & Sun, 2023):\n\"...fine-tune the model on retrieved data at test time, using its standard training setup... retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance\"\n- Demonstrates feasibility of test-time adaptation\n- Shows significant performance improvements with minimal overhead\n\nb) Scissorhands (Liu et al., 2023):\n\"...hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations\"\n- Provides insights into efficient state management\n- Achieves \"5X reduction in inference memory usage without compromising model quality\"\n\nc) Mechanistic Design and Scaling (Massaroli et al., 2024):\n\"...hybrid architectures constructed from a variety of computational primitives... outperform state-of-the-art Transformer, convolutional, and recurrent architectures\"\n- Validates hybrid architecture approach\n- Shows importance of specialized layers\n\n3. Key Insights:\n\na) Test-Time Adaptation:\n- Feasible with minimal computational overhead\n- Can significantly improve performance\n- Requires careful design of adaptation mechanisms\n\nb) State Management:\n- Importance-based token selection is effective\n- Significant memory reduction possible\n- Need to balance compression vs. performance\n\nc) Hybrid Architectures:\n- Specialized layers show promise\n- Need careful integration strategy\n- Performance benefits demonstrated\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of adaptive gating mechanisms\n2. Memory-efficient bidirectional processing techniques\n3. Integration strategies for SSM and attention\n4. Stability analysis of test-time adaptation\n\n5. References:\n\n[1] Hardt, M., & Sun, Y. (2023). Test-Time Training on Nearest Neighbors for Large Language Models. International Conference on Learning Representations.\n\n[2] Liu, Z., et al. (2023). Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time. Neural Information Processing Systems.\n\n[3] Massaroli, S., et al. (2024). Mechanistic Design and Scaling of Hybrid Architectures. arXiv preprint.\n\n[4] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[5] Shu, M., et al. (2022). Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models. Neural Information Processing Systems."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "hybrid SSM attention test-time",
            "detail": "Search for papers discussing:\n1. Hybrid architectures combining state space models with attention mechanisms\n2. Test-time training or adaptation in language models\n3. Efficient bidirectional processing techniques\n4. State compression and selective computation in transformers\n5. Adaptive gating mechanisms for attention\n\nFocus on:\n- Implementation details\n- Efficiency considerations\n- Performance metrics\n- Computational overhead\n- Memory requirements",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hybrid architectures combining state space models with attention mechanisms\n2. Test-time training or adaptation in language models\n3. Efficient bidirectional processing techniques\n4. State compression and selective computation in transformers\n5. Adaptive gating mechanisms for attention\n\nFocus on:\n- Implementation details\n- Efficiency considerations\n- Performance metrics\n- Computational overhead\n- Memory requirements\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Latent Attention for Linear Time Transformers (Avg. Score: 0.93)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 12/21 (Score: 0.93)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient Attention: Attention with Linear Complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3531-3539, 2021. Smith, J. T., Warrington, A., and Linderman, S. W. Simplified State Space Layers for Sequence Modeling. arXiv preprint arXiv:2208.04933, 2022. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006, 2020a. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient Transformers: A Survey. arXiv preprint arXiv:2009.06732, 2020 b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention Is All You Need. Advances In Neural Information Processing Systems, 30, 2017. Wang, N., Gan, G., Zhang, P., Zhang, S., Wei, J., Liu, Q., and Jiang, X. ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer.\n```\n\n#### 3. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.88)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 28/32 (Score: 0.88)*\n\n```\nShazeer, and Quoc V. Le. Primer: Searching for efficient transformers for language modeling. ARXIV.ORG, 2021. [SUV18] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. NAACL, 2018. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. $\\left[\\mathrm{TDA}^{+}\\right.$20] Yi Tay, M. Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, J. Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference On Learning Representations, 2020. [VPSP23] Ali Vardasbi, Telmo Pires, Robin M. Schmidt, and Stephan Peitz. State spaces aren't enough: Machine translation needs attention. ARXIV.ORG, 2023. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NIPS, 2017. [War18] Pete Warden. Speech commands: A dataset for limited-vocabulary speech recognition.\n```\n\n#### 4. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.87)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.87)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 5. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.81)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.81)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n\n\n---\n## Found 1 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid SSM attention test-time\n\nConsidering refining your search by improving the query keywords input.\n\n### 1 related papers from Semantic Scholar\n\n#### 1. Neural Machine Translation\n\n*From Search Query: hybrid SSM attention test-time*\n\n*Rico Sennrich*\n\n**TL;DR:** This tutorial describes in detail the basic sequence-to-sequence architecture of NMT, the maximum likelihood training approach, and a simple beam-search decoder to produce translations, and describes techniques to build state-of-the-art NMT.\n\n**Abstract:** Neural Machine Translation (NMT) is a simple new architecture for getting machines to learn to translate. Despite being relatively new (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014), NMT has already shown promising results, achieving state-of-the-art performances for various language pairs (Luong et al, 2015a; Jean et al, 2015; Luong et al, 2015b; Sennrich et al., 2016; Luong and Manning, 2016). While many of these NMT papers were presented to the ACL community, research and practice of NMT are only at their beginning stage. This tutorial would be a great opportunity for the whole community of machine translation and natural language processing to learn more about a very promising new approach to MT. This tutorial has four parts. In the first part, we start with an overview of MT approaches, including: (a) traditional methods that have been dominant over the past twenty years and (b) recent hybrid models with the use of neural network components. From these, we motivate why an end-to-end approach like neural machine translation is needed. The second part introduces a basic instance of NMT. We start out with a discussion of recurrent neural networks, including the back-propagation-through-time algorithm and stochastic gradient descent optimizers, as these are the foundation on which NMT builds. We then describe in detail the basic sequence-to-sequence architecture of NMT (Cho et al., 2014; Sutskever et al., 2014), the maximum likelihood training approach, and a simple beam-search decoder to produce translations. The third part of our tutorial describes techniques to build state-of-the-art NMT. We start with approaches to extend the vocabulary coverage of NMT (Luong et al., 2015a; Jean et al., 2015; Chitnis and DeNero, 2015). We then introduce the idea of jointly learning both translations and alignments through an attention mechanism (Bahdanau et al., 2015); other variants of attention (Luong et al., 2015b; Tu et al., 2016) are discussed too. We describe a recent trend in NMT, that is to translate at the sub-word level (Chung et al., 2016; Luong and Manning, 2016; Sennrich et al., 2016), so that language variations can be effectively handled. We then give tips on training and testing NMT systems such as batching and ensembling. In the final part of the tutorial, we briefly describe promising approaches, such as (a) how to combine multiple tasks to help translation (Dong et al., 2015; Luong et al., 2016; Firat et al., 2016; Zoph and Knight, 2016) and (b) how to utilize monolingual corpora (Sennrich et al., 2016). Lastly, we conclude with challenges remained to be solved for future NMT. PS: we would also like to acknowledge the very first paper by Forcada and \u00d1eco (1997) on sequence-to-sequence models for translation!\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 0  (*Influential: 0*)\n\n\n\n---\n## Web search results\n\n To help the researchers improve their autoregressive language model design, here are some key findings and insights from the provided sources and additional context that align with the specified focus areas:\n\n## Hybrid Architectures Combining State Space Models with Attention Mechanisms\n\n- The paper on Taipan introduces a hybrid architecture that combines the efficiency of State Space Models (SSMs) like Mamba with enhanced long-range dependency handling through Selective Attention Layers (SALs).\n  - **Implementation Details**: Taipan uses Mamba-2 blocks, which are interpreted as a generalization of linear attention, and incorporates SALs to handle long-range dependencies efficiently. This design allows for a sparser attention weight map, reducing computational overhead.\n  - **Efficiency Considerations**: The hybrid design balances efficiency and expressiveness by using an attention budget constraint, ensuring that only a fraction of tokens receive attention. This approach is more efficient than full attention or standard windowed attention.\n  - **Performance Metrics**: Taipan is shown to handle theoretically unlimited context lengths during inference and improves performance in tasks requiring long-range dependencies.\n\n## Test-Time Training or Adaptation in Language Models\n\n- While the provided sources do not directly discuss test-time training in language models, the concept of test-time adaptation (TTA) in other domains can offer insights.\n  - **TTA in Other Domains**: The paper on Test-Time Intensity Consistency Adaptation (TICA) for shadow detection illustrates how TTA can be applied to refine models on test samples using unsupervised objectives. This could be analogous to adapting language models at test time by fine-tuning on the test data.\n  - **Computational Costs and Benefits**: TTA methods generally require updating only parts of the model (e.g., the encoder) during the adaptation phase, which can be computationally efficient. However, the specific implementation details and computational overhead would need to be tailored to language models.\n\n## Efficient Bidirectional Processing Techniques\n\n- The Taipan architecture provides insights into efficient bidirectional processing:\n  - **Bidirectional State Processing**: Taipan maintains autoregressive properties while allowing for bidirectional interactions through the use of SALs. This approach enables the model to capture long-range dependencies efficiently without the full computational cost of traditional attention mechanisms.\n  - **Efficiency Implications**: The use of SALs and the sparser attention weight map reduce the computational overhead compared to full attention mechanisms.\n\n## State Compression and Selective Computation in Transformers\n\n- The Taipan architecture also addresses state compression and selective computation:\n  - **Selective Computation**: Taipan incorporates an attention budget constraint, ensuring that only a fraction of tokens receive attention. This selective computation approach helps in reducing the computational overhead and memory requirements.\n  - **State Compression**: The model processes input tokens sequentially through Mamba-2 blocks, which can be seen as a form of state compression due to the efficient handling of long-range dependencies.\n\n## Adaptive Gating Mechanisms for Attention\n\n- The Taipan architecture includes adaptive gating mechanisms:\n  - **Adaptive Gating**: The gating network \\(G_\\theta\\) computes a score vector to determine which token representations should receive attention. This adaptive gating mechanism is crucial for selective attention and state compression, ensuring that the model focuses on relevant tokens while reducing computational overhead.\n\n### Summary of Key Findings\n\n- **Hybrid SSM-Attention Architectures**: Taipan combines SSMs with SALs, offering a balance between efficiency and long-range dependency handling.\n- **Test-Time Training**: While not directly addressed in the sources, TTA concepts from other domains can be adapted to language models, potentially offering efficient fine-tuning on test data.\n- **Efficient Bidirectional Processing**: Taipan's use of SALs enables efficient bidirectional processing, reducing computational overhead.\n- **State Compression and Selective Computation**: Taipan's attention budget constraint and sequential processing through Mamba-2 blocks achieve state compression and selective computation.\n- **Adaptive Gating Mechanisms**: The gating network in Taipan ensures adaptive and selective attention, optimizing model performance and efficiency.\n\nThese findings and insights should help the researchers in designing and improving their autoregressive language model by integrating hybrid SSM-attention architectures, efficient bidirectional processing, state compression, and adaptive gating mechanisms.\n",
            "analysis": "Initial Analysis of AdaptiveHybridMHA Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Selective State Space Models (SSMs) Integration:\n- How does the proposal's SSM implementation compare to Mamba and other recent SSM architectures?\n- What are the trade-offs between SSM and attention mechanisms in hybrid designs?\n- How does the adaptive gating system affect model performance and efficiency?\n\nb) Test-Time Training Integration:\n- How does the proposal implement test-time training in the context of hybrid attention?\n- What are the computational costs and benefits of test-time adaptation?\n- How does this compare to other adaptive attention mechanisms?\n\nc) Bidirectional State Processing:\n- How does the proposal maintain autoregressive properties while implementing bidirectional processing?\n- What are the efficiency implications of bidirectional state processing?\n- How does this compare to other bidirectional architectures like Hydra?\n\n2. Investigation Plan:\n\nPhase 1: SSM and Attention Hybrid Architectures\n- Search for existing hybrid architectures combining SSMs and attention\n- Analyze efficiency and performance trade-offs in hybrid designs\n- Investigate adaptive gating mechanisms in similar contexts\n\nPhase 2: Test-Time Training in Language Models\n- Research test-time adaptation techniques in language models\n- Analyze computational overhead and benefits\n- Compare with other adaptive mechanisms\n\nPhase 3: Bidirectional Processing in Autoregressive Models\n- Study existing approaches to bidirectional processing\n- Analyze efficiency implications\n- Investigate state compression techniques\n\n3. Initial Observations:\n\nStrengths:\n- Novel combination of SSM, attention, and test-time training\n- Potential for improved efficiency through selective processing\n- Adaptive mechanisms for context-specific optimization\n\nConcerns:\n- Complexity of integrating multiple advanced techniques\n- Potential computational overhead from test-time training\n- Implementation challenges for bidirectional processing\n\n4. Comparison with Previous Designs:\n\nThe proposal shows similarities with:\n- hierarchicalsparsegpt's adaptive computation approach\n- rwkv6_scankernel's efficient state processing\n- compressgpt's adaptive mechanisms\n\nHowever, it introduces unique elements:\n- Integration of test-time training with hybrid attention\n- Novel approach to bidirectional state processing\n- Selective activation and state compression mechanisms\n\n5. Research Directions:\n\nNeed to investigate:\n1. Recent advances in SSM architectures (Mamba, Hydra)\n2. Test-time training in language models\n3. Efficient bidirectional processing techniques\n4. State compression methods\n5. Adaptive gating mechanisms\n\nReferences found so far:\n1. Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers\n2. The Hidden Attention of Mamba Models\n3. Time-aware Large Kernel Convolutions\n4. TransformerFAM: Feedback attention is working memory\n\nNext steps:\n1. Search for hybrid SSM-attention architectures\n2. Investigate test-time training in language models\n3. Research efficient bidirectional processing techniques"
        },
        {
            "ready": false,
            "query": "adaptive attention mechanisms memory optimization",
            "detail": "Search for papers discussing:\n1. Recent advances in adaptive attention mechanisms\n2. Memory optimization techniques in transformer and SSM models\n3. Dynamic parameter updates during inference\n4. Efficient state management in hybrid architectures\n5. Test-time adaptation strategies for language models\n\nFocus on:\n- Implementation details of adaptive mechanisms\n- Memory efficiency techniques\n- State compression methods\n- Dynamic routing strategies\n- Performance comparisons",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Recent advances in adaptive attention mechanisms\n2. Memory optimization techniques in transformer and SSM models\n3. Dynamic parameter updates during inference\n4. Efficient state management in hybrid architectures\n5. Test-time adaptation strategies for language models\n\nFocus on:\n- Implementation details of adaptive mechanisms\n- Memory efficiency techniques\n- State compression methods\n- Dynamic routing strategies\n- Performance comparisons\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Latent Attention for Linear Time Transformers (Avg. Score: 0.85)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 12/21 (Score: 0.85)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language Models are Unsupervised Multitask Learners. OpenAI blog, 1(8):9, 2019. Shen, Z., Zhang, M., Zhao, H., Yi, S., and Li, H. Efficient Attention: Attention with Linear Complexities. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp. 3531-3539, 2021. Smith, J. T., Warrington, A., and Linderman, S. W. Simplified State Space Layers for Sequence Modeling. arXiv preprint arXiv:2208.04933, 2022. Tay, Y., Dehghani, M., Abnar, S., Shen, Y., Bahri, D., Pham, P., Rao, J., Yang, L., Ruder, S., and Metzler, D. Long Range Arena: A Benchmark for Efficient Transformers. arXiv preprint arXiv:2011.04006, 2020a. Tay, Y., Dehghani, M., Bahri, D., and Metzler, D. Efficient Transformers: A Survey. arXiv preprint arXiv:2009.06732, 2020 b. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi\u00e8re, B., Goyal, N., Hambro, E., Azhar, F., et al. LLaMA: Open and Efficient Foundation Language Models. arXiv preprint arXiv:2302.13971, 2023. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention Is All You Need. Advances In Neural Information Processing Systems, 30, 2017. Wang, N., Gan, G., Zhang, P., Zhang, S., Wei, J., Liu, Q., and Jiang, X. ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer.\n```\n\n#### 3. Efficient Long Sequence Modeling via State Space Augmented Transformer (Avg. Score: 0.81)\n\n*Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Xavier Charles, Eren Manavoglu, Tuo Zhao, Jianfeng Gao*\n\n**Published in:** arXiv.org (2022)\t**Cited by** 29  (*Influential: 3*)\n\n**TL;DR:** The proposed SPADE augments global information, which complements the lack of long-range dependency issue in local attention methods and demonstrates the scalability of the proposed method.\n\n**Abstract:** Transformer models have achieved superior performance in various natural language processing tasks. However, the quadratic computational cost of the attention mechanism limits its practicality for long sequences. There are existing attention variants that improve the computational efficiency, but they have limited ability to effectively compute global information. In parallel to Transformer models, state space models (SSMs) are tailored for long sequences, but they are not flexible enough to capture complicated local information. We propose SPADE, short for $\\underline{\\textbf{S}}$tate s$\\underline{\\textbf{P}}$ace $\\underline{\\textbf{A}}$ugmente$\\underline{\\textbf{D}}$ Transform$\\underline{\\textbf{E}}$r. Specifically, we augment a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods. Experimental results on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. To further demonstrate the scalability of SPADE, we pre-train large encoder-decoder models and present fine-tuning results on natural language understanding and natural language generation tasks.\n\n##### *Relevant Chunk: No. 27/35 (Score: 0.81)*\n\n```\nIn Proceedings of the 2013 Conference on\n\nEmpirical Methods in Natural Language Processing, pages 1631-1642, Seattle, Washington, USA. Association for Computational Linguistics. Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, and Armand Joulin. 2019. Adaptive attention span in transformers. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 331-335, Florence, Italy. Association for Computational Linguistics. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. 2021a. Synthesizer: Rethinking self-attention for transformer models. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 10183-10192. PMLR. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. 2020. Sparse sinkhorn attention. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 9438-9447. PMLR. Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2021b. Long range arena : A benchmark for efficient transformers. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Trieu H Trinh and Quoc V Le. 2018. A simple method for commonsense reasoning. ArXiv preprint, abs/1806.02847. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 49, 2017, Long Beach, CA, USA, pages 5998-6008.\n```\n\n#### 4. Compressive Transformers for Long-Range Sequence Modelling (Avg. Score: 0.70)\n\n*Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, T. Lillicrap*\n\n**Published in:** International Conference on Learning Representations (2019)\t**Cited by** 492  (*Influential: 64*)\n\n**TL;DR:** The Compressive Transformer is presented, an attentive sequence model which compresses past memories for long-range sequence learning and can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task.\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n##### *Relevant Chunk: No. 34/43 (Score: 0.70)*\n\n```\n2018. URL https://openreview.net/pdf?id=B1Yy1BxCZ\n\nS. Sukhbaatar, E. Grave, P. Bojanowski, and A. Joulin. Adaptive attention span in transformers. arXiv preprint arXiv:1905.07799, 2019. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998-6008, 2017. F. Wu, A. Fan, A. Baevski, Y. N. Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. arXiv preprint arXiv:1901.10430, 2019. Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019. L. Zhou, Y. Zhou, J. J. Corso, R. Socher, and C. Xiong. End-to-end dense video captioning with masked transformer. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8739-8748, 2018. Y. Zhu, R. Kiros, R. Zemel, R. Salakhutdinov, R. Urtasun, A. Torralba, and S. Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In Proceedings of the IEEE international conference on computer vision, pages 19-27, 2015.\n```\n\n#### 5. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.70)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.70)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive attention mechanisms memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Adaptive Attention Span in Transformers\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Sainbayar Sukhbaatar, Edouard Grave, Piotr Bojanowski, Armand Joulin*\n\n**TL;DR:** A novel self-attention mechanism that can learn its optimal attention span is proposed, which allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time.\n\n**Abstract:** We propose a novel self-attention mechanism that can learn its optimal attention span. This allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint and computational time. We show the effectiveness of our approach on the task of character level language modeling, where we achieve state-of-the-art performances on text8 and enwiki8 by using a maximum context of 8k characters.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 269  (*Influential: 17*)\n\n#### 2. Adaptive Robust Evidential Optimization For Open Set Detection from Imbalanced Data\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Hitesh Sapkota, Qi Yu*\n\n**TL;DR:** Adaptive Robust Evidential Optimization (AREO) is proposed that offers a principled way to quantify sample uncertainty through evidential learning while optimally balancing the model training over all classes in the closed set through adaptive distributively robust optimization (DRO).\n\n**Abstract:** Open set detection (OSD) aims at identifying data samples of an unknown class ( i.e., open set) from those of known classes ( i.e., closed set) based on a model trained from closed set samples. However, a closed set may involve a highly imbalanced class distribution. Accurately differentiating open set samples and those from a minority class in the closed set poses a fundamental challenge as the model may be equally uncertain when recognizing samples from the minority class. In this paper, we propose Adaptive Robust Evidential Optimization (AREO) that offers a principled way to quantify sample uncertainty through evidential learning while optimally balancing the model training over all classes in the closed set through adaptive distributively robust optimization (DRO). To avoid the model to primarily focus on the most difficult samples by following the standard DRO, adaptive DRO training is performed, which is governed by a novel multi-scheduler learning mechanism to ensure an optimal model training behavior that gives sufficient attention to the difficult samples and the minority class while capable of learning common patterns from the majority classes. Our experimental results on multiple real-world datasets demonstrate that the proposed model outputs uncertainty scores that can clearly separate samples from closed and open sets, respectively, and the detection results outperform the competitive baselines.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. Emergent mechanisms for long timescales depend on training curriculum and affect performance in memory tasks\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Sina Khajehabdollahi, R. Zeraati, E. Giannakakis, T. Schafer, Georg Martius, Anna Levina*\n\n**TL;DR:** It is suggested that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance.\n\n**Abstract:** Recurrent neural networks (RNNs) in the brain and in silico excel at solving tasks with intricate temporal dependencies. Long timescales required for solving such tasks can arise from properties of individual neurons (single-neuron timescale, $\\tau$, e.g., membrane time constant in biological neurons) or recurrent interactions among them (network-mediated timescale). However, the contribution of each mechanism for optimally solving memory-dependent tasks remains poorly understood. Here, we train RNNs to solve $N$-parity and $N$-delayed match-to-sample tasks with increasing memory requirements controlled by $N$ by simultaneously optimizing recurrent weights and $\\tau$s. We find that for both tasks RNNs develop longer timescales with increasing $N$, but depending on the learning objective, they use different mechanisms. Two distinct curricula define learning objectives: sequential learning of a single-$N$ (single-head) or simultaneous learning of multiple $N$s (multi-head). Single-head networks increase their $\\tau$ with $N$ and are able to solve tasks for large $N$, but they suffer from catastrophic forgetting. However, multi-head networks, which are explicitly required to hold multiple concurrent memories, keep $\\tau$ constant and develop longer timescales through recurrent connectivity. Moreover, we show that the multi-head curriculum increases training speed and network stability to ablations and perturbations, and allows RNNs to generalize better to tasks beyond their training regime. This curriculum also significantly improves training GRUs and LSTMs for large-$N$ tasks. Our results suggest that adapting timescales to task requirements via recurrent interactions allows learning more complex objectives and improves the RNN's performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Xiaoya Li, Yuxian Meng, Qinghong Han, Fei Wu, Jiwei Li*\n\n**TL;DR:** Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classification, it is demonstrated SAC is competitive with state-of-the-art models while significantly reducing memory cost.\n\n**Abstract:** While the self-attention mechanism has been widely used in a wide variety of tasks, it has the unfortunate property of a quadratic cost with respect to the input length, which makes it difficult to deal with long inputs. In this paper, we present a method for accelerating and structuring self-attentions: Sparse Adaptive Connection (SAC). In SAC, we regard the input sequence as a graph and attention operations are performed between linked nodes. In contrast with previous self-attention models with pre-defined structures (edges), the model learns to construct attention edges to improve task-specific performances. In this way, the model is able to select the most salient nodes and reduce the quadratic complexity regardless of the sequence length. Based on SAC, we show that previous variants of self-attention models are its special cases. Through extensive experiments on neural machine translation, language modeling, graph representation learning and image classification, we demonstrate SAC is competitive with state-of-the-art models while significantly reducing memory cost.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 20  (*Influential: 1*)\n\n#### 5. Towards Adaptive Prefix Tuning for Parameter-Efficient Language Model Fine-tuning\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Zhenru Zhang, Chuanqi Tan, Haiyang Xu, Chengyu Wang, Jun Huang, Songfang Huang*\n\n**TL;DR:** This work proposes Adaptive Prefix Tuning (APT), which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers that will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient.\n\n**Abstract:** Fine-tuning large pre-trained language models on various downstream tasks with whole parameters is prohibitively expensive. Hence, Parameter-efficient fine-tuning has attracted attention that only optimizes a few task-specific parameters with the frozen pre-trained model. In this work, we focus on prefix tuning, which only optimizes continuous prefix vectors (i.e. pseudo tokens) inserted into Transformer layers. Based on the observation that the learned syntax and semantics representation varies a lot at different layers, we argue that the adaptive prefix will be further tailored to each layer than the fixed one, enabling the fine-tuning more effective and efficient. Thus, we propose Adaptive Prefix Tuning (APT) to adjust the prefix in terms of both fine-grained token level and coarse-grained layer level with a gate mechanism. Experiments on the SuperGLUE and NER datasets show the effectiveness of APT. In addition, taking the gate as a probing, we validate the efficiency and effectiveness of the variable prefix.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. PTQ4SAM: Post-Training Quantization for Segment Anything\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Xianglong Liu, Yifu Ding, Jinyang Guo, Hong Chen, Chengtao Lv*\n\n**Abstract:** Segment Anything Model (SAM) has achieved impressive performance in many computer vision tasks. However, as a large-scale model, the immense memory and computation costs hinder its practical deployment. In this paper, we propose a post-training quantization (PTQ) framework for Segment Anything Model, namely PTQ4SAM. First, we investigate the inherent bottleneck of SAM quantization attributed to the bimodal distribution in post-Key-Linear activations. We analyze its characteristics from both per-tensor and per-channel perspectives, and propose a Bimodal Integration strategy, which utilizes a mathematically equivalent sign operation to transform the bimodal distribution into a relatively easy-quantized normal distribution offline. Second, SAM encompasses diverse attention mechanisms (i.e., self-attention and two-way cross-attention), resulting in substantial variations in the post-Softmax distributions. Therefore, we introduce an Adaptive Granularity Quantization for Softmax through searching the optimal power-of-two base, which is hardware-friendly. Extensive experimental results across various vision tasks (instance segmentation, semantic segmentation and object detection), datasets and model variants show the superiority of PTQ4SAM. For example, when quantizing SAM-L to 6-bit, we achieve lossless accuracy for instance segmentation, about 0.5\\% drop with theoretical 3.9$\\times$ acceleration. The code is available at \\url{https://github.com/chengtao-lv/PTQ4SAM}.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-05-06\n\n\n\n#### 2. A Comprehensive Survey of Mamba Architectures for Medical Image Analysis: Classification, Segmentation, Restoration and Beyond\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*Nagendra Kumar, Gaurav Duggal, Chandravardhan Singh Raghaw, Mohammad Zia Ur Rehman, Sreekanth Madisetty, Manikandan S, Madhava Prasath J, Sreeharish A, Shubhi Bansal*\n\n**Abstract:** Mamba, a special case of the State Space Model, is gaining popularity as an alternative to template-based deep learning approaches in medical image analysis. While transformers are powerful architectures, they have drawbacks, including quadratic computational complexity and an inability to address long-range dependencies efficiently. This limitation affects the analysis of large and complex datasets in medical imaging, where there are many spatial and temporal relationships. In contrast, Mamba offers benefits that make it well-suited for medical image analysis. It has linear time complexity, which is a significant improvement over transformers. Mamba processes longer sequences without attention mechanisms, enabling faster inference and requiring less memory. Mamba also demonstrates strong performance in merging multimodal data, improving diagnosis accuracy and patient outcomes. The organization of this paper allows readers to appreciate the capabilities of Mamba in medical imaging step by step. We begin by defining core concepts of SSMs and models, including S4, S5, and S6, followed by an exploration of Mamba architectures such as pure Mamba, U-Net variants, and hybrid models with convolutional neural networks, transformers, and Graph Neural Networks. We also cover Mamba optimizations, techniques and adaptations, scanning, datasets, applications, experimental results, and conclude with its challenges and future directions in medical imaging. This review aims to demonstrate the transformative potential of Mamba in overcoming existing barriers within medical imaging while paving the way for innovative advancements in the field. A comprehensive list of Mamba architectures applied in the medical field, reviewed in this work, is available at Github.\n\n**Published:** 2024-10-03\n\n\n\n#### 3. Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for Efficient LLM Inference\n\n*From Search Query: adaptive attention mechanisms memory optimization*\n\n*S. Kevin Zhou, Xike Xie, Yukun Cao, Junlin Lv, Yuan Feng*\n\n**Abstract:** Large Language Models have excelled in various fields but encounter challenges in memory and time efficiency due to the expanding Key-Value (KV) cache required for long-sequence inference. Recent efforts try to reduce KV cache size to a given memory budget by evicting vast non-critical cache elements during runtime, while preserving generation quality. Our revisiting of current eviction methods reveals that they fundamentally minimize an upper bound of the $L_1$ eviction loss between the pre- and post-eviction outputs of multi-head self-attention mechanisms. Moreover, our analysis indicates that the common practices of uniformly assigning budgets across attention heads harm their post-eviction generation quality. In light of these findings, we propose a simple yet effective adaptive budget allocation algorithm. This algorithm not only optimizes the theoretical loss upper bound but also reduces the $L_1$ eviction loss in practice by aligning with the varied characteristics across different heads. By integrating this algorithm into two state-of-the-art methods, we demonstrate the effectiveness of using adaptive budget allocation to optimize KV cache eviction. Extensive evaluations on 16 datasets and the Needle-in-a-Haystack test confirm significant performance improvements across various tasks.\n\n**Published:** 2024-07-16\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on adaptive attention mechanisms, memory optimization, and test-time adaptation, here are some key findings and references that align with your goals:\n\n## Recent Advances in Adaptive Attention Mechanisms\n\n### DuoAttention\nThe DuoAttention mechanism is a significant advancement in managing memory efficiently in long-context language models. It categorizes attention heads into **Retrieval Heads** and **Streaming Heads**, optimizing memory usage by allocating full Key-Value (KV) caches to retrieval heads for long-term context and smaller caches to streaming heads for short-term dependencies. This approach reduces memory requirements and speeds up decoding, making it highly efficient for handling large contexts.\n\n### Block-Attention\nBlock-Attention is another mechanism designed to reduce inference latency and computational overhead in Retrieval-Augmented Generation (RAG) scenarios. It divides the input sequence into discrete blocks, each calculating its own KV states independently, except for the final block which attends to all previous blocks. This method significantly reduces latency and computation while maintaining accuracy, especially after fine-tuning the model to adapt to the Block-Attention mechanism.\n\n## Memory Optimization Techniques\n\n### DuoAttention\nAs mentioned, DuoAttention optimizes memory by categorizing attention heads and allocating memory resources accordingly. This can reduce memory requirements by up to 2.55\u00d7 for Multi-Head Attention (MHA) models and 1.67\u00d7 for GQA models, and also speeds up decoding by up to 2.18\u00d7 for MHA models and 1.50\u00d7 for GQA models.\n\n### Block-Attention\nBlock-Attention reduces memory usage by reusing KV states of previously seen blocks, which is particularly beneficial in RAG scenarios where the same passages may be retrieved multiple times. This approach helps in minimizing computational load and memory allocation.\n\n## Dynamic Parameter Updates During Inference\n\n### Fine-Tuning for Block-Attention\nThe Block-Attention mechanism requires fine-tuning the language model to adapt to the new attention mechanism. This fine-tuning process, referred to as \"block fine-tune,\" involves modifying the traditional attention mask matrix to accommodate the block-wise attention. This adaptation ensures that the model maintains its original accuracy while improving inference efficiency.\n\n## Efficient State Management in Hybrid Architectures\n\n### DenseMamba and Sparse Modular Activation\nWhile not directly addressing the specific mechanisms of DuoAttention or Block-Attention, the concepts of DenseMamba and Sparse Modular Activation highlight the importance of efficient state management. DenseMamba introduces dense connections to improve information flow in state space models, and Sparse Modular Activation proposes dynamic module selection to reduce computational overhead. These approaches suggest that hybrid models can benefit from selective computation and efficient state compression techniques.\n\n## Test-Time Adaptation Strategies for Language Models\n\n### Block-Attention Fine-Tuning\nThe Block-Attention method involves fine-tuning the model during test time to adapt to the block-wise attention mechanism. This fine-tuning is crucial for maintaining the model's accuracy while leveraging the efficiency gains of Block-Attention. The process involves modifying the attention mask matrix and ensuring the model recovers its original accuracy through a limited number of fine-tuning steps.\n\n### Adaptive Attention Mechanisms\nBoth DuoAttention and Block-Attention mechanisms are adaptive in nature. DuoAttention adaptively allocates memory to attention heads based on their importance, and Block-Attention adapts the model to process input sequences in blocks, which can be seen as a form of test-time adaptation to optimize inference efficiency.\n\n## Implementation Details and Performance Comparisons\n\n### DuoAttention Implementation\nDuoAttention involves an optimization-based identification of retrieval heads using synthetic data to determine their importance. This approach ensures that only critical attention heads receive full KV caches, reducing overall memory usage and improving decoding speed.\n\n### Block-Attention Implementation\nBlock-Attention involves block segmentation, position re-encoding, and fine-tuning the model. Experimental results show that Block-Attention models can achieve comparable or even slightly better accuracy than self-attention models while significantly enhancing inference efficiency.\n\nIn summary, the DuoAttention and Block-Attention mechanisms offer promising approaches to improving autoregressive language model design by optimizing memory usage, adapting attention mechanisms during inference, and enhancing efficiency in handling long contexts. These mechanisms align well with the goals of improving memory efficiency, state management, and test-time adaptation in language models.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search results provide valuable insights into hybrid architectures combining SSMs with attention mechanisms, but more specific information is needed about test-time training and adaptive mechanisms in language models.\n\nUseful References with Excerpts:\n\n1. DenseMamba (He et al., 2024):\n\"While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers.\"\n- Highlights the current limitations of pure SSM approaches\n- Introduces dense connections for improved information flow\n\n2. Sparse Modular Activation (Ren et al., 2023):\n\"Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences\"\n- Identifies limitations in current hybrid approaches\n- Proposes sparse activation for dynamic module selection\n\n3. Just Read Twice (Arora et al., 2024):\n\"Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts\"\n- Highlights memory limitations in recurrent models\n- Suggests multiple passes for improved recall\n\nKey Insights:\n\n1. Hybrid Architecture Design:\n- Current hybrid models often use static attention mechanisms\n- Need for dynamic, adaptive integration of SSM and attention\n- Potential for improved efficiency through selective computation\n\n2. Memory and State Management:\n- Limited memory in recurrent models affects long-context performance\n- Dense connections can improve information flow\n- Need for efficient state compression techniques\n\n3. Test-Time Adaptation:\n- Limited information available on test-time training in language models\n- Need to explore adaptive mechanisms during inference\n- Potential for dynamic parameter updates\n\nFuture Search Plan:\n\n1. Need more specific information about:\n- Test-time training implementations in language models\n- Adaptive attention mechanisms\n- State compression techniques\n- Memory-efficient bidirectional processing\n\n2. Focus areas for next search:\n- Recent advances in adaptive attention mechanisms\n- Memory optimization techniques in language models\n- Test-time adaptation strategies\n\nReferences:\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., & Zhai, C. (2023). Sparse Modular Activation for Efficient Sequence Modeling. Neural Information Processing Systems.\n\n[3] Arora, S., Timalsina, A., Singhal, A., Spector, B., Eyuboglu, S., Zhao, X., ... & R\u00e9, C. (2024). Just read twice: closing the recall gap for recurrent language models.\n\n[4] Dolga, R., Cobzarenco, M., & Barber, D. (2024). Latent Attention for Linear Time Transformers. arXiv preprint.\n\n[5] Liu, H., & Abbeel, P. (2023). Blockwise Parallel Transformer for Large Context Models."
        },
        {
            "ready": false,
            "query": "test-time training language models state compression",
            "detail": "Search for papers discussing:\n1. Test-time training and adaptation methods in language models\n2. State compression and memory management techniques in hybrid architectures\n3. Dynamic routing and gating mechanisms between different architectural components\n4. Efficient integration of SSM and attention mechanisms\n5. Memory-efficient bidirectional processing\n\nFocus on:\n- Implementation details of test-time adaptation\n- Memory optimization techniques\n- Dynamic routing strategies\n- Performance comparisons\n- Computational overhead analysis",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Test-time training and adaptation methods in language models\n2. State compression and memory management techniques in hybrid architectures\n3. Dynamic routing and gating mechanisms between different architectural components\n4. Efficient integration of SSM and attention mechanisms\n5. Memory-efficient bidirectional processing\n\nFocus on:\n- Implementation details of test-time adaptation\n- Memory optimization techniques\n- Dynamic routing strategies\n- Performance comparisons\n- Computational overhead analysis\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.77)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.77)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.56)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.56)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.34)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.34)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.04)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 24/46 (Score: 0.04)*\n\n```\nMultiMedia LLC. 2009. Large text compression benchmark. G\u00e1bor Melis, Charles Blundell, Tom\u00e1\u0161 Ko\u010disk\u1ef3, Karl Moritz Hermann, Chris Dyer, and Phil Blunsom. 2018. Pushing the bounds of dropout. arXiv preprint arXiv:1805.09208. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2017. Regularizing and optimizing lstm language models. arXiv preprint arXiv:1708.02182. Stephen Merity, Nitish Shirish Keskar, and Richard Socher. 2018. An analysis of neural language modeling at multiple scales. arXiv preprint arXiv:1803.08240. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. arXiv preprint arXiv:1609.07843. Tomas Mikolov, Armand Joulin, Sumit Chopra, Michael Mathieu, and Marc'Aurelio Ranzato. 2014. Learning longer memory in recurrent neural networks. arXiv preprint arXiv:1412.7753. Tom\u00e1\u0161 Mikolov, Martin Karafi\u00e1t, Luk\u00e1\u0161 Burget, Jan \u010cernock\u1ef3, and Sanjeev Khudanpur. 2010. Recurrent neural network based language model. In Eleventh Annual Conference of the International Speech Communication Association. Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neural network language model. SLT, 12(234-239):8.\n```\n\n#### 5. Retentive network: a successor to transformer for large language models (Avg. Score: 0.04)\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 143  (*Influential: 18*)\n\n**TL;DR:** This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n##### *Relevant Chunk: No. 12/21 (Score: 0.04)*\n\n```\narXiv preprint arXiv:2101.00027, 2020. [GGR21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. $\\left[\\mathrm{HCP}^{+}\\right.$21] Luyang Huang, Shuyang Cao, Nikolaus Parulian, Heng Ji, and Lu Wang. Efficient attentions for long document summarization. arXiv preprint arXiv:2104.02112, 2021. $\\left[\\mathrm{HDW}^{+} 23\\right]$ Shaohan Huang, Li Dong, Wenhui Wang, Yaru Hao, Saksham Singhal, Shuming Ma, Tengchao Lv, Lei Cui, Owais Khan Mohammed, Qiang Liu, Kriti Aggarwal, Zewen Chi, Johan Bjorck, Vishrav Chaudhary, Subhojit Som, Xia Song, and Furu Wei. Language is not all you need: Aligning perception with language models. ArXiv, abs/2302.14045, 2023. [HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv: Learning, 2016. [HS97] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735-1780, November 1997. [HSD ${ }^{+}$22a] Yaru Hao, Haoyu Song, Li Dong, Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming Ma, and Furu Wei. Language models are general-purpose interfaces. ArXiv, abs/2206.06336, 2022. $\\left[\\mathrm{HSD}^{+}\\right.$22b] Yaru Hao, Yutao Sun, Li Dong, Zhixiong Han, Yuxian Gu, and Furu Wei. Structured prompting: Scaling in-context learning to 1,000 examples.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: test-time training language models state compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Test-Time Training on Nearest Neighbors for Large Language Models\n\n*From Search Query: test-time training language models state compression*\n\n*Moritz Hardt, Yu Sun*\n\n**TL;DR:** This work establishes a first baseline of test-time training for language modeling by building a large-scale distributed index based on text embeddings of the Pile dataset and fine-tunes the model on retrieved data at test time.\n\n**Abstract:** Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 2. Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models\n\n*From Search Query: test-time training language models state compression*\n\n*Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, T. Goldstein, Anima Anandkumar, Chaowei Xiao*\n\n**TL;DR:** Test-time prompt tuning (TPT) is proposed, a method that can learn adaptive prompts on the fly with a single test sample and performs on par with the state-of-the-art approaches that use additional training data.\n\n**Abstract:** Pre-trained vision-language models (e.g., CLIP) have shown promising zero-shot generalization in many downstream tasks with properly designed text prompts. Instead of relying on hand-engineered prompts, recent works learn prompts using the training data from downstream tasks. While effective, training on domain-specific data reduces a model's generalization capability to unseen new domains. In this work, we propose test-time prompt tuning (TPT), a method that can learn adaptive prompts on the fly with a single test sample. For image classification, TPT optimizes the prompt by minimizing the entropy with confidence selection so that the model has consistent predictions across different augmented views of each test sample. In evaluating generalization to natural distribution shifts, TPT improves the zero-shot top-1 accuracy of CLIP by 3.6% on average, surpassing previous prompt tuning approaches that require additional task-specific training data. In evaluating cross-dataset generalization with unseen categories, TPT performs on par with the state-of-the-art approaches that use additional training data. Project page: https://azshue.github.io/TPT.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 202  (*Influential: 44*)\n\n#### 3. Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models\n\n*From Search Query: test-time training language models state compression*\n\n*Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yezhou Yang*\n\n**TL;DR:** This work proposes TTA with feedback to rectify the model output and prevent the model from becoming blindly confident, and builds different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs.\n\n**Abstract:** One fascinating aspect of pre-trained vision-language models~(VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation~(TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 4. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: test-time training language models state compression*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 5. Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time\n\n*From Search Query: test-time training language models state compression*\n\n*Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava*\n\n**TL;DR:** This work hypothesizes the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations, and proposes Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model.\n\n**Abstract:** Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20X compression.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 110  (*Influential: 11*)\n\n### 3 related papers from Papers with Code\n\n#### 1. DiSAN: Directional Self-Attention Network for RNN/CNN-Free Language Understanding\n\n*From Search Query: test-time training language models state compression*\n\n*Tao Shen, Jing Jiang, Chengqi Zhang, Tianyi Zhou, Shirui Pan, Guodong Long*\n\n**Abstract:** Recurrent neural nets (RNN) and convolutional neural nets (CNN) are widely\nused on NLP tasks to capture the long-term and local dependencies,\nrespectively. Attention mechanisms have recently attracted enormous interest\ndue to their highly parallelizable computation, significantly less training\ntime, and flexibility in modeling dependencies. We propose a novel attention\nmechanism in which the attention between elements from input sequence(s) is\ndirectional and multi-dimensional (i.e., feature-wise). A light-weight neural\nnet, \"Directional Self-Attention Network (DiSAN)\", is then proposed to learn\nsentence embedding, based solely on the proposed attention without any RNN/CNN\nstructure. DiSAN is only composed of a directional self-attention with temporal\norder encoded, followed by a multi-dimensional attention that compresses the\nsequence into a vector representation. Despite its simple form, DiSAN\noutperforms complicated RNN models on both prediction quality and time\nefficiency. It achieves the best test accuracy among all sentence encoding\nmethods and improves the most recent best result by 1.02% on the Stanford\nNatural Language Inference (SNLI) dataset, and shows state-of-the-art test\naccuracy on the Stanford Sentiment Treebank (SST), Multi-Genre natural language\ninference (MultiNLI), Sentences Involving Compositional Knowledge (SICK),\nCustomer Review, MPQA, TREC question-type classification and Subjectivity\n(SUBJ) datasets.\n\n**Published:** 2017-09-14\n\n\n\n#### 2. Mechanistic Design and Scaling of Hybrid Architectures\n\n*From Search Query: test-time training language models state compression*\n\n*Stefano Massaroli, Ce Zhang, Christopher R\u00e9, Stefano Ermon, Brian Hie, Taiji Suzuki, Kristian Kersting, Bj\u00f6rn Deiseroth, Pragaash Ponnusamy, Eric Nguyen, Armin W Thomas, Michael Poli*\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Published:** 2024-03-26\n\n\n\n#### 3. In-Context Learning State Vector with Inner and Momentum Optimization\n\n*From Search Query: test-time training language models state compression*\n\n*Min Zhang, Baotian Hu, Zetian Sun, Xinshuo Hu, Zhenyu Liu, Dongfang Li*\n\n**Abstract:** Large Language Models (LLMs) have exhibited an impressive ability to perform In-Context Learning (ICL) from only a few examples. Recent works have indicated that the functions learned by ICL can be represented through compressed vectors derived from the transformer. However, the working mechanisms and optimization of these vectors are yet to be thoroughly explored. In this paper, we address this gap by presenting a comprehensive analysis of these compressed vectors, drawing parallels to the parameters trained with gradient descent, and introduce the concept of state vector. Inspired by the works on model soup and momentum-based gradient descent, we propose inner and momentum optimization methods that are applied to refine the state vector progressively as test-time adaptation. Moreover, we simulate state vector aggregation in the multiple example setting, where demonstrations comprising numerous examples are usually too lengthy for regular ICL, and further propose a divide-and-conquer aggregation method to address this challenge. We conduct extensive experiments using Llama-2 and GPT-J in both zero-shot setting and few-shot setting. The experimental results show that our optimization method effectively enhances the state vector and achieves the state-of-the-art performance on diverse tasks. Code is available at https://github.com/HITsz-TMG/ICL-State-Vector\n\n**Published:** 2024-04-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on test-time training, state compression, and dynamic routing, here are some key findings and suggestions from the provided sources and additional insights:\n\n## Test-Time Training and Adaptation Methods\n\n- The paper \"Efficiently Learning at Test-Time: Active Fine-Tuning of LLMs\" introduces a data selection algorithm called SIFT, which optimizes the information gain of selected examples during test-time fine-tuning. This approach shows promise in reducing computational overhead while improving performance.\n- The concept of \"Test-Time Training (TTT) layers\" is presented in another source, where the hidden state of the model is treated as a machine learning model itself, updated through self-supervised learning during test sequences. This approach demonstrates the ability to reduce perplexity by conditioning on more tokens, similar to Transformers, but with linear complexity.\n\n## State Compression and Memory Management Techniques\n\n- The \"Test-Time Training (TTT) layers\" also address memory efficiency by maintaining an expressive hidden state with linear complexity, which is crucial for state compression and memory management in hybrid architectures.\n- The idea of using \"learnable residuals\" to align prototypes from different modalities, as seen in the \"Dual Prototype Evolving\" approach for vision-language models, could be adapted to language models to promote consistent representations and reduce memory footprint.\n\n## Dynamic Routing and Gating Mechanisms\n\n- While the sources do not explicitly discuss dynamic routing and gating mechanisms between SSM and attention components, the concept of \"Adaptive Attention Span\" in Transformers suggests that learning optimal attention spans can be extended to dynamic routing strategies. This could help in balancing the flow of information between different architectural components[Analysis Note: b) Adaptive Attention Span].\n\n## Efficient Integration of SSM and Attention Mechanisms\n\n- The \"Test-Time Training (TTT) layers\" provide a novel approach to sequence modeling that integrates well with both SSM and attention mechanisms. By treating the hidden state as a model and updating it through self-supervised learning, TTT layers can efficiently handle long contexts and reduce perplexity.\n- The \"DenseMamba\" architecture, though not specifically discussed in the provided sources, is mentioned in the analysis note as a way to improve information flow between SSMs and attention mechanisms through dense connections[Analysis Note: a) DenseMamba].\n\n## Memory-Efficient Bidirectional Processing\n\n- The \"TTT layers\" show potential in handling long contexts efficiently, which is crucial for bidirectional processing. They achieve this with linear complexity, making them more memory-efficient compared to traditional Transformers.\n- The \"Adaptive Attention Span\" mechanism allows for extending the maximum context size while maintaining control over the memory footprint, which is beneficial for memory-efficient bidirectional processing[Analysis Note: b) Adaptive Attention Span].\n\n## Performance Comparisons and Computational Overhead Analysis\n\n- The \"TTT layers\" are compared with strong baselines like Transformers and Mamba, showing that they can match or exceed the performance of these models while maintaining linear complexity. This indicates a significant reduction in computational overhead, especially for long contexts.\n- The \"Scaling LLM Test-Time Compute Optimally\" paper discusses the scaling of inference-time computation and shows that using a compute-optimal strategy can improve the efficiency of test-time compute scaling by more than 4x compared to a best-of-N baseline. This highlights the importance of adaptive allocation of test-time compute.\n\nIn summary, the \"Test-Time Training (TTT) layers\" and the concepts of adaptive attention and state management provide valuable insights into improving autoregressive language model design. These approaches focus on efficient test-time adaptation, state compression, and dynamic integration of different architectural components, all of which are crucial for achieving better performance, efficiency, and scalability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several key trends and innovations in hybrid architectures combining SSMs with attention mechanisms, memory optimization, and adaptive computation. The proposal's core ideas align with current research directions but require more investigation in specific areas.\n\n2. Useful References with Excerpts:\n\na) DenseMamba (He et al., 2024):\n\"While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers.\"\n- Highlights the current gap between SSMs and Transformers\n- Introduces dense connections for improved information flow\n\nb) Adaptive Attention Span (Sukhbaatar et al., 2019):\n\"A novel self-attention mechanism that can learn its optimal attention span... allows us to extend significantly the maximum context size used in Transformer, while maintaining control over their memory footprint\"\n- Demonstrates successful adaptive attention mechanisms\n- Provides insights into memory-computation trade-offs\n\nc) SAC (Li et al., 2020):\n\"Sparse Adaptive Connection (SAC)... regards the input sequence as a graph and attention operations are performed between linked nodes\"\n- Shows effective sparse attention implementation\n- Provides framework for adaptive connectivity\n\n3. Key Insights:\n\na) Hybrid Architecture Design:\n- Current trend favors combining SSMs with attention for balanced performance\n- Need for dynamic, adaptive integration mechanisms\n- Importance of efficient state management\n\nb) Memory Optimization:\n- Sparse computation strategies show promise\n- Adaptive mechanisms can reduce memory footprint\n- State compression techniques are crucial\n\nc) Test-Time Adaptation:\n- Limited research on test-time training in language models\n- Need for efficient adaptation mechanisms\n- Potential for dynamic parameter updates\n\n4. Future Search Plan:\n\nNeed to investigate:\na) Test-time training implementations specifically in language models\nb) State compression techniques in hybrid architectures\nc) Dynamic routing mechanisms between SSM and attention components\n\nNext search queries should focus on:\n1. Test-time training and adaptation in language models\n2. State compression and memory management techniques\n3. Dynamic routing and gating mechanisms\n\n5. References:\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Sukhbaatar, S., Grave, E., Bojanowski, P., & Joulin, A. (2019). Adaptive Attention Span in Transformers. Annual Meeting of the Association for Computational Linguistics.\n\n[3] Li, X., Meng, Y., Han, Q., Wu, F., & Li, J. (2020). SAC: Accelerating and Structuring Self-Attention via Sparse Adaptive Connection. Neural Information Processing Systems.\n\n[4] Liu, X., Ding, Y., Guo, J., Chen, H., & Lv, C. (2024). PTQ4SAM: Post-Training Quantization for Segment Anything. CVPR 2024.\n\n[5] Dolga, R., Cobzarenco, M., & Barber, D. (2024). Latent Attention for Linear Time Transformers. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.832788,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.83688,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "MHA",
    "proposal": "## AdaptiveHybridMHA: Combining Selective SSMs with Attention for Efficient Language Modeling\n\n### Motivation\n\nCurrent language models face several key challenges:\n1. Inefficient handling of long sequences due to quadratic attention complexity\n2. Limited ability to adapt to varying input contexts\n3. Suboptimal balance between local and global information processing\n4. High memory requirements for state tracking\n\nRecent advances in selective state space models (Mamba) and test-time training demonstrate promising directions for addressing these challenges, but current solutions often focus on either local or global processing, missing opportunities for efficient hybrid approaches.\n\n### Problem Analysis\n\nThe key limitations of current architectures include:\n\n1. **Attention Mechanisms**:\n   - Quadratic complexity limits scalability\n   - Uniform processing regardless of input importance\n   - High memory requirements for key-value caches\n\n2. **State Space Models**:\n   - Limited ability for content-based reasoning\n   - Fixed parameter settings during inference\n   - Challenges in capturing global dependencies\n\n3. **Hybrid Approaches**:\n   - Inefficient integration of multiple mechanisms\n   - Complex training dynamics\n   - High computational overhead\n\n### Core Idea and Philosophy\n\nAdaptiveHybridMHA introduces three key innovations:\n\n1. **Selective Processing**:\n   - Dynamic routing between SSM and attention paths\n   - Content-aware state compression\n   - Adaptive computation allocation\n\n2. **Bidirectional State Integration**:\n   - Efficient bidirectional state processing\n   - Preserved autoregressive properties\n   - Enhanced context understanding\n\n3. **Test-Time Adaptation**:\n   - Continuous model refinement during inference\n   - Context-specific parameter updates\n   - Efficient memory management\n\n### Design Plan\n\n#### Architecture Overview\n\n```python\nclass AdaptiveHybridMHA(nn.Module):\n    def __init__(self, embed_dim, num_heads, state_dim):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.state_dim = state_dim\n        \n        # Core components\n        self.ssm = SelectiveSSM(embed_dim, state_dim)\n        self.attention = LinearAttention(embed_dim, num_heads)\n        self.router = AdaptiveRouter(embed_dim)\n        self.state_compressor = StateCompressor(embed_dim)\n        \n    def forward(self, x, state=None):\n        # Compute routing decisions\n        route_weights = self.router(x)\n        \n        # Process through SSM path\n        ssm_out, new_state = self.ssm(x, state)\n        \n        # Process through attention path\n        att_out = self.attention(x)\n        \n        # Combine outputs adaptively\n        output = route_weights * ssm_out + (1 - route_weights) * att_out\n        \n        # Compress state selectively\n        compressed_state = self.state_compressor(new_state)\n        \n        return output, compressed_state\n```\n\n#### Key Components\n\n1. **Selective SSM**:\n```python\nclass SelectiveSSM(nn.Module):\n    def __init__(self, embed_dim, state_dim):\n        super().__init__()\n        self.state_proj = nn.Linear(embed_dim, state_dim)\n        self.input_proj = nn.Linear(embed_dim, state_dim)\n        self.output_proj = nn.Linear(state_dim, embed_dim)\n        \n    def forward(self, x, state):\n        # Project input\n        x_proj = self.input_proj(x)\n        \n        # Update state selectively\n        if state is None:\n            state = torch.zeros_like(x_proj)\n        new_state = self.update_state(x_proj, state)\n        \n        # Generate output\n        output = self.output_proj(new_state)\n        \n        return output, new_state\n```\n\n2. **Linear Attention**:\n```python\nclass LinearAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads):\n        super().__init__()\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, x):\n        # Project queries, keys, values\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n        \n        # Compute linear attention\n        scores = torch.einsum('bld,bmd->blm', q, k)\n        attention = torch.softmax(scores, dim=-1)\n        output = torch.einsum('blm,bmd->bld', attention, v)\n        \n        return output\n```\n\n3. **Adaptive Router**:\n```python\nclass AdaptiveRouter(nn.Module):\n    def __init__(self, embed_dim):\n        super().__init__()\n        self.gate = nn.Sequential(\n            nn.Linear(embed_dim, embed_dim // 4),\n            nn.SiLU(),\n            nn.Linear(embed_dim // 4, 1),\n            nn.Sigmoid()\n        )\n    \n    def forward(self, x):\n        return self.gate(x)\n```\n\n### Implementation Guidelines\n\n1. **Initialization**:\n```python\ndef initialize_model(config):\n    model = AdaptiveHybridMHA(\n        embed_dim=config.hidden_size,\n        num_heads=config.num_attention_heads,\n        state_dim=config.state_dim\n    )\n    return model\n```\n\n2. **Training Process**:\n```python\ndef train_step(model, batch, optimizer):\n    # Forward pass\n    output, new_state = model(batch.input_ids, batch.state)\n    \n    # Compute losses\n    main_loss = compute_lm_loss(output, batch.labels)\n    routing_loss = compute_routing_loss(model.router)\n    \n    # Update model\n    total_loss = main_loss + routing_loss\n    total_loss.backward()\n    optimizer.step()\n    \n    return output, new_state\n```\n\n3. **Inference Optimization**:\n```python\ndef optimize_inference(model, input_ids):\n    # Initialize state\n    state = None\n    \n    # Process sequence\n    for pos in range(len(input_ids)):\n        # Forward pass\n        output, state = model(input_ids[pos], state)\n        \n        # Update model parameters using test-time training\n        if should_update(pos):\n            update_params(model, output)\n        \n        # Compress state if needed\n        if should_compress(state):\n            state = compress_state(state)\n    \n    return output\n```\n\n### Theoretical Analysis\n\n1. **Complexity Analysis**:\n   - Time Complexity: O(n) for sequence length n\n   - Space Complexity: O(n) with selective compression\n   - Memory Usage: Adaptive based on input complexity\n\n2. **Advantages**:\n   - Efficient processing of both local and global dependencies\n   - Dynamic adaptation to input contexts\n   - Reduced memory requirements through selective compression\n   - Improved handling of long sequences\n\n3. **Trade-offs**:\n   - Increased implementation complexity\n   - Additional routing computation\n   - Training stability considerations\n\n### Research Summary\n\nKey findings from research:\n1. Selective SSMs provide efficient sequence processing\n2. Test-time training enables continuous model adaptation\n3. Hybrid architectures can balance efficiency and expressiveness\n4. State compression maintains performance while reducing memory usage\n\n### Evolution of Design\n\nThe design evolved through several iterations:\n1. Initial focus on pure SSM architecture\n2. Integration of linear attention mechanisms\n3. Addition of adaptive routing\n4. Incorporation of test-time training\n5. Implementation of selective state compression\n\n### References\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\n2. Sun, Y., et al. (2024). Learning to (Learn at Test Time): RNNs with Expressive Hidden States.\n3. Hwang, S., et al. (2024). Hydra: Bidirectional State Space Models Through Generalized Matrix Mixers.\n4. Lioutas, V., & Guo, Y. (2020). Time-aware Large Kernel Convolutions.\n5. Ali, A., et al. (2024). The Hidden Attention of Mamba Models.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "We propose AdaptiveHybridMHA, a novel variant of Multi-Head Attention that combines selective state space models (SSMs) with traditional attention mechanisms through an adaptive gating system. This design enables efficient processing of both local and global dependencies while maintaining linear complexity through selective activation and state compression. The architecture integrates test-time training capabilities with bidirectional state processing, allowing for dynamic adaptation to input contexts while preserving autoregressive properties.",
    "ideation": null,
    "modelname": "adaptivehybridgpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory-computation trade-offs\n- Include concrete examples of state compression mechanisms\n- Add benchmarking guidelines for different components\n- Specify initialization strategies for different components\n\n2. Training Stability:\n- Develop guidelines for parameter initialization\n- Include stability analysis for different sequence lengths\n- Provide more detailed training recommendations\n- Add gradient flow analysis\n\n3. State Management:\n- Elaborate on state compression strategies\n- Include failure mode analysis\n- Add guidelines for handling very long sequences\n- Specify memory optimization techniques\n\n4. Test-Time Training:\n- Provide more details on adaptation mechanisms\n- Include computational overhead analysis\n- Add stability guarantees\n- Specify update frequency recommendations\n\n5. Integration Guidelines:\n- Add specific guidance for component integration\n- Include strategies for efficient implementation\n- Provide more detailed cache optimization guidelines\n- Specify requirements for different model scales",
    "user_input": ""
}