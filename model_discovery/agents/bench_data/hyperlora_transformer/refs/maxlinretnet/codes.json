{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 1536,\n    'n_block': 29\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 2048,\n    'n_block': 28\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 14\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MaxLinMultiScaleRetention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = RetNetMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass RetNetMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        hidden_ratio = 2\n        intermediate_size = int(hidden_size * hidden_ratio * 2 / 3)\n        intermediate_size = 256 * ((intermediate_size + 256 - 1) // 256)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size *\n            2, bias=False, device=device, dtype=dtype)\n        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size,\n            bias=False, device=device, dtype=dtype)\n        self.act_fn = ACT2FN['swish']\n\n    def _forward(self, X, **Z):\n        y = self.gate_proj(X)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        x = self.down_proj(z)\n        return x\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\nimport torch.utils.checkpoint\n\n\nclass MaxLinMultiScaleRetention(GAUBase):\n    \"\"\"\n    MaxLinMultiScaleRetention GAU enhances the MultiScaleRetention unit by integrating\n    max-margin token selection and dynamic linear attention mechanisms for improved\n    discriminative power and computational efficiency.\n\n    This unit redefines the attention scoring mechanism to maximize the margin between\n    relevant and irrelevant tokens and employs dynamic linear attention to reduce\n    computational complexity from quadratic to linear with respect to sequence length,\n    while ensuring causality.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location of a block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        device (torch.device, optional): Device to initialize the unit on.\n        dtype (torch.dtype, optional): Datatype of the unit parameters.\n        hidden_size (int, optional): Hidden size of the model. Defaults to embed_dim.\n        num_heads (int, optional): Number of attention heads. Defaults to 8.\n        norm_eps (float, optional): Epsilon value for RMSNorm. Defaults to 1e-5.\n        max_margin (float, optional): Margin value for max-margin token selection. Defaults to 1.0.\n\n    **Attributes:**\n        q_proj (nn.Linear): Query projection layer.\n        k_proj (nn.Linear): Key projection layer.\n        v_proj (nn.Linear): Value projection layer.\n        margin (nn.Parameter): Max-margin parameter for attention adjustment.\n        rotary (RotaryPositionalEmbeddings): Rotary positional embeddings for queries and keys.\n        g_norm (RMSNorm): Normalization layer for the gated output.\n        gate_fn (function): Activation function for the gating mechanism.\n        o_proj (nn.Linear): Output projection layer.\n\n    **Example:**\n\n        Here's how you might instantiate and use this GAU:\n\n            batch_size = 2\n            seq_len = 4\n            embed_dim = 32\n            num_heads = 4\n\n            X = torch.randn(batch_size, seq_len, embed_dim)\n            kwarg_all = {}\n            block_loc = (0, 1)\n\n            gau = MaxLinMultiScaleRetention(embed_dim=embed_dim, block_loc=block_loc,\n                                            kwarg_all=kwarg_all, num_heads=num_heads)\n\n            Y, Z = gau(X)\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads=8, norm_eps=\n        1e-05, max_margin=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        assert self.head_dim * num_heads == embed_dim, 'embed_dim must be divisible by num_heads'\n        self.q_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.k_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.v_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.margin = nn.Parameter(torch.tensor(max_margin), requires_grad=\n            False)\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_dim).to(**\n            self.factory_kwargs)\n        self.g_norm = RMSNorm(self.head_dim, eps=norm_eps).to(**self.\n            factory_kwargs)\n        self.gate_fn = ACT2FN['swish']\n        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=\n            False, **self.factory_kwargs)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of the MaxLinMultiScaleRetention GAU.\n\n        Args:\n            X (torch.Tensor): Input tensor of shape (batch_size, seq_len, embed_dim).\n            **Z: Intermediate variables (unused).\n\n        Returns:\n            torch.Tensor: Output tensor of the same shape as X.\n            dict: Updated intermediate variables.\n        \"\"\"\n        Y = torch.utils.checkpoint.checkpoint(self._checkpointed_forward, X)\n        Z_ = {}\n        return Y, Z_\n\n    def _checkpointed_forward(self, X):\n        B, L, D = X.shape\n        Q = self.q_proj(X)\n        K = self.k_proj(X)\n        V = self.v_proj(X)\n        Q = Q.view(B, L, self.num_heads, self.head_dim)\n        K = K.view(B, L, self.num_heads, self.head_dim)\n        V = V.view(B, L, self.num_heads, self.head_dim)\n        Q = self.rotary(Q)\n        K = self.rotary(K)\n        Q = Q.permute(0, 2, 1, 3)\n        K = K.permute(0, 2, 1, 3)\n        V = V.permute(0, 2, 1, 3)\n        attention_scores = torch.matmul(Q, K.transpose(-2, -1)\n            ) / self.head_dim ** 0.5\n        attention_scores = attention_scores - self.margin\n        L = Q.size(-2)\n        causal_mask = torch.tril(torch.ones(L, L, device=Q.device, dtype=\n            torch.bool))\n        attention_scores = attention_scores.masked_fill(~causal_mask, float\n            ('-inf'))\n        attention_weights = torch.softmax(attention_scores, dim=-1)\n        attention_output = torch.matmul(attention_weights, V)\n        phi_Q = F.elu(Q) + 1\n        phi_K = F.elu(K) + 1\n        cumulative_phi_K = torch.cumsum(phi_K, dim=-2)\n        cumulative_phi_KV = torch.cumsum(phi_K * V, dim=-2)\n        linear_attention = phi_Q * cumulative_phi_KV / (cumulative_phi_K + \n            1e-08)\n        combined_output = attention_output + linear_attention\n        gated_output = self.g_norm(combined_output)\n        X_reshaped = X.view(B, L, self.num_heads, self.head_dim).permute(0,\n            2, 1, 3)\n        gated_output = gated_output * self.gate_fn(X_reshaped)\n        gated_output = gated_output.permute(0, 2, 1, 3).contiguous().view(B,\n            L, D)\n        Y = self.o_proj(gated_output) + X\n        return Y\n\n\ngab_config = {'norm_eps': 1e-05, 'hidden_size': None, 'num_heads': 8,\n    'max_margin': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 27\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}