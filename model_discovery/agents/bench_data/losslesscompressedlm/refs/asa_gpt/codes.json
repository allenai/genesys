{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = AdaptiveSparseAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass AdaptiveSparseAttention(GAUBase):\n    \"\"\"\n    Adaptive Sparse Attention (ASA) implementation that combines dynamic sparsity,\n    hierarchical experts, and efficient routing for improved attention computation.\n    \n    Key features:\n    - Dynamic sparsity through causal learnable routing\n    - Hierarchical expert integration with causality preservation\n    - Efficient parameter sharing\n    - Compatible with RoPE positional embeddings\n    \n    Args:\n        embed_dim (int): Input embedding dimension\n        block_loc (tuple): Location of block in model\n        kwarg_all (dict): Additional arguments\n        n_heads (int): Number of attention heads\n        num_experts (int): Number of expert groups\n        expert_dim (int): Dimension of each expert\n        causal (bool): Whether to use causal attention\n        num_heads_kv (int, optional): Number of key/value heads for GQA\n        head_dim (int, optional): Dimension of each attention head\n        qkv_proj_bias (bool): Whether to use bias in QKV projection\n        out_proj_bias (bool): Whether to use bias in output projection\n        router_epsilon (float): Small value for router stability\n        device (torch.device, optional): Device to place tensors\n        dtype (torch.dtype, optional): Data type of tensors\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, num_experts: int=4, expert_dim: int=None, causal:\n        bool=True, num_heads_kv: int=None, head_dim: int=None,\n        qkv_proj_bias: bool=True, out_proj_bias: bool=True, router_epsilon:\n        float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        self.head_dim = (head_dim if head_dim is not None else embed_dim //\n            n_heads)\n        self.causal = causal\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.num_experts = num_experts\n        self.expert_dim = (expert_dim if expert_dim is not None else self.\n            head_dim * 2)\n        self.router_epsilon = router_epsilon\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        self.qkv_proj = nn.Linear(embed_dim, qkv_dim, bias=qkv_proj_bias,\n            **self.factory_kwargs)\n        self.out_proj = nn.Linear(out_dim, embed_dim, bias=out_proj_bias,\n            **self.factory_kwargs)\n        router_hidden = max(32, embed_dim // 8)\n        self.router = nn.Sequential(nn.Linear(embed_dim, router_hidden, **\n            self.factory_kwargs), nn.ReLU(), nn.Linear(router_hidden,\n            num_experts, **self.factory_kwargs))\n        self.expert_gates = nn.ModuleList([nn.Linear(self.head_dim, self.\n            expert_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self.expert_values = nn.ModuleList([nn.Linear(self.expert_dim, self\n            .head_dim, **self.factory_kwargs) for _ in range(num_experts)])\n        self._init_experts()\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_experts(self):\n        \"\"\"Initialize expert networks for better stability\"\"\"\n        for gate, value in zip(self.expert_gates, self.expert_values):\n            nn.init.xavier_uniform_(gate.weight)\n            if gate.bias is not None:\n                nn.init.zeros_(gate.bias)\n            nn.init.xavier_uniform_(value.weight)\n            if value.bias is not None:\n                nn.init.zeros_(value.bias)\n\n    def _compute_routing_weights(self, x: torch.Tensor, seq_len: int\n        ) ->torch.Tensor:\n        \"\"\"Compute routing weights while preserving causality\"\"\"\n        B = x.size(0)\n        if self.causal:\n            x_cum_sum = torch.cumsum(x, dim=1)\n            positions = torch.arange(1, seq_len + 1, device=x.device, dtype\n                =x.dtype).view(1, -1, 1)\n            x_pooled = x_cum_sum / positions\n        else:\n            x_pooled = x.mean(dim=1, keepdim=True).expand(-1, seq_len, -1)\n        router_logits = self.router(x_pooled)\n        routing_weights = F.softmax(router_logits + self.router_epsilon, dim=-1\n            )\n        if self.training:\n            noise_scale = 0.1 * torch.arange(seq_len, device=x.device\n                ) / seq_len\n            noise = torch.randn_like(routing_weights) * noise_scale.view(1,\n                -1, 1)\n            routing_weights = F.softmax(router_logits + noise + self.\n                router_epsilon, dim=-1)\n        return routing_weights\n\n    def _apply_experts(self, x: torch.Tensor, routing_weights: torch.Tensor\n        ) ->torch.Tensor:\n        \"\"\"Apply expert transformations with routing\"\"\"\n        B, H, L, D = x.shape\n        expert_out = torch.zeros_like(x)\n        x = F.layer_norm(x, (D,))\n        for i in range(self.num_experts):\n            weight = routing_weights[..., i].view(B, 1, L, 1)\n            gate_input = x.view(-1, D)\n            expert_gate = self.expert_gates[i](gate_input).view(B, H, L, -1)\n            expert_gate = F.gelu(expert_gate)\n            expert_value = self.expert_values[i](expert_gate.view(-1, self.\n                expert_dim))\n            expert_value = expert_value.view(B, H, L, D)\n            expert_out += expert_value * weight\n        return expert_out\n\n    def _forward(self, X: torch.Tensor, **Z) ->torch.Tensor:\n        \"\"\"\n        Forward pass implementing adaptive sparse attention with strict causality.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Additional arguments passed through\n            \n        Returns:\n            Output tensor of shape (batch_size, seq_len, embed_dim)\n        \"\"\"\n        B, L, _ = X.shape\n        qkv = self.qkv_proj(X)\n        q, k, v = qkv.split([self.num_heads * self.head_dim, self.\n            num_heads_kv * self.head_dim, self.num_heads_kv * self.head_dim\n            ], dim=-1)\n        q = rearrange(q, 'b l (h d) -> b h l d', h=self.num_heads)\n        k = rearrange(k, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        v = rearrange(v, 'b l (h d) -> b h l d', h=self.num_heads_kv)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = repeat(k, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        v = repeat(v, 'b h l d -> b (h r) l d', r=self.num_heads // self.\n            num_heads_kv)\n        routing_weights = self._compute_routing_weights(X, L)\n        q = self._apply_experts(q, routing_weights)\n        k = self._apply_experts(k, routing_weights)\n        v = self._apply_experts(v, routing_weights)\n        scale = 1.0 / math.sqrt(self.head_dim)\n        scores = torch.matmul(q, k.transpose(-2, -1)) * scale\n        if self.causal:\n            causal_mask = torch.triu(torch.ones(L, L, device=X.device),\n                diagonal=1).bool()\n            scores.masked_fill_(causal_mask, float('-inf'))\n        attn = F.softmax(scores, dim=-1)\n        context = torch.matmul(attn, v)\n        output = rearrange(context, 'b h l d -> b l (h d)')\n        output = self.out_proj(output)\n        return output, Z\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000, 'max_seq_len': 4096,\n    'hidden_features': None, 'out_features': None, 'activation': None,\n    'bias': False, 'multiple_of': 128, 'n_heads': 8, 'num_experts': 4,\n    'expert_dim': None, 'causal': True, 'num_heads_kv': None, 'head_dim':\n    None, 'qkv_proj_bias': True, 'out_proj_bias': True, 'router_epsilon': 1e-05\n    }\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}