{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = RetNet(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom torchtune.modules import RMSNorm\n\n\nclass RetNet(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, norm_eps: float=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.attn = MultiScaleRetention(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mlp_norm = RMSNorm(self.hidden_size, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.mlp = StreamRetNetMLP(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        hidden_states = self.attn_norm(X)\n        X = self.attn(hidden_states, **Z)[0] + X\n        hidden_states = self.mlp_norm(X)\n        X = self.mlp(hidden_states, **Z)[0] + X\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\nfrom einops import rearrange, repeat\nfrom torchtune.modules import RotaryPositionalEmbeddings, RMSNorm\n\n\nclass MultiScaleRetention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_size=None, num_heads: int=8,\n        norm_eps: float=1e-05, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        hidden_size = hidden_size if hidden_size is not None else embed_dim\n        self.hidden_size = hidden_size\n        self.num_heads = num_heads\n        self.num_kv_heads = num_heads\n        self.num_kv_groups = self.num_heads // self.num_kv_heads\n        self.key_dim = hidden_size\n        self.value_dim = hidden_size * 2\n        self.key_dim_per_group = self.key_dim // self.num_kv_groups\n        self.value_dim_per_group = self.value_dim // self.num_kv_groups\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.q_proj = nn.Linear(hidden_size, self.key_dim, bias=False,\n            device=device, dtype=dtype)\n        self.k_proj = nn.Linear(hidden_size, self.key_dim_per_group, bias=\n            False, device=device, dtype=dtype)\n        self.v_proj = nn.Linear(hidden_size, self.value_dim_per_group, bias\n            =False, device=device, dtype=dtype)\n        self.g_proj = nn.Linear(hidden_size, self.value_dim, bias=False,\n            device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, hidden_size, bias=False,\n            device=device, dtype=dtype)\n        self.g_norm = RMSNorm(self.head_v_dim, eps=norm_eps).to(device=\n            device, dtype=dtype)\n        self.gate_fn = ACT2FN['swish']\n        self.rotary = RotaryPositionalEmbeddings(dim=self.head_qk_dim).to(\n            device=device, dtype=dtype)\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        module._is_hf_initialized = True\n\n    def naive_retention(self, q, k, v):\n        orig_type = q.dtype\n        q, k, v = q.float(), k.float(), v.float()\n        _, n_heads, seq_len, d_head = q.shape\n        s = (1 - q.new_tensor(2.0, dtype=torch.float).pow(-5.0 - q.\n            new_tensor(range(n_heads), dtype=torch.float))).log2()\n        n = q.new_tensor(range(seq_len), dtype=torch.float)\n        n = torch.exp2((n.unsqueeze(-1) - n) * s.view(-1, 1, 1)) * n.unsqueeze(\n            -1).ge(n)\n        s = torch.einsum('bhqd,bhkd,hqk->bhqk', q * d_head ** -0.5, k, n.to\n            (q.dtype))\n        o = torch.einsum('bhqk,bhkd->bhqd', s, v)\n        return o.to(orig_type)\n\n    def _forward(self, X, **Z):\n        q = self.q_proj(X)\n        k = self.k_proj(X)\n        v = self.v_proj(X)\n        q = rearrange(q, '... (h d) -> ... h d', h=self.num_heads)\n        k = rearrange(k, '... (h d) -> ... h d', h=self.num_kv_heads)\n        q = self.rotary(q)\n        k = self.rotary(k)\n        q = q.transpose(1, 2)\n        if self.num_kv_groups > 1:\n            k = repeat(k, 'b t h d -> b (h g) t d', h=self.num_kv_heads, g=\n                self.num_kv_groups)\n            v = repeat(v, 'b t (h d) -> b (h g) t d', h=self.num_kv_heads,\n                g=self.num_kv_groups)\n        else:\n            k, v = rearrange(k, 'b t h d -> b h t d'), rearrange(v,\n                'b t (h d) -> b h t d', h=self.num_kv_heads)\n        o = self.naive_retention(q, k, v)\n        o = rearrange(o, 'b h l d -> b l h d')\n        g = self.g_proj(X)\n        o = rearrange(self.g_norm(o), 'b l h d -> b l (h d)')\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        return o\n\n\nimport torch.nn.functional as F\nfrom transformers.activations import ACT2FN\n\n\nclass StreamRetNetMLP(GAUBase):\n    \"\"\"\n    StreamRetNetMLP is an optimized MLP unit for RetNet that introduces attention sink integration,\n    adaptive tiling, and selective KV caching mechanisms for efficient streaming inference.\n    The MLP processes the input sequence along with attention sinks and handles memory efficiently\n    during both training and inference.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments, used to initialize child units.\n        sink_size (int, optional): Number of attention sink tokens. Default is 4.\n        tile_size (int, optional): Size of each tile for adaptive tiling. Default is 128.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, sink_size: int=4, tile_size: int=128, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, self.\n            hidden_size, **self.factory_kwargs))\n        self.gate_proj = nn.Linear(self.hidden_size, self.hidden_size * 4,\n            bias=False, **self.factory_kwargs)\n        self.down_proj = nn.Linear(self.hidden_size * 2, self.hidden_size,\n            bias=False, **self.factory_kwargs)\n        self.act_fn = ACT2FN['swish']\n        self.tile_manager = TileManager(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.kv_cache = AdaptiveKVCache(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n\n    def _process_sinks(self, X):\n        sink_tokens = self.sink_tokens.unsqueeze(0).expand(X.size(0), -1, -1)\n        return sink_tokens\n\n    def _compute_block(self, tile, sink_attn):\n        combined = torch.cat([sink_attn, tile], dim=1)\n        y = self.gate_proj(combined)\n        gate, y = y.chunk(2, -1)\n        z = self.act_fn(gate) * y\n        o = self.down_proj(z)\n        tile_output = o[:, self.sink_size:, :]\n        return tile_output\n\n    def _forward(self, X, **Z):\n        sink_attn = self._process_sinks(X)\n        tiled_output, Z_tile = self.tile_manager.forward(X, compute_fn=self\n            ._compute_block, sink_attn=sink_attn, **Z)\n        Y_kv, Z_kv = self.kv_cache.forward(tiled_output, **Z)\n        Z.update(Z_tile)\n        Z.update(Z_kv)\n        Y = Y_kv\n        return Y, Z\n\n\nfrom torch import nn\nfrom transformers.activations import ACT2FN\n\n\nclass AdaptiveKVCache(GAUBase):\n    \"\"\"\n    AdaptiveKVCache manages intelligent caching of key-value pairs for efficient streaming inference.\n    It updates the cache based on the importance of the input.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, threshold: float=0.5, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.threshold = threshold\n\n    def _forward(self, X, **Z):\n        scores = self.importance_estimator(X)\n        scores = torch.sigmoid(scores)\n        mask = scores > self.threshold\n        if 'kv_cache' in Z:\n            cache = Z['kv_cache']\n        else:\n            cache = []\n        cache_entries = []\n        for b in range(X.size(0)):\n            selected = X[b][mask[b].squeeze(-1)]\n            cache_entries.append(selected)\n        if cache_entries:\n            cache.append(cache_entries)\n            Z_ = {'kv_cache': cache}\n        else:\n            Z_ = Z\n        Y = X * scores\n        return Y, Z_\n\n\nfrom torch import nn\n\n\nclass TileManager(GAUBase):\n    \"\"\"\n    TileManager implements the adaptive tiling mechanism for efficient memory access.\n    It processes the input sequence by dividing it into tiles and applying a compute function\n    to each tile along with the attention sinks.\n\n    Args:\n        embed_dim (int): Dimension of the embeddings.\n        block_loc (tuple): Location of the block within the network, (layer_idx, n_block).\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        tile_size (int): Size of each tile for tiling.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, tile_size: int=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.tile_size = tile_size\n\n    def _forward(self, X, compute_fn, sink_attn, **Z):\n        tiles = torch.split(X, self.tile_size, dim=1)\n        outputs = []\n        for tile in tiles:\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n        tiled_output = torch.cat(outputs, dim=1)\n        return tiled_output, Z\n\n\ngab_config = {'hidden_size': None, 'num_heads': 8, 'norm_eps': 1e-06,\n    'sink_size': 4, 'tile_size': 128, 'threshold': 0.5}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}