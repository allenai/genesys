{
    "variantname": "ControlVariateLinearAttention",
    "review": "The ControlVariateLinearAttention (CVLA) proposal presents an innovative approach to enhancing linear attention mechanisms through the integration of control variates and dynamic memory compression. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\n1. CLARITY (4.5/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n- Well-defined objectives and motivation\n- Detailed mathematical formulations\n- Clear implementation guidelines\n- Comprehensive architectural descriptions\nHowever, some aspects of the control variate integration with memory compression could be more thoroughly explained.\n\n2. INNOVATION (4.3/5):\nThe proposal demonstrates significant innovation through:\n- Novel integration of control variates with linear attention\n- Unique approach to dynamic memory compression\n- Advanced hardware optimization strategies\nComparison with existing research shows several unique contributions:\n- Unlike Lightning Attention-2's block-based approach, CVLA introduces variance reduction\n- Different from Linear Attention Sequence Parallelism's communication focus, CVLA emphasizes memory efficiency\n- Distinct from existing control variate methods, CVLA combines variance reduction with memory compression\n\n3. FEASIBILITY (4.0/5):\nThe design appears implementable within the given framework:\n+ Clear implementation guidelines and pseudo-code\n+ Well-defined mathematical formulations\n+ Modular architecture\n- However, some concerns exist regarding:\n  * Integration complexity of control variates with memory compression\n  * Training stability with combined variance reduction and compression\n  * Memory management complexity\n\n4. SCALABILITY (4.2/5):\nStrong potential for efficient scaling:\n+ Linear complexity through optimized attention mechanisms\n+ Efficient memory usage through dynamic compression\n+ Adaptive resource allocation\n- Considerations:\n  * Need for careful management of control variate overhead\n  * Potential memory bottlenecks in very large models\n  * Communication overhead between components\n\n5. ACCURACY AND ROBUSTNESS (4.1/5):\nExpected positive impact on model performance:\n+ Enhanced stability through variance reduction\n+ Improved handling of varied input patterns\n+ Better adaptation to different contexts\n- Potential concerns:\n  * Need for careful tuning of control variate coefficients\n  * Impact of compression on model quality\n  * Balance between variance reduction and information preservation\n\n6. EFFICIENCY (4.4/5):\nSignificant efficiency improvements:\n+ Reduced computational complexity through variance reduction\n+ Optimized memory usage via dynamic compression\n+ Hardware-efficient implementation strategy\n- Considerations:\n  * Overhead from control variate computation\n  * Additional computation for compression management",
    "search_stack": [
        {
            "ready": false,
            "query": "variance reduction transformer attention, dynamic memory management language models, efficient sparse attention patterns",
            "detail": "Find papers discussing:\n1. Variance reduction techniques specifically applied to transformer attention mechanisms\n2. Dynamic memory management approaches in language models, particularly those dealing with variable sequence lengths\n3. Methods for combining sparse and dense attention patterns efficiently\n4. Implementation details of low-rank approximations in attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Variance reduction techniques specifically applied to transformer attention mechanisms\n2. Dynamic memory management approaches in language models, particularly those dealing with variable sequence lengths\n3. Methods for combining sparse and dense attention patterns efficiently\n4. Implementation details of low-rank approximations in attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.94)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 17/38 (Score: 0.94)*\n\n```\nBehnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP (1), pp. 2664-2674. Association for Computational Linguistics, 2020 . Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004. 05150. Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/ abs/2112.04426. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 2. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.92)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 13/30 (Score: 0.92)*\n\n```\narXiv preprint arXiv:2304.07327, 2023. Woosuk Kwon, Sehoon Kim, Michael W. Mahoney, Joseph Hassoun, Kurt Keutzer, and Amir Gholami. A fast post-training pruning framework for transformers, 2022. Heejun Lee, Minki Kang, Youngwan Lee, and Sung Ju Hwang. Sparse token transformer with attention back tracking. In The Eleventh International Conference on Learning Representations, 2023. Juho Lee, Yoonho Lee, Jungtaek Kim, Adam R. Kosiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A framework for attention-based permutation-invariant neural networks, 2019. Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transformers. AI Open, 2022. Andr\u00e9 Martins, Ant\u00f3nio Farinhas, Marcos Treviso, Vlad Niculae, Pedro Aguiar, and Mario Figueiredo. Sparse and continuous attention mechanisms. Advances in Neural Information Processing Systems, 33:20989-21001, 2020. Lorenzo Noci, Sotiris Anagnostidis, Luca Biggio, Antonio Orvieto, Sidak Pal Singh, and Aurelien Lucchi. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse.\n```\n\n#### 3. Reformer: The Efficient Transformer (Avg. Score: 0.92)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.92)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n#### 4. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.90)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.90)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 5. \u221e-former: Infinite Memory Transformer (Avg. Score: 0.89)\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins*\n\n**Published in:** Volume 1 (2022)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 23/32 (Score: 0.89)*\n\n```\nIn Proc. of the National Academy of Sciences. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random Feature Attention. In Proc. ICLR (To appear). Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, and Timothy P Lillicrap. 2016. Scaling memoryaugmented neural networks with sparse reads and writes. In Proc. NeurIPS. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive Transformers for Long-Range Sequence Modelling. In Proc. ICLR. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and DaCheng Juan. 2020a. Sparse sinkhorn attention. In Proc. ICML. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020b. Efficient transformers: A survey. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS. Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Proc. NeurIPS. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proc. EMNLP-IJCNLP. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. 2021. Cluster-Former: Clustering-based Sparse Transformer for Question Answering.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: variance reduction transformer attention, dynamic memory management language models, efficient sparse attention patterns\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings\n\n*From Search Query: variance reduction transformer attention*\n\n*Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge*\n\n**TL;DR:** This work demonstrates that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance, and derives the underlying distribution of each step within a transformer layer.\n\n**Abstract:** The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 2. Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top\n\n*From Search Query: variance reduction transformer attention*\n\n*Eduard A. Gorbunov, Samuel Horv\u00e1th, Peter Richt'arik, Gauthier Gidel*\n\n**TL;DR:** Theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-\u0141ojasiewicz loss functions are derived and theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-\u0141ojasiewicz loss functions are derived.\n\n**Abstract:** Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning. However, many fruitful directions, such as the usage of variance reduction for achieving robustness and communication compression for reducing communication costs, remain weakly explored in the \ufb01eld. This work addresses this gap and proposes Byz-VR-MARINA \u2013a new Byzantine-tolerant method with variance reduction and compression. A key mes-sage of our paper is that variance reduction is key to \ufb01ghting Byzantine workers more effectively. At the same time, communication compression is a bonus that makes the process more communication ef\ufb01cient. We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-\u0141ojasiewicz loss functions. Unlike the con-current Byzantine-robust methods with variance reduction and/or compression, our complexity results are tight and do not rely on restrictive assumptions such as boundedness of the gradients or limited compression. Moreover, we provide the \ufb01rst analysis of a Byzantine-tolerant method supporting non-uniform sampling of stochastic gradients. Numerical experiments corroborate our theoretical \ufb01ndings.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 2*)\n\n#### 3. SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget\n\n*From Search Query: dynamic memory management language models*\n\n*Rui Kong, Yuanchun Li, Qingtian Feng, Weijun Wang, L. Kong, Yunxin Liu*\n\n**TL;DR:** The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts.\n\n**Abstract:** Mixture of experts (MoE) is a popular technique to improve capacity of Large Language Models (LLMs) with conditionally-activated parallel experts. However, serving MoE models on memory-constrained devices is challenging due to the large parameter size. Typical solutions such as memory swapping or expert pruning may lead to significantly higher latency or severe accuracy loss. In this paper, we introduce SwapMoE, a framework for efficient serving of MoE-based large language models with tunable memory budgets. The main idea of SwapMoE is to keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts. Experiments have shown that SwapMoE can reduce the memory footprint while maintaining reasonable accuracy. For example, on text summarization tasks with Switch Transformer, SwapMoE can reduce the memory consumption from 14.2 GiB to 4.7 GiB, together with 50\\% latency reduction and a slight Rouge-2 score drop of 0.041.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 1*)\n\n#### 4. Towards Teachable Reasoning Systems: Using a Dynamic Memory of User Feedback for Continual System Improvement\n\n*From Search Query: dynamic memory management language models*\n\n*Bhavana Dalvi, Oyvind Tafjord, Peter Clark*\n\n**TL;DR:** The approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied corrections toerroneous model beliefs that users identify during interaction, leading to improved system\u2019s performance over time.\n\n**Abstract:** Our goal is a teachable reasoning system for question-answering (QA), where a user can interact with faithful answer explanations, and correct its errors so that the system improves over time. Our approach is to augment a QA model with a dynamic memory of user feedback, containing user-supplied corrections toerroneous model beliefs that users identify during interaction. Retrievals from memory are used as additional context for QA, to help avoid previous mistakes in similar new situations - a novel application of memory-based continuous learning. With simulated feedback, we find that our system (called TeachMe) continually improves with time, and without model retraining, requiring feedback on only 25% of training examples to reach within 1% of the upper-bound (feedback on all examples). Similarly, in experiments with real users, we observe a similar trend, with performance improving by over 15% on a hidden test set after teaching. This suggests new opportunities for using frozen language models in an interactive setting where users can inspect, debug, and correct the model\u2019s beliefs, leading to improved system\u2019s performance over time.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 2*)\n\n#### 5. Larimar: Large Language Models with Episodic Memory Control\n\n*From Search Query: dynamic memory management language models*\n\n*Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aur'elie Lozano, Georgios Kollias, V. Chenthamarakshan, Jir\u00ed Navr\u00e1til, Soham Dan, Pin-Yu Chen*\n\n**TL;DR:** Larimar is presented - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory that allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning.\n\n**Abstract:** Efficient and accurate updating of knowledge stored in Large Language Models (LLMs) is one of the most pressing research challenges today. This paper presents Larimar - a novel, brain-inspired architecture for enhancing LLMs with a distributed episodic memory. Larimar's memory allows for dynamic, one-shot updates of knowledge without the need for computationally expensive re-training or fine-tuning. Experimental results on multiple fact editing benchmarks demonstrate that Larimar attains accuracy comparable to most competitive baselines, even in the challenging sequential editing setup, but also excels in speed - yielding speed-ups of 8-10x depending on the base LLM - as well as flexibility due to the proposed architecture being simple, LLM-agnostic, and hence general. We further provide mechanisms for selective fact forgetting, information leakage prevention, and input context length generalization with Larimar and show their effectiveness. Our code is available at https://github.com/IBM/larimar\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 6. ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer\n\n*From Search Query: efficient sparse attention patterns*\n\n*Ning Wang, Guobing Gan, Peng Zhang, Shuai Zhang, Junqiu Wei, Qun Liu, Xin Jiang*\n\n**TL;DR:** A neural clustering method which can be seamlessly integrated into the Self-Attention Mechanism in Transformer, which groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency.\n\n**Abstract:** Recently, a lot of research has been carried out to improve the efficiency of Transformer. Among them, the sparse pattern-based method is an important branch of efficient Transformers. However, some existing sparse methods usually use fixed patterns to select words, without considering similarities between words. Other sparse methods use clustering patterns to select words, but the clustering process is separate from the training process of the target task, which causes a decrease in effectiveness. To address these limitations, we design a neural clustering method, which can be seamlessly integrated into the Self-Attention Mechanism in Transformer. The clustering task and the target task are jointly trained and optimized to benefit each other, leading to significant effectiveness improvement. In addition, our method groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently, which improves the efficiency. We verified our method on machine translation, text classification, natural language inference, and text matching tasks. Experimental results show that our method outperforms two typical sparse attention methods, Reformer and Routing Transformer while having a comparable or even better time and memory efficiency.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 7. SparseBERT: Rethinking the Importance Analysis in Self-attention\n\n*From Search Query: efficient sparse attention patterns*\n\n*Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, J. Kwok*\n\n**TL;DR:** A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions, and a proof is provided showing that these diagonal elements can indeed be removed without deteriorating model performance.\n\n**Abstract:** Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 46  (*Influential: 2*)\n\n#### 8. The Elastic Lottery Ticket Hypothesis\n\n*From Search Query: efficient sparse attention patterns*\n\n*Xiaohan Chen, Yu Cheng, Shuohang Wang, Zhe Gan, Jingjing Liu, Zhangyang Wang*\n\n**TL;DR:** The Elastic Lottery Ticket Hypothesis (E-LTH) is articulated: by mindfully replicating and re-ordering layers for one network, its corresponding winning ticket could be stretched into a subnetwork for another deeper network from the same family, whose performance is nearly the same competitive as the latter's winning ticket directly found by IMP.\n\n**Abstract:** Lottery Ticket Hypothesis (LTH) raises keen attention to identifying sparse trainable subnetworks, or winning tickets, which can be trained in isolation to achieve similar or even better performance compared to the full models. Despite many efforts being made, the most effective method to identify such winning tickets is still Iterative Magnitude-based Pruning (IMP), which is computationally expensive and has to be run thoroughly for every different network. A natural question that comes in is: can we\"transform\"the winning ticket found in one network to another with a different architecture, yielding a winning ticket for the latter at the beginning, without re-doing the expensive IMP? Answering this question is not only practically relevant for efficient\"once-for-all\"winning ticket finding, but also theoretically appealing for uncovering inherently scalable sparse patterns in networks. We conduct extensive experiments on CIFAR-10 and ImageNet, and propose a variety of strategies to tweak the winning tickets found from different networks of the same model family (e.g., ResNets). Based on these results, we articulate the Elastic Lottery Ticket Hypothesis (E-LTH): by mindfully replicating (or dropping) and re-ordering layers for one network, its corresponding winning ticket could be stretched (or squeezed) into a subnetwork for another deeper (or shallower) network from the same family, whose performance is nearly the same competitive as the latter's winning ticket directly found by IMP. We have also extensively compared E-LTH with pruning-at-initialization and dynamic sparse training methods, as well as discussed the generalizability of E-LTH to different model families, layer types, and across datasets. Code is available at https://github.com/VITA-Group/ElasticLTH.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 30  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond\n\n*From Search Query: variance reduction transformer attention*\n\n*DaCheng Tao, Jing Zhang, Yufei Xu, Qiming Zhang*\n\n**Abstract:** Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and can learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. The proposed two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline transformer models and concurrent works. Besides, we scale up our ViTAE model to 644M parameters and obtain the state-of-the-art classification performance, i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the best 91.2% Top-1 accuracy on ImageNet real validation set, without using extra private data.\n\n**Published:** 2022-02-21\n\n\n\n#### 2. ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias\n\n*From Search Query: variance reduction transformer attention*\n\n*DaCheng Tao, Jing Zhang, Qiming Zhang, Yufei Xu*\n\n**Abstract:** Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a novel Vision Transformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at GitHub.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-07\n\n\n\n#### 3. Efficient Memory Management for Large Language Model Serving with PagedAttention\n\n*From Search Query: dynamic memory management language models*\n\n*Ion Stoica, Hao Zhang, Joseph E. Gonzalez, Cody Hao Yu, Lianmin Zheng, Ying Sheng, Siyuan Zhuang, Zhuohan Li, Woosuk Kwon*\n\n**Abstract:** High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm\n\n**Published:** 2023-09-12\n\n\n\n#### 4. MemGPT: Towards LLMs as Operating Systems\n\n*From Search Query: dynamic memory management language models*\n\n*Ion Stoica, Kevin Lin, Joseph E. Gonzalez, Sarah Wooders, Shishir G. Patil, Vivian Fang, Charles Packer*\n\n**Abstract:** Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.\n\n**Published:** 2023-10-12\n\n\n\n#### 5. Self-Attentive Sequential Recommendation\n\n*From Search Query: efficient sparse attention patterns*\n\n*Wang-Cheng Kang, Julian McAuley*\n\n**Abstract:** Sequential dynamics are a key feature of many modern recommender systems,\nwhich seek to capture the `context' of users' activities on the basis of\nactions they have performed recently. To capture such patterns, two approaches\nhave proliferated: Markov Chains (MCs) and Recurrent Neural Networks (RNNs).\nMarkov Chains assume that a user's next action can be predicted on the basis of\njust their last (or last few) actions, while RNNs in principle allow for\nlonger-term semantics to be uncovered. Generally speaking, MC-based methods\nperform best in extremely sparse datasets, where model parsimony is critical,\nwhile RNNs perform better in denser datasets where higher model complexity is\naffordable. The goal of our work is to balance these two goals, by proposing a\nself-attention based sequential model (SASRec) that allows us to capture\nlong-term semantics (like an RNN), but, using an attention mechanism, makes its\npredictions based on relatively few actions (like an MC). At each time step,\nSASRec seeks to identify which items are `relevant' from a user's action\nhistory, and use them to predict the next item. Extensive empirical studies\nshow that our method outperforms various state-of-the-art sequential models\n(including MC/CNN/RNN-based approaches) on both sparse and dense datasets.\nMoreover, the model is an order of magnitude more efficient than comparable\nCNN/RNN-based models. Visualizations on attention weights also show how our\nmodel adaptively handles datasets with various density, and uncovers meaningful\npatterns in activity sequences.\n\n**Published:** 2018-08-20\n\n\n\n#### 6. MInference 1.0: Accelerating Pre-filling for Long-Context LLMs via Dynamic Sparse Attention\n\n*From Search Query: efficient sparse attention patterns*\n\n*Lili Qiu, Yuqing Yang, Chin-Yew Lin, Dongsheng Li, Amir H. Abdi, Zhenhua Han, Surin Ahn, Xufang Luo, Qianhui Wu, Chengruidong Zhang, Yucheng Li, Huiqiang Jiang*\n\n**Abstract:** The computational challenges of Large Language Model (LLM) inference remain a significant barrier to their widespread deployment, especially as prompt lengths continue to increase. Due to the quadratic complexity of the attention computation, it takes 30 minutes for an 8B LLM to process a prompt of 1M tokens (i.e., the pre-filling stage) on a single A100 GPU. Existing methods for speeding up prefilling often fail to maintain acceptable accuracy or efficiency when applied to long-context LLMs. To address this gap, we introduce MInference (Milliontokens Inference), a sparse calculation method designed to accelerate pre-filling of long-sequence processing. Specifically, we identify three unique patterns in long-context attention matrices-the A-shape, Vertical-Slash, and Block-Sparsethat can be leveraged for efficient sparse computation on GPUs. We determine the optimal pattern for each attention head offline and dynamically build sparse indices based on the assigned pattern during inference. With the pattern and sparse indices, we perform efficient sparse attention calculations via our optimized GPU kernels to significantly reduce the latency in the pre-filling stage of long-context LLMs. Our proposed technique can be directly applied to existing LLMs without any modifications to the pre-training setup or additional fine-tuning. By evaluating on a wide range of downstream tasks, including InfiniteBench, RULER, PG-19, and Needle In A Haystack, and models including LLaMA-3-1M, GLM4-1M, Yi-200K, Phi-3-128K, and Qwen2-128K, we demonstrate that MInference effectively reduces inference latency by up to 10x for pre-filling on an A100, while maintaining accuracy. Our code is available at https://aka.ms/MInference.\n\n**Published:** 2024-07-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on the areas of variance reduction, dynamic memory management, efficient sparse and dense attention patterns, and low-rank approximations, here are some relevant findings and suggestions based on the provided sources and additional insights:\n\n## Variance Reduction Techniques in Transformer Attention Mechanisms\n\nWhile the provided sources do not directly address variance reduction techniques specific to transformer attention mechanisms, the concept of variance reduction is often linked to optimization methods. The paper on \"From Attention to Activation: Unravelling the Enigmas of Large Transformers\" introduces *OrthoAdam*, an optimizer that uses orthogonal matrices to transform gradients, which can help in reducing outlier activations and potentially stabilize training.\n\nFor explicit variance control, researchers might explore adapting variance reduction techniques from other domains, such as those used in stochastic gradient descent (SGD) methods. However, specific applications to transformer attention mechanisms would require further investigation.\n\n## Dynamic Memory Management Approaches\n\nDynamic memory management is crucial for handling variable sequence lengths efficiently. Here are some approaches:\n\n- **Dynamic Token Pruning**: The \"Sparse Token Transformer with Attention Back Tracking\" suggests a mechanism for dynamic sparsity by pruning tokens based on attention scores. This can help in reducing memory usage for long sequences (though not directly mentioned, it aligns with the concept).\n- **Efficient Linear Attention Parallelization**: The \"Linear Attention Sequence Parallelism\" demonstrates efficient point-to-point communication for linear attention, which can be optimized for memory usage by parallelizing computations effectively.\n\n## Methods for Combining Sparse and Dense Attention Patterns\n\nCombining sparse and dense attention patterns can enhance the expressiveness of the attention mechanism while maintaining efficiency:\n\n- **Differential Transformer**: The \"Differential Transformer\" introduces a differential attention mechanism that can be seen as a way to combine different attention patterns efficiently. It partitions the query and key vectors into two groups and computes two separate softmax attention maps, then subtracts these maps to eliminate attention noise. This approach can be adapted to combine sparse and dense attention patterns by applying different attention mechanisms to different parts of the input.\n\n## Implementation Details of Low-Rank Approximations in Attention Mechanisms\n\nLow-rank approximations can significantly improve the efficiency of attention mechanisms:\n\n- **Low-Rank Transformer**: The \"Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer\" demonstrates the successful application of low-rank approximations in transformer architectures. This involves approximating the attention matrix using lower-dimensional representations, which can be combined with linear attention mechanisms for better efficiency.\n\n## Additional Insights\n\n- **Attention Noise Reduction**: The \"Differential Transformer\" also highlights the importance of reducing attention noise, which can be achieved through differential denoising. This method encourages models to focus on critical information and can be particularly useful in combining sparse and dense attention patterns efficiently.\n- **Orthogonal Optimizers**: The use of *OrthoAdam* optimizer, as mentioned earlier, can help in reducing outlier activations and improving the stability of the model, which is crucial for robust and efficient training.\n\nBy integrating these techniques\u2014dynamic token pruning for memory efficiency, differential attention mechanisms for combining sparse and dense patterns, low-rank approximations for efficient attention computation, and orthogonal optimizers for variance control\u2014the researchers can significantly enhance the performance, efficiency, and adaptability of their autoregressive language model.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design AdaptiveLinearAttention (ALA) presents a strong foundation by combining linear attention with context-adaptive encoding and meta-learning. However, there are several areas where improvements could be made:\n\n1. Variance Control: While ALA uses meta-learning for adaptation, it lacks explicit variance control mechanisms which could affect training stability and generalization.\n\n2. Memory Efficiency: The current design could benefit from more sophisticated memory management techniques, especially for long sequences.\n\n3. Attention Mechanism: The linear attention mechanism could be enhanced with more expressive patterns while maintaining efficiency.\n\n4. Adaptability: Though it includes context-adaptive encoding, there's room for more sophisticated dynamic adaptation mechanisms.\n\nKey References and Excerpts:\n\n1. From \"Sparse Token Transformer with Attention Back Tracking\":\n\"LTPPruneToken implements token pruning based on attention scores, providing a mechanism for dynamic sparsity:\n```python\ndef forward(self, x, attention_score, attention_mask):\n    score = torch.mean(torch.mean(attention_score, dim=1), dim=1)\n    self.last_mask = torch.sigmoid((score - self.threshold) / self.temperature)\n```\"\nThis suggests a potential approach for dynamic token selection.\n\n2. From \"Linear Attention Sequence Parallelism\":\n\"The implementation shows efficient point-to-point communication for linear attention:\n```python\ndef lasp_forward(q, k, v, s, kv):\n    # Linear complexity attention with efficient parallelization\n    o = torch.empty((b, h, n, e), dtype=q.dtype, device=q.device)\n    # ... efficient implementation details\n```\"\nThis demonstrates techniques for efficient linear attention computation.\n\n3. From \"The pitfalls of next-token prediction\":\nKey insight: \"teacher-forcing can simply fail to learn an accurate next-token predictor in certain classes of tasks\" - suggests need for more robust prediction mechanisms.\n\n4. From \"Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer\":\nDemonstrates successful application of low-rank approximations in transformer architectures while maintaining performance.\n\nKey Insights and Analysis:\n\n1. Memory Management:\n- Dynamic token pruning from the Sparse Token Transformer could be adapted for efficient memory usage\n- Linear attention parallelization techniques from LASP could improve computational efficiency\n\n2. Attention Enhancement:\n- Low-rank approximations could be combined with linear attention for better efficiency\n- Back-tracking mechanisms could improve attention accuracy\n\n3. Variance Control:\n- Need to address the teacher forcing issues highlighted in \"The pitfalls of next-token prediction\"\n- Could incorporate explicit variance reduction techniques\n\n4. Adaptability:\n- Token pruning could be made more context-aware\n- Memory management could be dynamically adjusted based on sequence characteristics\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Variance reduction techniques in transformer architectures\n- Dynamic memory management strategies\n- Advanced parallelization methods for attention mechanisms\n- Techniques for combining low-rank approximations with linear attention\n\n2. Specific areas to search:\n- Recent advances in variance control for transformers\n- Memory-efficient attention mechanisms\n- Dynamic sparsity patterns in language models\n\nReferences:\n\n1. Lee, H., Kang, M., Lee, Y., & Hwang, S. J. (2023). Sparse Token Transformer with Attention Back Tracking. International Conference on Learning Representations.\n\n2. Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., & Zhong, Y. (2024). Linear Attention Sequence Parallelism. arXiv.org.\n\n3. Bachmann, G., & Nagarajan, V. (2024). The pitfalls of next-token prediction. arXiv.org.\n\n4. Winata, G. I., Cahyawijaya, S., Lin, Z., Liu, Z., & Fung, P. (2019). Lightweight and Efficient End-to-End Speech Recognition Using Low-Rank Transformer. IEEE International Conference on Acoustics, Speech, and Signal Processing."
        },
        {
            "ready": false,
            "query": "variance reduction neural networks, hybrid attention mechanisms transformer, memory compression transformer models",
            "detail": "Find specific implementations and theoretical analyses of:\n1. Variance reduction techniques in neural networks, particularly those applicable to attention mechanisms\n2. Hybrid attention mechanisms that combine multiple attention patterns\n3. Memory compression methods for transformer models, especially those dealing with attention state",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and theoretical analyses of:\n1. Variance reduction techniques in neural networks, particularly those applicable to attention mechanisms\n2. Hybrid attention mechanisms that combine multiple attention patterns\n3. Memory compression methods for transformer models, especially those dealing with attention state\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Reformer: The Efficient Transformer (Avg. Score: 0.85)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.85)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n#### 2. \u221e-former: Infinite Memory Transformer (Avg. Score: 0.57)\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 F. T. Martins*\n\n**Published in:** Volume 1 (2022)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** N/A\n\n##### *Relevant Chunk: No. 23/32 (Score: 0.59)*\n\n```\nIn Proc. of the National Academy of Sciences. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2021. Random Feature Attention. In Proc. ICLR (To appear). Ofir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Jack W Rae, Jonathan J Hunt, Tim Harley, Ivo Danihelka, Andrew Senior, Greg Wayne, Alex Graves, and Timothy P Lillicrap. 2016. Scaling memoryaugmented neural networks with sparse reads and writes. In Proc. NeurIPS. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive Transformers for Long-Range Sequence Modelling. In Proc. ICLR. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68. Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and DaCheng Juan. 2020a. Sparse sinkhorn attention. In Proc. ICML. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. 2020b. Efficient transformers: A survey. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. NeurIPS. Apoorv Vyas, Angelos Katharopoulos, and Fran\u00e7ois Fleuret. 2020. Fast transformers with clustered attention. In Proc. NeurIPS. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal Adversarial Triggers for Attacking and Analyzing NLP. In Proc. EMNLP-IJCNLP. Shuohang Wang, Luowei Zhou, Zhe Gan, Yen-Chun Chen, Yuwei Fang, Siqi Sun, Yu Cheng, and Jingjing Liu. 2021. Cluster-Former: Clustering-based Sparse Transformer for Question Answering.\n```\n\n##### *Relevant Chunk: No. 17/32 (Score: 0.56)*\n\n```\nIn Proc. $A C L$. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. 2020. Location Attention for Extrapolation to Longer Sequences. In Proc. ACL. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. 2021. Augmenting Transformers with KNN-Based Composite Memory for Dialog. Transactions of the Association for Computational Linguistics. Ant\u00f3nio Farinhas, Andr\u00e9 F. T. Martins, and P. Aguiar. 2021. Multimodal Continuous Visual Attention Mechanisms. Edouard Grave, Armand Joulin, and Nicolas Usunier. 2016. Improving Neural Language Models with a Continuous Cache. In Proc. ICLR. Alex Graves, Greg Wayne, and Ivo Danihelka. 2014. Neural turing machines. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. 2015. Learning to transduce with unbounded memory. Proc. NeurIPS. Maosheng Guo, Yu Zhang, and Ting Liu. 2019. Gaussian Transformer: A Lightweight Approach for Natural Language Inference.\n```\n\n#### 3. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.57)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.57)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 4. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.51)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 17/38 (Score: 0.51)*\n\n```\nBehnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP (1), pp. 2664-2674. Association for Computational Linguistics, 2020 . Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004. 05150. Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/ abs/2112.04426. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: variance reduction neural networks, hybrid attention mechanisms transformer, memory compression transformer models\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Training Structured Neural Networks Through Manifold Identification and Variance Reduction\n\n*From Search Query: variance reduction neural networks*\n\n*Zih-Syuan Huang, Ching-pei Lee*\n\n**TL;DR:** It is proved that after a finite number of iterations, all iterates of RMDA possess a desired structure identical to that induced by the regularizer at the stationary point of asymptotic convergence, even in the presence of engineering tricks like data augmentation and dropout that complicate the training process.\n\n**Abstract:** This paper proposes an algorithm (RMDA) for training neural networks (NNs) with a regularization term for promoting desired structures. RMDA does not incur computation additional to proximal SGD with momentum, and achieves variance reduction without requiring the objective function to be of the finite-sum form. Through the tool of manifold identification from nonlinear optimization, we prove that after a finite number of iterations, all iterates of RMDA possess a desired structure identical to that induced by the regularizer at the stationary point of asymptotic convergence, even in the presence of engineering tricks like data augmentation and dropout that complicate the training process. Experiments on training NNs with structured sparsity confirm that variance reduction is necessary for such an identification, and show that RMDA thus significantly outperforms existing methods for this task. For unstructured sparsity, RMDA also outperforms a state-of-the-art pruning method, validating the benefits of training structured NNs through regularization.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 2. Stochastic Training of Graph Convolutional Networks with Variance Reduction\n\n*From Search Query: variance reduction neural networks*\n\n*Jianfei Chen, Jun Zhu, Le Song*\n\n**TL;DR:** Control variate based algorithms which allow sampling an arbitrarily small neighbor size are developed and a new theoretical guarantee for these algorithms to converge to a local optimum of GCN is proved.\n\n**Abstract:** Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 463  (*Influential: 68*)\n\n#### 3. Unbiased Stochastic Proximal Solver for Graph Neural Networks with Equilibrium States\n\n*From Search Query: variance reduction neural networks*\n\n*Mingjie Li, Yifei Wang, Yisen Wang, Zhouchen Lin*\n\n**TL;DR:** Two unbiased stochastic proximal solvers inspired by the stochastic proximal gradient descent method and its variance reduction variant called USP and USP-VR solvers are proposed and theoretically prove that their solvers are unbiased, which can converge to the same solution as the original solvers for unfolded GNNs and implicit GNNs.\n\n**Abstract:** Graph Neural Networks (GNNs) are widely used deep learning models that can extract meaningful representations from graph datasets and achieve great success in many machine learning tasks. Among them, graph neural networks with iterative iterations like unfolded GNNs and implicit GNNs can effectively capture longrange dependencies in graphs and demonstrate superior performance on large graphs since they can mathematically ensure its convergence to some nontrivial solution after lots of aggregations. However, the aggregation time for such models costs a lot as they need to aggregate the full graph in each update. Such weakness limits the scalability of the implicit graph models. To tackle such limitations, we propose two unbiased stochastic proximal solvers inspired by the stochastic proximal gradient descent method and its variance reduction variant called USP and USP-VR solvers. From the point of stochastic optimization, we theoretically prove that our solvers are unbiased, which can converge to the same solution as the original solvers for unfolded GNNs and implicit GNNs. Furthermore, the computation complexities for unfolded GNNs and implicit GNNs with our proposed solvers are significantly less than their vanilla versions. Experiments on various large graph datasets show that our proposed solvers are more efficient and can achieve state-of-the-art performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 4. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: hybrid attention mechanisms transformer*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: hybrid attention mechanisms transformer*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 6. Logical Languages Accepted by Transformer Encoders with Hard Attention\n\n*From Search Query: hybrid attention mechanisms transformer*\n\n*Pablo Barcel\u00f3, A. Kozachinskiy, A. W. Lin, Vladimir Podolskii*\n\n**TL;DR:** UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates, and it is shown that AHAT encoder can recognize all languages of their logic even when the authors enrich it with counting terms.\n\n**Abstract:** We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\\sf TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from ${\\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a. Parikh images).\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 7. Deep Compression of Pre-trained Transformer Models\n\n*From Search Query: memory compression transformer models*\n\n*Naigang Wang, Chia-Chun Liu, Swagath Venkataramani, Sanchari Sen, Chia-Yu Chen, K. E. Maghraoui, Vijayalakshmi Srinivasan, Le Chang*\n\n**TL;DR:** Methods to deeply compress pre-trained transformer models across three major application domains: NLP, speech, and vision are introduced and critical initialization strategies for quantization-and sparsity-aware \ufb01ne-tuning are identified.\n\n**Abstract:** Pre-trained transformer models have achieved remarkable success in natural language processing (NLP) and have recently become competitive alternatives to Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) in vision and speech tasks, respectively. Due to their excellent computational ef\ufb01-ciency and scalability, transformer models can be trained on exceedingly large amounts of data at the expense of tremendous growth in model size. As high performance, large-scale, and pre-trained transformer models become increasingly available for users to download and \ufb01ne-tune for customized downstream tasks, their deployment becomes challenging due to the vast amount of operations and large memory footprint. To address this challenge, we introduce methods to deeply compress pre-trained transformer models across three major application domains: NLP, speech, and vision. Speci\ufb01cally, we quantize transformer backbones down to 4-bit and further achieve 50% \ufb01ne-grained structural sparsity on pre-trained BERT, Wav2vec2.0, and Vision Transformer (ViT) models to demonstrate 16x compression while maintaining model accuracy. This is achieved by identifying critical initialization strategies for quantization-and sparsity-aware \ufb01ne-tuning as well as developing novel techniques such as quantizers with a zero-preserving format and scheduled dropout. These hardware-friendly techniques need only to be applied in the \ufb01ne-tuning phase for downstream tasks, which renders them especially suitable for acceleration and deployment of pre-trained transformer models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 0*)\n\n#### 8. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: memory compression transformer models*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n#### 9. Compressed Context Memory For Online Language Model Interaction\n\n*From Search Query: memory compression transformer models*\n\n*Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song*\n\n**TL;DR:** This paper proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments.\n\n**Abstract:** This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Optimal Control of Agent-Based Dynamics under Deep Galerkin Feedback Laws\n\n*From Search Query: variance reduction neural networks*\n\n*Frederik Kelbel*\n\n**Abstract:** Ever since the concepts of dynamic programming were introduced, one of the most difficult challenges has been to adequately address high-dimensional control problems. With growing dimensionality, the utilisation of Deep Neural Networks promises to circumvent the issue of an otherwise exponentially increasing complexity. The paper specifically investigates the sampling issues the Deep Galerkin Method is subjected to. It proposes a drift relaxation-based sampling approach to alleviate the symptoms of high-variance policy approximations. This is validated on mean-field control problems; namely, the variations of the opinion dynamics presented by the Sznajd and the Hegselmann-Krause model. The resulting policies induce a significant cost reduction over manually optimised control functions and show improvements on the Linear-Quadratic Regulator problem over the Deep FBSDE approach.\n\n**Published:** 2024-06-13\n\n\n\n#### 2. Differentiable Tree Search Network\n\n*From Search Query: variance reduction neural networks*\n\n*Wee Sun Lee, Dixant Mittal*\n\n**Abstract:** In decision-making problems with limited training data, policy functions approximated using deep neural networks often exhibit suboptimal performance. An alternative approach involves learning a world model from the limited data and determining actions through online search. However, the performance is adversely affected by compounding errors arising from inaccuracies in the learned world model. While methods like TreeQN have attempted to address these inaccuracies by incorporating algorithmic inductive biases into the neural network architectures, the biases they introduce are often weak and insufficient for complex decision-making tasks. In this work, we introduce Differentiable Tree Search Network (D-TSN), a novel neural network architecture that significantly strengthens the inductive bias by embedding the algorithmic structure of a best-first online search algorithm. D-TSN employs a learned world model to conduct a fully differentiable online search. The world model is jointly optimized with the search algorithm, enabling the learning of a robust world model and mitigating the effect of prediction inaccuracies. Further, we note that a naive incorporation of best-first search could lead to a discontinuous loss function in the parameter space. We address this issue by adopting a stochastic tree expansion policy, formulating search tree expansion as another decision-making task, and introducing an effective variance reduction technique for the gradient computation. We evaluate D-TSN in an offline-RL setting with a limited training data scenario on Procgen games and grid navigation task, and demonstrate that D-TSN outperforms popular model-free and model-based baselines.\n\n**Published:** 2024-01-22\n\n\n\n#### 3. ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers\n\n*From Search Query: hybrid attention mechanisms transformer*\n\n*Baoyuan Wang, Shusheng Yang, Xinggang Wang, Jingfeng Yao*\n\n**Abstract:** Recently, plain vision Transformers (ViTs) have shown impressive performance on various computer vision tasks, thanks to their strong modeling capacity and large-scale pretraining. However, they have not yet conquered the problem of image matting. We hypothesize that image matting could also be boosted by ViTs and present a new efficient and robust ViT-based matting system, named ViTMatte. Our method utilizes (i) a hybrid attention mechanism combined with a convolution neck to help ViTs achieve an excellent performance-computation trade-off in matting tasks. (ii) Additionally, we introduce the detail capture module, which just consists of simple lightweight convolutions to complement the detailed information required by matting. To the best of our knowledge, ViTMatte is the first work to unleash the potential of ViT on image matting with concise adaptation. It inherits many superior properties from ViT to matting, including various pretraining strategies, concise architecture design, and flexible inference strategies. We evaluate ViTMatte on Composition-1k and Distinctions-646, the most commonly used benchmark for image matting, our method achieves state-of-the-art performance and outperforms prior matting works by a large margin.\n\n**Published:** 2023-05-24\n\n\n\n#### 4. Next-ViT: Next Generation Vision Transformer for Efficient Deployment in Realistic Industrial Scenarios\n\n*From Search Query: hybrid attention mechanisms transformer*\n\n*Xin Pan, Min Zheng, Rui Wang, Xuefeng Xiao, Xing Wang, Huixia Li, Wei Li, Xin Xia, Jiashi Li*\n\n**Abstract:** Due to the complex attention mechanisms and model design, most existing vision Transformers (ViTs) can not perform as efficiently as convolutional neural networks (CNNs) in realistic industrial deployment scenarios, e.g. TensorRT and CoreML. This poses a distinct challenge: Can a visual neural network be designed to infer as fast as CNNs and perform as powerful as ViTs? Recent works have tried to design CNN-Transformer hybrid architectures to address this issue, yet the overall performance of these works is far away from satisfactory. To end these, we propose a next generation vision Transformer for efficient deployment in realistic industrial scenarios, namely Next-ViT, which dominates both CNNs and ViTs from the perspective of latency/accuracy trade-off. In this work, the Next Convolution Block (NCB) and Next Transformer Block (NTB) are respectively developed to capture local and global information with deployment-friendly mechanisms. Then, Next Hybrid Strategy (NHS) is designed to stack NCB and NTB in an efficient hybrid paradigm, which boosts performance in various downstream tasks. Extensive experiments show that Next-ViT significantly outperforms existing CNNs, ViTs and CNN-Transformer hybrid architectures with respect to the latency/accuracy trade-off across various vision tasks. On TensorRT, Next-ViT surpasses ResNet by 5.5 mAP (from 40.4 to 45.9) on COCO detection and 7.7% mIoU (from 38.8% to 46.5%) on ADE20K segmentation under similar latency. Meanwhile, it achieves comparable performance with CSWin, while the inference speed is accelerated by 3.6x. On CoreML, Next-ViT surpasses EfficientFormer by 4.6 mAP (from 42.6 to 47.2) on COCO detection and 3.5% mIoU (from 45.1% to 48.6%) on ADE20K segmentation under similar latency. Our code and models are made public at: https://github.com/bytedance/Next-ViT\n\n**Published:** 2022-07-12\n\n\n\n#### 5. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: memory compression transformer models*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 6. Memory AMP\n\n*From Search Query: memory compression transformer models*\n\n*Brian M. Kurkoski, Shunqi Huang, Lei Liu*\n\n**Abstract:** Approximate message passing (AMP) is a low-cost iterative parameter-estimation technique for certain high-dimensional linear systems with non-Gaussian distributions. AMP only applies to independent identically distributed (IID) transform matrices, but may become unreliable (e.g., perform poorly or even diverge) for other matrix ensembles, especially for ill-conditioned ones. To solve this issue, orthogonal/vector AMP (OAMP/VAMP) was proposed for general right-unitarily-invariant matrices. However, the Bayes-optimal OAMP/VAMP (BO-OAMP/VAMP) requires a high-complexity linear minimum mean square error (MMSE) estimator. This prevents OAMP/VAMP from being used in large-scale systems. To address the drawbacks of AMP and BO-OAMP/VAMP, this paper offers a memory AMP (MAMP) framework based on the orthogonality principle, which ensures that estimation errors in MAMP are asymptotically IID Gaussian. To realize the required orthogonality for MAMP, we provide an orthogonalization procedure for the local memory estimators. In addition, we propose a Bayes-optimal MAMP (BO-MAMP), in which a long-memory matched filter is used for interference suppression. The complexity of BO-MAMP is comparable to AMP. To asymptotically characterize the performance of BO-MAMP, a state evolution is derived. The relaxation parameters and damping vector in BO-MAMP are optimized based on state evolution. Most crucially, the state evolution of the optimized BO-MAMP converges to the same fixed point as that of the high-complexity BO-OAMP/VAMP for all right-unitarily-invariant matrices, and achieves the Bayes optimal MSE predicted by the replica method if its state evolution has a unique fixed point. Finally, simulations are provided to verify the theoretical results' validity and accuracy.\n\n**Published:** 2020-12-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on variance reduction, hybrid attention mechanisms, and memory compression, here are some key insights and potential implementations based on the provided analysis and additional research:\n\n## Variance Reduction Techniques in Neural Networks\n\n### Implicit Position Encoding\nThe concept of latent positional information in self-attention variance, as discussed in \"Latent Positional Information\" by Chi et al., suggests that transformer models can inherently encode strong positional information without explicit positional embeddings. This can reduce model complexity and potentially stabilize the attention mechanism[1,.\n\n### Explicit Variance Control\nFor explicit variance control, techniques such as those used in reinforcement learning can be adapted. For instance, the use of Gaussian perturbations in the value function, as seen in the context of reinforcement learning, can serve as a regularization technique to reduce policy overfitting and address potential distributional shifts. This idea can be translated to attention mechanisms by introducing noise or regularization terms to stabilize the attention weights.\n\n## Hybrid Attention Mechanisms\n\n### Combining Sparse and Dense Attention\nThe findings from \"SparseBERT\" indicate that diagonal elements in the attention map are less important, suggesting that sparse attention patterns can be effective. Combining sparse and dense attention patterns, as hinted in the analysis, can balance efficiency and performance. For example, using sparse attention for long-range dependencies and dense attention for local dependencies could be a hybrid approach.\n\n### Neural Clustering\n\"ClusterFormer\" introduces a neural clustering approach that groups words with strong dependencies into the same cluster and performs attention independently for each cluster. This method can be combined with other attention mechanisms to create a hybrid approach that leverages both efficiency and effectiveness.\n\n## Memory Compression Methods for Transformer Models\n\n### Dynamic Token Selection and Pruning\nTechniques like those in \"SwapMoE\" and \"PagedAttention\" demonstrate effective dynamic memory management. By keeping a small dynamic set of important tokens or experts in main memory, the model can significantly reduce memory usage while maintaining performance. This can be adapted to attention patterns by dynamically selecting and pruning tokens based on their importance.\n\n### Hierarchical Memory Structures\nImplementing hierarchical memory structures, as suggested in the analysis, can help balance speed and capacity. This involves using a multi-level memory architecture where less frequently accessed tokens are stored in slower but larger memory, while frequently accessed tokens are kept in faster but smaller memory.\n\n### Efficient KV Cache Management\n\"PagedAttention\" introduces near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests. This efficient memory management technique can be applied to transformer models to reduce memory usage, especially in the context of attention mechanisms where key-value pairs are critical.\n\n## Additional Considerations\n\n### Attention Mechanism Modules\nThe integration of attention mechanisms, as discussed in the context of image processing and natural language processing, can be highly beneficial. For instance, using feature-based attention or attention mechanisms that focus on specific parts of the input data can enhance the model's performance by prioritizing more relevant information.\n\nBy combining these insights, researchers can develop an autoregressive language model that:\n\n- Uses implicit or explicit variance control mechanisms to stabilize the attention weights.\n- Employs hybrid attention mechanisms that combine sparse and dense patterns, along with neural clustering.\n- Implements dynamic token selection, pruning, and hierarchical memory structures to optimize memory usage.\n- Utilizes efficient KV cache management techniques to minimize memory waste.\n\nThese approaches can help achieve better performance, efficiency, and scalability in autoregressive language models.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for improving the AdaptiveLinearAttention (ALA) design, particularly in areas of variance control, memory efficiency, and attention mechanisms:\n\n1. Variance Control and Stability:\n- The concept of latent positional information in attention variance (Chi et al., 2023) suggests potential for implicit position encoding\n- Variance reduction through careful attention pattern design could improve stability\n- Need to explore more explicit variance control mechanisms\n\n2. Memory Management:\n- Dynamic token selection and pruning shows promise (SwapMoE, Larimar)\n- PagedAttention's memory management techniques could be adapted\n- Potential for hierarchical memory structures\n\n3. Attention Mechanisms:\n- ClusterFormer's neural clustering approach offers interesting efficiency gains\n- SparseBERT's findings about diagonal attention importance are noteworthy\n- Potential for combining sparse and dense patterns efficiently\n\nUseful References with Excerpts:\n\n1. From \"Latent Positional Information\" (Chi et al., 2023):\n\"a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance\"\n- Suggests potential for removing explicit position embeddings\n\n2. From \"SwapMoE\" (Kong et al., 2023):\n\"keep a small dynamic set of important experts, namely Virtual Experts, in the main memory for inference, while seamlessly maintaining how the Virtual Experts map to the actual experts\"\n- Demonstrates effective dynamic memory management\n\n3. From \"ClusterFormer\" (Wang et al., 2022):\n\"groups the words with strong dependencies into the same cluster and performs the attention mechanism for each cluster independently\"\n- Shows potential for efficient attention computation\n\n4. From \"SparseBERT\" (Shi et al., 2021):\n\"diagonal elements in the attention map are the least important compared with other attention positions\"\n- Important insight for attention pattern design\n\n5. From \"PagedAttention\" (Stoica et al., 2023):\n\"near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests\"\n- Efficient memory management techniques\n\nKey Insights and Analysis:\n\n1. Memory Efficiency:\n- Combining PagedAttention's paging concepts with dynamic token selection could provide efficient memory management\n- Hierarchical memory structures could help balance speed and capacity\n- Virtual expert concept from SwapMoE could be adapted for attention patterns\n\n2. Attention Mechanism:\n- ClusterFormer's neural clustering could be combined with linear attention\n- SparseBERT's findings about diagonal attention suggest potential optimizations\n- Need to balance sparse and dense attention patterns\n\n3. Variance Control:\n- Implicit position encoding through attention variance could reduce model complexity\n- Need to explore explicit variance reduction techniques\n- Potential for adaptive variance control based on input characteristics\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Explicit variance reduction techniques in neural networks\n2. Advanced clustering methods for attention mechanisms\n3. Hybrid attention patterns combining sparse and dense approaches\n4. Memory compression techniques for transformer models\n\nReferences:\n\n1. Chi, T., Fan, T., Chen, L., Rudnicky, A., & Ramadge, P. (2023). Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings. Annual Meeting of the Association for Computational Linguistics.\n\n2. Kong, R., Li, Y., Feng, Q., Wang, W., Kong, L., & Liu, Y. (2023). SwapMoE: Serving Off-the-shelf MoE-based Large Language Models with Tunable Memory Budget. Annual Meeting of the Association for Computational Linguistics.\n\n3. Wang, N., Gan, G., Zhang, P., Zhang, S., Wei, J., Liu, Q., & Jiang, X. (2022). ClusterFormer: Neural Clustering Attention for Efficient and Effective Transformer. Annual Meeting of the Association for Computational Linguistics.\n\n4. Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., & Kwok, J. (2021). SparseBERT: Rethinking the Importance Analysis in Self-attention. International Conference on Machine Learning.\n\n5. Stoica, I., Zhang, H., Gonzalez, J.E., Yu, C.H., Zheng, L., Sheng, Y., Zhuang, S., Li, Z., & Kwon, W. (2023). Efficient Memory Management for Large Language Model Serving with PagedAttention."
        },
        {
            "ready": false,
            "query": "control variate attention mechanism, theoretical analysis hybrid attention, advanced memory compression transformer",
            "detail": "Find papers discussing:\n1. Implementation details and theoretical analysis of control variates in attention mechanisms\n2. Mathematical foundations of hybrid attention patterns and their convergence properties\n3. Advanced techniques for memory compression in transformer models, particularly those dealing with attention state compression",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Implementation details and theoretical analysis of control variates in attention mechanisms\n2. Mathematical foundations of hybrid attention patterns and their convergence properties\n3. Advanced techniques for memory compression in transformer models, particularly those dealing with attention state compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.93)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.93)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.86)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.86)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. Transformer Quality in Linear Time (Avg. Score: 0.41)\n\n*Weizhe Hua, Zihang Dai, Hanxiao Liu, Quoc V. Le*\n\n**Published in:** International Conference on Machine Learning (2022)\t**Cited by** 152  (*Influential: 36*)\n\n**TL;DR:** This work revisit the design choices in Transformers, and proposes a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss, and a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality.\n\n**Abstract:** We revisit the design choices in Transformers, and propose methods to address their weaknesses in handling long sequences. First, we propose a simple layer named gated attention unit, which allows the use of a weaker single-head attention with minimal quality loss. We then propose a linear approximation method complementary to this new layer, which is accelerator-friendly and highly competitive in quality. The resulting model, named FLASH, matches the perplexity of improved Transformers over both short (512) and long (8K) context lengths, achieving training speedups of up to 4.9$\\times$ on Wiki-40B and 12.1$\\times$ on PG-19 for auto-regressive language modeling, and 4.8$\\times$ on C4 for masked language modeling.\n\n##### *Relevant Chunk: No. 18/29 (Score: 0.41)*\n\n```\nCoRR, abs/1910.05895, 2019. URL http://arxiv.org/ $\\mathrm{abs} / 1910.05895$. Peng, H. et al. Random feature attention. In ICLR, 2021. Rae, J. W., Potapenko, A., Jayakumar, S. M., and Lillicrap, T. P. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1-67, 2020. URL http://jmlr. org/papers/v21/20-074.html. Ramachandran, P., Zoph, B., and Le, Q. V. Searching for activation functions. CoRR, abs/1710.05941, 2017. URL http://arxiv.org/abs/1710.05941. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., and Dai, B. Combiner: Full attention transformer with sparse computation cost.\n```\n\n#### 4. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.36)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 6/24 (Score: 0.36)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [12] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [13] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. [14] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. [15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. [16] Facebook. Fully Sharded Data Parallel: faster AI training with fewer GPUs - engineering.fb.com. https://engineering.fb.com/2021/07/15/open-source/fsdp/.\n```\n\n#### 5. Faster Causal Attention Over Large Sequences Through Sparse Flash Attention (Avg. Score: 0.35)\n\n*Matteo Pagliardini, Daniele Paliotta, Martin Jaggi, Franccois Fleuret*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work extends FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention, leading to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAtt attention.\n\n**Abstract:** Transformer-based language models have found many diverse applications requiring them to process sequences of increasing length. For these applications, the causal self-attention -- which is the only component scaling quadratically w.r.t. the sequence length -- becomes a central concern. While many works have proposed schemes to sparsify the attention patterns and reduce the computational overhead of self-attention, those are often limited by implementations concerns and end up imposing a simple and static structure over the attention matrix. Conversely, implementing more dynamic sparse attentions often results in runtimes significantly slower than computing the full attention using the Flash implementation from Dao et al. (2022). We extend FlashAttention to accommodate a large class of attention sparsity patterns that, in particular, encompass key/query dropping and hashing-based attention. This leads to implementations with no computational complexity overhead and a multi-fold runtime speedup on top of FlashAttention. Even with relatively low degrees of sparsity, our method improves visibly upon FlashAttention as the sequence length increases. Without sacrificing perplexity, we increase the training speed of a transformer language model by $2.0\\times$ and $3.3\\times$ for sequences of respectively $8k$ and $16k$ tokens.\n\n##### *Relevant Chunk: No. 17/38 (Score: 0.35)*\n\n```\nBehnke, M. and Heafield, K. Losing heads in the lottery: Pruning transformer attention in neural machine translation. In EMNLP (1), pp. 2664-2674. Association for Computational Linguistics, 2020 . Beltagy, I., Peters, M. E., and Cohan, A. Longformer: The long-document transformer. CoRR, abs/2004.05150, 2020. URL https://arxiv.org/abs/2004. 05150. Bender, E. M., Gebru, T., McMillan-Major, A., and Shmitchell, S. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, FAccT '21, pp. 610-623, New York, NY, USA, 2021. Association for Computing Machinery. ISBN 9781450383097. doi: 10.1145/3442188.3445922. URL https://doi.org/10.1145/3442188.3445922. Borgeaud, S., Mensch, A., Hoffmann, J., Cai, T., Rutherford, E., Millican, K., van den Driessche, G., Lespiau, J., Damoc, B., Clark, A., de Las Casas, D., Guy, A., Menick, J., Ring, R., Hennigan, T., Huang, S., Maggiore, L., Jones, C., Cassirer, A., Brock, A., Paganini, M., Irving, G., Vinyals, O., Osindero, S., Simonyan, K., Rae, J. W., Elsen, E., and Sifre, L. Improving language models by retrieving from trillions of tokens. CoRR, abs/2112.04426, 2021. URL https://arxiv.org/ abs/2112.04426. Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers. CoRR, abs/1904.10509, 2019. URL http://arxiv.org/abs/1904.10509. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., Belanger, D., Colwell, L. J., and Weller, A. Rethinking attention with performers. CoRR, abs/2009.14794, 2020. URL https://arxiv.org/abs/2009.14794. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: control variate attention mechanism, theoretical analysis hybrid attention, advanced memory compression transformer\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Fine-tune BERT with Sparse Self-Attention Mechanism\n\n*From Search Query: control variate attention mechanism*\n\n*Baiyun Cui, Yingming Li, Ming Chen, Zhongfei Zhang*\n\n**TL;DR:** A novel Sparse Self-Attention Fine-tuning model (referred as SSAF) is developed which integrates sparsity into self-attention mechanism to enhance the fine-tuned performance of BERT.\n\n**Abstract:** In this paper, we develop a novel Sparse Self-Attention Fine-tuning model (referred as SSAF) which integrates sparsity into self-attention mechanism to enhance the fine-tuning performance of BERT. In particular, sparsity is introduced into the self-attention by replacing softmax function with a controllable sparse transformation when fine-tuning with BERT. It enables us to learn a structurally sparse attention distribution, which leads to a more interpretable representation for the whole input. The proposed model is evaluated on sentiment analysis, question answering, and natural language inference tasks. The extensive experimental results across multiple datasets demonstrate its effectiveness and superiority to the baseline methods.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 65  (*Influential: 7*)\n\n#### 2. ABC: Attention with Bounded-memory Control\n\n*From Search Query: control variate attention mechanism*\n\n*Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, Noah A. Smith*\n\n**TL;DR:** This work shows that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and it outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.\n\n**Abstract:** Transformer architectures have achieved state- of-the-art results on a variety of natural language processing (NLP) tasks. However, their attention mechanism comes with a quadratic complexity in sequence lengths, making the computational overhead prohibitive, especially for long sequences. Attention context can be seen as a random-access memory with each token taking a slot. Under this perspective, the memory size grows linearly with the sequence length, and so does the overhead of reading from it. One way to improve the efficiency is to bound the memory size. We show that disparate approaches can be subsumed into one abstraction, attention with bounded-memory control (ABC), and they vary in their organization of the memory. ABC reveals new, unexplored possibilities. First, it connects several efficient attention variants that would otherwise seem apart. Second, this abstraction gives new insights\u2014an established approach (Wang et al., 2020b) previously thought to not be applicable in causal attention, actually is. Last, we present a new instance of ABC, which draws inspiration from existing ABC approaches, but replaces their heuristic memory-organizing functions with a learned, contextualized one. Our experiments on language modeling, machine translation, and masked language model finetuning show that our approach outperforms previous efficient attention models; compared to the strong transformer baselines, it significantly improves the inference time and space efficiency with no or negligible accuracy loss.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 22  (*Influential: 3*)\n\n#### 3. A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity\n\n*From Search Query: theoretical analysis hybrid attention*\n\n*Hongkang Li, M. Wang, Sijia Liu, Pin-Yu Chen*\n\n**TL;DR:** It is shown that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations, and it is proved that a training process using stochastic gradient descent (SGD) leads to a sparse attention map.\n\n**Abstract:** Vision Transformers (ViTs) with self-attention modules have recently achieved great empirical success in many vision tasks. Due to non-convex interactions across layers, however, theoretical learning and generalization analysis is mostly elusive. Based on a data model characterizing both label-relevant and label-irrelevant tokens, this paper provides the first theoretical analysis of training a shallow ViT, i.e., one self-attention layer followed by a two-layer perceptron, for a classification task. We characterize the sample complexity to achieve a zero generalization error. Our sample complexity bound is positively correlated with the inverse of the fraction of label-relevant tokens, the token noise level, and the initial model error. We also prove that a training process using stochastic gradient descent (SGD) leads to a sparse attention map, which is a formal verification of the general intuition about the success of attention. Moreover, this paper indicates that a proper token sparsification can improve the test performance by removing label-irrelevant and/or noisy tokens, including spurious correlations. Empirical experiments on synthetic data and CIFAR-10 dataset justify our theoretical results and generalize to deeper ViTs.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 48  (*Influential: 4*)\n\n#### 4. SparseBERT: Rethinking the Importance Analysis in Self-attention\n\n*From Search Query: theoretical analysis hybrid attention*\n\n*Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, J. Kwok*\n\n**TL;DR:** A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions, and a proof is provided showing that these diagonal elements can indeed be removed without deteriorating model performance.\n\n**Abstract:** Transformer-based models are popularly used in natural language processing (NLP). Its core component, self-attention, has aroused widespread interest. To understand the self-attention mechanism, a direct method is to visualize the attention map of a pre-trained model. Based on the patterns observed, a series of efficient Transformers with different sparse attention masks have been proposed. From a theoretical perspective, universal approximability of Transformer-based models is also recently proved. However, the above understanding and analysis of self-attention is based on a pre-trained model. To rethink the importance analysis in self-attention, we study the significance of different positions in attention matrix during pre-training. A surprising result is that diagonal elements in the attention map are the least important compared with other attention positions. We provide a proof showing that these diagonal elements can indeed be removed without deteriorating model performance. Furthermore, we propose a Differentiable Attention Mask (DAM) algorithm, which further guides the design of the SparseBERT. Extensive experiments verify our interesting findings and illustrate the effect of the proposed algorithm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 46  (*Influential: 2*)\n\n#### 5. JoMA: Demystifying Multilayer Transformers via JOint Dynamics of MLP and Attention\n\n*From Search Query: theoretical analysis hybrid attention*\n\n*Yuandong Tian, Yiping Wang, Zhenyu (Allen) Zhang, Beidi Chen, Simon S. Du*\n\n**TL;DR:** JoMA removes unrealistic assumptions in previous analysis and predicts that the attention first becomes sparse, then dense, then dense in the presence of nonlinear activations in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time.\n\n**Abstract:** We propose Joint MLP/Attention (JoMA) dynamics, a novel mathematical framework to understand the training procedure of multilayer Transformer architectures. This is achieved by integrating out the self-attention layer in Transformers, producing a modified dynamics of MLP layers only. JoMA removes unrealistic assumptions in previous analysis (e.g., lack of residual connection) and predicts that the attention first becomes sparse (to learn salient tokens), then dense (to learn less salient tokens) in the presence of nonlinear activations, while in the linear case, it is consistent with existing works that show attention becomes sparse over time. We leverage JoMA to qualitatively explains how tokens are combined to form hierarchies in multilayer Transformers, when the input tokens are generated by a latent hierarchical generative model. Experiments on models trained from real-world dataset (Wikitext2/Wikitext103) and various pre-trained models (OPT, Pythia) verify our theoretical findings. Code can be found in https://github.com/facebookresearch/luckmatters/tree/yuandong3.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 1*)\n\n#### 6. Deep Compression of Pre-trained Transformer Models\n\n*From Search Query: advanced memory compression transformer*\n\n*Naigang Wang, Chia-Chun Liu, Swagath Venkataramani, Sanchari Sen, Chia-Yu Chen, K. E. Maghraoui, Vijayalakshmi Srinivasan, Le Chang*\n\n**TL;DR:** Methods to deeply compress pre-trained transformer models across three major application domains: NLP, speech, and vision are introduced and critical initialization strategies for quantization-and sparsity-aware \ufb01ne-tuning are identified.\n\n**Abstract:** Pre-trained transformer models have achieved remarkable success in natural language processing (NLP) and have recently become competitive alternatives to Convolution Neural Networks (CNN) and Recurrent Neural Networks (RNN) in vision and speech tasks, respectively. Due to their excellent computational ef\ufb01-ciency and scalability, transformer models can be trained on exceedingly large amounts of data at the expense of tremendous growth in model size. As high performance, large-scale, and pre-trained transformer models become increasingly available for users to download and \ufb01ne-tune for customized downstream tasks, their deployment becomes challenging due to the vast amount of operations and large memory footprint. To address this challenge, we introduce methods to deeply compress pre-trained transformer models across three major application domains: NLP, speech, and vision. Speci\ufb01cally, we quantize transformer backbones down to 4-bit and further achieve 50% \ufb01ne-grained structural sparsity on pre-trained BERT, Wav2vec2.0, and Vision Transformer (ViT) models to demonstrate 16x compression while maintaining model accuracy. This is achieved by identifying critical initialization strategies for quantization-and sparsity-aware \ufb01ne-tuning as well as developing novel techniques such as quantizers with a zero-preserving format and scheduled dropout. These hardware-friendly techniques need only to be applied in the \ufb01ne-tuning phase for downstream tasks, which renders them especially suitable for acceleration and deployment of pre-trained transformer models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 0*)\n\n#### 7. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: advanced memory compression transformer*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 8. LoRAP: Transformer Sub-Layers Deserve Differentiated Structured Compression for Large Language Models\n\n*From Search Query: advanced memory compression transformer*\n\n*Guangyan Li, Yongqiang Tang, Wensheng Zhang*\n\n**TL;DR:** This study designs a mixed compression model, which organically combines Low-Rank matrix approximation And structured Pruning (LoRAP), and proposes an input activation weighted singular value decomposition method to strengthen the low-rank characteristic of MHA sub-layer.\n\n**Abstract:** Large language models (LLMs) show excellent performance in difficult tasks, but they often require massive memories and computational resources. How to reduce the parameter scale of LLMs has become research hotspots. In this study, we make an important observation that the multi-head self-attention (MHA) sub-layer of Transformer exhibits noticeable low-rank structure, while the feed-forward network (FFN) sub-layer does not. With this regard, we design a mixed compression model, which organically combines Low-Rank matrix approximation And structured Pruning (LoRAP). For the MHA sub-layer, we propose an input activation weighted singular value decomposition method to strengthen the low-rank characteristic. Furthermore, we discover that the weight matrices in MHA sub-layer have different low-rank degrees. Thus, a novel parameter allocation scheme according to the discrepancy of low-rank degrees is devised. For the FFN sub-layer, we propose a gradient-free structured channel pruning method. During the pruning, we get an interesting finding that the least important 1% of parameter actually play a vital role in model performance. Extensive evaluations on zero-shot perplexity and zero-shot task classification indicate that our proposal is superior to previous structured compression rivals under multiple compression ratios.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Linear Complexity Randomized Self-attention Mechanism\n\n*From Search Query: control variate attention mechanism*\n\n*Lingpeng Kong, Chong Wang, Lin Zheng*\n\n**Abstract:** Recently, random feature attentions (RFAs) are proposed to approximate the softmax attention in linear time and space complexity by linearizing the exponential kernel. In this paper, we first propose a novel perspective to understand the bias in such approximation by recasting RFAs as self-normalized importance samplers. This perspective further sheds light on an \\emph{unbiased} estimator for the whole softmax attention, called randomized attention (RA). RA constructs positive random features via query-specific distributions and enjoys greatly improved approximation fidelity, albeit exhibiting quadratic complexity. By combining the expressiveness in RA and the efficiency in RFA, we develop a novel linear complexity self-attention mechanism called linear randomized attention (LARA). Extensive experiments across various domains demonstrate that RA and LARA significantly improve the performance of RFAs by a substantial margin.\n\n**Published:** 2022-04-10\n\n\n\n#### 2. Efficient Attention via Control Variates\n\n*From Search Query: control variate attention mechanism*\n\n*Lingpeng Kong, Chong Wang, Jianbo Yuan, Lin Zheng*\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Published:** 2023-02-09\n\n\n\n#### 3. Sparse, Dense, and Attentional Representations for Text Retrieval\n\n*From Search Query: theoretical analysis hybrid attention*\n\n*Jacob Eisenstein, Yi Luan, Michael Collins, Kristina Toutanova*\n\n**Abstract:** Dual encoders perform retrieval by encoding documents and queries into dense lowdimensional vectors, scoring each document by its inner product with the query. We investigate the capacity of this architecture relative to sparse bag-of-words models and attentional neural networks. Using both theoretical and empirical analysis, we establish connections between the encoding dimension, the margin between gold and lower-ranked documents, and the document length, suggesting limitations in the capacity of fixed-length encodings to support precise retrieval of long documents. Building on these insights, we propose a simple neural model that combines the efficiency of dual encoders with some of the expressiveness of more costly attentional architectures, and explore sparse-dense hybrids to capitalize on the precision of sparse retrieval. These models outperform strong alternatives in large-scale retrieval.\n\n**Published:** 2020-05-01\n\n\n\n#### 4. Zoology: Measuring and Improving Recall in Efficient Language Models\n\n*From Search Query: theoretical analysis hybrid attention*\n\n*Christopher R\u00e9, Atri Rudra, James Zou, Michael Poli, Isys Johnson, Aman Timalsina, Sabri Eyuboglu, Simran Arora*\n\n**Abstract:** Attention-free language models that combine gating and convolutions are growing in popularity due to their efficiency and increasingly competitive performance. To better understand these architectures, we pretrain a suite of 17 attention and \"gated-convolution\" language models, finding that SoTA gated-convolution architectures still underperform attention by up to 2.1 perplexity points on the Pile. In fine-grained analysis, we find 82% of the gap is explained by each model's ability to recall information that is previously mentioned in-context, e.g. \"Hakuna Matata means no worries Hakuna Matata it means no\" $\\rightarrow$ \"??\". On this task, termed \"associative recall\", we find that attention outperforms gated-convolutions by a large margin: a 70M parameter attention model outperforms a 1.4 billion parameter gated-convolution model on associative recall. This is surprising because prior work shows gated convolutions can perfectly solve synthetic tests for AR capability. To close the gap between synthetics and real language, we develop a new formalization of the task called multi-query associative recall (MQAR) that better reflects actual language. We perform an empirical and theoretical study of MQAR that elucidates differences in the parameter-efficiency of attention and gated-convolution recall. Informed by our analysis, we evaluate simple convolution-attention hybrids and show that hybrids with input-dependent sparse attention patterns can close 97.4% of the gap to attention, while maintaining sub-quadratic scaling. Our code is accessible at: https://github.com/HazyResearch/zoology.\n\n**Published:** 2023-12-08\n\n\n\n#### 5. Extreme Compression of Large Language Models via Additive Quantization\n\n*From Search Query: advanced memory compression transformer*\n\n*Dan Alistarh, Artem Babenko, Elias Frantar, Denis Kuznedelev, Andrei Panferov, Vage Egiazarian*\n\n**Abstract:** The emergence of accurate open large language models (LLMs) has led to a race towards performant quantization techniques which can enable their execution on end-user devices. In this paper, we revisit the problem of \"extreme\" LLM compression-defined as targeting extremely low bit counts, such as 2 to 3 bits per parameter-from the point of view of classic methods in Multi-Codebook Quantization (MCQ). Our algorithm, called AQLM, generalizes the classic Additive Quantization (AQ) approach for information retrieval to advance the state-of-the-art in LLM compression, via two innovations: 1) learned additive quantization of weight matrices in input-adaptive fashion, and 2) joint optimization of codebook parameters across each transformer blocks. Broadly, AQLM is the first scheme that is Pareto optimal in terms of accuracy-vs-model-size when compressing to less than 3 bits per parameter, and significantly improves upon all known schemes in the extreme compression (2bit) regime. In addition, AQLM is practical: we provide fast GPU and CPU implementations of AQLM for token generation, which enable us to match or outperform optimized FP16 implementations for speed, while executing in a much smaller memory footprint.\n\n**Published:** 2024-01-11\n\n\n\n#### 6. The Efficiency Spectrum of Large Language Models: An Algorithmic Survey\n\n*From Search Query: advanced memory compression transformer*\n\n*Luming Liang, Ilya Zharkov, Zhihui Zhu, Guangzhi Wang, Jinxin Zhou, Yiqi Zhong, Jiachen Jiang, Haidong Zhu, Tianyi Chen, Tianyu Ding*\n\n**Abstract:** The rapid growth of Large Language Models (LLMs) has been a driving force in transforming various domains, reshaping the artificial general intelligence landscape. However, the increasing computational and memory demands of these models present substantial challenges, hindering both academic research and practical applications. To address these issues, a wide array of methods, including both algorithmic and hardware solutions, have been developed to enhance the efficiency of LLMs. This survey delivers a comprehensive review of algorithmic advancements aimed at improving LLM efficiency. Unlike other surveys that typically focus on specific areas such as training or model compression, this paper examines the multi-faceted dimensions of efficiency essential for the end-to-end algorithmic development of LLMs. Specifically, it covers various topics related to efficiency, including scaling laws, data utilization, architectural innovations, training and tuning strategies, and inference techniques. This paper aims to serve as a valuable resource for researchers and practitioners, laying the groundwork for future innovations in this critical research area. Our repository of relevant references is maintained at url{https://github.com/tding1/Efficient-LLM-Survey}.\n\n**Published:** 2023-12-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on control variate attention mechanisms, hybrid attention patterns, and advanced memory compression techniques, here are some key findings and references that can guide the researcher:\n\n### Control Variate Attention Mechanism\n\n1. **Variance Reduction and Stability**:\n   - The concept of control variates can be applied to reduce variance in attention mechanisms, similar to how it is used in stochastic optimization methods. For instance, the paper \"Stochastic Training of Graph Convolutional Networks\" discusses control variate based algorithms that can be adapted for attention mechanisms to improve training stability and reduce variance.\n\n2. **Theoretical Analysis**:\n   - While the provided sources do not directly discuss control variates in attention mechanisms, the principle of using control variates to reduce variance can be theoretically extended to attention computations. This involves analyzing how the variance of attention weights can be reduced using control variates, potentially leading to more stable and efficient training processes.\n\n### Hybrid Attention Mechanisms\n\n1. **Combining Sparse and Dense Attention**:\n   - The \"Reformer: The Efficient Transformer\" paper highlights the benefits of leveraging sparsity in attention layers and using product-key attention to increase the key space, which can reduce memory requirements and improve efficiency[Key References and Excerpts].\n\n2. **Theoretical Analysis and Convergence Properties**:\n   - The \"HybridBERT\" paper demonstrates the effectiveness of combining self-attention and additive attention mechanisms, which can provide theoretical guarantees and improve pretraining accuracy. This hybrid approach can be analyzed for its convergence properties, ensuring that the combined mechanisms do not compromise the model's stability or performance.\n\n3. **Neural Clustering and Hard Attention**:\n   - Neural clustering for efficient attention computation and hard attention mechanisms with theoretical guarantees are discussed in the context of improving efficiency and maintaining expressiveness. These methods can be integrated into hybrid attention architectures to enhance performance and reduce computational overhead[Key Insights and Analysis].\n\n### Advanced Memory Compression Techniques\n\n1. **Compressed Context Memory**:\n   - The \"Compressed Context Memory For Online Language Model Interaction\" paper proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space. This technique is crucial for reducing memory requirements and improving the efficiency of transformer models[Key References and Excerpts].\n\n2. **Dynamic Token Selection and Pruning**:\n   - Dynamic token selection and pruning can significantly reduce memory requirements. Techniques such as those discussed in \"Deep Compression of Pre-trained Transformer Models\" can be adapted to optimize KV cache management and reduce the memory footprint of the model.\n\n3. **Efficient KV Cache Optimization**:\n   - Efficient KV cache optimization is critical for performance. Using memristor-based in-memory computing, as described in the paper on \"Efficient memristor accelerator for transformer self-attention,\" can optimize matrix operations and reduce the computational and memory overhead associated with attention mechanisms.\n\n### Integration and Future Directions\n\n- **Probabilistic Sparse Attention**: The \"MPSA-Conformer-CTC/Attention\" model introduces a probabilistic sparse attention mechanism that reduces computational complexity and memory usage while enhancing performance. This approach can be integrated into hybrid attention architectures to further optimize memory and computational efficiency.\n\n- **Local Attention Mechanism**: The \"Local Attention Mechanism\" paper proposes an efficient attention mechanism tailored for time series analysis, which exploits local and long-term temporal dependencies. This mechanism can be adapted for autoregressive language models to minimize computational and memory overheads while preserving positional information.\n\nBy integrating these techniques\u2014control variate attention mechanisms for variance reduction, hybrid attention architectures for efficiency and expressiveness, and advanced memory compression techniques for optimizing memory usage\u2014the researcher can develop a more efficient, scalable, and high-performing autoregressive language model.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for improving the AdaptiveLinearAttention (ALA) design, particularly around variance reduction, hybrid attention mechanisms, and memory optimization:\n\n1. Variance Control and Stability:\n- Implicit positional encoding through attention variance patterns\n- Control variate techniques for variance reduction\n- Stochastic optimization methods with convergence guarantees\n\n2. Hybrid Attention Architectures:\n- Combining sparse and dense attention patterns\n- Neural clustering for efficient attention computation\n- Hard attention mechanisms with theoretical guarantees\n\n3. Memory Management:\n- Compressed context memory systems\n- Dynamic token selection and pruning\n- Efficient KV cache optimization\n\nKey References and Excerpts:\n\n1. From \"Reformer: The Efficient Transformer\":\n\"leveraging sparsity in the attention layers has proved fruitful... using product-key attention to increase the key space has also been used to reduce memory requirements\"\nThis suggests potential for combining sparse patterns with key space optimization.\n\n2. From \"Compressed Context Memory\":\n\"proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space\"\nDemonstrates effective memory compression techniques.\n\n3. From \"Stochastic Training of Graph Convolutional Networks\":\n\"develop control variate based algorithms which allow sampling an arbitrarily small neighbor size... prove new theoretical guarantee for these algorithms to converge\"\nShows potential for variance reduction through control variates.\n\n4. From \"HybridBERT\":\n\"combine self-attention and additive attention mechanisms together with sub-layer normalization... attains twice the pretraining accuracy\"\nDemonstrates benefits of hybrid attention approaches.\n\nKey Insights and Analysis:\n\n1. Variance Reduction:\n- Control variates could be integrated into attention computation\n- Stochastic optimization techniques could improve training stability\n- Implicit position encoding could reduce model complexity\n\n2. Hybrid Attention:\n- Combining multiple attention types can improve efficiency\n- Neural clustering can reduce computation while maintaining expressiveness\n- Hard attention can provide theoretical guarantees\n\n3. Memory Optimization:\n- Continuous compression of context memory is feasible\n- Dynamic token selection can reduce memory requirements\n- Efficient KV cache management is critical for performance\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific implementations of control variates in attention mechanisms\n2. Theoretical analysis of hybrid attention patterns\n3. Advanced memory compression techniques for transformers\n4. Integration methods for multiple attention mechanisms\n\nReferences:\n\n1. Kitaev, N., Kaiser, L., & Levskaya, A. (2020). Reformer: The Efficient Transformer. International Conference on Learning Representations.\n\n2. Kim, J., Yeom, J., Yun, S., & Song, H.O. (2023). Compressed Context Memory For Online Language Model Interaction. International Conference on Learning Representations.\n\n3. Chen, J., Zhu, J., & Song, L. (2017). Stochastic Training of Graph Convolutional Networks with Variance Reduction. International Conference on Machine Learning.\n\n4. Srinivasagan, G., & Ostermann, S. (2024). HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms. North American Chapter of the Association for Computational Linguistics.\n\n5. Wang, N., Liu, C., Venkataramani, S., Sen, S., Chen, C., Maghraoui, K.E., Srinivasan, V., & Chang, L. (2022). Deep Compression of Pre-trained Transformer Models. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "transformer control variate implementation, memory compression attention mechanism, theoretical convergence hybrid attention",
            "detail": "Find specific papers discussing:\n1. Implementation details of control variates in transformer attention mechanisms\n2. Memory compression techniques specifically designed for attention mechanisms\n3. Theoretical analysis of convergence properties for hybrid attention patterns\n4. Hardware-efficient implementations of compressed attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific papers discussing:\n1. Implementation details of control variates in transformer attention mechanisms\n2. Memory compression techniques specifically designed for attention mechanisms\n3. Theoretical analysis of convergence properties for hybrid attention patterns\n4. Hardware-efficient implementations of compressed attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.97)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.97)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.93)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.93)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.69)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 14/40 (Score: 0.69)*\n\n```\non pp. 1-4, 12, 16, 19, 29, 30). [13] Songlin Yang et al. \"Gated Linear Attention Transformers with Hardware-Efficient Training\". In: arXiv preprint arXiv:2312.06635 (2023) (cit.\n```\n\n#### 4. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.60)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 35/41 (Score: 0.60)*\n\n```\nIn NAACL, 2018. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and Efficient Posttraining Quantization for Large Language Models. In ICML, 2023. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A Nystr\u00f6m-based Algorithm for Approximating Self-attention. In AAAI, 2021. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated Linear Attention Transformers with Hardware-efficient Training. arXiv preprint arXiv:2312.06635, 2023. You, H., Sun, Z., Shi, H., Yu, Z., Zhao, Y., Zhang, Y., Li, C., Li, B., and Lin, Y. ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 273-286. IEEE, 2023a. You, H., Xiong, Y., Dai, X., Wu, B., Zhang, P., Fan, H., Vajda, P., and Lin, Y. C. Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference. In CVPR, 2023b. You, H., Shi, H., Guo, Y., and Lin, Y. ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. Advances in Neural Information Processing Systems, 36, 2024. Zeng, Z., Xiong, Y., Ravi, S., Acharya, S., Fung, G. M., and Singh, V. You Only Sample (almost) Once: Linear Cost Self-attention via Bernoulli Sampling. In ICML, 2021. Zhang, X., Zhao, J., and LeCun, Y. Character-level Convolutional Networks for Text Classification.\n```\n\n#### 5. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.60)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.60)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n\n\n---\n## Found 13 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: transformer control variate implementation, memory compression attention mechanism, theoretical convergence hybrid attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Recurrent Memory Transformer\n\n*From Search Query: transformer control variate implementation*\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 85  (*Influential: 10*)\n\n#### 2. SPARTA: Efficient Open-Domain Question Answering via Sparse Transformer Matching Retrieval\n\n*From Search Query: transformer control variate implementation*\n\n*Tiancheng Zhao, Xiaopeng Lu, Kyusong Lee*\n\n**TL;DR:** SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc.\n\n**Abstract:** We introduce SPARTA, a novel neural retrieval method that shows great promise in performance, generalization, and interpretability for open-domain question answering. Unlike many neural ranking methods that use dense vector nearest neighbor search, SPARTA learns a sparse representation that can be efficiently implemented as an Inverted Index. The resulting representation enables scalable neural retrieval that does not require expensive approximate vector search and leads to better performance than its dense counterpart. We validated our approaches on 4 open-domain question answering (OpenQA) tasks and 11 retrieval question answering (ReQA) tasks. SPARTA achieves new state-of-the-art results across a variety of open-domain question answering tasks in both English and Chinese datasets, including open SQuAD, CMRC and etc. Analysis also confirms that the proposed method creates human interpretable representation and allows flexible control over the trade-off between performance and efficiency.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 66  (*Influential: 10*)\n\n#### 3. Neural Rule-Execution Tracking Machine For Transformer-Based Text Generation\n\n*From Search Query: transformer control variate implementation*\n\n*Yufei Wang, Can Xu, Huang Hu, Chongyang Tao, Stephen Wan, M. Dras, Mark Johnson, Daxin Jiang*\n\n**TL;DR:** A novel module named Neural Rule-Execution Tracking Machine that can be equipped into various transformer-based generators to leverage multiple rules simultaneously to guide the neural generation model for superior generation performance in a unified and scalable way is proposed.\n\n**Abstract:** Sequence-to-Sequence (S2S) neural text generation models, especially the pre-trained ones (e.g., BART and T5), have exhibited compelling performance on various natural language generation tasks. However, the black-box nature of these models limits their application in tasks where specific rules (e.g., controllable constraints, prior knowledge) need to be executed. Previous works either design specific model structure (e.g., Copy Mechanism corresponding to the rule\"the generated output should include certain words in the source input\") or implement specialized inference algorithm (e.g., Constrained Beam Search) to execute particular rules through the text generation. These methods require careful design case-by-case and are difficult to support multiple rules concurrently. In this paper, we propose a novel module named Neural Rule-Execution Tracking Machine that can be equipped into various transformer-based generators to leverage multiple rules simultaneously to guide the neural generation model for superior generation performance in a unified and scalable way. Extensive experimental results on several benchmarks verify the effectiveness of our proposed model in both controllable and general text generation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 4. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: memory compression attention mechanism*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 5. Prototype memory and attention mechanisms for few shot image generation\n\n*From Search Query: memory compression attention mechanism*\n\n*Tianqin Li, Zijie Li, Andrew Luo, Harold Rockwell, A. Farimani, T. Lee*\n\n**TL;DR:** The results demonstrate the feasibility of the idea that these super-sparse complex feature detectors of macaque monkeys can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Abstract:** Recent discoveries indicate that the neural codes in the super\ufb01cial layers of the primary visual cortex (V1) of macaque monkeys are complex, diverse and super-sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing during the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized through a memory-based attention operation. Integrating this mechanism, we propose Memory Concept Attention ( MoCA ) to improve few shot image generation quality. We show that having a prototype memory with attention mechanisms can improve image synthesis quality, learn interpretable visual concept clusters, and improve the robustness of the model. Our results demonstrate the feasibility of the idea that these super-sparse complex feature detectors can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 6. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: memory compression attention mechanism*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 7. What Improves the Generalization of Graph Transformers? A Theoretical Dive into the Self-attention and Positional Encoding\n\n*From Search Query: theoretical convergence hybrid attention*\n\n*Hongkang Li, Meng Wang, Tengfei Ma, Sijia Liu, Zaixi Zhang, Pin-Yu Chen*\n\n**TL;DR:** It is demonstrated that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers.\n\n**Abstract:** Graph Transformers, which incorporate self-attention and positional encoding, have recently emerged as a powerful architecture for various graph learning tasks. Despite their impressive performance, the complex non-convex interactions across layers and the recursive graph structure have made it challenging to establish a theoretical foundation for learning and generalization. This study introduces the first theoretical investigation of a shallow Graph Transformer for semi-supervised node classification, comprising a self-attention layer with relative positional encoding and a two-layer perceptron. Focusing on a graph data model with discriminative nodes that determine node labels and non-discriminative nodes that are class-irrelevant, we characterize the sample complexity required to achieve a desirable generalization error by training with stochastic gradient descent (SGD). This paper provides the quantitative characterization of the sample complexity and number of iterations for convergence dependent on the fraction of discriminative nodes, the dominant patterns, and the initial model errors. Furthermore, we demonstrate that self-attention and positional encoding enhance generalization by making the attention map sparse and promoting the core neighborhood during training, which explains the superior feature representation of Graph Transformers. Our theoretical results are supported by empirical experiments on synthetic and real-world benchmarks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 8  (*Influential: 2*)\n\n#### 8. In-Context Convergence of Transformers\n\n*From Search Query: theoretical convergence hybrid attention*\n\n*Yu Huang, Yuan Cheng, Yingbin Liang*\n\n**TL;DR:** This work takes the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes.\n\n**Abstract:** Transformers have recently revolutionized many domains in modern machine learning and one salient discovery is their remarkable in-context learning capability, where models can solve an unseen task by utilizing task-specific prompts without further parameters fine-tuning. This also inspired recent theoretical studies aiming to understand the in-context learning mechanism of transformers, which however focused only on linear transformers. In this work, we take the first step toward studying the learning dynamics of a one-layer transformer with softmax attention trained via gradient descent in order to in-context learn linear function classes. We consider a structured data model, where each token is randomly sampled from a set of feature vectors in either balanced or imbalanced fashion. For data with balanced features, we establish the finite-time convergence guarantee with near-zero prediction error by navigating our analysis over two phases of the training dynamics of the attention map. More notably, for data with imbalanced features, we show that the learning dynamics take a stage-wise convergence process, where the transformer first converges to a near-zero prediction error for the query tokens of dominant features, and then converges later to a near-zero prediction error for the query tokens of under-represented features, respectively via one and four training phases. Our proof features new techniques for analyzing the competing strengths of two types of attention weights, the change of which determines different training phases.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 42  (*Influential: 11*)\n\n#### 9. Rethinking Attention with Performers\n\n*From Search Query: theoretical convergence hybrid attention*\n\n*K. Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tam\u00e1s Sarl\u00f3s, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy J. Colwell, Adrian Weller*\n\n**TL;DR:** Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness are introduced.\n\n**Abstract:** We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 1322  (*Influential: 184*)\n\n### 4 related papers from Papers with Code\n\n#### 1. End-to-end Optimized Image Compression\n\n*From Search Query: transformer control variate implementation*\n\n*Johannes Ball\u00e9, Valero Laparra, Eero P. Simoncelli*\n\n**Abstract:** We describe an image compression method, consisting of a nonlinear analysis\ntransformation, a uniform quantizer, and a nonlinear synthesis transformation.\nThe transforms are constructed in three successive stages of convolutional\nlinear filters and nonlinear activation functions. Unlike most convolutional\nneural networks, the joint nonlinearity is chosen to implement a form of local\ngain control, inspired by those used to model biological neurons. Using a\nvariant of stochastic gradient descent, we jointly optimize the entire model\nfor rate-distortion performance over a database of training images, introducing\na continuous proxy for the discontinuous loss function arising from the\nquantizer. Under certain conditions, the relaxed loss function may be\ninterpreted as the log likelihood of a generative model, as implemented by a\nvariational autoencoder. Unlike these models, however, the compression model\nmust operate at any given point along the rate-distortion curve, as specified\nby a trade-off parameter. Across an independent set of test images, we find\nthat the optimized method generally exhibits better rate-distortion performance\nthan the standard JPEG and JPEG 2000 compression methods. More importantly, we\nobserve a dramatic improvement in visual quality for all images at all bit\nrates, which is supported by objective quality estimates using MS-SSIM.\n\n**Published:** 2016-11-05\n\n\n\n#### 2. SC VALL-E: Style-Controllable Zero-Shot Text to Speech Synthesizer\n\n*From Search Query: transformer control variate implementation*\n\n*Yong-Hoon Choi, Seongho Hong, Daegyeom Kim*\n\n**Abstract:** Expressive speech synthesis models are trained by adding corpora with diverse speakers, various emotions, and different speaking styles to the dataset, in order to control various characteristics of speech and generate the desired voice. In this paper, we propose a style control (SC) VALL-E model based on the neural codec language model (called VALL-E), which follows the structure of the generative pretrained transformer 3 (GPT-3). The proposed SC VALL-E takes input from text sentences and prompt audio and is designed to generate controllable speech by not simply mimicking the characteristics of the prompt audio but by controlling the attributes to produce diverse voices. We identify tokens in the style embedding matrix of the newly designed style network that represent attributes such as emotion, speaking rate, pitch, and voice intensity, and design a model that can control these attributes. To evaluate the performance of SC VALL-E, we conduct comparative experiments with three representative expressive speech synthesis models: global style token (GST) Tacotron2, variational autoencoder (VAE) Tacotron2, and original VALL-E. We measure word error rate (WER), F0 voiced error (FVE), and F0 gross pitch error (F0GPE) as evaluation metrics to assess the accuracy of generated sentences. For comparing the quality of synthesized speech, we measure comparative mean option score (CMOS) and similarity mean option score (SMOS). To evaluate the style control ability of the generated speech, we observe the changes in F0 and mel-spectrogram by modifying the trained tokens. When using prompt audio that is not present in the training data, SC VALL-E generates a variety of expressive sounds and demonstrates competitive performance compared to the existing models. Our implementation, pretrained models, and audio samples are located on GitHub.\n\n**Published:** 2023-07-20\n\n\n\n#### 3. Compressive Transformers for Long-Range Sequence Modelling\n\n*From Search Query: memory compression attention mechanism*\n\n*Siddhant M. Jayakumar, Anna Potapenko, Jack W. Rae, Timothy P. Lillicrap*\n\n**Abstract:** We present the Compressive Transformer, an attentive sequence model which compresses past memories for long-range sequence learning. We find the Compressive Transformer obtains state-of-the-art language modelling results in the WikiText-103 and Enwik8 benchmarks, achieving 17.1 ppl and 0.97 bpc respectively. We also find it can model high-frequency speech effectively and can be used as a memory mechanism for RL, demonstrated on an object matching task. To promote the domain of long-range sequence learning, we propose a new open-vocabulary language modelling benchmark derived from books, PG-19.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-11-13\n\n\n\n#### 4. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention\n\n*From Search Query: memory compression attention mechanism*\n\n*Siddharth Gopal, Manaal Faruqui, Tsendsuren Munkhdalai*\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n**Published:** 2024-04-10\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, particularly focusing on the specified areas, here are some key findings and references that align with the given details:\n\n## Implementation Details of Control Variates in Transformer Attention Mechanisms\n\n- The paper \"Linear Complexity Randomized Self-attention\" by Kong et al. (2022) provides insights into using control variates to reduce variance in attention approximations. It discusses recasting Random Feature Attention (RFA) as self-normalized importance samplers, which can be seen as a form of control variate to stabilize attention approximations.\n- The concept of using control variates is also touched upon in the context of \"Attention with Bounded-memory Control\" (ABC) by Peng et al. (2021), which explores efficient attention variants that could be combined with linear attention for better stability.\n\n## Memory Compression Techniques for Attention Mechanisms\n\n- \"Dynamic Memory Compression\" (DMC) by Nawrot et al. (2024) is a crucial reference here. DMC proposes a method to compress memory for attention mechanisms, allowing for longer contexts and larger batches within a given memory budget. This can serve as a drop-in replacement for KV caching, significantly reducing the memory footprint.\n- Hierarchical memory structures and efficient KV cache management are also highlighted in the analysis as potential methods for memory optimization.\n\n## Theoretical Analysis of Convergence Properties for Hybrid Attention Patterns\n\n- Theoretical guarantees for the convergence of hybrid attention mechanisms can be found in papers that analyze the combination of sparse and dense attention patterns. For instance, \"SparseBERT\" by Shi et al. (2021) provides insights into the importance analysis in self-attention, which can be extended to understand the convergence properties of hybrid attention mechanisms.\n- \"JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention\" by Tian et al. (2023) offers a theoretical understanding of the dynamics of transformers, which can be applied to analyze the convergence of hybrid attention patterns.\n\n## Hardware-Efficient Implementations of Compressed Attention Mechanisms\n\n- For hardware-efficient implementations, the focus should be on optimizing the compressed attention mechanisms for inference. \"Dynamic Memory Compression\" by Nawrot et al. (2024) not only discusses the compression technique but also its potential for accelerated inference, which is crucial for hardware efficiency.\n- Additionally, papers on efficient transformer architectures, such as those discussing neural clustering and hierarchical memory structures, can provide insights into how to optimize these mechanisms for hardware efficiency.\n\n### Key References\n\n- **Peng, H., Kasai, J., Pappas, N., Yogatama, D., Wu, Z., Kong, L., Schwartz, R., & Smith, N.A. (2021). ABC: Attention with Bounded-memory Control.** Annual Meeting of the Association for Computational Linguistics.\n- **Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference.** International Conference on Machine Learning.\n- **Kong, L., Wang, C., & Zheng, L. (2022). Linear Complexity Randomized Self-attention Mechanism.** International Conference on Machine Learning.\n- **Tian, Y., Wang, Y., Zhang, Z., Chen, B., & Du, S.S. (2023). JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention.** International Conference on Learning Representations.\n\nThese references and insights should help the researchers in implementing control variates, optimizing memory, analyzing convergence properties of hybrid attention, and achieving hardware-efficient implementations for their autoregressive language model design.\n",
            "analysis": "Summary of Analysis:\n\nAfter analyzing three rounds of search results, several promising directions have emerged for improving the AdaptiveLinearAttention (ALA) design:\n\n1. Control Variate Integration:\n- Potential to reduce variance in attention approximations\n- Can be combined with linear attention for better stability\n- Theoretical foundations from stochastic optimization\n\n2. Memory Optimization:\n- Dynamic compression techniques for attention states\n- Efficient KV cache management\n- Hierarchical memory structures\n\n3. Hybrid Attention Mechanisms:\n- Combining sparse and dense patterns\n- Neural clustering for efficiency\n- Theoretical guarantees for convergence\n\nKey References and Excerpts:\n\n1. From \"ABC: Attention with Bounded-memory Control\" (Peng et al., 2021):\n\"ABC reveals new, unexplored possibilities... connects several efficient attention variants that would otherwise seem apart.\"\nShows potential for bounded memory attention with control mechanisms.\n\n2. From \"SparseBERT\" (Shi et al., 2021):\n\"diagonal elements in the attention map are the least important compared with other attention positions\"\nImportant insight for attention pattern design.\n\n3. From \"Dynamic Memory Compression\" (Nawrot et al., 2024):\n\"DMC can serve as a drop-in replacement for KV caching... to fit longer contexts and larger batches within any given memory budget.\"\nDemonstrates effective memory compression techniques.\n\n4. From \"Linear Complexity Randomized Self-attention\" (Kong et al., 2022):\n\"recasting RFAs as self-normalized importance samplers... sheds light on an unbiased estimator\"\nShows potential for variance reduction in linear attention.\n\nKey Insights and Analysis:\n\n1. Memory Management:\n- Dynamic compression can significantly reduce memory footprint\n- Hierarchical structures could balance speed and capacity\n- KV cache optimization crucial for performance\n\n2. Variance Control:\n- Control variates can stabilize attention approximations\n- Importance sampling provides theoretical framework\n- Need to balance accuracy and computational overhead\n\n3. Attention Mechanisms:\n- Hybrid patterns can combine benefits of sparse and dense attention\n- Neural clustering offers efficient computation\n- Theoretical guarantees important for stability\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific implementations of control variates in transformer architectures\n2. Integration methods for memory compression with linear attention\n3. Theoretical analysis of hybrid attention convergence\n4. Hardware optimization techniques for compressed attention\n\nReferences:\n\n1. Peng, H., Kasai, J., Pappas, N., Yogatama, D., Wu, Z., Kong, L., Schwartz, R., & Smith, N.A. (2021). ABC: Attention with Bounded-memory Control. Annual Meeting of the Association for Computational Linguistics.\n\n2. Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., & Kwok, J. (2021). SparseBERT: Rethinking the Importance Analysis in Self-attention. International Conference on Machine Learning.\n\n3. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n4. Kong, L., Wang, C., & Zheng, L. (2022). Linear Complexity Randomized Self-attention Mechanism. International Conference on Machine Learning.\n\n5. Li, H., Wang, M., Liu, S., & Chen, P.Y. (2023). A Theoretical Understanding of shallow Vision Transformers: Learning, Generalization, and Sample Complexity. International Conference on Learning Representations.\n\n6. Tian, Y., Wang, Y., Zhang, Z., Chen, B., & Du, S.S. (2023). JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "\"variance reduction attention mechanisms\", \"memory computation trade-offs\", \"attention stability analysis\"",
            "detail": "Search for specific implementations and analyses of:\n1. Variance reduction techniques in attention mechanisms\n2. Memory-computation trade-offs in neural networks\n3. Stability analysis methods for attention mechanisms\n4. Integration strategies for combining different optimization techniques\n5. Quantitative evaluations of attention mechanism performance",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific implementations and analyses of:\n1. Variance reduction techniques in attention mechanisms\n2. Memory-computation trade-offs in neural networks\n3. Stability analysis methods for attention mechanisms\n4. Integration strategies for combining different optimization techniques\n5. Quantitative evaluations of attention mechanism performance\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.55)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 10/40 (Score: 0.59)*\n\n```\nWe refer the readers to Appendix C. 6 for further details regarding the task and models. ### 4.7 LANGUAGE MODELLING\n\nWe perform experiments on the WikiText-103 data corpus (Merity et al., 2016) for the language modeling task. Here, the task is to predict probabilities for next or masked words, evaluated through perplexity. Quantitative Results: We use 6-layered transformer models with parameter. We plot the validation perplexity against epochs in Figure 7 which highlights that our proposed attention mechanism not only outperforms the baseline but also converges faster. Further, we see that our proposed model obtains test perplexity $38.8_{ \\pm 0.0}$ as opposed to baseline's perplexity $39.6_{ \\pm 0.3}$. We refer the reader to Appendix C. 7 for further details. ## 5 DISCUSSION AND CONCLUSION\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5de3d65737610c02fb8cg-09.jpg?height=429&width=549&top_left_y=1146&top_left_x=1189)\n\nFigure 7: Performance on Language Modeling (WikiText103). We illustrate that our proposed mechanism outperforms the standard multi-head attention. Summary. In this work, we revisit Multi-Head Attention, a popular attention mechanism, and highlight its shortcomings due to the rigid association between search and retrieval mechanisms. We argue that this rigid coupling hinders re-usability of parameters and reduces the expressivity of the model. To mitigate this, we propose a novel mechanism which uses a value retrieval mechanism to flexibly compose searches and retrievals. Experiments on various tasks show that our proposed method outperforms standard multi-head transformers, while often using only a fraction of retrievals. Complexity. While our proposed mechanism requires additional parameters for the computation of value scores, we highlight that this increase is often minuscule compared to the total number of parameters. Crucially, we note that this light increase in parameters per search mechanism is easily offset by reducing the number of retrievals needed. For all our experiments, our proposed models offer similar capacity as the baselines unless stated otherwise. This highlights that the improved performance is due to flexible composition of search and retrieval and not number of parameters. We discuss computational complexity in detail in Appendix B.5. Limitations and Conclusion. Motivated by the need for efficient factorization of knowledge and dynamic reusability of learned pieces of computations, we propose Compositional Attention, a first step towards flexible composition of search and retrieval.\n```\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.51)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 2. Repeat After Me: Transformers are Better than State Space Models at Copying (Avg. Score: 0.02)\n\n*Samy Jelassi, David Brandfonbrener, S. Kakade, Eran Malach*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 4*)\n\n**TL;DR:** It is proved that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state, and a fundamental gap between transformers and GSSMs on tasks of practical interest is suggested.\n\n**Abstract:** Transformers are the dominant architecture for sequence modeling, but there is growing interest in models that use a fixed-size latent state that does not depend on the sequence length, which we refer to as\"generalized state space models\"(GSSMs). In this paper we show that while GSSMs are promising in terms of inference-time efficiency, they are limited compared to transformer models on tasks that require copying from the input context. We start with a theoretical analysis of the simple task of string copying and prove that a two layer transformer can copy strings of exponential length while GSSMs are fundamentally limited by their fixed-size latent state. Empirically, we find that transformers outperform GSSMs in terms of efficiency and generalization on synthetic tasks that require copying the context. Finally, we evaluate pretrained large language models and find that transformer models dramatically outperform state space models at copying and retrieving information from context. Taken together, these results suggest a fundamental gap between transformers and GSSMs on tasks of practical interest.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.02)*\n\n```\narXiv preprint arXiv:2301.10743, 2023. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Dao, T., Fu, D., Ermon, S., Rudra, A., and R\u00e9, C. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Del\u00e9tang, G., Ruoss, A., Grau-Moya, J., Genewein, T., Wenliang, L. K., Catt, E., Hutter, M., Legg, S., and Ortega, P. A. Neural networks and the chomsky hierarchy. arXiv preprint arXiv:2207.02098, 2022. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Edelman, B. L., Goel, S., Kakade, S., and Zhang, C. Inductive biases and variable creation in self-attention mechanisms. In International Conference on Machine Learning, pp. 5793-5831. PMLR, 2022. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N.,\net al. The pile: An 800 gb dataset of diverse text for language modeling.\n```\n\n#### 3. Attention as an RNN (Avg. Score: 0.02)\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Mohamed Osama Ahmed, Y. Bengio, Greg Mori*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Aaren is introduced, an attention-based module that can not only be trained in parallel but also be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs).\n\n**Abstract:** The advent of Transformers marked a significant breakthrough in sequence modelling, providing a highly performant architecture capable of leveraging GPU parallelism. However, Transformers are computationally expensive at inference time, limiting their applications, particularly in low-resource settings (e.g., mobile and embedded devices). Addressing this, we (1) begin by showing that attention can be viewed as a special Recurrent Neural Network (RNN) with the ability to compute its \\textit{many-to-one} RNN output efficiently. We then (2) show that popular attention-based models such as Transformers can be viewed as RNN variants. However, unlike traditional RNNs (e.g., LSTMs), these models cannot be updated efficiently with new tokens, an important property in sequence modelling. Tackling this, we (3) introduce a new efficient method of computing attention's \\textit{many-to-many} RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we (4) introduce \\textbf{Aaren}, an attention-based module that can not only (i) be trained in parallel (like Transformers) but also (ii) be updated efficiently with new tokens, requiring only constant memory for inferences (like traditional RNNs). Empirically, we show Aarens achieve comparable performance to Transformers on $38$ datasets spread across four popular sequential problem settings: reinforcement learning, event forecasting, time series classification, and time series forecasting tasks while being more time and memory-efficient.\n\n##### *Relevant Chunk: No. 15/34 (Score: 0.02)*\n\n```\n(Left) Memory Usage Comparison. (Right) Cumulative Time Comparison. only a vector. Measuring this empirically in comparable models, we found that Transformers used $3,152,384$ parameters. In contrast, the equivalent Aarens used 3, 152, 896 parameters, representing only a marginal $\\sim 0.016 \\%$ parameter increase - a minor trade-off for the significant gains in memory and time complexities. ## 5 Related Work\n\nClosest to Aaren are approximations of attention such as those by RWKV (Peng et al., 2023), RetNet (Sun et al., 2023), and Linear Transformer (Katharopoulos et al., 2020). These models proposed linearizations of the standard softmax-based attention that allow them to be formulated as an RNN. However, in doing so, these models also encode an exponential factor that biases tokens based on their timestamp, limiting their potential applications. In contrast, Aaren leverages an exact re-formulation of softmax attention as an RNN, allowing the model itself to compute the weight of each token. Feng et al. (2023) showed attention can be computed recurrently, using it to compress set-based inputs. Rabe and Staats (2022) introduced a recurrent formulation of attention, showing that self-attention can be computed efficiently. Katharopoulos et al. (2020) showed that Transformers with a causal mask can be viewed as an RNN. In contrast, we (1) show a more general result whereas any attention model can be viewed as an RNN. Furthermore, we (2) introduce Aaren, a new attention formulation based on parallel prefix sums, that achieves competitive results with that of Transformers while being more efficient. The problem of computing prefix scans/sums has been well studied with various efficient parallelized algorithms proposed for computing them. Since Aaren only requires the output of the prefix scan, any efficient algorithm for computing it can be used. In this work, we outlined the method by Hillis and Steele (1986). This method is time efficient for parallel computation, requiring $\\log _{2}(N)$ sequential steps and $\\mathcal{O}(N \\log (N))$ overall computation. In contrast, the method by Ladner and Fischer (1980) use mores sequential steps (specifically, $2 \\log _{2}(N)-2$ ) but only performs $\\mathcal{O}(N)$ overall computation. For a more in-depth introduction to parallel prefix sums algorithms, we refer the reader to the following work by Blelloch (1990). In this work, we applied Transformers to a subset of applications. For a broad overview of the applications of Transformers, we refer the reader to the following survey by Islam et al. (2023). For an overview of different transformer models applied to the specific settings considered in this paper, we refer the reader to the following surveys (1) on transformers in reinforcement learning by Li et al. (2023) and (2) on transformers in event forecasting, time series forecasting, time series classification, and more by Wen et al. (2022). ## 6 Conclusion\n\nIn this work, we showed that attention can be formulated as an RNN whereas the conventional way of computing attention is a parallelized method of computing its many-to-one RNN output. Building on the RNN formulation, we showed that existing attention-based models can be formulated as RNNs. However, unlike traditional RNNs such as LSTMs and GRUs, these methods cannot be updated efficiently with new tokens. Addressing this, we introduced a new parallelized method of computing attention's many-to-many RNN output based on the parallel prefix scan algorithm. Building on the new attention formulation, we introduced Aaren, a new module that can not only (i) be trained in parallel (like Transformers) but also (ii) be efficiently updated at inference time, thereby requiring only constant memory (like RNNs). Empirically, we showed that Aarens achieve performance competitive with Transformers on 38 datasets spread across four sequential data settings: reinforcement learning, event forecasting, time series classification, and time series forecasting. Finally, we empirically show that Aarens are significantly more time and memory-efficient than Transformers. ## References\n\nAgarwal, P., Rahman, A. A., St-Charles, P.-L., Prince, S. J., and Kahou, S. E. (2023). Transformers in reinforcement learning: A survey.\n```\n\n#### 4. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.01)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.01)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"variance reduction attention mechanisms\", \"memory computation trade-offs\", \"attention stability analysis\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. EF-BV: A Unified Theory of Error Feedback and Variance Reduction Mechanisms for Biased and Unbiased Compression in Distributed Optimization\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*Laurent Condat, Kai Yi, Peter Richt'arik*\n\n**TL;DR:** This paper unifies two classes of compression operators into a single framework and proposes a new algorithm, recovering DIANA and EF21 as particular cases, and proves its linear convergence under certain conditions.\n\n**Abstract:** In distributed or federated optimization and learning, communication between the different computing units is often the bottleneck and gradient compression is widely used to reduce the number of bits sent within each communication round of iterative methods. There are two classes of compression operators and separate algorithms making use of them. In the case of unbiased random compressors with bounded variance (e.g., rand-k), the DIANA algorithm of Mishchenko et al. (2019), which implements a variance reduction technique for handling the variance introduced by compression, is the current state of the art. In the case of biased and contractive compressors (e.g., top-k), the EF21 algorithm of Richt\\'arik et al. (2021), which instead implements an error-feedback mechanism, is the current state of the art. These two classes of compression schemes and algorithms are distinct, with different analyses and proof techniques. In this paper, we unify them into a single framework and propose a new algorithm, recovering DIANA and EF21 as particular cases. Our general approach works with a new, larger class of compressors, which has two parameters, the bias and the variance, and includes unbiased and biased compressors as particular cases. This allows us to inherit the best of the two worlds: like EF21 and unlike DIANA, biased compressors, like top-k, whose good performance in practice is recognized, can be used. And like DIANA and unlike EF21, independent randomness at the compressors allows to mitigate the effects of compression, with the convergence rate improving when the number of parallel workers is large. This is the first time that an algorithm with all these features is proposed. We prove its linear convergence under certain conditions. Our approach takes a step towards better understanding of two so-far distinct worlds of communication-efficient distributed learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 17  (*Influential: 2*)\n\n#### 2. Variance Reduction is an Antidote to Byzantines: Better Rates, Weaker Assumptions and Communication Compression as a Cherry on the Top\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*Eduard A. Gorbunov, Samuel Horv\u00e1th, Peter Richt'arik, Gauthier Gidel*\n\n**TL;DR:** Theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-\u0141ojasiewicz loss functions are derived and theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-\u0141ojasiewicz loss functions are derived.\n\n**Abstract:** Byzantine-robustness has been gaining a lot of attention due to the growth of the interest in collaborative and federated learning. However, many fruitful directions, such as the usage of variance reduction for achieving robustness and communication compression for reducing communication costs, remain weakly explored in the \ufb01eld. This work addresses this gap and proposes Byz-VR-MARINA \u2013a new Byzantine-tolerant method with variance reduction and compression. A key mes-sage of our paper is that variance reduction is key to \ufb01ghting Byzantine workers more effectively. At the same time, communication compression is a bonus that makes the process more communication ef\ufb01cient. We derive theoretical convergence guarantees for Byz-VR-MARINA outperforming previous state-of-the-art for general non-convex and Polyak-\u0141ojasiewicz loss functions. Unlike the con-current Byzantine-robust methods with variance reduction and/or compression, our complexity results are tight and do not rely on restrictive assumptions such as boundedness of the gradients or limited compression. Moreover, we provide the \ufb01rst analysis of a Byzantine-tolerant method supporting non-uniform sampling of stochastic gradients. Numerical experiments corroborate our theoretical \ufb01ndings.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 35  (*Influential: 2*)\n\n#### 3. Improved Convergence Rate of Stochastic Gradient Langevin Dynamics with Variance Reduction and its Application to Optimization\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*Yuri Kinoshita, Taiji Suzuki*\n\n**TL;DR:** Two variants of the Stochastic Variance Reduced Gradient Langevin Dynamics are studied and it is proved their convergence to the objective distribution in terms of KL-divergence under the sole assumptions of smoothness and Log-Sobolev inequality.\n\n**Abstract:** The stochastic gradient Langevin Dynamics is one of the most fundamental algorithms to solve sampling problems and non-convex optimization appearing in several machine learning applications. Especially, its variance reduced versions have nowadays gained particular attention. In this paper, we study two variants of this kind, namely, the Stochastic Variance Reduced Gradient Langevin Dynamics and the Stochastic Recursive Gradient Langevin Dynamics. We prove their convergence to the objective distribution in terms of KL-divergence under the sole assumptions of smoothness and Log-Sobolev inequality which are weaker conditions than those used in prior works for these algorithms. With the batch size and the inner loop length set to $\\sqrt{n}$, the gradient complexity to achieve an $\\epsilon$-precision is $\\tilde{O}((n+dn^{1/2}\\epsilon^{-1})\\gamma^2 L^2\\alpha^{-2})$, which is an improvement from any previous analyses. We also show some essential applications of our result to non-convex optimization.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 13  (*Influential: 2*)\n\n#### 4. ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Iman Mirzadeh, Keivan Alizadeh-Vahid, Sachin Mehta, C. C. D. Mundo, Oncel Tuzel, Golnoosh Samei, Mohammad Rastegari, Mehrdad Farajtabar*\n\n**TL;DR:** It is demonstrated that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer during inference, which is particularly valuable during the memory-bound inference step, where efficiency is paramount.\n\n**Abstract:** Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 36  (*Influential: 5*)\n\n#### 5. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 6. A Graph is Worth 1-bit Spikes: When Graph Contrastive Learning Meets Spiking Neural Networks\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Jintang Li, Huizhe Zhang, Ruofan Wu, Zulun Zhu, Liang Chen, Zibin Zheng, Baokun Wang, Changhua Meng*\n\n**TL;DR:** This work proposes SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance, and provides theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts.\n\n**Abstract:** While contrastive self-supervised learning has become the de-facto learning paradigm for graph neural networks, the pursuit of higher task accuracy requires a larger hidden dimensionality to learn informative and discriminative full-precision representations, raising concerns about computation, memory footprint, and energy consumption burden (largely overlooked) for real-world applications. This work explores a promising direction for graph contrastive learning (GCL) with spiking neural networks (SNNs), which leverage sparse and binary characteristics to learn more biologically plausible and compact representations. We propose SpikeGCL, a novel GCL framework to learn binarized 1-bit representations for graphs, making balanced trade-offs between efficiency and performance. We provide theoretical guarantees to demonstrate that SpikeGCL has comparable expressiveness with its full-precision counterparts. Experimental results demonstrate that, with nearly 32x representation storage compression, SpikeGCL is either comparable to or outperforms many fancy state-of-the-art supervised and self-supervised methods across several graph benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 7. Transformers as Algorithms: Generalization and Stability in In-context Learning\n\n*From Search Query: \"attention stability analysis\"*\n\n*Yingcong Li, M. E. Ildiz, Dimitris Papailiopoulos, Samet Oymak*\n\n**TL;DR:** This work formalizes in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time and identifies an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner.\n\n**Abstract:** In-context learning (ICL) is a type of prompting where a transformer model operates on a sequence of (input, output) examples and performs inference on-the-fly. In this work, we formalize in-context learning as an algorithm learning problem where a transformer model implicitly constructs a hypothesis function at inference-time. We first explore the statistical aspects of this abstraction through the lens of multitask learning: We obtain generalization bounds for ICL when the input prompt is (1) a sequence of i.i.d. (input, label) pairs or (2) a trajectory arising from a dynamical system. The crux of our analysis is relating the excess risk to the stability of the algorithm implemented by the transformer. We characterize when transformer/attention architecture provably obeys the stability condition and also provide empirical verification. For generalization on unseen tasks, we identify an inductive bias phenomenon in which the transfer learning risk is governed by the task complexity and the number of MTL tasks in a highly predictable manner. Finally, we provide numerical evaluations that (1) demonstrate transformers can indeed implement near-optimal algorithms on classical regression problems with i.i.d. and dynamic data, (2) provide insights on stability, and (3) verify our theoretical predictions.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 121  (*Influential: 6*)\n\n#### 8. Transformers Learn Nonlinear Features In Context: Nonconvex Mean-field Dynamics on the Attention Landscape\n\n*From Search Query: \"attention stability analysis\"*\n\n*Juno Kim, Taiji Suzuki*\n\n**TL;DR:** This paper studies the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer, and proves in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign.\n\n**Abstract:** Large language models based on the Transformer architecture have demonstrated impressive capabilities to learn in context. However, existing theoretical studies on how this phenomenon arises are limited to the dynamics of a single layer of attention trained on linear regression tasks. In this paper, we study the optimization of a Transformer consisting of a fully connected layer followed by a linear attention layer. The MLP acts as a common nonlinear representation or feature map, greatly enhancing the power of in-context learning. We prove in the mean-field and two-timescale limit that the infinite-dimensional loss landscape for the distribution of parameters, while highly nonconvex, becomes quite benign. We also analyze the second-order stability of mean-field dynamics and show that Wasserstein gradient flow almost always avoids saddle points. Furthermore, we establish novel methods for obtaining concrete improvement rates both away from and near critical points. This represents the first saddle point analysis of mean-field dynamics in general and the techniques are of independent interest.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 11  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Differential Transformer\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*Tianzhu Ye, Li Dong, Yuqing Xia, Yutao Sun, Yi Zhu, Gao Huang, Furu Wei*\n\n**Abstract:** Transformer tends to overallocate attention to irrelevant context. In this\nwork, we introduce Diff Transformer, which amplifies attention to the relevant\ncontext while canceling noise. Specifically, the differential attention\nmechanism calculates attention scores as the difference between two separate\nsoftmax attention maps. The subtraction cancels noise, promoting the emergence\nof sparse attention patterns. Experimental results on language modeling show\nthat Diff Transformer outperforms Transformer in various settings of scaling up\nmodel size and training tokens. More intriguingly, it offers notable advantages\nin practical applications, such as long-context modeling, key information\nretrieval, hallucination mitigation, in-context learning, and reduction of\nactivation outliers. By being less distracted by irrelevant context, Diff\nTransformer can mitigate hallucination in question answering and text\nsummarization. For in-context learning, Diff Transformer not only enhances\naccuracy but is also more robust to order permutation, which was considered as\na chronic robustness issue. The results position Diff Transformer as a highly\neffective and promising architecture to advance large language models.\n\n**Published:** 2024-10-07T17:57:38Z  (*Updated: 2024-10-07T17:57:38Z*)\n\n\n\n#### 2. RetrievalAttention: Accelerating Long-Context LLM Inference via Vector\n  Retrieval\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*Di Liu, Meng Chen, Baotong Lu, Huiqiang Jiang, Zhenhua Han, Qianxi Zhang, Qi Chen, Chengruidong Zhang, Bailu Ding, Kai Zhang, Chen Chen, Fan Yang, Yuqing Yang, Lili Qiu*\n\n**Abstract:** Transformer-based Large Language Models (LLMs) have become increasingly\nimportant. However, due to the quadratic time complexity of attention\ncomputation, scaling LLMs to longer contexts incurs extremely slow inference\nlatency and high GPU memory consumption for caching key-value (KV) vectors.\nThis paper proposes RetrievalAttention, a training-free approach to both\naccelerate attention computation and reduce GPU memory consumption. By\nleveraging the dynamic sparsity of attention mechanism, RetrievalAttention\nproposes to use approximate nearest neighbor search (ANNS) indexes for KV\nvectors in CPU memory and retrieves the most relevant ones with vector search\nduring generation. Unfortunately, we observe that the off-the-shelf ANNS\nindexes are often ineffective for such retrieval tasks due to the\nout-of-distribution (OOD) between query vectors and key vectors in attention\nmechanism. RetrievalAttention addresses the OOD challenge by designing an\nattention-aware vector search algorithm that can adapt to the distribution of\nquery vectors. Our evaluation shows that RetrievalAttention only needs to\naccess 1--3% of data while maintaining high model accuracy. This leads to\nsignificant reduction in the inference cost of long-context LLMs with much\nlower GPU memory footprint. In particular, RetrievalAttention only needs a\nsingle NVIDIA RTX4090 (24GB) for serving 128K tokens in LLMs with 8B\nparameters, which is capable of generating one token in 0.188 seconds.\n\n**Published:** 2024-09-16T17:59:52Z  (*Updated: 2024-09-18T13:11:13Z*)\n\n\n\n#### 3. Training Language Models with Memory Augmentation\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Zexuan Zhong, Tao Lei, Danqi Chen*\n\n**Abstract:** Recent work has improved language models (LMs) remarkably by equipping them\nwith a non-parametric memory component. However, most existing approaches only\nintroduce mem-ories at testing time or represent them using a separately\ntrained encoder, resulting in suboptimal training of the language model. In\nthis work, we present TRIME, a novel yet simple training approach designed for\ntraining LMs with memory augmentation. Our approach uses a training objective\nthat directly takes in-batch examples as accessible memory. We also present new\nmethods for memory construction and data batching, which are used for adapting\nto different sets of memories--local, long-term, and external memory--at\ntesting time. We evaluate TRIME on multiple language modeling and machine\ntranslation benchmarks and show that it is able to achieve significant\nimprovements across all the settings. Concretely, TRIME reduces the perplexity\nfrom 18.70 to 15.37 on WIKITEXT-103, by effectively leveraging a large memory\nset from the training corpus. Compared to standard LM training, TRIME adds\nnegligible computational overhead and is compatible with different neural\narchitectures, making it a versatile solution for training memory-augmented\nLMs.\n\n**Published:** 2022-05-25T11:37:29Z  (*Updated: 2022-11-29T16:30:09Z*)\n\n\n\n#### 4. Implicit Memory Transformer for Computationally Efficient Simultaneous\n  Speech Translation\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Matthew Raffel, Lizhong Chen*\n\n**Abstract:** Simultaneous speech translation is an essential communication task difficult\nfor humans whereby a translation is generated concurrently with oncoming speech\ninputs. For such a streaming task, transformers using block processing to break\nan input sequence into segments have achieved state-of-the-art performance at a\nreduced cost. Current methods to allow information to propagate across\nsegments, including left context and memory banks, have faltered as they are\nboth insufficient representations and unnecessarily expensive to compute. In\nthis paper, we propose an Implicit Memory Transformer that implicitly retains\nmemory through a new left context method, removing the need to explicitly\nrepresent memory with memory banks. We generate the left context from the\nattention output of the previous segment and include it in the keys and values\nof the current segment's attention calculation. Experiments on the MuST-C\ndataset show that the Implicit Memory Transformer provides a substantial\nspeedup on the encoder forward pass with nearly identical translation quality\nwhen compared with the state-of-the-art approach that employs both left context\nand memory banks.\n\n**Published:** 2023-07-03T22:20:21Z  (*Updated: 2023-07-03T22:20:21Z*)\n\n\n\n#### 5. Sparse Attention with Linear Units\n\n*From Search Query: \"attention stability analysis\"*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more\ninterpretable by replacing the softmax function in the attention with its\nsparse variants. In this work, we introduce a novel, simple method for\nachieving sparsity in attention: we replace the softmax activation with a ReLU,\nand show that sparsity naturally emerges from such a formulation. Training\nstability is achieved with layer normalization with either a specialized\ninitialization or an additional gating function. Our model, which we call\nRectified Linear Attention (ReLA), is easy to implement and more efficient than\npreviously proposed sparse attention mechanisms. We apply ReLA to the\nTransformer and conduct experiments on five machine translation tasks. ReLA\nachieves translation performance comparable to several strong baselines, with\ntraining and decoding speed similar to that of the vanilla attention. Our\nanalysis shows that ReLA delivers high sparsity rate and head diversity, and\nthe induced cross attention achieves better accuracy with respect to\nsource-target word alignment than recent sparsified softmax-based models.\nIntriguingly, ReLA heads also learn to attend to nothing (i.e. 'switch off')\nfor some queries, which is not possible with sparsified softmax alternatives.\n\n**Published:** 2021-04-14T17:52:38Z  (*Updated: 2021-10-06T14:04:59Z*)\n\n\n\n#### 6. DeepNet: Scaling Transformers to 1,000 Layers\n\n*From Search Query: \"attention stability analysis\"*\n\n*Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, Furu Wei*\n\n**Abstract:** In this paper, we propose a simple yet effective method to stabilize\nextremely deep Transformers. Specifically, we introduce a new normalization\nfunction (DeepNorm) to modify the residual connection in Transformer,\naccompanying with theoretically derived initialization. In-depth theoretical\nanalysis shows that model updates can be bounded in a stable way. The proposed\nmethod combines the best of two worlds, i.e., good performance of Post-LN and\nstable training of Pre-LN, making DeepNorm a preferred alternative. We\nsuccessfully scale Transformers up to 1,000 layers (i.e., 2,500 attention and\nfeed-forward network sublayers) without difficulty, which is one order of\nmagnitude deeper than previous deep Transformers. Remarkably, on a multilingual\nbenchmark with 7,482 translation directions, our 200-layer model with 3.2B\nparameters significantly outperforms the 48-layer state-of-the-art model with\n12B parameters by 5 BLEU points, which indicates a promising scaling direction.\n\n**Published:** 2022-03-01T15:36:38Z  (*Updated: 2022-03-01T15:36:38Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. ViTAEv2: Vision Transformer Advanced by Exploring Inductive Bias for Image Recognition and Beyond\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*DaCheng Tao, Jing Zhang, Yufei Xu, Qiming Zhang*\n\n**Abstract:** Vision transformers have shown great potential in various computer vision tasks owing to their strong capability to model long-range dependency using the self-attention mechanism. Nevertheless, they treat an image as a 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance, which is instead learned implicitly from large-scale training data with longer training schedules. In this paper, we propose a Vision Transformer Advanced by Exploring intrinsic IB from convolutions, i.e., ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and can learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. The proposed two kinds of cells are stacked in both isotropic and multi-stage manners to formulate two families of ViTAE models, i.e., the vanilla ViTAE and ViTAEv2. Experiments on the ImageNet dataset as well as downstream tasks on the MS COCO, ADE20K, and AP10K datasets validate the superiority of our models over the baseline transformer models and concurrent works. Besides, we scale up our ViTAE model to 644M parameters and obtain the state-of-the-art classification performance, i.e., 88.5% Top-1 classification accuracy on ImageNet validation set and the best 91.2% Top-1 accuracy on ImageNet real validation set, without using extra private data.\n\n**Published:** 2022-02-21\n\n\n\n#### 2. ViTAE: Vision Transformer Advanced by Exploring Intrinsic Inductive Bias\n\n*From Search Query: \"variance reduction attention mechanisms\"*\n\n*DaCheng Tao, Jing Zhang, Qiming Zhang, Yufei Xu*\n\n**Abstract:** Transformers have shown great potential in various computer vision tasks owing to their strong capability in modeling long-range dependency using the self-attention mechanism. Nevertheless, vision transformers treat an image as 1D sequence of visual tokens, lacking an intrinsic inductive bias (IB) in modeling local visual structures and dealing with scale variance. Alternatively, they require large-scale training data and longer training schedules to learn the IB implicitly. In this paper, we propose a novel Vision Transformer Advanced by Exploring intrinsic IB from convolutions, ie, ViTAE. Technically, ViTAE has several spatial pyramid reduction modules to downsample and embed the input image into tokens with rich multi-scale context by using multiple convolutions with different dilation rates. In this way, it acquires an intrinsic scale invariance IB and is able to learn robust feature representation for objects at various scales. Moreover, in each transformer layer, ViTAE has a convolution block in parallel to the multi-head self-attention module, whose features are fused and fed into the feed-forward network. Consequently, it has the intrinsic locality IB and is able to learn local features and global dependencies collaboratively. Experiments on ImageNet as well as downstream tasks prove the superiority of ViTAE over the baseline transformer and concurrent works. Source code and pretrained models will be available at GitHub.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-07\n\n\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Christopher R\u00e9, Atri Rudra, Stefano Ermon, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n**Published:** 2022-05-27\n\n\n\n#### 4. EfficientViT: Memory Efficient Vision Transformer with Cascaded Group Attention\n\n*From Search Query: \"memory computation trade-offs\"*\n\n*Yixuan Yuan, Han Hu, Yuqing Yang, Ningxin Zheng, Houwen Peng, Xinyu Liu*\n\n**Abstract:** Vision transformers have shown great success due to their high model capabilities. However, their remarkable performance is accompanied by heavy computation costs, which makes them unsuitable for real-time applications. In this paper, we propose a family of high-speed vision transformers named EfficientViT. We find that the speed of existing transformer models is commonly bounded by memory inefficient operations, especially the tensor reshaping and element-wise functions in MHSA. Therefore, we design a new building block with a sandwich layout, i.e., using a single memory-bound MHSA between efficient FFN layers, which improves memory efficiency while enhancing channel communication. Moreover, we discover that the attention maps share high similarities across heads, leading to computational redundancy. To address this, we present a cascaded group attention module feeding attention heads with different splits of the full feature, which not only saves computation cost but also improves attention diversity. Comprehensive experiments demonstrate EfficientViT outperforms existing efficient models, striking a good trade-off between speed and accuracy. For instance, our EfficientViT-M5 surpasses MobileNetV3-Large by 1.9% in accuracy, while getting 40.4% and 45.2% higher throughput on Nvidia V100 GPU and Intel Xeon CPU, respectively. Compared to the recent efficient model MobileViT-XXS, EfficientViT-M2 achieves 1.8% superior accuracy, while running 5.8x/3.7x faster on the GPU/CPU, and 7.4x faster when converted to ONNX format. Code and models are available at https://github.com/microsoft/Cream/tree/main/EfficientViT.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-05-11\n\n\n\n#### 5. Lumina-T2X: Transforming Text into Any Modality, Resolution, and Duration via Flow-based Large Diffusion Transformers\n\n*From Search Query: \"attention stability analysis\"*\n\n*Rongjie Huang, Longtian Qiu, Dongyang Liu, Hongsheng Li, Yu Qiao, Jingwen He, He Tong, Weicai Ye, Tianshuo Yang, Zhengkai Jiang, Wenqi Shao, Junlin Xi, Renrui Zhang, Shijie Geng, Chen Lin, Yuhang Zhang, Xu Luo, Ruoyi Du, Le Zhuo, Peng Gao*\n\n**Abstract:** Sora unveils the potential of scaling Diffusion Transformer for generating photorealistic images and videos at arbitrary resolutions, aspect ratios, and durations, yet it still lacks sufficient implementation details. In this technical report, we introduce the Lumina-T2X family - a series of Flow-based Large Diffusion Transformers (Flag-DiT) equipped with zero-initialized attention, as a unified framework designed to transform noise into images, videos, multi-view 3D objects, and audio clips conditioned on text instructions. By tokenizing the latent spatial-temporal space and incorporating learnable placeholders such as [nextline] and [nextframe] tokens, Lumina-T2X seamlessly unifies the representations of different modalities across various spatial-temporal resolutions. This unified approach enables training within a single framework for different modalities and allows for flexible generation of multimodal data at any resolution, aspect ratio, and length during inference. Advanced techniques like RoPE, RMSNorm, and flow matching enhance the stability, flexibility, and scalability of Flag-DiT, enabling models of Lumina-T2X to scale up to 7 billion parameters and extend the context window to 128K tokens. This is particularly beneficial for creating ultra-high-definition images with our Lumina-T2I model and long 720p videos with our Lumina-T2V model. Remarkably, Lumina-T2I, powered by a 5-billion-parameter Flag-DiT, requires only 35% of the training computational costs of a 600-million-parameter naive DiT. Our further comprehensive analysis underscores Lumina-T2X's preliminary capability in resolution extrapolation, high-resolution editing, generating consistent 3D views, and synthesizing videos with seamless transitions. We expect that the open-sourcing of Lumina-T2X will further foster creativity, transparency, and diversity in the generative AI community.\n\n**Published:** 2024-05-09\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on variance reduction in attention mechanisms, memory-computation trade-offs, stability analysis, and integration strategies, here are some key findings and suggestions based on the provided sources and additional insights:\n\n## Variance Reduction Techniques in Attention Mechanisms\n\n- The paper \"From Attention to Activation: Unravelling the Enigmas of Large Language Models\" introduces a novel optimizer called OrthoAdam, which utilizes orthogonal matrices to transform gradients. This approach helps reduce the kurtosis of hidden states in Transformer models, effectively mitigating large outlier activations and improving stability.\n- The concept of control variates, as discussed in \"Efficient Attention via Control Variates,\" can be applied to reduce variance in attention mechanisms. This involves decomposing the attention calculation into a sum of multiple control variate estimators, which can help balance variance reduction and computational overhead [Analysis Note].\n\n## Memory-Computation Trade-offs in Neural Networks\n\n- The \"Lightning Attention-2\" and \"Linear Attention Sequence Parallelism\" papers highlight the use of tiling and block-based computation strategies. These methods separate intra-block and inter-block components in linear attention calculations, reducing computational overhead and memory requirements. They also leverage efficient point-to-point communication mechanisms to decrease communication overhead[Analysis Note].\n- Dynamic compression techniques for attention states, as mentioned in the analysis note, are crucial for achieving a balance between compression ratio and model performance. Hardware-aware optimization strategies, such as optimizing memory access patterns and parallel computation techniques, are also essential[Analysis Note].\n\n## Stability Analysis Methods for Attention Mechanisms\n\n- The paper \"From Attention to Activation: Unravelling the Enigmas of Large Language Models\" provides a detailed analysis of the stability issues in Transformer models, such as the dominance of the first token in attention heads and the occurrence of large outlier activations. The authors propose a reformulation of the softmax function (softmax-1) and the use of OrthoAdam to mitigate these issues, ensuring the model's stability and performance.\n- The \"Length-Induced Embedding Collapse in Transformer-based Models\" paper introduces Temperature Scaling (TempScale) to mitigate the length collapse phenomenon in self-attention mechanisms. This method adjusts the attention scores to increase the variance of the attention score matrix, leading to more diverse embeddings and improved stability.\n\n## Integration Strategies for Combining Different Optimization Techniques\n\n- Integrating control variates with compressed states is a promising approach. The \"Efficient Attention via Control Variates\" paper shows how control variates can be combined with linear attention to reduce variance. This can be further integrated with dynamic compression strategies to optimize both memory and computation [Analysis Note].\n- The use of OrthoAdam in conjunction with other optimization techniques, such as softmax-1, can help in stabilizing the model and reducing outlier activations. This integration can be crucial for maintaining performance while optimizing for memory and computation.\n\n## Quantitative Evaluations of Attention Mechanism Performance\n\n- The \"Length-Induced Embedding Collapse in Transformer-based Models\" paper provides quantitative evaluations by analyzing the effect of TempScale on the attention score matrix. The authors show that a smaller temperature leads to a larger variance in the attention score matrix, resulting in more diverse embeddings and alleviating length collapse.\n- The analysis note mentions the importance of quantitative analysis of memory-computation trade-offs, which can be achieved through experiments that measure the performance impact of different compression ratios and computational strategies[Analysis Note].\n\nIn summary, to improve autoregressive language model design, researchers should focus on:\n\n- Implementing variance reduction techniques such as control variates and OrthoAdam.\n- Optimizing memory-computation trade-offs using block-based computation, dynamic compression, and hardware-aware strategies.\n- Conducting stability analysis using methods like softmax-1 and TempScale.\n- Integrating different optimization techniques to balance performance, memory, and computation.\n- Performing quantitative evaluations to assess the impact of these strategies on model performance.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights into three key aspects of the ControlVariateLinearAttention (CVLA) proposal:\n\na) Control Variate Integration:\n- Novel approaches to variance reduction in attention mechanisms\n- Implementation strategies for control variates in neural networks\n- Theoretical foundations for combining control variates with linear attention\n\nb) Memory Compression:\n- Dynamic compression techniques for attention states\n- Trade-offs between compression ratio and model performance\n- Hardware-aware optimization strategies\n\nc) Hardware Optimization:\n- Block-based computation strategies\n- Memory access patterns optimization\n- Efficient parallel computation techniques\n\n2. Useful References with Key Excerpts:\n\nFrom \"Lightning Attention-2\" (Qin et al., 2024):\n\"We leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks.\"\n\nFrom \"Linear Attention Sequence Parallelism\" (Sun et al., 2024):\n\"We design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP.\"\n\nFrom \"Efficient Attention via Control Variates\" (Zheng et al., 2023):\n\"Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence.\"\n\n3. Key Insights:\n\na) Control Variate Implementation:\n- Need for careful balance between variance reduction and computational overhead\n- Importance of adaptive control variate coefficients\n- Potential for hierarchical control variate structures\n\nb) Memory Management:\n- Trade-off between compression ratio and model performance\n- Dynamic compression strategies crucial for efficiency\n- Importance of hardware-aware memory management\n\nc) Hardware Optimization:\n- Block-based computation shows promise\n- Memory access patterns critical for performance\n- Parallel computation strategies essential\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Specific implementations of variance reduction in attention mechanisms\n2. Quantitative analysis of memory-computation trade-offs\n3. Integration strategies for control variates with compressed states\n4. Stability analysis methods for attention mechanisms\n5. Hardware-specific optimization techniques\n\n5. References:\n\n1. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n2. Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., & Zhong, Y. (2024). Linear Attention Sequence Parallelism. arXiv preprint.\n\n3. Zheng, L., Yuan, J., Wang, C., & Kong, L. (2023). Efficient Attention via Control Variates. International Conference on Learning Representations.\n\n4. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint.\n\n5. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. International Conference on Machine Learning."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"control variate attention variance reduction\", \"dynamic memory compression transformers\", \"hardware efficient linear attention\"",
            "detail": "Examine the integration of control variates in attention mechanisms, focusing on variance reduction techniques and their impact on training stability. Include analysis of memory compression methods and hardware optimization strategies for attention mechanisms.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExamine the integration of control variates in attention mechanisms, focusing on variance reduction techniques and their impact on training stability. Include analysis of memory compression methods and hardware optimization strategies for attention mechanisms.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.25)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.25)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 2. Efficient Attention via Control Variates (Avg. Score: 0.08)\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**Published in:** International Conference on Learning Representations (2023)\t**Cited by** 15  (*Influential: 1*)\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n##### *Relevant Chunk: No. 17/52 (Score: 0.10)*\n\n```\narXiv preprint arXiv:2101.00027, 2020. Tomas Geffner and Justin Domke. Using large ensembles of control variates for variational inference. Advances in Neural Information Processing Systems, 31, 2018. Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum? id=SyzKd1bCW. Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning.\n```\n\n##### *Relevant Chunk: No. 52/52 (Score: 0.06)*\n\n```\n2021). ## E. 4 EXPERIMENTAL SETTINGS OF EFFICIENCY COMPARISON\n\nFor the simulation experiment conducted in $\\S 5.3$ we adopt the same transformer architecture across all attention variants. In particular, it uses 8 transformer layers, 192 embedding dimensions, and 2 attention heads so that longer sequences can fit into our devices. The batch size is set to 64 across\n\n[^4]![](https://cdn.mathpix.com/cropped/2024_09_12_5520e6e7b143a538a399g-32.jpg?height=545&width=1414&top_left_y=266&top_left_x=353)\n\nFigure 2: Left and right: Additional empirical memory consumption and running time comparison for different attention mechanisms under various sequence lengths. 8 V100 GPUs, and the statistics are computed by averaging the results of 30 runs. Besides, in our ablation study, the efficiency metrics reported in Table 6 and Table 7 are evaluated under the same setup used during training. Remark on Modeling Short Sequences. Unfortunately, similar to most previous efficient attention baselines, EVA also runs slower than softmax attention under shorter sequences (e.g., length of 128 or 256), but it soon catches up in running speed, and the reduction of memory consumption is still significant. Besides, in short-sequence settings (such as the case of DeiT-Tiny/Small with sequences of 196 tokens), EVA often performs on par with or better than conventional softmax attention (see Table 1, whereas most previous attention variants usually perform much worse. This implies EVA can achieve a better trade-off between efficiency and quality: for short sequences, EVA is possible to achieve stronger performance competitive with softmax attention (despite in longer running time); while for long sequences, EVA can be run much faster with less memory. Comparison to Memory-efficient Attention Mechanisms. In this section, we conduct an empirical efficiency comparison between efficient approximate attention methods and FlashAttention, one of the memory-efficient attention mechanisms (Rabe \\& Staats, 2021, Dao et al., 2022) with optimized memory accesses. FlashAttention computes the exact softmax attention in an online manner without materializing the full attention matrix, achieving linear memory complexity with respect to sequence lengths; besides, both runtime and memory usage are further improved by minimizing IO accesses. We benchmark different attention modules on one NVIDIA GeForce RTX 3090 GPU, where we measure the memory usage and runtime of running a single attention block, consisting of 8 attention heads with 512 embedding dimension size, for both a forward and backward pass. As shown in Figure 2, we observe that FlashAttention achieves significant memory usage reduction for softmax attention approximation and even consumes much less memory than all considered approximate baselines under all sequence lengths. In terms of runtime, we notice that FlashAttention runs faster than most attention baselines under sequence lengths less than 2048 despite scaling quadratically, but EVA, along with other more efficient approximate variants, begin to catch up at longer sequence lengths. This implies that the quadratic computational costs of softmax attention still bottleneck its runtime performance, aligning with one of the main findings in Dao et al. (2022). According to this empirical study, we observe that FlashAttention offers a general and effective technique to speed up softmax attention; since many approximate variants (including EVA) exhibit a similar formulation to softmax attention (e.g., Equation 16), we expect they can also benefit from the optimized online softmax calculation technique and memory accesses of FlashAttention (Dao et al., 2022). ## F EXPERIMENTS on Long RANGE ArEnA\n\nLong Range Arena (LRA; Tay et al. 2021) is a lightweight benchmark that assesses the ability of efficient attention methods to model long sequences in diverse domains. We follow the same hyper-parameter setup as Xiong et al. (2021b) to re-evaluate all attention baselines and report the\n\nTable 15: Classification accuracy (\\%) on LRA benchmark with different efficient attention mechanisms. | Model | ListOps | Text | Retrieval | Image | Pathfinder | Avg. |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: |\n| Softmax | $\\mathbf{3 8 . 6 6}$ | 64.91 | 80.70 | 40.61 | 68.29 | 58.63 |\n| Linformer | 38.21 | 53.91 | 77.66 | 39.40 | 66.44 | 55.12 |\n| Performer | 29.84 | $\\mathbf{6 5 . 3 0}$ | 77.70 | 38.29 | 66.39 | 55.50 |\n| Reformer | 27.12 | 63.90 | 78.08 | 42.40 | 51.90 | 52.69 |\n| Scatterbrain | 38.21 | 64.04 | 77.83 | 42.51 | 60.62 | 56.64 |\n| Combiner | 38.26 | 63.98 | 81.47 | 42.80 | 55.94 | 56.49 |\n| LARA | 37.10 | 64.62 | 80.82 | 38.99 | 68.96 | 58.10 |\n| Nystr\u00f6mformer | 38.46 | $\\mathbf{6 5 . 2 8}$ | 80.44 | 39.71 | 68.98 | 58.57 |\n| Local | 38.46 | 63.70 | 80.71 | 42.25 | 68.46 | 58.72 |\n| Long-short | 38.56 | 63.46 | $\\mathbf{8 1 . 7 3}$ | 40.54 | $\\mathbf{7 1 . 2 8}$ | 59.11 |\n| EVA | $\\mathbf{3 8 . 6 1}$ | 64.31 | 80.21 | $\\mathbf{4 3 . 2 4}$ | 70.90 | $\\mathbf{5 9 . 4 5}$ |\n\ncomparison in Table 15. We observe that EVA largely improves previous RFA methods such as Performer (Choromanski et al. 2021) and LARA (Zheng et al., 2022b), and performs competitively with full softmax attention. Notably, EVA even achieves better average results over all tasks, with higher accuracy on Image and Pathfinder benchmarks, suggesting its capability of capturing long-term dependencies. For LRA benchmark, we set all attention-specific hyper-parameters to 128 (e.g., the number of landmarks in Nystr\u00f6mformer (Xiong et al. 2021b) and LARA (Zheng et al., 2022b), the window size in local attention and Combiner (Ren et al., 2021), etc.). We set $|E|=128$ and $C=64$ by default for EVA without any further tuning and find this setup works well. ## G Connections to Other AtTEntion Mechanisms\n\n## G. 1 RFA, SoftmAx AtTEntion, And EVA\n\nAs mentioned in our main text, one of the main contributions of this work is to develop a more general framework that bridges RFA and conventional softmax attention. To see how EVA (Equation 13) achieves this goal formally, note that if either $|E|=M$ or $C=M$, EVA would be equivalent to standard softmax attention; while if we set $|E|=0$ and $C=1$, EVA would recover vanilla RFA. ## G. 2 CONNECTIONS TO LARA\n\nNotably, EVA and LARA (Zheng et al. 2022b) are two efficient attention mechanisms that are both built upon the self-normalized importance sampling (SNIS) formulation of RFAs. LARA (Zheng et al. 2022b) puts the main focus on the proposal distribution used in SNIS and tries to design importance sampling proposals that are closer to the true underlying distribution. The proposed usage of multiple proposals further improves the estimation quality of SNIS and achieves strong empirical performance while still keeping linear complexity. In contrast to LARA, in this work we do not focus on the design choice of proposals used in importance sampling but aim to generalize the SNIS formulation further via control variates. As demonstrated in \\$3.2 our theory clearly delineates how the gap between such SNIS estimation and softmax attention can be closed by manipulating control variates. Since LARA and RFA are both SNIS estimators (their main difference lies in the choice of proposal distributions), our generalization also applies to LARA. To summarize, compared with LARA, EVA is a more general framework and improves conventional RFA from an orthogonal perspective. ## G. 3 Connections to Clustered AtTEntion\n\nClustered attention (Vyas et al. 2020) is an efficient attention mechanism that first clusters the set of queries into multiple groups, computes the mean centroid of each group, and then performs attention between query centroids and original key-value pairs. This framework is fast and effective and enjoys well-bounded approximation error. Clustered attention and EVA share some similarities in two aspects. First, both of them adopt the partitioning technique to reduce the computational complexity while remaining effective; and secondly, both observe that the efficient attention mechanism can be improved by refining the approximation over specific elements. For instance, clustered attention can be improved (Vyas et al., 2020) by selecting top- $k$ key-value pairs that are most relevant to each centroid and then refining the approximation by recomputing attention weights over these keys using original queries; while EVA notices that we can directly employ the optimal control variate coefficient for a subset of key-value pairs $(m \\in E)$ while still remaining efficient, which yields a more accurate approximation. Nevertheless, our main technical contribution is to develop a control variate formulation in the context of RFA and demonstrate that how RFA can be further improved locally. On the other hand, while clustered attention (Vyas et al. 2020) clusters queries, EVA partitions key-value pairs. This property makes EVA more amenable to the case of autoregressive language modeling since we do not impose clustering structures over the query set, and thus the causal relation among queries can be well maintained. ## G. 4 CONNECTIONS TO COMBINER\n\nCombiner (Ren et al. 2021) is a recently proposed attention mechanism that also partitions the sequence into chunks combined with local attention. The key difference between EVA and Combiner is the motivation, where Combiner introduces a structured factorization over the attention probability distribution, while our approach is built from the control variate perspective. ## G. 5 CONNECTIONS TO SCATTERbRAIN\n\nIn this section, we show that Scatterbrain (Chen et al., 2021a) can be cast as a special case of our framework EVA, although they are proposed based on quite different motivations. A Brief Review of Scatterbrain. Scatterbrain (Chen et al, 2021a) notes that sparse attention and RFA can approximate sharp and flat regions of the softmax attention matrix well, respectively. Based on this insight, Scatterbrain is proposed to first compute a Performer approximation to softmax attention and then cancel out the approximation error on critical regions via a sparse mechanism. Specifically, Scatterbrain (Chen et al. 2021a) defines a sparse matrix $\\mathbf{S} \\in \\mathbb{R}^{N \\times M}$ ) so that for each $(n, m) \\in \\mathbf{S}$ that indexes a non-zero entry. For notational simplicity, we also denote $\\operatorname{Supp}(\\mathbf{S})=\\left\\{(i, j) \\mid S_{i j} \\neq 0\\right\\}$ and $\\operatorname{Supp}_{n}(\\mathbf{S})=\\left\\{m \\mid S_{n m} \\neq 0\\right\\}$. With random features $\\phi(\\cdot, \\cdot)$ defined in Appendix A we let\n\n$$\nS_{n m}=\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)-\\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)\n$$\n\nWe then add it back to the approximate output:\n\n$$\n\\begin{aligned}\ny_{n}^{\\prime} & =\\sum_{m=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\mathbf{S V} \\\\\n& =\\sum_{m=1}^{M} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} S_{n m^{\\prime}} \\mathbf{v}_{m^{\\prime}} \\\\\n& =\\sum_{m \\notin \\operatorname{Supp}_{n}(\\mathbf{S})} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right) \\mathbf{v}_{m^{\\prime}}\n\\end{aligned}\n$$\n\nThe sparse mechanism can be thought of as modeling the error due to RFA and eliminating it on the support of $\\mathbf{S}$. After the correction step, Scatterbrain further adds a post-hoc normalization step to obtain a normalized attention output:\n\n$$\ny_{n}=\\frac{\\sum_{m \\notin \\operatorname{Supp}_{n}(\\mathbf{S})} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right) \\mathbf{v}_{m^{\\prime}}}{\\sum_{m \\notin \\operatorname{Supp}_{n}(\\mathbf{S})} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)+\\sum_{m^{\\prime} \\in \\operatorname{Supp}_{n}(\\mathbf{S})} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\n$$\n\nIntuitively, Scatterbrain (Chen et al. 2021a) produces accurate approximation in the support of the sparse matrix and remains the random feature approximation outside the support. Scatterbrain is a Special Case of EVA. For notational convenience, we denote $E:=\\operatorname{Supp}_{n}(\\mathbf{S})$. According to Proposition 1, suppose we employ optimal coefficients $\\widehat{\\boldsymbol{\\beta}}_{m}$ for all entries in $\\operatorname{Supp}_{n}(\\mathbf{S})$, and use the same coefficient $\\widehat{\\boldsymbol{\\beta}}$ for all the remaining entries (in other words, we let $C=1$ and the whole index set is only partitioned into two subsets $\\{E,[M] \\backslash E\\})$. Then we have\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_5520e6e7b143a538a399g-35.jpg?height=138&width=1160&top_left_y=495&top_left_x=472)\n\nAnd the resulting estimator overall becomes\n\n$$\n\\begin{aligned}\n\\widetilde{g}(\\boldsymbol{\\omega}) & =\\sum_{m=1}^{M} \\widetilde{g}_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\widetilde{g}_{m}(\\boldsymbol{\\omega})+\\sum_{m \\notin E} \\widetilde{g}_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})+\\widehat{\\boldsymbol{\\beta}} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\sum_{m \\notin E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}}\\left(1-\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right)\n\\end{aligned}\n$$\n\nScatterbrain (Chen et al. 2021a) can be a special case of this estimation algorithm if we set the proposal distribution to $q(\\omega)=\\mathcal{N}(\\omega ; 0, \\mathbf{I})$, and estimate the normalizing constant as follows. $$\n\\begin{aligned}\nZ & =\\mathbb{E}_{\\omega \\sim q(\\omega)}\\left[\\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I})\\left(\\sum_{m \\in E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)+\\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)\\right)}{q(\\omega)}\\right] \\\\\n& =\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\mathbb{E}_{\\omega \\sim q(\\omega)}\\left[\\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I}) \\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)}{q(\\omega)}\\right] \\\\\n& \\approx \\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} \\frac{\\mathcal{N}(\\omega ; 0, \\mathbf{I}) \\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right)}{q\\left(\\omega_{s}\\right)} \\\\\n& =\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\frac{1}{S} \\sum_{s=1}^{S} \\sum_{m \\notin E} \\xi\\left(\\mathbf{q}_{n}, \\omega\\right)^{\\top} \\xi\\left(\\mathbf{k}_{m}, \\omega\\right) \\\\\n& =\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{m \\notin E} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\\\\n& :=\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{m \\notin E} \\widetilde{h}_{m}(\\boldsymbol{\\omega})\n\\end{aligned}\n$$\n\nwhere we define $\\widetilde{h}_{m}(\\boldsymbol{\\omega})=Z h_{m}(\\boldsymbol{\\omega})$, as in this case\n\n$$\n\\begin{aligned}\n& g(\\boldsymbol{\\omega})=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)} f\\left(\\omega_{s}\\right)=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{Z} \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m} \\\\\n& h(\\boldsymbol{\\omega})=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{p_{n}\\left(\\omega_{s}\\right)}{q\\left(\\omega_{s}\\right)}=\\frac{1}{S} \\sum_{s=1}^{S} \\frac{1}{Z} \\sum_{m=1}^{M} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right)\n\\end{aligned}\n$$\n\nWith these specifications, we obtain\n\n$$\n\\begin{aligned}\n\\widetilde{g}(\\boldsymbol{\\omega}) & =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}}\\left(1-\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z}\\right) \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\frac{Z-\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)}{Z} \\\\\n& \\approx \\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\frac{\\sum_{m \\notin E} \\widetilde{h}_{m}(\\boldsymbol{\\omega})}{Z} \\\\\n& =\\sum_{m \\in E} \\frac{\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E}\\left(g_{m}(\\boldsymbol{\\omega})-\\widehat{\\boldsymbol{\\beta}} h_{m}(\\boldsymbol{\\omega})\\right)+\\widehat{\\boldsymbol{\\beta}} \\sum_{m \\notin E} h_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E} g_{m}(\\boldsymbol{\\omega}) \\\\\n& =\\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E} \\frac{\\frac{1}{S} \\sum_{s=1}^{S} \\xi\\left(\\mathbf{q}_{n}, \\omega_{s}\\right) \\xi\\left(\\mathbf{k}_{m}, \\omega_{s}\\right) \\mathbf{v}_{m}}{Z} \\\\\n& =\\frac{\\sum_{m \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m}}{Z}+\\sum_{m \\notin E} \\frac{\\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}}{Z} \\\\\n& \\approx \\frac{\\sum_{m \\notin E} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right) \\mathbf{v}_{m}+\\sum_{m^{\\prime} \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right) \\mathbf{v}_{m^{\\prime}}}{\\sum_{m \\notin E} \\boldsymbol{\\phi}\\left(\\mathbf{q}_{n}, \\boldsymbol{\\omega}\\right)^{\\top} \\boldsymbol{\\phi}\\left(\\mathbf{k}_{m}, \\boldsymbol{\\omega}\\right)+\\sum_{m^{\\prime} \\in E} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m^{\\prime}}\\right)}\n\\end{aligned}\n$$\n\nwhich is equivalent to Scatterbrain (Equation 30). Note that this equivalence would hold irrespective of the choice of shared coefficients $\\widehat{\\boldsymbol{\\beta}}$, which possibly indicates that the formulation of Scatterbrain limits the potential benefit of optimizing control variates under our framework. [^0]:    ${ }^{*}$ The majority of this work was done while these authors were at Bytedance. ${ }^{1}$ Our code and models are available at this link\n\n[^1]:    ${ }^{2}$ Note that the expectation of individual control variates $h_{m}(\\cdot)$ is still in closed form as $\\mathbb{E}\\left[h_{m}(\\boldsymbol{\\omega})\\right]=$ $\\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) / Z$. The derivation can be found in Appendix B. 3\n\n[^2]:    ${ }^{3}$ Here we add the superscript $n$ to reflect the dependence on query position $n$. [^3]:    ${ }^{4}$ we retain the repeated augmentation technique in training PVT to be consistent with the original training protocol in Wang et al. (2021b). [^4]:    ${ }^{5}$ The setup in Baevski \\& Auli (2019) can be found in the corresponding Fairseq training script: https://github.com/pytorch/fairseq/blob/master/examples/language_ model/README. adaptive_inputs.md\n\n\n```\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 24/49 (Score: 0.01)*\n\n```\nAdvances in neural information processing systems, 9, 1996. [32] Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. [33] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [34] Sheng-Chun Kao, Suvinay Subramanian, Gaurav Agrawal, and Tushar Krishna. An optimized dataflow for mitigating attention performance bottlenecks. arXiv preprint arXiv:2107.06419, 2021. [35] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are RNNs: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.01)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 3/27 (Score: 0.01)*\n\n```\nAlong this line, previous studies mainly investigated from the model expressivity and training stability perspectives. On the one hand, Dong et al. [6] and Noci et al. [19] initiated the discussion of attention localization and theoretically showed that a network with self-attention layers without skip connections exponentially loses the rank of hidden layers; the fact indicates that the model expressivity shall be immediately lost with more self-attention layers stacked. On the other hand, Zhai et al. [31] empirically found that attention entropy-averaged Shannon entropy of an attention probability matrix-correlates with training stability. Specifically, a training loss curve tends to fall into a plateau when attention entropy is low. Since higher entropy indicates near-uniform attention weights, their finding apparently suggests that localized attention may lead the learning dynamics to a plateau. Up until now, these two failure modes have been discussed independently with slightly different notions of attention localization, and hence, our understanding of the blessing and curse of attention localization remains elusive. To better comprehend, we characterize self-attention patterns by attention parameter matrices to reconcile the two collapse modes. We formulate the concept of localization by signal propagation probability (Section 3), which describes how likely the signal of a specific input token propagates to the gradient of a training objective. If the signal propagation probability is high for a few numbers of tokens only, attention is regarded to be localized. We show that the localization mode can be characterized by the eigenspectrum of attention weight matrices (Section 4). Specifically, attention is localized in the above sense when the eigenspectrum of the query-key parameter matrix has a non-zero mean and a small variance. Furthermore, the small eigenspectrum variance is relevant to both the rank collapse and entropy collapse (Section 5), and thus, we give a unified perspective of the two notions of attention collapse. For this reason, we argue that attention collapse and its performance can be viewed more transparently based on the eigenspectrum variance. Lastly, we verified the correlation of the eigenspectrum and the model performance in the experiments with the WikiText dataset [16] by introducing a regularization scheme called LocAteR.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"control variate attention variance reduction\", \"dynamic memory compression transformers\", \"hardware efficient linear attention\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Stochastic Training of Graph Convolutional Networks with Variance Reduction\n\n*From Search Query: \"control variate attention variance reduction\"*\n\n*Jianfei Chen, Jun Zhu, Le Song*\n\n**TL;DR:** Control variate based algorithms which allow sampling an arbitrarily small neighbor size are developed and a new theoretical guarantee for these algorithms to converge to a local optimum of GCN is proved.\n\n**Abstract:** Graph convolutional networks (GCNs) are powerful deep neural networks for graph-structured data. However, GCN computes the representation of a node recursively from its neighbors, making the receptive field size grow exponentially with the number of layers. Previous attempts on reducing the receptive field size by subsampling neighbors do not have a convergence guarantee, and their receptive field size per node is still in the order of hundreds. In this paper, we develop control variate based algorithms which allow sampling an arbitrarily small neighbor size. Furthermore, we prove new theoretical guarantee for our algorithms to converge to a local optimum of GCN. Empirical results show that our algorithms enjoy a similar convergence with the exact algorithm using only two neighbors per node. The runtime of our algorithms on a large Reddit dataset is only one seventh of previous neighbor sampling algorithms.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 463  (*Influential: 68*)\n\n#### 2. Approximation Based Variance Reduction for Reparameterization Gradients\n\n*From Search Query: \"control variate attention variance reduction\"*\n\n*Tomas Geffner, Justin Domke*\n\n**TL;DR:** A control variate is presented that is applicable for any reparameterizable distribution with known mean and covariance matrix, e.g. Gaussians with any covariance structure, that leads to large improvements in gradient variance and optimization convergence for inference with non-factorized variational distributions.\n\n**Abstract:** Flexible variational distributions improve variational inference but are harder to optimize. In this work we present a control variate that is applicable for any reparameterizable distribution with known mean and covariance matrix, e.g. Gaussians with any covariance structure. The control variate is based on a quadratic approximation of the model, and its parameters are set using a double-descent scheme by minimizing the gradient estimator's variance. We empirically show that this control variate leads to large improvements in gradient variance and optimization convergence for inference with non-factorized variational distributions.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 3. On the Theory of Variance Reduction for Stochastic Gradient Monte Carlo\n\n*From Search Query: \"control variate attention variance reduction\"*\n\n*Niladri S. Chatterji, Nicolas Flammarion, Yian Ma, Yi-An Ma, P. Bartlett, Michael I. Jordan*\n\n**TL;DR:** These methods are analyzed under a uniform set of assumptions on the log-posterior distribution, assuming it to be smooth, strongly convex and Hessian Lipschitz, to provide convergence guarantees in Wasserstein distance for a variety of variance-reduction methods.\n\n**Abstract:** We provide convergence guarantees in Wasserstein distance for a variety of variance-reduction methods: SAGA Langevin diffusion, SVRG Langevin diffusion and control-variate underdamped Langevin diffusion. We analyze these methods under a uniform set of assumptions on the log-posterior distribution, assuming it to be smooth, strongly convex and Hessian Lipschitz. This is achieved by a new proof technique combining ideas from finite-sum optimization and the analysis of sampling methods. Our sharp theoretical bounds allow us to identify regimes of interest where each method performs better than the others. Our theory is verified with experiments on real-world and synthetic datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 83  (*Influential: 12*)\n\n#### 4. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Piotr Nawrot, Adrian La'ncucki, Marcin Chochowski, David Tarjan, E. Ponti*\n\n**TL;DR:** Dynamic Memory Compression can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs). However, generation remains inefficient due to the need to store in memory a cache of key-value representations for past tokens, whose size scales linearly with the input sequence length and batch size. As a solution, we propose Dynamic Memory Compression (DMC), a method for online key-value cache compression at inference time. Most importantly, the model learns to apply different compression ratios in different heads and layers. We retrofit pre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers, achieving up to 7x throughput increase during auto-regressive inference on an NVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible percentage of the original data without adding any extra parameters. DMC preserves the original downstream performance with up to 4x cache compression, outperforming up-trained grouped-query attention (GQA) and key-value eviction policies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded gains. Hence, DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 2*)\n\n#### 6. Context Compression for Auto-regressive Transformers with Sentinel Tokens\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Siyu Ren, Qi Jia, Kenny Q. Zhu*\n\n**TL;DR:** This work proposes a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context.\n\n**Abstract:** The quadratic complexity of the attention module makes it gradually become the bulk of compute in Transformer-based LLMs during generation. Moreover, the excessive key-value cache that arises when dealing with long inputs also brings severe issues on memory footprint and inference latency. In this work, we propose a plug-and-play approach that is able to incrementally compress the intermediate activation of a specified span of tokens into compact ones, thereby reducing both memory and computational cost when processing subsequent context. Experiments on both in-domain language modeling and zero-shot open-ended document generation demonstrate the advantage of our approach over sparse attention baselines in terms of fluency, n-gram matching, and semantic similarity. At last, we comprehensively profile the benefit of context compression on improving the system throughout. Code is available at https://github.com/DRSY/KV_Compression.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 7. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"hardware efficient linear attention\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: \"hardware efficient linear attention\"*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. Bandit Structured Prediction for Neural Sequence-to-Sequence Learning\n\n*From Search Query: \"control variate attention variance reduction\"*\n\n*Julia Kreutzer, Artem Sokolov, Stefan Riezler*\n\n**Abstract:** Bandit structured prediction describes a stochastic optimization framework\nwhere learning is performed from partial feedback. This feedback is received in\nthe form of a task loss evaluation to a predicted output structure, without\nhaving access to gold standard structures. We advance this framework by lifting\nlinear bandit learning to neural sequence-to-sequence learning problems using\nattention-based recurrent neural networks. Furthermore, we show how to\nincorporate control variates into our learning algorithms for variance\nreduction and improved generalization. We present an evaluation on a neural\nmachine translation task that shows improvements of up to 5.89 BLEU points for\ndomain adaptation from simulated bandit feedback.\n\n**Published:** 2017-04-21T11:56:00Z  (*Updated: 2018-12-13T17:00:18Z*)\n\n\n\n#### 2. Variational Autoencoders for Semi-supervised Text Classification\n\n*From Search Query: \"control variate attention variance reduction\"*\n\n*Weidi Xu, Haoze Sun, Chao Deng, Ying Tan*\n\n**Abstract:** Although semi-supervised variational autoencoder (SemiVAE) works in image\nclassification task, it fails in text classification task if using vanilla LSTM\nas its decoder. From a perspective of reinforcement learning, it is verified\nthat the decoder's capability to distinguish between different categorical\nlabels is essential. Therefore, Semi-supervised Sequential Variational\nAutoencoder (SSVAE) is proposed, which increases the capability by feeding\nlabel into its decoder RNN at each time-step. Two specific decoder structures\nare investigated and both of them are verified to be effective. Besides, in\norder to reduce the computational complexity in training, a novel optimization\nmethod is proposed, which estimates the gradient of the unlabeled objective\nfunction by sampling, along with two variance reduction techniques.\nExperimental results on Large Movie Review Dataset (IMDB) and AG's News corpus\nshow that the proposed approach significantly improves the classification\naccuracy compared with pure-supervised classifiers, and achieves competitive\nperformance against previous advanced methods. State-of-the-art results can be\nobtained by integrating other pretraining-based methods.\n\n**Published:** 2016-03-08T13:24:45Z  (*Updated: 2016-11-24T08:18:31Z*)\n\n\n\n#### 3. DCT: Dynamic Compressive Transformer for Modeling Unbounded Sequence\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Kai-Po Chang, Wei-Yun Ma*\n\n**Abstract:** In this paper, we propose Dynamic Compressive Transformer (DCT), a\ntransformer-based framework for modeling the unbounded sequence. In contrast to\nthe previous baselines which append every sentence representation to memory,\nconditionally selecting and appending them is a more reasonable solution to\ndeal with unlimited long sequences. Our model uses a policy that determines\nwhether the sequence should be kept in memory with a compressed state or\ndiscarded during the training process. With the benefits of retaining\nsemantically meaningful sentence information in the memory system, our\nexperiment results on Enwik8 benchmark show that DCT outperforms the previous\nstate-of-the-art (SOTA) model.\n\n**Published:** 2021-10-10T15:21:19Z  (*Updated: 2021-10-10T15:21:19Z*)\n\n\n\n#### 4. LoMA: Lossless Compressed Memory Attention\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.\n\n**Published:** 2024-01-16T09:18:46Z  (*Updated: 2024-02-04T03:14:08Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient linear attention\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Linear Attention Sequence Parallelism\n\n*From Search Query: \"hardware efficient linear attention\"*\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single GPU. However, existing SP\nmethods do not take advantage of linear attention features, resulting in\nsub-optimal parallelism efficiency and usability for linear attention-based\nlanguage models. In this paper, we introduce Linear Attention Sequence Parallel\n(LASP), an efficient SP method tailored to linear attention-based language\nmodels. Specifically, we design an efficient point-to-point communication\nmechanism to leverage the right-product kernel trick of linear attention, which\nsharply decreases the communication overhead of SP. We also enhance the\npractical efficiency of LASP by performing kernel fusion and intermediate state\ncaching, making the implementation of LASP hardware-friendly on GPU clusters.\nFurthermore, we meticulously ensure the compatibility of sequence-level LASP\nwith all types of batch-level data parallel methods, which is vital for\ndistributed training on large clusters with long sequences and large batches.\nWe conduct extensive experiments on two linear attention-based models with\nvarying sequence lengths and GPU cluster sizes. LASP scales sequence length up\nto 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than\nexisting SP methods while being significantly faster. The code is available at\nhttps://github.com/OpenNLPLab/LASP.\n\n**Published:** 2024-04-03T17:33:21Z  (*Updated: 2024-04-03T17:33:21Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Compound Word Transformer: Learning to Compose Full-Song Music over Dynamic Directed Hypergraphs\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Yi-Hsuan Yang, Yin-Cheng Yeh, Jen-Yu Liu, Wen-Yi Hsiao*\n\n**Abstract:** To apply neural sequence models such as the Transformers to music generation tasks, one has to represent a piece of music by a sequence of tokens drawn from a finite set of pre-defined vocabulary. Such a vocabulary usually involves tokens of various types. For example, to describe a musical note, one needs separate tokens to indicate the note's pitch, duration, velocity (dynamics), and placement (onset time) along the time grid. While different types of tokens may possess different properties, existing models usually treat them equally, in the same way as modeling words in natural languages. In this paper, we present a conceptually different approach that explicitly takes into account the type of the tokens, such as note types and metric types. And, we propose a new Transformer decoder architecture that uses different feed-forward heads to model tokens of different types. With an expansion-compression trick, we convert a piece of music to a sequence of compound words by grouping neighboring tokens, greatly reducing the length of the token sequences. We show that the resulting model can be viewed as a learner over dynamic directed hypergraphs. And, we employ it to learn to compose expressive Pop piano music of full-song length (involving up to 10K individual tokens per song), both conditionally and unconditionally. Our experiment shows that, compared to state-of-the-art models, the proposed model converges 5--10 times faster at training (i.e., within a day on a single GPU with 11 GB memory), and with comparable quality in the generated music.\n\n**Published:** 2021-01-07\n\n\n\n#### 2. Stateful ODE-Nets using Basis Function Expansions\n\n*From Search Query: \"dynamic memory compression transformers\"*\n\n*Michael W. Mahoney, Liam Hodgkinson, N. Benjamin Erichson, Alejandro Queiruga*\n\n**Abstract:** The recently-introduced class of ordinary differential equation networks (ODE-Nets) establishes a fruitful connection between deep learning and dynamical systems. In this work, we reconsider formulations of the weights as continuous-in-depth functions using linear combinations of basis functions which enables us to leverage parameter transformations such as function projections. In turn, this view allows us to formulate a novel stateful ODE-Block that handles stateful layers. The benefits of this new ODE-Block are twofold: first, it enables incorporating meaningful continuous-in-depth batch normalization layers to achieve state-of-the-art performance; second, it enables compressing the weights through a change of basis, without retraining, while maintaining near state-of-the-art performance and reducing both inference time and memory footprint. Performance is demonstrated by applying our stateful ODE-Block to (a) image classification tasks using convolutional units and (b) sentence-tagging tasks using transformer encoder units.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-21\n\n\n\n#### 3. EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n\n*From Search Query: \"hardware efficient linear attention\"*\n\n*Chuang Gan, Junyan Li, Muyan Hu, Song Han, Han Cai*\n\n**Abstract:** High-resolution dense prediction enables many appealing real-world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel multi-scale linear attention. Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our multi-scale linear attention achieves the global receptive field and multi-scale learning (two desirable features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9x higher throughput on A100 GPU while achieving slightly better zero-shot instance segmentation performance on COCO.\n\n**Published:** 2022-05-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using the concepts of control variates, dynamic memory compression, and hardware-efficient linear attention, here are some key points and directions for further research:\n\n## Control Variate Integration in Attention Mechanisms\n\n### Variance Reduction Effectiveness\nControl variates are a variance reduction technique that can be integrated into attention mechanisms to improve training stability and efficiency. The idea is to use a control variate to adjust the estimator of the gradient, reducing its variance. For example, in the context of Monte Carlo methods, control variates have been used to reduce variance by leveraging the correlation between different estimators.\n\n- **Implementation Complexity and Overhead**: The complexity of integrating control variates into linear attention mechanisms needs to be evaluated. This involves studying how to construct and update the control variate efficiently, potentially using techniques like multiple importance sampling to combine different sampling distributions.\n\n- **Impact on Training Stability**: Analyzing the stability of training with control variates is crucial. This can involve comparing the convergence rates and stability of models trained with and without control variates. Research on control variates in other deep learning contexts can provide insights into how to adapt these methods for attention mechanisms.\n\n## Dynamic Memory Compression\n\n### Adaptive Compression Techniques\nDynamic memory compression is essential for reducing the memory footprint of large language models, especially during inference.\n\n- **State-of-the-Art Approaches**: Recent works like \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\" by Nawrot et al. (2024) provide techniques for compressing memory in large language models. These methods can be adapted for attention mechanisms to reduce memory usage while preserving performance.\n\n- **Memory Efficiency Gains and Information Preservation**: The trade-offs between compression ratios and the preservation of information need to be studied. Techniques such as quantization, pruning, and knowledge distillation can be explored to achieve efficient memory compression without significant performance degradation.\n\n- **Computational Overhead**: The computational overhead of dynamic compression techniques must be evaluated. This includes the time and resources required to compress and decompress attention states during training and inference.\n\n## Hardware Optimization\n\n### Hardware-Efficient Linear Attention\n\n- **Memory Access Patterns**: Optimizing memory access patterns is critical for efficient hardware utilization. Studies like \"Linear Attention Sequence Parallelism\" by Sun et al. (2024) can provide insights into how to design attention mechanisms that are friendly to hardware constraints, such as parallel computation and memory access optimization.\n\n- **Parallel Computation Efficiency**: Efficient parallel computation strategies can significantly improve the performance of attention mechanisms. This involves designing models that can leverage GPU or TPU architectures effectively, minimizing communication overhead and maximizing parallel processing.\n\n- **Hardware Utilization Improvements**: Best practices for hardware-optimized attention implementations include using techniques like batch processing, tensor parallelism, and pipeline parallelism. These methods can be combined with control variates and dynamic memory compression to achieve highly efficient and scalable models.\n\n## Future Research Directions\n\n### Specific Examples of Control Variate Applications\nTo find more specific examples of control variate applications in attention mechanisms, researchers can explore literature on variance reduction techniques in deep learning. For instance, studies on Monte Carlo methods and their applications in deep learning can provide valuable insights.\n\n### Optimal Compression Strategies\nIdentifying optimal compression strategies for attention states involves comparing different compression techniques (e.g., quantization, pruning, knowledge distillation) and evaluating their impact on model performance. This can be done through empirical studies and benchmarking different compression methods.\n\n### Hardware Optimization Techniques for Variance Reduction\nDiscovering hardware optimization techniques specifically for variance reduction in attention mechanisms requires a deep dive into hardware-aware design principles. This includes studying how control variates can be implemented efficiently on various hardware platforms and how memory access patterns can be optimized to reduce variance.\n\n### Stability Analysis of Control Variate Methods\nStability analysis of control variate methods is crucial for ensuring that the integration of control variates does not introduce instability during training. This involves theoretical analysis and empirical studies to understand the convergence properties of models using control variates.\n\nBy focusing on these areas, researchers can develop a novel autoregressive language model that combines control variates, dynamic memory compression, and hardware-efficient linear attention, leading to improved performance, stability, and scalability.\n",
            "analysis": "Initial Analysis of ControlVariateLinearAttention (CVLA) Proposal:\n\n1. Key Components Requiring Investigation:\n\na) Control Variate Integration:\n- Novel combination with linear attention\n- Variance reduction effectiveness\n- Implementation complexity and overhead\n- Impact on training stability\n\nb) Dynamic Memory Compression:\n- Adaptive compression techniques\n- Memory efficiency gains\n- Information preservation\n- Computational overhead\n\nc) Hardware Optimization:\n- Memory access patterns\n- Parallel computation efficiency\n- Hardware utilization improvements\n\n2. Comparison with Previous Designs:\n\nThe proposal appears distinct from previous siblings:\n- Unlike SelectiveStateLinearAttention (SSLA) which focuses on state space models, CVLA emphasizes variance reduction\n- Different from HyperMetaLearner which uses hypernetworks, CVLA employs control variates\n- Novel integration of memory compression with variance reduction\n\n3. Research Questions:\n\na) Variance Reduction:\n- How effective are control variates in linear attention settings?\n- What are the trade-offs between variance reduction and computational overhead?\n- How does this compare to other variance reduction techniques?\n\nb) Memory Management:\n- What are state-of-the-art approaches to dynamic memory compression?\n- How do different compression strategies affect model performance?\n- What are the optimal compression ratios for different scenarios?\n\nc) Hardware Efficiency:\n- What are current best practices for hardware-optimized attention implementations?\n- How do different memory access patterns affect performance?\n- What are effective strategies for parallel computation in attention mechanisms?\n\n4. Investigation Plan:\n\nPhase 1: Control Variate Analysis\n- Research existing control variate applications in deep learning\n- Examine variance reduction techniques in attention mechanisms\n- Study integration methods with linear attention\n\nPhase 2: Memory Compression Study\n- Investigate dynamic compression techniques\n- Analyze memory-efficient attention implementations\n- Research adaptive memory management strategies\n\nPhase 3: Hardware Optimization Review\n- Study hardware-aware attention designs\n- Examine parallel computation strategies\n- Research memory access optimization techniques\n\nInitial search results suggest this is a novel approach to combining control variates with linear attention, though individual components have been explored separately. The integration of dynamic memory compression with variance reduction appears particularly innovative.\n\nReferences found so far:\n\n1. \"Linear Complexity Randomized Self-attention Mechanism\" (Kong et al., 2022)\n- Discusses linear attention implementations\n- Provides baseline for comparison\n\n2. \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\" (Nawrot et al., 2024)\n- Presents memory compression techniques\n- Relevant for compression component\n\n3. \"ABC: Attention with Bounded-memory Control\" (Peng et al., 2021)\n- Explores memory-efficient attention\n- Offers insights into state management\n\n4. \"Linear Attention Sequence Parallelism\" (Sun et al., 2024)\n- Discusses hardware optimization for linear attention\n- Relevant for implementation strategy\n\nFuture search needed to:\n1. Find more specific examples of control variate applications in attention mechanisms\n2. Identify optimal compression strategies for attention states\n3. Discover hardware optimization techniques for variance reduction\n4. Examine stability analysis of control variate methods"
        },
        {
            "ready": false,
            "query": "\"control variate attention implementation examples\", \"attention state compression techniques\", \"parallel computation attention mechanisms\"",
            "detail": "Search for specific implementations of control variates in attention mechanisms, focusing on:\n1. Practical examples of control variate integration\n2. Methods for variance reduction in attention computation\n3. Techniques for efficient parallel implementation\n4. State compression strategies for attention mechanisms\n5. Hardware optimization techniques for attention computation",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific implementations of control variates in attention mechanisms, focusing on:\n1. Practical examples of control variate integration\n2. Methods for variance reduction in attention computation\n3. Techniques for efficient parallel implementation\n4. State compression strategies for attention mechanisms\n5. Hardware optimization techniques for attention computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.65)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.65)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 2. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.55)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.55)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n#### 3. Softmax Attention with Constant Cost per Token (Avg. Score: 0.53)\n\n*Franz A. Heinsen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work proposes a simple modification to the conventional attention mechanism applied by Transformers, which quantifies pairwise query-key similarity with scaled dot-products with the logarithms of scaled dot-products of exponentials, and linearizes attention with exponential kernel feature maps.\n\n**Abstract:** We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.\n\n##### *Relevant Chunk: No. 3/8 (Score: 0.53)*\n\n```\n(2021), and Poli et al. (2023). More recently, generalized state space models that build on previous research (Martin and Cundy, 2017) (Gu et al., 2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state (Peng et al., 2023) (Gu and Dao, 2023) (Katsch, 2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention (Jelassi et al., 2024). ### 1.1 Modifying Attention\n\nWe find that a simple modification to conventional attention linearizes it (Katharopoulos et al., 2020) with exponential kernel feature maps, and we show that this modification renders attention expressible as a composition of log-sums of exponentials, with a fixed-size latent space, for sequential application with constant cost per token. We implement our modification, verify that it works, and conclude that it is a promising alternative. The modification we propose is:\n\n$$\n\\begin{gathered}\n\\text { modified }(Q, K, V):= \\\\\n\\text { Attention }(Q) \\\\\n\\text { Softmax }\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V\n\\end{gathered}\n$$\n\nwhere queries $Q$, keys $K$ and values $V$ have $n_{Q} \\times d_{K}, n_{K} \\times d_{K}$, and $n_{K} \\times d_{V}$ elements, respectively, and $c$ is a scalar constant, all in $\\mathbb{R}$. We compute all exponentials elementwise. ### 1.2 As Log-Sums of Exponentials\n\nIn Section 2, we prove that\n\n$$\n\\begin{aligned}\n& \\text { modified } \\\\\n& \\operatorname{Attention}(Q, K, V)=\\exp (\\log S-\\log Z) \\text {, }\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nThe elementwise sums are over compatible dimensions, broadcasting over all other dimensions, from left to right-e.g., before reduction, the broadcasted elementwise sum $K^{T}+\\log V$ has $d_{K} \\times n_{K} \\times d_{V}$ elements. The functions $\\operatorname{LSE}_{\\left[d_{K}\\right]}(\\cdot)$ and $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$ compute log-sums of exponentials over the dimension indexed by $\\left(1,2, \\ldots, d_{K}\\right)$ and $\\left(1,2, \\ldots, n_{K}\\right)$, respectively. If any of $V$ 's elements are negative, $\\log V$ is complex, and therefore so is $\\log S$, but all Softmax mixtures of $V$ remain over $\\mathbb{R}$ because they are a composition of operations under which $\\mathbb{R}$ is closed (1). ### 1.3 Autoregressive Case\n\nFor autoregressive attention, in which $n_{Q}=n_{K}$ and for each query at step $t$ we compute attention only over $t$ trailing tokens, we note that in (3), all sequential dependencies are modeled by the logsums computed with $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$, so we can compute autoregressive $\\log S$ and $\\log Z$ with:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times n_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\times n_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nwhere the function $\\operatorname{LCSE}_{\\left[n_{K}\\right]}(\\cdot)$ computes a logcumulative-sum of exponentials over the dimension indexed by and $\\left(1,2, \\ldots, n_{K}\\right)$. For sequential application, given a new query $Q_{t}$ at step $t$, we need only the end-states of the two log-cumulative-sums of exponentials:\n\n$$\n\\begin{aligned}\n& \\log S_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(S)}}_{d_{K} \\times d_{V}}) \\\\\n& \\log Z_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(Z)}}_{d_{K}})\n\\end{aligned}\n$$\n\nwhere hidden states $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the states of the two log-cumulative-sums at step $t$ :\n\n$$\n\\begin{aligned}\n& H_{t}^{(S)}=\\log \\left(\\exp \\left(H_{t-1}^{(S)}\\right)+\\exp \\left(K_{t}+\\log V_{t}\\right)\\right) \\\\\n& H_{t}^{(Z)}=\\log \\left(\\exp \\left(H_{t-1}^{(Z)}\\right)+\\exp \\left(K_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwith zeros as their initial condition:\n\n$$\n\\begin{aligned}\n& H_{0}^{(S)}=\\{0\\}^{d_{K} \\times d_{V}} \\\\\n& H_{0}^{(Z)}=\\{0\\}^{d_{K}}\n\\end{aligned}\n$$\n\nTogether, $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ hold the latent, or hidden, state of autoregressive attention's computation at step $t$. They enable us to compute autoregressive attention sequentially with constant time and space complexity per token, $\\mathcal{O}(1)$. ### 1.4 Non-Autoregressive Case\n\nFor non-autoregressive attention, in which $n_{Q}$ may differ from $n_{K}$ and for each query we compute attention over all tokens in the sequence, we compute $\\log S$ and $\\log Z$ with (3). For sequential application, in which we add a new token to the input context at step $t$, with key $K_{t}$ and value $V_{t}$, we compute $\\log S$ and $\\log Z$ for all queries from the updated hidden states:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(S)}\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(Z)}\\right)\n\\end{aligned}\n$$\n\nwhere $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the hidden states at step $t$ (6), with zeros as their initial condition (7). ## 2 Proof\n\nGiven a query $q$ and a key $k$ in $\\mathbb{R}^{d_{K}}$, the logarithm of the dot-product of their exponentials is $\\log \\left(\\sum(\\exp (q) \\odot \\exp (k))\\right)=\\operatorname{LSE}(q+k)$, where $\\odot$ denotes an elementwise product. Log-sums of exponentials are associative and commutative, making the proof fairly straightforward. For clarity's sake, we walk step-by-step through a sequence of algebraic manipulations. We start by expanding the Softmax function in (1) and simplifying the resulting expression. We obtain a form of linear attention (Katharopoulos et al., 2020) with exponential kernel feature maps:\n\n$$\n\\begin{gathered}\n\\operatorname{Softmax}\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V= \\\\\n{\\left[\\frac{\\exp (Q) \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\exp (Q) \\exp (K)^{T}}\\right] V}\n\\end{gathered}\n$$\n\nwhere $\\sum_{\\left[n_{K}\\right]}$ normalizes each row to a probability distribution. The scaling constant $\\exp (c)$ disappears because it becomes a common divisor of numerator and denominator expressions. Note that the feature function corresponding to the exponential kernel is infinite dimensional. Substitute the dot-products of exponentiated queries and exponentiated keys with equivalent explicit summations over elementwise products:\n\n$$\n\\left[\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\\right] V\n$$\n\nwhere the elementwise product $\\odot$ is over compatible dimensions, broadcasting over any other dimensions, from left to right, such that the broadcasted elementwise product $\\exp (Q) \\odot \\exp (K)^{T}$ has $n_{Q} \\times d_{K} \\times n_{K}$ elements. ${ }^{2}$\nExpress matrix multiplication with $V$ as a summation over broadcasted elementwise products:\n\n$$\n\\frac{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T} \\odot V}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\n$$\n\nBoth $\\exp (K)^{T}$ and $V$ have a dimension indexed by $\\left(1,2, \\ldots, n_{K}\\right)$, but $\\exp (Q)$ does not, so we can sum over that dimension before broadcastmultiplying elementwise with $\\exp (Q)$ :\n\n$$\n\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V}{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}}\n$$\n\nDefine $S$ and $Z$ as the expressions that compute numerators and denominators, respectively,\n\n$$\n\\begin{aligned}\n& S:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V \\\\\n& Z:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}\n\\end{aligned}\n$$\n\nand take their logarithms. We obtain:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)\\right),\n\\end{aligned}\n$$\n\nwhich is the same as (3). [^1]\n## 3 Implementation\n\nAs proof of concept, we implement our attention mechanism for both autoregressive applications (e.g., generative language modeling) and nonautoregressive applications (e.g., masked language modeling). For simplicity and expediency, we limit our implementation in two significant ways: First, we restrict $V$ to elements $\\geq 0$ to avoid dealing with complex floating-point numbers, which incur greater overhead and are more cumbersome to manipulate than real floating-point numbers with existing software infrastructure. Second, when computing autoregressive attention over $n_{K}$ tokens, we first compute all $n_{K}$ hidden states with a parallel scan, and then reduce them, which is space-inefficient but easier to implement with existing software infrastructure. ${ }^{3}$\n\nWe apply our implementation in a small generative language model ( 125 M parameters, 50257 token ids, 768 embedding features). For numerical stability, in each layer we compute $\\log V$ over $\\mathbb{R}$ directly, with a dense feed-forward transformation of token states, implicitly defining $V$ as $\\log V$ 's exponential but never actually computing it. To remain in $\\mathbb{R}$, we use the logarithm of attention as input to subsequent transformations in the layer, i.e., the input to subsequent transformations is $\\log S-\\log Z$ instead of $\\exp (\\log S-\\log Z)$. Please see our published code for all model details. We train the model on 300B tokens from The Pile (Gao et al., 2020) with a conventional sequence length of 1024 tokens, and obtain a crossentropy loss of 2.47 , competitive with state-of-theart generative language models of similar size. ## 4 Conclusions\n\nBy all indications, our attention mechanism is a promising alternative to the conventional one, but the evidence we have so far is too scant to be conclusive. An adequate comparison requires addressing our implementation's temporary limitations and evaluating models with one to several orders of magnitude more parameters on a diverse set of benchmarks and downstream tasks. [^2]\n## References\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR abs/1904.10509. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 4. Efficient Attention via Control Variates (Avg. Score: 0.49)\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**Published in:** International Conference on Learning Representations (2023)\t**Cited by** 15  (*Influential: 1*)\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n##### *Relevant Chunk: No. 41/52 (Score: 0.49)*\n\n```\nThis also validates the effectiveness of adopting a finer-grained treatment over control variates. Partitioning Strategy. EVA significantly improves random feature approximation by trying to locally estimate each subset of tokens, which is a much easier task than approximating the whole sequence as in previous RFA methods. To achieve this, EVA partitions the whole token sequence into multiple subsets according to the current query position $n$, which is denoted by $\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N} \\square^{3}$ For elements in subset $E^{n}$, we optimize the control variate coefficient to give an exact estimate for each single token $m \\in E^{n}$. In addition, we impose T5-style relative positional encoding (Raffel et al. 2020a) over elements in $E^{n}$. While for some other subset $\\mathcal{P}_{c}$, we employ the shared coefficient to approximate all tokens belonging to $\\mathcal{P}_{c}$. We assume all $E^{1}, \\ldots, E^{N}$ are of the same cardinality $K$, and $\\left|\\mathcal{P}_{c}^{n}\\right|$ is the same for any $c=1, \\ldots, C$ and $n=1, \\ldots, N$. The partition strategy $\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N}$ is decided based on a simple criterion:\n\n- for $E^{n}$, it contains $K$ local neighbors with respect to each query $n$. To further simplify implementation and reduce memory usage, we chunk the whole sequence into contiguous blocks of size $K$, and all adjacent queries belonging to the same block will share this block as the subset $E^{n}$;\n- as for $\\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}$, we follow a similar treatment by splitting the complement $[M] \\backslash E^{n}$ into $C$ contiguous chunks of the same size. For ease of implementation, we simply partition the whole index set $[M]$ into multiple groups instead of $[M] \\backslash E^{n}$, which circumvents the overload for explicitly performing set difference operations in practical implementation. Although this leads to extra approximation error, this amounts to putting more attention weights on tokens belonging to the subset $E$ and we found this approximation does not lead to performance degradation (Table 8). ## D A CAuSAl VARIANT OF EVA\n\nIn this section, we describe the causal variant of EVA, where each query can only attend to historical tokens. Thanks to the partitioning scheme, all future information with respect to the current query token can be masked conveniently. Following the formulation of EVA, we partition the whole sequence into $C+1$ subsets $\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}$ with respect to each query $\\mathbf{q}_{n}$. To fulfill the\n\n[^2]```\nAlgorithm 1 Pseudo-code for EVA\n    Input: the randomized mapping \\(\\xi(\\cdot, \\cdot)\\), queries \\(\\mathbf{Q}:=\\left\\{\\mathbf{q}_{n}\\right\\}_{n=1}^{N}\\), keys \\(\\mathbf{K}:=\\left\\{\\mathbf{k}_{m}\\right\\}_{m=1}^{M}\\), values\n    \\(\\mathbf{V}:=\\left\\{\\mathbf{v}_{m}\\right\\}_{m=1}^{M}\\) and partitions of the sequence \\(\\left\\{E^{n}, \\mathcal{P}_{1}^{n}, \\mathcal{P}_{2}^{n}, \\ldots, \\mathcal{P}_{C}^{n}\\right\\}_{n=1}^{N} ;\\)\n    Output: attention output \\(\\mathbf{Y}:=\\left\\{\\mathbf{y}_{n}\\right\\}_{n=1}^{N}\\);\n    for \\(c=1,2, \\ldots, C\\) do\n        Compute \\(\\widetilde{\\mathbf{k}}_{c}\\) according to Equation 26\n        Compute \\(q_{c}(\\omega)\\) according to Equation 28\n        Sample \\(\\omega_{c} \\sim q_{c}(\\omega) ; \\quad \\triangleright\\) During inference, simply set \\(\\omega_{c}=\\mathbb{E}_{q_{c}(\\omega)}[\\omega]\\)\n        Compute \\(\\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega})=\\sum_{m \\in \\mathcal{P}_{c}^{n}} \\frac{\\xi\\left(\\mathbf{k}_{m}, \\omega_{c}\\right)}{\\sum_{m \\in \\mathcal{P}_{c}^{n}} \\xi\\left(\\mathbf{k}_{m}, \\omega_{c}\\right)} \\mathbf{v}_{m}\\)\n    end for\n    for \\(n=1,2, \\ldots, N\\) do\n        Compute \\(\\mathcal{S}=\\sum_{m \\in E^{n}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right) \\mathbf{v}_{m} ; \\triangleright\\) Compute attention scores in the selected subset \\(E\\)\n        Compute \\(\\mathcal{R}=\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right) \\widehat{\\boldsymbol{\\beta}}_{c}(\\boldsymbol{\\omega}) ; \\quad \\triangleright\\) Compute approx. expected control variates\n        Compute \\(Z=\\sum_{m \\in E^{n}} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\mathbf{k}_{m}\\right)+\\sum_{c=1}^{C} \\exp \\left(\\mathbf{q}_{n}^{\\top} \\widetilde{\\mathbf{k}}_{c}\\right)\\);\n        Compute \\(\\mathbf{y}_{n}=(\\mathcal{S}+\\mathcal{R}) / Z\\)\n    end for\n    Return \\(\\mathbf{Y}:=\\left[\\mathbf{y}_{1}, \\ldots, \\mathbf{y}_{N}\\right]\\)\n```\n\ncausal requirement, we design two different types of masking matrices to deal with both $E^{n}$ and $\\left\\{\\mathcal{P}_{c}^{n}\\right\\}_{c=1}^{C}$ respectively. - For $E^{n}$, we adopt a single lower-triangular matrix with shape $K \\times K$ (recall that each set $E^{n}$ is of size $K$ ) to mask future tokens locally, similar to the case of standard decoder softmax attention. Future tokens that do not belong to $E^{n}$ are handled by masking functions for $\\left\\{\\mathcal{P}_{c}^{n}\\right\\}_{c=1}^{C}$, as described below. - For $\\left\\{\\mathcal{P}_{c}^{n}\\right\\}_{c=1}^{C}$, we make use of the fact $n \\in E^{n}$. Since any $\\mathcal{P}_{c}^{n}$ and $E^{n}$ are disjoint, we only need to mask all subsets $\\mathcal{P}_{c}^{n}$ that appear after $E^{n}$. This amounts to first allocating a lower-triangular matrix with shape $C \\times C$, and then conducting future masking at a subset level. The pseudo-code for the causal variant of EVA is listed in Algorithm2. ## E EXPERIMENTAL DETAILS\n\nAll of our experiments are conducted with at most 16 NVIDIA V100 GPUs. ## E. 1 Efficient AtTENTION BASELINES\n\nWe compare our proposed attention mechanism EVA against various baselines:\n\n- Performer (Choromanski et al., 2021), which uses the plain random features to approximate softmax attention;\n- LARA (Zheng et al. 2022b), an advanced RF approximation that makes use of multiple adaptive proposals to construct the SNIS estimator;\n- Linformer (Wang et al.\n```\n\n#### 5. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.27)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.27)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"control variate attention implementation examples\", \"attention state compression techniques\", \"parallel computation attention mechanisms\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. The mechanistic basis of data dependence and abrupt learning in an in-context classification task\n\n*From Search Query: \"control variate attention implementation examples\"*\n\n*Gautam Reddy*\n\n**TL;DR:** A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.\n\n**Abstract:** Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence. In-context learning contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 4*)\n\n#### 2. Motif-based Graph Self-Supervised Learning for Molecular Property Prediction\n\n*From Search Query: \"control variate attention implementation examples\"*\n\n*Zaixin Zhang, Qi Liu, Hao Wang, Chengqiang Lu, Chee-Kong Lee*\n\n**TL;DR:** This work designs a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary and introduces a general motif-based generative pre-training framework in which GNNs are asked to make topological and label predictions.\n\n**Abstract:** Predicting molecular properties with data-driven methods has drawn much attention in recent years. Particularly, Graph Neural Networks (GNNs) have demonstrated remarkable success in various molecular generation and prediction tasks. In cases where labeled data is scarce, GNNs can be pre-trained on unlabeled molecular data to first learn the general semantic and structural information before being fine-tuned for specific tasks. However, most existing self-supervised pre-training frameworks for GNNs only focus on node-level or graph-level tasks. These approaches cannot capture the rich information in subgraphs or graph motifs. For example, functional groups (frequently-occurred subgraphs in molecular graphs) often carry indicative information about the molecular properties. To bridge this gap, we propose Motif-based Graph Self-supervised Learning (MGSSL) by introducing a novel self-supervised motif generation framework for GNNs. First, for motif extraction from molecular graphs, we design a molecule fragmentation method that leverages a retrosynthesis-based algorithm BRICS and additional rules for controlling the size of motif vocabulary. Second, we design a general motif-based generative pre-training framework in which GNNs are asked to make topological and label predictions. This generative framework can be implemented in two different ways, i.e., breadth-first or depth-first. Finally, to take the multi-scale information in molecular graphs into consideration, we introduce a multi-level self-supervised pre-training. Extensive experiments on various downstream benchmark tasks show that our methods outperform all state-of-the-art baselines.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 202  (*Influential: 21*)\n\n#### 3. EF21-P and Friends: Improved Theoretical Communication Complexity for Distributed Optimization with Bidirectional Compression\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Kaja Gruntkowska, A. Tyurin, Peter Richt'arik*\n\n**TL;DR:** This work obtains novel methods supporting bidirectional compression (both from the server to the workers and vice versa) that enjoy new state-of-the-art theoretical communication complexity for convex and nonconvex problems.\n\n**Abstract:** In this work we focus our attention on distributed optimization problems in the context where the communication time between the server and the workers is non-negligible. We obtain novel methods supporting bidirectional compression (both from the server to the workers and vice versa) that enjoy new state-of-the-art theoretical communication complexity for convex and nonconvex problems. Our bounds are the first that manage to decouple the variance/error coming from the workers-to-server and server-to-workers compression, transforming a multiplicative dependence to an additive one. Moreover, in the convex regime, we obtain the first bounds that match the theoretical communication complexity of gradient descent. Even in this convex regime, our algorithms work with biased gradient estimators, which is non-standard and requires new proof techniques that may be of independent interest. Finally, our theoretical results are corroborated through suitable experiments.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 19  (*Influential: 2*)\n\n#### 4. COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan*\n\n**TL;DR:** This paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.\n\n**Abstract:** Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\\times$ speedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the existing works.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 5. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou*\n\n**TL;DR:** This work presents a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation, and demonstrates that the monolingual model outperforms state-of-the-art baselines in different parameter size of student models.\n\n**Abstract:** Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 1000  (*Influential: 149*)\n\n#### 6. Bifurcated Attention for Single-Context Large-Batch Sampling\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Jiacheng Guo, Liangfu Chen, Parminder Bhatia, Ramesh Nallapati, Sudipta Sengupta, Bing Xiang*\n\n**TL;DR:** Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process, leading to lower latency and improving suitability for real-time applications.\n\n**Abstract:** In our study, we present bifurcated attention , a method developed for language model inference in single-context batch sampling contexts. This approach aims to reduce redundant memory IO costs, a significant factor in latency for high batch sizes and long context lengths. Bifurcated attention achieves this by dividing the attention mechanism during incremental decoding into two distinct GEMM operations, focusing on the KV cache from prefill and the decoding process. This method ensures precise computation and maintains the usual computational load (FLOPs) of standard attention mechanisms, but with reduced memory IO. Bifurcated attention is also compatible with multi-query attention mechanism known for reduced memory IO for KV cache, further enabling higher batch size and context length. The resulting efficiency leads to lower latency, improving suitability for real-time applications, e.g., enabling massively-parallel answer generation without substantially increasing latency, enhancing performance when integrated with post-processing techniques such as reranking.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. Sub-Linear Memory: How to Make Performers SLiM\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Valerii Likhosherstov, K. Choromanski, Jared Davis, Xingyou Song, Adrian Weller*\n\n**TL;DR:** A thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, results in a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of $L$ (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting.\n\n**Abstract:** The Transformer architecture has revolutionized deep learning on sequential data, becoming ubiquitous in state-of-the-art solutions for a wide variety of applications. Yet vanilla Transformers are notoriously resource-expensive, requiring $O(L^2)$ in serial time and memory as functions of input length $L$. Recent works proposed various linear self-attention mechanisms, scaling only as $O(L)$ for serial computation. We perform a thorough analysis of recent Transformer mechanisms with linear self-attention, Performers, in terms of overall computational complexity. We observe a remarkable computational flexibility: forward and backward propagation can be performed with no approximations using sublinear memory as a function of $L$ (in addition to negligible storage for the input sequence), at a cost of greater time complexity in the parallel setting. In the extreme case, a Performer consumes only $O(1)$ memory during training, and still requires $O(L)$ time. This discovered time-memory tradeoff can be used for training or, due to complete backward-compatibility, for fine-tuning on a low-memory device, e.g. a smartphone or an earlier-generation GPU, thus contributing towards decentralized and democratized deep learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 8. Branchformer: Parallel MLP-Attention Architectures to Capture Local and Global Context for Speech Recognition and Understanding\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Yifan Peng, Siddharth Dalmia, Ian Lane, Shinji Watanabe*\n\n**TL;DR:** This work proposes a more flexible, interpretable and customizable encoder alternative, Branchformer, with parallel branches for modeling various ranged dependencies in end-to-end speech processing, including the ability to have variable inference complexity in a single trained model.\n\n**Abstract:** Conformer has proven to be effective in many speech processing tasks. It combines the benefits of extracting local dependencies using convolutions and global dependencies using self-attention. Inspired by this, we propose a more flexible, interpretable and customizable encoder alternative, Branchformer, with parallel branches for modeling various ranged dependencies in end-to-end speech processing. In each encoder layer, one branch employs self-attention or its variant to capture long-range dependencies, while the other branch utilizes an MLP module with convolutional gating (cgMLP) to extract local relationships. We conduct experiments on several speech recognition and spoken language understanding benchmarks. Results show that our model outperforms both Transformer and cgMLP. It also matches with or outperforms state-of-the-art results achieved by Conformer. Furthermore, we show various strategies to reduce computation thanks to the two-branch architecture, including the ability to have variable inference complexity in a single trained model. The weights learned for merging branches indicate how local and global dependencies are utilized in different layers, which benefits model designing.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 109  (*Influential: 9*)\n\n### 6 related papers from ArXiv\n\n#### 1. Natural Language Generation with Neural Variational Models\n\n*From Search Query: \"control variate attention implementation examples\"*\n\n*Hareesh Bahuleyan*\n\n**Abstract:** In this thesis, we explore the use of deep neural networks for generation of\nnatural language. Specifically, we implement two sequence-to-sequence neural\nvariational models - variational autoencoders (VAE) and variational\nencoder-decoders (VED). VAEs for text generation are difficult to train due to\nissues associated with the Kullback-Leibler (KL) divergence term of the loss\nfunction vanishing to zero. We successfully train VAEs by implementing\noptimization heuristics such as KL weight annealing and word dropout. We also\ndemonstrate the effectiveness of this continuous latent space through\nexperiments such as random sampling, linear interpolation and sampling from the\nneighborhood of the input. We argue that if VAEs are not designed\nappropriately, it may lead to bypassing connections which results in the latent\nspace being ignored during training. We show experimentally with the example of\ndecoder hidden state initialization that such bypassing connections degrade the\nVAE into a deterministic model, thereby reducing the diversity of generated\nsentences. We discover that the traditional attention mechanism used in\nsequence-to-sequence VED models serves as a bypassing connection, thereby\ndeteriorating the model's latent space. In order to circumvent this issue, we\npropose the variational attention mechanism where the attention context vector\nis modeled as a random variable that can be sampled from a distribution. We\nshow empirically using automatic evaluation metrics, namely entropy and\ndistinct measures, that our variational attention model generates more diverse\noutput sentences than the deterministic attention model. A qualitative analysis\nwith human evaluation study proves that our model simultaneously produces\nsentences that are of high quality and equally fluent as the ones generated by\nthe deterministic attention counterpart.\n\n**Published:** 2018-08-27T19:40:53Z  (*Updated: 2018-08-27T19:40:53Z*)\n\n\n\n#### 2. Named Tensor Notation\n\n*From Search Query: \"control variate attention implementation examples\"*\n\n*David Chiang, Alexander M. Rush, Boaz Barak*\n\n**Abstract:** We propose a notation for tensors with named axes, which relieves the author,\nreader, and future implementers of machine learning models from the burden of\nkeeping track of the order of axes and the purpose of each. The notation makes\nit easy to lift operations on low-order tensors to higher order ones, for\nexample, from images to minibatches of images, or from an attention mechanism\nto multiple attention heads.\n  After a brief overview and formal definition of the notation, we illustrate\nit through several examples from modern machine learning, from building blocks\nlike attention and convolution to full models like Transformers and LeNet. We\nthen discuss differential calculus in our notation and compare with some\nalternative notations. Our proposals build on ideas from many previous papers\nand software libraries. We hope that our notation will encourage more authors\nto use named tensors, resulting in clearer papers and more precise\nimplementations.\n\n**Published:** 2021-02-25T22:21:30Z  (*Updated: 2023-01-17T19:52:28Z*)\n\n\n\n#### 3. MLKD-BERT: Multi-level Knowledge Distillation for Pre-trained Language\n  Models\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Ying Zhang, Ziheng Yang, Shufan Ji*\n\n**Abstract:** Knowledge distillation is an effective technique for pre-trained language\nmodel compression. Although existing knowledge distillation methods perform\nwell for the most typical model BERT, they could be further improved in two\naspects: the relation-level knowledge could be further explored to improve\nmodel performance; and the setting of student attention head number could be\nmore flexible to decrease inference time. Therefore, we are motivated to\npropose a novel knowledge distillation method MLKD-BERT to distill multi-level\nknowledge in teacher-student framework. Extensive experiments on GLUE benchmark\nand extractive question answering tasks demonstrate that our method outperforms\nstate-of-the-art knowledge distillation methods on BERT. In addition, MLKD-BERT\ncan flexibly set student attention head number, allowing for substantial\ninference time decrease with little performance drop.\n\n**Published:** 2024-07-03T03:03:30Z  (*Updated: 2024-07-03T03:03:30Z*)\n\n\n\n#### 4. Autoregressive Knowledge Distillation through Imitation Learning\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Alexander Lin, Jeremy Wohlwend, Howard Chen, Tao Lei*\n\n**Abstract:** The performance of autoregressive models on natural language generation tasks\nhas dramatically improved due to the adoption of deep, self-attentive\narchitectures. However, these gains have come at the cost of hindering\ninference speed, making state-of-the-art models cumbersome to deploy in\nreal-world, time-sensitive settings. We develop a compression technique for\nautoregressive models that is driven by an imitation learning perspective on\nknowledge distillation. The algorithm is designed to address the exposure bias\nproblem. On prototypical language generation tasks such as translation and\nsummarization, our method consistently outperforms other distillation\nalgorithms, such as sequence-level knowledge distillation. Student models\ntrained with our method attain 1.4 to 4.8 BLEU/ROUGE points higher than those\ntrained from scratch, while increasing inference speed by up to 14 times in\ncomparison to the teacher model.\n\n**Published:** 2020-09-15T17:43:02Z  (*Updated: 2020-10-29T00:40:45Z*)\n\n\n\n#### 5. MUSE: Parallel Multi-Scale Attention for Sequence to Sequence Learning\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, Liangchen Luo*\n\n**Abstract:** In sequence to sequence learning, the self-attention mechanism proves to be\nhighly effective, and achieves significant improvements in many tasks. However,\nthe self-attention mechanism is not without its own flaws. Although\nself-attention can model extremely long dependencies, the attention in deep\nlayers tends to overconcentrate on a single token, leading to insufficient use\nof local information and difficultly in representing long sequences. In this\nwork, we explore parallel multi-scale representation learning on sequence data,\nstriving to capture both long-range and short-range language structures. To\nthis end, we propose the Parallel MUlti-Scale attEntion (MUSE) and MUSE-simple.\nMUSE-simple contains the basic idea of parallel multi-scale sequence\nrepresentation learning, and it encodes the sequence in parallel, in terms of\ndifferent scales with the help from self-attention, and pointwise\ntransformation. MUSE builds on MUSE-simple and explores combining convolution\nand self-attention for learning sequence representations from more different\nscales. We focus on machine translation and the proposed approach achieves\nsubstantial performance improvements over Transformer, especially on long\nsequences. More importantly, we find that although conceptually simple, its\nsuccess in practice requires intricate considerations, and the multi-scale\nattention must build on unified semantic space. Under common setting, the\nproposed model achieves substantial performance and outperforms all previous\nmodels on three main machine translation tasks. In addition, MUSE has potential\nfor accelerating inference due to its parallelism. Code will be available at\nhttps://github.com/lancopku/MUSE\n\n**Published:** 2019-11-17T09:36:07Z  (*Updated: 2019-11-17T09:36:07Z*)\n\n\n\n#### 6. Linear Attention Sequence Parallelism\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single GPU. However, existing SP\nmethods do not take advantage of linear attention features, resulting in\nsub-optimal parallelism efficiency and usability for linear attention-based\nlanguage models. In this paper, we introduce Linear Attention Sequence Parallel\n(LASP), an efficient SP method tailored to linear attention-based language\nmodels. Specifically, we design an efficient point-to-point communication\nmechanism to leverage the right-product kernel trick of linear attention, which\nsharply decreases the communication overhead of SP. We also enhance the\npractical efficiency of LASP by performing kernel fusion and intermediate state\ncaching, making the implementation of LASP hardware-friendly on GPU clusters.\nFurthermore, we meticulously ensure the compatibility of sequence-level LASP\nwith all types of batch-level data parallel methods, which is vital for\ndistributed training on large clusters with long sequences and large batches.\nWe conduct extensive experiments on two linear attention-based models with\nvarying sequence lengths and GPU cluster sizes. LASP scales sequence length up\nto 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than\nexisting SP methods while being significantly faster. The code is available at\nhttps://github.com/OpenNLPLab/LASP.\n\n**Published:** 2024-04-03T17:33:21Z  (*Updated: 2024-04-03T17:33:21Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Feature Selection in the Contrastive Analysis Setting\n\n*From Search Query: \"control variate attention implementation examples\"*\n\n*Anonymous*\n\n**Abstract:** Contrastive analysis (CA) refers to the exploration of variations uniquely enriched in a target dataset as compared to a corresponding background dataset generated from sources of variation that are irrelevant to a given task. For example, a biomedical data analyst may wish to find a small set of genes to use as a proxy for variations in genomic data only present among patients with a given disease (target) as opposed to healthy control subjects (background). However, as of yet the problem of feature selection in the CA setting has received little attention from the machine learning community. In this work we present contrastive feature selection (CFS), a method for performing feature selection in the CA setting. We motivate our approach with a novel information-theoretic analysis of representation learning in the CA setting, and we empirically validate CFS on a semi-synthetic dataset and four real-world biomedical datasets. We find that our method consistently outperforms previously proposed state-of-the-art supervised and fully unsupervised feature selection methods not designed for the CA setting. An open-source implementation of our method is available at https://github.com/suinleelab/CFS.\n\n**Conference:** feature-selection-in-the-contrastive-analysis\n\n**Published:** 2023-10-27\n\n\n\n#### 2. Powerpropagation: A sparsity inducing weight reparameterisation\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Yee Whye Teh, Peter E. Latham, Razvan Pascanu, Siddhant M. Jayakumar, Jonathan Schwarz*\n\n**Abstract:** The training of sparse neural networks is becoming an increasingly important tool for reducing the computational footprint of models at training and evaluation, as well enabling the effective scaling up of models. Whereas much work over the years has been dedicated to specialised pruning techniques, little attention has been paid to the inherent effect of gradient based training on model sparsity. In this work, we introduce Powerpropagation, a new weight-parameterisation for neural networks that leads to inherently sparse models. Exploiting the behaviour of gradient descent, our method gives rise to weight updates exhibiting a \"rich get richer\" dynamic, leaving low-magnitude parameters largely unaffected by learning. Models trained in this manner exhibit similar performance, but have a distribution with markedly higher density at zero, allowing more parameters to be pruned safely. Powerpropagation is general, intuitive, cheap and straight-forward to implement and can readily be combined with various other techniques. To highlight its versatility, we explore it in two very different settings: Firstly, following a recent line of work, we investigate its effect on sparse training for resource-constrained settings. Here, we combine Powerpropagation with a traditional weight-pruning technique as well as recent state-of-the-art sparse-to-sparse algorithms, showing superior performance on the ImageNet benchmark. Secondly, we advocate the use of sparsity in overcoming catastrophic forgetting, where compressed representations allow accommodating a large number of tasks at fixed model capacity. In all cases our reparameterisation considerably increases the efficacy of the off-the-shelf methods.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-10-01\n\n\n\n#### 3. FlexGen: High-Throughput Generative Inference of Large Language Models with a Single GPU\n\n*From Search Query: \"attention state compression techniques\"*\n\n*Ce Zhang, Ion Stoica, Christopher R\u00e9, Percy Liang, Joseph E. Gonzalez, Clark Barrett, Beidi Chen, Zhiqiang Xie, Daniel Y. Fu, Max Ryabinin, Zhuohan Li, Binhang Yuan, Lianmin Zheng, Ying Sheng*\n\n**Abstract:** The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen\n\n**Published:** 2023-03-13\n\n\n\n#### 4. Co-Scale Conv-Attentional Image Transformers\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Zhuowen Tu, Tyler Chang, Yifan Xu, Weijian Xu*\n\n**Abstract:** In this paper, we present Co-scale conv-attentional image Transformers (CoaT), a Transformer-based image classifier equipped with co-scale and conv-attentional mechanisms. First, the co-scale mechanism maintains the integrity of Transformers' encoder branches at individual scales, while allowing representations learned at different scales to effectively communicate with each other; we design a series of serial and parallel blocks to realize the co-scale mechanism. Second, we devise a conv-attentional mechanism by realizing a relative position embedding formulation in the factorized attention module with an efficient convolution-like implementation. CoaT empowers image Transformers with enriched multi-scale and contextual modeling capabilities. On ImageNet, relatively small CoaT models attain superior classification results compared with similar-sized convolutional neural networks and image/vision Transformers. The effectiveness of CoaT's backbone is also illustrated on object detection and instance segmentation, demonstrating its applicability to downstream computer vision tasks.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-13\n\n\n\n#### 5. Retentive Network: A Successor to Transformer for Large Language Models\n\n*From Search Query: \"parallel computation attention mechanisms\"*\n\n*Furu Wei, Jianyong Wang, Jilong Xue, Yuqing Xia, Shuming Ma, Shaohan Huang, Li Dong, Yutao Sun*\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n**Published:** 2023-07-17\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, focusing on the integration of control variates, attention state compression, and parallel computation, here are some specific points and references that can be useful:\n\n### Control Variate Implementation and Variance Reduction\n\n- **Theoretical Analysis and Practical Examples**:\n  Control variates can be integrated into attention mechanisms to reduce variance in attention approximations. For example, the work by Zheng et al. (2023) on \"Efficient Attention via Control Variates\" shows how control variates can be used to decompose the attention computation into multiple control variate estimators, each for an element in the sequence. This approach can help in stabilizing the attention weights and reducing variance.\n\n- **Stability Analysis**:\n  To ensure the stability of control variate methods, it is crucial to analyze the covariance between the control variate and the original estimator. This analysis can help in optimizing the parameters of the control variate to maximize variance reduction. Theoretical stability analysis can be found in papers that discuss the application of control variates in deep learning contexts.\n\n### Attention State Compression Techniques\n\n- **Dynamic Memory Compression (DMC)**:\n  DMC, as discussed by Nawrot et al. (2024), can be used to compress attention states efficiently. This method serves as a drop-in replacement for KV caching in existing large language models (LLMs), allowing for longer contexts and larger batches within a given memory budget. The trade-off between compression ratio and model performance needs careful consideration.\n\n- **Information Preservation**:\n  Studies on information preservation in compressed attention states are crucial. Techniques such as quantization, pruning, or other forms of compression need to be evaluated for their impact on model performance. Adaptive compression strategies that adjust the compression ratio based on the importance of the attention states can also be explored.\n\n### Parallel Computation and Hardware Optimization\n\n- **Block-Based Computation**:\n  The \"Lightning Attention\" approach by Qin et al. (2024) splits the attention calculation into intra-blocks and inter-blocks, using conventional attention for intra-blocks and linear attention kernel tricks for inter-blocks. This block-based computation can significantly improve efficiency and is a good example of parallel computation strategies.\n\n- **Memory Access Optimization**:\n  Optimizing memory access patterns is critical for performance. The use of memristor-based in-memory computing, as described in the Nature article on \"Efficient memristor accelerator for transformer self-attention,\" can perform matrix operations in an analog method, integrating computation into memory. This approach reduces the number of MAC (Multiply-Accumulate) operations and improves efficiency.\n\n- **Hardware-Aware Designs**:\n  Papers like \"Gated Linear Attention Transformers with Hardware-Efficient Training\" by Yang et al. (2023) provide insights into hardware-aware attention designs. These designs focus on optimizing the attention mechanisms for specific hardware architectures, which can lead to significant improvements in training and inference efficiency.\n\n### Additional Considerations\n\n- **Parallelism in Linear Attention**:\n  The work on \"Linear Attention Sequence Parallelism\" by Sun et al. (2024) discusses how to parallelize linear attention computations effectively. This involves breaking down the attention computation into parallelizable components, which can be executed simultaneously to speed up the overall process.\n\nBy focusing on these areas, researchers can develop more efficient, scalable, and robust autoregressive language models that leverage control variates for variance reduction, dynamic memory compression for state management, and optimized parallel computation strategies for hardware efficiency.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search results provide valuable insights into three key components of the ControlVariateLinearAttention (CVLA) proposal: control variates for variance reduction, dynamic memory compression, and hardware-efficient linear attention implementations. However, more specific information is needed in certain areas.\n\nUseful References with Key Excerpts:\n\n1. Control Variates & Variance Reduction:\n- From \"Efficient Attention via Control Variates\" (Zheng et al., 2023):\n\"Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence.\"\n\n2. Dynamic Memory Compression:\n- From \"Dynamic Memory Compression\" (Nawrot et al., 2024):\n\"DMC can serve as a drop-in replacement for KV caching in existing LLMs to fit longer contexts and larger batches within any given memory budget.\"\n\n3. Hardware-Efficient Linear Attention:\n- From \"Lightning Attention\" (Qin et al., 2024):\n\"We split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks.\"\n\nKey Insights:\n\n1. Control Variate Integration:\n- Control variates can effectively reduce variance in attention approximations\n- Need more specific examples of implementation in attention mechanisms\n- Integration with linear attention requires careful consideration\n\n2. Memory Compression:\n- Dynamic compression can significantly reduce memory footprint\n- Trade-off between compression ratio and model performance\n- Need more details on compression strategies for attention states\n\n3. Hardware Optimization:\n- Block-based computation can improve efficiency\n- Memory access patterns crucial for performance\n- Need more information on parallel computation strategies\n\nFuture Search Plan:\n\n1. Control Variate Implementation:\n- Need more specific examples of control variate applications in attention mechanisms\n- Search for theoretical analysis of variance reduction in deep learning\n- Look for stability analysis of control variate methods\n\n2. Memory Compression:\n- Search for compression techniques specifically for attention mechanisms\n- Look for studies on information preservation in compressed attention\n- Find examples of adaptive compression strategies\n\n3. Hardware Efficiency:\n- Search for implementation details of efficient linear attention\n- Look for studies on memory access optimization\n- Find examples of successful hardware-aware attention designs\n\nReferences:\n\n1. Zheng, L., Yuan, J., Wang, C., & Kong, L. (2023). Efficient Attention via Control Variates. International Conference on Learning Representations.\n\n2. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n3. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention. International Conference on Machine Learning.\n\n4. Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., & Zhong, Y. (2024). Linear Attention Sequence Parallelism. arXiv preprint.\n\n5. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"hierarchical control variate attention\", \"memory compression optimization\", \"hardware efficient attention implementation\"",
            "detail": "Search for specific implementations and analyses of:\n1. Hierarchical control variate structures in neural networks\n2. Memory-computation trade-offs in attention mechanisms\n3. Hardware-specific optimization techniques for attention computation\n4. Integration strategies for combining control variates with compressed states\n5. Stability analysis methods for variance reduction in attention",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for specific implementations and analyses of:\n1. Hierarchical control variate structures in neural networks\n2. Memory-computation trade-offs in attention mechanisms\n3. Hardware-specific optimization techniques for attention computation\n4. Integration strategies for combining control variates with compressed states\n5. Stability analysis methods for variance reduction in attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Compositional Attention: Disentangling Search and Retrieval (Avg. Score: 0.31)\n\n*Sarthak Mittal, S. Raparthy, I. Rish, Yoshua Bengio, Guillaume Lajoie*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel attention mechanism, called Compositional Attention, that replaces the standard head structure, and demonstrates that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings.\n\n**Abstract:** Multi-head, key-value attention is the backbone of the widely successful Transformer model and its variants. This attention mechanism uses multiple parallel key-value attention blocks (called heads), each performing two fundamental computations: (1) search - selection of a relevant entity from a set via query-key interactions, and (2) retrieval - extraction of relevant features from the selected entity via a value matrix. Importantly, standard attention heads learn a rigid mapping between search and retrieval. In this work, we first highlight how this static nature of the pairing can potentially: (a) lead to learning of redundant parameters in certain tasks, and (b) hinder generalization. To alleviate this problem, we propose a novel attention mechanism, called Compositional Attention, that replaces the standard head structure. The proposed mechanism disentangles search and retrieval and composes them in a dynamic, flexible and context-dependent manner through an additional soft competition stage between the query-key combination and value pairing. Through a series of numerical experiments, we show that it outperforms standard multi-head attention on a variety of tasks, including some out-of-distribution settings. Through our qualitative analysis, we demonstrate that Compositional Attention leads to dynamic specialization based on the type of retrieval needed. Our proposed mechanism generalizes multi-head attention, allows independent scaling of search and retrieval, and can easily be implemented in lieu of standard attention heads in any network architecture.\n\n##### *Relevant Chunk: No. 23/40 (Score: 0.31)*\n\n```\narXiv preprint arXiv:2012.14601, 2020. Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhudinov, Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In International conference on machine learning, pp. 2048-2057. PMLR, 2015. ## APPENDIX\n\n## A RELATED WORK\n\nThe advent of transformer-like models have led to advancements on various flavours of attention based models. This revolution first started with augmenting Recurrent Neural Networks (RNNs) with a form of semi-parametric memory structure through attention (Bahdanau et al., 2015) and it soon led to people questioning the need for recurrence. This line of questioning resulted in a famous class of models that get rid of recurrence in favour of just parallel self-attention computations that are quite efficient to do on modern hardware (Vaswani et al., 2017). We briefly discuss the various advances along these lines and distinguish how our proposed attention algorithm is different from them. ## A. 1 ATTENTION\n\nAttention has been a major component of human cognition which allows humans to selectively process relevant information from the plethora of sensory stimulus we receive. The idea of selecting relevant features from a sea of information allows us to make predictions in both a robust as well as compute efficient way. Inspired from neural cognition, there have been a lot of efforts in trying to introduce a notion of attention to relevant states of the input for reliable downstream prediction ( Xu et al., 2015; Luong et al., 2015; Kerg et al., 2020). A major problem in Recurrent Neural Networks based systems is the problem of vanishing and exploding gradients that happens due to improper credit assignment in the model. This is because RNNs model all the information seen up to a certain time through a parametric fixed sized vector which undergoes repeated computations over all time steps. This makes the system brittle to changes in sequence lengths or in presence of long sequence of distracting information. A way to solve this problem was to move away from parametric representations of the entire past and instead rely on dynamic semi-parametric \"memory\" to allow these models to look back whenever needed (Graves et al., 2014; Bahdanau et al., 2015). These works aimed at augmenting recurrence with self-attention and demonstrated that when combined with these cognition-inspired inductive biases, ML systems were able to extrapolate much better to larger sequence lengths. Following this, there has been a lot of recent work that then aimed to remove recurrence between timesteps and rely solely on querying information through self-attention. Recent advances on multiple domains (Vaswani et al., 2017; Dosovitskiy et al., 2020; Ding et al., 2020; Locatello et al., 2020) showcased that removing recurrence from the picture and relying solely on parallel computations not only leads to significant improvements in performance and generalization but is also easier and faster to train on current hardware. Since the advent of these transformer based models built fundamentally on multi-head attention, the role of attention has become increasingly important across various domains like vision, language and reinforcement learning. It has also led to a lot of research on various architectural choices in fully attention-based systems, some of which we discuss in Appendix A.2. It is, however, important to note that there has been some research that highlight the need for recurrence jointly with self-attention for solving certain logical reasoning tasks efficiently (Hudson \\& Manning, 2018; Selvakumar et al., 2018; Webb et al., 2020). ## A. 2 TRANSFORMER VARIANTS\n\nThe ubiquity of self-attention models in the current ML community has led to tremendous research aimed at incorporating different inductive biases in the attention mechanism used; namely in the multi-head attention. Most of these variants aim to alter multi-head attention in a way that would remove the quadratic time complexity computational bottleneck that is present in standard multi-head attention. However, there are certain works that aim more on the fundamental inductive biases that the attention encodes as opposed to computational benefits. We discuss some of these variants here. Reducing Computational Complexity. Given a set of $n$ vectors, the standard multi-head attention aims to create an $n \\times n$ attention matrix that takes quadratic complexity to compute. This bottleneck prevents usage of self-attention when $n$ is large. In light of this, a lot of recent research aims to reduce this quadratic complexity to $n \\log n$ or linear complexity. This is often achieved by either introducing some restrictions in the $n \\times n$ attention matrix through locality sensitive hashing (Kitaev et al., 2020),\nsparsity (Child et al., 2019), low rank approximation (Wang et al., 2020) or through random features for approximation of softmax (Choromanski et al., 2020).\n```\n\n#### 2. Efficient Attention via Control Variates (Avg. Score: 0.14)\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**Published in:** International Conference on Learning Representations (2023)\t**Cited by** 15  (*Influential: 1*)\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n##### *Relevant Chunk: No. 17/52 (Score: 0.33)*\n\n```\narXiv preprint arXiv:2101.00027, 2020. Tomas Geffner and Justin Domke. Using large ensembles of control variates for variational inference. Advances in Neural Information Processing Systems, 31, 2018. Will Grathwohl, Dami Choi, Yuhuai Wu, Geoff Roeder, and David Duvenaud. Backpropagation through the void: Optimizing control variates for black-box gradient estimation. In International Conference on Learning Representations, 2018. URL https://openreview. net/forum? id=SyzKd1bCW. Evan Greensmith, Peter L Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning.\n```\n\n##### *Relevant Chunk: No. 14/52 (Score: 0.05)*\n\n```\narXiv preprint arXiv:1409.0473, 2014. Jack Baker, Paul Fearnhead, Emily B Fox, and Christopher Nemeth. Control variates for stochastic gradient mcmc. Statistics and Computing, 29(3):599-615, 2019. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Ond\u0159ej Bojar, Christian Buck, Christian Federmann, Barry Haddow, Philipp Koehn, Johannes Leveling, Christof Monz, Pavel Pecina, Matt Post, Herve Saint-Amand, et al. Findings of the 2014 workshop on statistical machine translation. In Proceedings of the ninth workshop on statistical machine translation, pp. 12-58, 2014. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1877-1901. Curran Associates, Inc., 2020. URL https://proceedings.neurips. cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\n\nBeidi Chen, Tri Dao, Eric Winsor, Zhao Song, Atri Rudra, and Christopher R\u00e9. Scatterbrain: Unifying sparse and low-rank attention. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021a. Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, Atri Rudra, and Christopher Re. Pixelated butterfly: Simple and efficient sparse training for neural network models. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum? id=Nfl-iXa-y7R\n\nChun-Fu Chen, Rameswar Panda, and Quanfu Fan. Regionvit: Regional-to-local attention for vision transformers.\n```\n\n##### *Relevant Chunk: No. 33/52 (Score: 0.04)*\n\n```\nIn I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. URL https://proceedings.neurips.cc/paper/2017/file/ ebd6d2f5d60ff9afaeda1a81fc53e2d0-Paper.pdf. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pp. 5998-6008, 2017. Nikos Vlassis, Ashok Chandrashekar, Fernando Amat, and Nathan Kallus. Control variates for slate off-policy evaluation.\n```\n\n#### 3. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.11)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.11)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical control variate attention\", \"memory compression optimization\", \"hardware efficient attention implementation\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Efficient Attention via Control Variates\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 2. Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Wenhu Chen, Jianshu Chen, Pengda Qin, Xifeng Yan, William Yang Wang*\n\n**TL;DR:** A multi-layer hierarchical graph is exploited to build a hierarchical disentangled self-attention network, where each act is represented as a root-to-leaf route on the graph, and combinatorially many dialog act semantics can be modeled to control the neural response generation.\n\n**Abstract:** Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios is shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the largescale Multi-Domain-WOZ dataset, our algorithm can yield an improvement of over 5.0 BLEU score, and in human evaluation, it also significantly outperforms other baselines over various metrics including consistency, etc.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 124  (*Influential: 24*)\n\n#### 3. Universal Morphology Control via Contextual Modulation\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Zheng Xiong, Jacob Beck, Shimon Whiteson*\n\n**TL;DR:** A fixed attention mechanism that solely depends on the morphology to modulate the interactions between different limbs in a robot, which improves learning performance on a diverse set of training robots, but also generalizes better to unseen morphologies in a zero-shot fashion.\n\n**Abstract:** Learning a universal policy across different robot morphologies can significantly improve learning efficiency and generalization in continuous control. However, it poses a challenging multi-task reinforcement learning problem, as the optimal policy may be quite different across robots and critically depend on the morphology. Existing methods utilize graph neural networks or transformers to handle heterogeneous state and action spaces across different morphologies, but pay little attention to the dependency of a robot's control policy on its morphology context. In this paper, we propose a hierarchical architecture to better model this dependency via contextual modulation, which includes two key submodules: (1) Instead of enforcing hard parameter sharing across robots, we use hypernetworks to generate morphology-dependent control parameters; (2) We propose a fixed attention mechanism that solely depends on the morphology to modulate the interactions between different limbs in a robot. Experimental results show that our method not only improves learning performance on a diverse set of training robots, but also generalizes better to unseen morphologies in a zero-shot fashion.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 4. Network Memory Footprint Compression Through Jointly Learnable Codebooks and Mappings\n\n*From Search Query: \"memory compression optimization\"*\n\n*Edouard Yvinec, Arnaud Dapogny, K\u00e9vin Bailly*\n\n**TL;DR:** The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.\n\n**Abstract:** The massive interest in deep neural networks (DNNs) for both computer vision and natural language processing has been sparked by the growth in computational power. However, this led to an increase in the memory footprint, to a point where it can be challenging to simply load a model on commodity devices such as mobile phones. To address this limitation, quantization is a favored solution as it maps high precision tensors to a low precision, memory efficient format. In terms of memory footprint reduction, its most effective variants are based on codebooks. These methods, however, suffer from two limitations. First, they either define a single codebook for each tensor, or use a memory-expensive mapping to multiple codebooks. Second, gradient descent optimization of the mapping favors jumps toward extreme values, hence not defining a proximal search. In this work, we propose to address these two limitations. First, we initially group similarly distributed neurons and leverage the re-ordered structure to either apply different scale factors to the different groups, or map weights that fall in these groups to several codebooks, without any mapping overhead. Second, stemming from this initialization, we propose a joint learning of the codebook and weight mappings that bears similarities with recent gradient-based post-training quantization techniques. Third, drawing estimation from straight-through estimation techniques, we introduce a novel gradient update definition to enable a proximal search of the codebooks and their mappings. The proposed jointly learnable codebooks and mappings (JLCM) method allows a very efficient approximation of any DNN: as such, a Llama 7B can be compressed down to 2Go and loaded on 5-year-old smartphones.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Fast as CHITA: Neural Network Pruning with Combinatorial Optimization\n\n*From Search Query: \"memory compression optimization\"*\n\n*Riade Benbaki, Wenyu Chen, X. Meng, Hussein Hazimeh, N. Ponomareva, Zhe Zhao, R. Mazumder*\n\n**TL;DR:** This work proposes a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint, and extends the classical Optimal Brain Surgeon framework, CHITA, which leads to significantly better sparsity-accuracy tradeoffs than competing methods.\n\n**Abstract:** The sheer size of modern neural networks makes model serving a serious computational challenge. A popular class of compression techniques overcomes this challenge by pruning or sparsifying the weights of pretrained networks. While useful, these techniques often face serious tradeoffs between computational requirements and compression quality. In this work, we propose a novel optimization-based pruning framework that considers the combined effect of pruning (and updating) multiple weights subject to a sparsity constraint. Our approach, CHITA, extends the classical Optimal Brain Surgeon framework and results in significant improvements in speed, memory, and performance over existing optimization-based approaches for network pruning. CHITA's main workhorse performs combinatorial optimization updates on a memory-friendly representation of local quadratic approximation(s) of the loss function. On a standard benchmark of pretrained models and datasets, CHITA leads to significantly better sparsity-accuracy tradeoffs than competing methods. For example, for MLPNet with only 2% of the weights retained, our approach improves the accuracy by 63% relative to the state of the art. Furthermore, when used in conjunction with fine-tuning SGD steps, our method achieves significant accuracy gains over the state-of-the-art approaches.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 0*)\n\n#### 6. Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming\n\n*From Search Query: \"memory compression optimization\"*\n\n*Jinuk Kim, Yeonwoo Jeong, Deokjae Lee, Hyun Oh Song*\n\n**TL;DR:** A subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent Convolution operations for efficient end-to-end inference latency is proposed.\n\n**Abstract:** Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by TensorRT for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in MobileNetV2 on the ImageNet dataset. Specifically, we achieve $1.41\\times$ speed-up with $0.11$\\%p accuracy gain in MobileNetV2-1.0 on the ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 7. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: \"hardware efficient attention implementation\"*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. ShiftAddNAS: Hardware-Inspired Search for More Accurate and Efficient Neural Networks\n\n*From Search Query: \"hardware efficient attention implementation\"*\n\n*Haoran You, Baopu Li, Huihong Shi, Y. Fu, Yingyan Lin*\n\n**TL;DR:** This work proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs and integrates the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs.\n\n**Abstract:** Neural networks (NNs) with intensive multiplications (e.g., convolutions and transformers) are capable yet power hungry, impeding their more extensive deployment into resource-constrained devices. As such, multiplication-free networks, which follow a common practice in energy-efficient hardware implementation to parameterize NNs with more efficient operators (e.g., bitwise shifts and additions), have gained growing attention. However, multiplication-free networks usually under-perform their vanilla counterparts in terms of the achieved accuracy. To this end, this work advocates hybrid NNs that consist of both powerful yet costly multiplications and efficient yet less powerful operators for marrying the best of both worlds, and proposes ShiftAddNAS, which can automatically search for more accurate and more efficient NNs. Our ShiftAddNAS highlights two enablers. Specifically, it integrates (1) the first hybrid search space that incorporates both multiplication-based and multiplication-free operators for facilitating the development of both accurate and efficient hybrid NNs; and (2) a novel weight sharing strategy that enables effective weight sharing among different operators that follow heterogeneous distributions (e.g., Gaussian for convolutions vs. Laplacian for add operators) and simultaneously leads to a largely reduced supernet size and much better searched networks. Extensive experiments and ablation studies on various models, datasets, and tasks consistently validate the efficacy of ShiftAddNAS, e.g., achieving up to a +7.7% higher accuracy or a +4.9 better BLEU score compared to state-of-the-art NN, while leading to up to 93% or 69% energy and latency savings, respectively. Codes and pretrained models are available at https://github.com/RICE-EIC/ShiftAddNAS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 12  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. Neural Attention-Aware Hierarchical Topic Model\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Yuan Jin, He Zhao, Ming Liu, Lan Du, Wray Buntine*\n\n**Abstract:** Neural topic models (NTMs) apply deep neural networks to topic modelling.\nDespite their success, NTMs generally ignore two important aspects: (1) only\ndocument-level word count information is utilized for the training, while more\nfine-grained sentence-level information is ignored, and (2) external semantic\nknowledge regarding documents, sentences and words are not exploited for the\ntraining. To address these issues, we propose a variational autoencoder (VAE)\nNTM model that jointly reconstructs the sentence and document word counts using\ncombinations of bag-of-words (BoW) topical embeddings and pre-trained semantic\nembeddings. The pre-trained embeddings are first transformed into a common\nlatent topical space to align their semantics with the BoW embeddings. Our\nmodel also features hierarchical KL divergence to leverage embeddings of each\ndocument to regularize those of their sentences, thereby paying more attention\nto semantically relevant sentences. Both quantitative and qualitative\nexperiments have shown the efficacy of our model in 1) lowering the\nreconstruction errors at both the sentence and document levels, and 2)\ndiscovering more coherent topics from real-world datasets.\n\n**Published:** 2021-10-14T05:42:32Z  (*Updated: 2021-10-14T05:42:32Z*)\n\n\n\n#### 2. Cascaded Head-colliding Attention\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Lin Zheng, Zhiyong Wu, Lingpeng Kong*\n\n**Abstract:** Transformers have advanced the field of natural language processing (NLP) on\na variety of important tasks. At the cornerstone of the Transformer\narchitecture is the multi-head attention (MHA) mechanism which models pairwise\ninteractions between the elements of the sequence. Despite its massive success,\nthe current framework ignores interactions among different heads, leading to\nthe problem that many of the heads are redundant in practice, which greatly\nwastes the capacity of the model. To improve parameter efficiency, we\nre-formulate the MHA as a latent variable model from a probabilistic\nperspective. We present cascaded head-colliding attention (CODA) which\nexplicitly models the interactions between attention heads through a\nhierarchical variational distribution. We conduct extensive experiments and\ndemonstrate that CODA outperforms the transformer baseline, by $0.6$ perplexity\non \\texttt{Wikitext-103} in language modeling, and by $0.6$ BLEU on\n\\texttt{WMT14 EN-DE} in machine translation, due to its improvements on the\nparameter efficiency.\\footnote{Our implementation is publicly available at\n\\url{https://github.com/LZhengisme/CODA}.}\n\n**Published:** 2021-05-31T10:06:42Z  (*Updated: 2021-05-31T10:06:42Z*)\n\n\n\n#### 3. LoMA: Lossless Compressed Memory Attention\n\n*From Search Query: \"memory compression optimization\"*\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU\nmemory and computational resources when handling long contexts. While sparsify\nthe Key-Value (KV) cache of transformer model is a typical strategy to\nalleviate resource usage, it unavoidably results in the loss of information. We\nintroduce Lossless Compressed Memory Attention (LoMA), a novel approach that\nenables lossless compression of the KV cache, thereby reducing the memory and\ncomputational demands during autoregressive generation. LoMA incorporates a\nspecialized training or fine-tuning precedure alongside an autoregressive\ngeneration algorithm optimized for the compressed context. Our method\ncompresses the KV cache after every $tc$ generated tokens with a compression\nratio of $c$ and a target compressed length $t$, and this process occurs within\na single inference pass without dependency on auxiliary models. We engineered\nan efficient training scheme involving specific inputs, attention masks, and\nposition identifiers to instill this compression capability. Experimental\nvalidation has demonstrated that LoMA significantly reducing computational\nconsumption and memory usage through achieving lossless KV cache compression.\n\n**Published:** 2024-01-16T09:18:46Z  (*Updated: 2024-02-04T03:14:08Z*)\n\n\n\n#### 4. CompAct: Compressed Activations for Memory-Efficient LLM Training\n\n*From Search Query: \"memory compression optimization\"*\n\n*Yara Shamshoum, Nitzan Hodos, Yuval Sieradzki, Assaf Schuster*\n\n**Abstract:** We introduce CompAct, a technique that reduces peak memory utilization on GPU\nby 25-30% for pretraining and 50% for fine-tuning of LLMs. Peak device memory\nis a major limiting factor in training LLMs, with various recent works aiming\nto reduce model memory. However most works don't target the largest component\nof allocated memory during training: the model's compute graph, which is stored\nfor the backward pass. By storing low-rank, compressed activations to be used\nin the backward pass we greatly reduce the required memory, unlike previous\nmethods which only reduce optimizer overheads or the number of trained\nparameters. Our compression uses random projection matrices, thus avoiding\nadditional memory overheads. Comparisons with previous techniques for either\npretraining or fine-tuning show that CompAct substantially improves existing\ncompute-performance tradeoffs. We expect CompAct's savings to scale even higher\nfor larger models.\n\n**Published:** 2024-10-20T10:24:38Z  (*Updated: 2024-10-20T10:24:38Z*)\n\n\n\n#### 5. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient attention implementation\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 6. Linear Attention Sequence Parallelism\n\n*From Search Query: \"hardware efficient attention implementation\"*\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long\nsequences that exceed the memory limit of a single GPU. However, existing SP\nmethods do not take advantage of linear attention features, resulting in\nsub-optimal parallelism efficiency and usability for linear attention-based\nlanguage models. In this paper, we introduce Linear Attention Sequence Parallel\n(LASP), an efficient SP method tailored to linear attention-based language\nmodels. Specifically, we design an efficient point-to-point communication\nmechanism to leverage the right-product kernel trick of linear attention, which\nsharply decreases the communication overhead of SP. We also enhance the\npractical efficiency of LASP by performing kernel fusion and intermediate state\ncaching, making the implementation of LASP hardware-friendly on GPU clusters.\nFurthermore, we meticulously ensure the compatibility of sequence-level LASP\nwith all types of batch-level data parallel methods, which is vital for\ndistributed training on large clusters with long sequences and large batches.\nWe conduct extensive experiments on two linear attention-based models with\nvarying sequence lengths and GPU cluster sizes. LASP scales sequence length up\nto 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than\nexisting SP methods while being significantly faster. The code is available at\nhttps://github.com/OpenNLPLab/LASP.\n\n**Published:** 2024-04-03T17:33:21Z  (*Updated: 2024-04-03T17:33:21Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. VirtuosoNet: A Hierarchical RNN-based System for Modeling Expressive Piano Performance\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Juhan Nam, Kyogu Lee, Yoojin Kim, Taegyun Kwon, Dasaem Jeong*\n\n**Abstract:** In this paper, we present our application of deep neural network to modeling piano performance, which imitates the expressive control of tempo, dynamics, articulations and pedaling from pianists. Our model consists of recurrent neural networks with hierarchical attention and conditional variational autoencoder. The model takes a sequence of note-level score features extracted from MusicXML as input and predicts piano performance features of the corresponding notes. To render musical expressions consistently over long-term sections, we first predict tempo and dynamics in measure-level and, based on the result, refine them in note-level. The evaluation through listening test shows that our model achieves a more human-like expressiveness compared to previous models. We also share the dataset we used for the experiment.\n\n**Proceeding:** ismir-2019-11\n\n**Published:** 2019-11-04\n\n\n\n#### 2. Hierarchical Attention Fusion for Geo-Localization\n\n*From Search Query: \"hierarchical control variate attention\"*\n\n*Dongfang Liu, Yingjie Chen, Yiming Cui, Liqi Yan*\n\n**Abstract:** Geo-localization is a critical task in computer vision. In this work, we cast the geo-localization as a 2D image retrieval task. Current state-of-the-art methods for 2D geo-localization are not robust to locate a scene with drastic scale variations because they only exploit features from one semantic level for image representations. To address this limitation, we introduce a hierarchical attention fusion network using multi-scale features for geo-localization. We extract the hierarchical feature maps from a convolutional neural network (CNN) and organically fuse the extracted features for image representations. Our training is self-supervised using adaptive weights to control the attention of feature emphasis from each hierarchical level. Evaluation results on the image retrieval and the large-scale geo-localization benchmarks indicate that our method outperforms the existing state-of-the-art methods. Code is available here: \\url{https://github.com/YanLiqi/HAF}.\n\n**Published:** 2021-02-18\n\n\n\n#### 3. Pareto Optimal Compression of Genomic Dictionaries, with or without Random Access in Main Memory\n\n*From Search Query: \"memory compression optimization\"*\n\n*Anonymous*\n\n**Abstract:** Motivation: A Genomic Dictionary, i.e., the set of the k-mers appearing in a genome, is a fundamental source of genomic information: its collection is the first step in strategic computational methods ranging from assembly to sequence comparison and phylogeny. Unfortunately, it is costly to store. This motivates some recent studies regarding the compression of those k-mer sets. However, such an area does not have the maturity of genomic compression, lacking an homogeneous and methodologically sound experimental foundation that allows to fairly compare the relative merits of the available solutions, and that takes into account also the rich choices of compression methods that can be used. Results: We provide such a foundation here, supporting it with an extensive set of experiments that use reference datasets and a carefully selected set of representative data compressors. Our results highlight the spectrum of compressor choices one has in terms of Pareto Optimality of compression vs. post-processing, this latter being important when the Dictionary needs to be decompressed many times. In addition to the useful indications, not available elsewhere, that this study offers to the researchers interested in storing k-mer dictionaries in compressed form, a software system that can be readily used to explore the Pareto Optimal solutions available r a given Dictionary is also provided. Availability: The software system is available at https://github.com/GenGrim76/Pareto-Optimal-GDC, together with user manuals and installation instructions. Contact: raffaele.giancarlo@unipa.it Supplementary information: Additional data are available in the Supplementary Material.\n\n**Published:** 2022-12-06\n\n\n\n#### 4. SBNet: Sparse Blocks Network for Fast Inference\n\n*From Search Query: \"memory compression optimization\"*\n\n*Raquel Urtasun, Mengye Ren, Bin Yang, Andrei Pokrovsky*\n\n**Abstract:** Conventional deep convolutional neural networks (CNNs) apply convolution\noperators uniformly in space across all feature maps for hundreds of layers -\nthis incurs a high computational cost for real-time applications. For many\nproblems such as object detection and semantic segmentation, we are able to\nobtain a low-cost computation mask, either from a priori problem knowledge, or\nfrom a low-resolution segmentation network. We show that such computation masks\ncan be used to reduce computation in the high-resolution main network. Variants\nof sparse activation CNNs have previously been explored on small-scale tasks\nand showed no degradation in terms of object classification accuracy, but often\nmeasured gains in terms of theoretical FLOPs without realizing a practical\nspeed-up when compared to highly optimized dense convolution implementations.\nIn this work, we leverage the sparsity structure of computation masks and\npropose a novel tiling-based sparse convolution algorithm. We verified the\neffectiveness of our sparse CNN on LiDAR-based 3D object detection, and we\nreport significant wall-clock speed-ups compared to dense convolution without\nnoticeable loss of accuracy.\n\n**Conference:** sbnet-sparse-blocks-network-for-fast-1\n\n**Published:** 2018-01-07\n\n\n\n#### 5. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks\n\n*From Search Query: \"hardware efficient attention implementation\"*\n\n*Guoqi Li, Jason K. Eshraghian, Qihang Zhao, Rui-Jie Zhu*\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n**Published:** 2023-02-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the concepts of hierarchical control variate attention, memory compression optimization, and hardware-efficient attention implementation, here are some key points and references that can guide the research:\n\n## Hierarchical Control Variate Structures in Neural Networks\n\n- The concept of hierarchical control variates can be extended from the idea of using multiple control variate estimators for each element in a sequence. This involves decomposing the attention mechanism into hierarchical layers, each applying control variates to reduce variance. The work by Zheng et al. (2023) on \"Efficient Attention via Control Variates\" provides a foundation for this, suggesting that hierarchical structures could be built by layering multiple control variate estimators[5,.\n\n## Memory-Computation Trade-offs in Attention Mechanisms\n\n- Dynamic memory compression techniques are crucial for optimizing memory usage in attention mechanisms. The paper by Nawrot et al. (2024) on \"Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\" discusses adaptive compression strategies that balance compression ratio and model performance. This work highlights the trade-offs between computational overhead and memory usage, which is essential for optimizing attention mechanisms.\n\n## Hardware-Specific Optimization Techniques for Attention Computation\n\n- Hardware-aware optimization techniques, such as block-based computation and optimized memory access patterns, are vital for efficient attention computation. The \"Lightning Attention-2\" paper by Qin et al. (2024) and the \"Linear Attention Sequence Parallelism\" paper by Sun et al. (2024) provide insights into tiling strategies, intra-block and inter-block computations, and point-to-point communication mechanisms that reduce communication overhead and optimize hardware usage[5,.\n\n## Integration Strategies for Combining Control Variates with Compressed States\n\n- Integrating control variates with compressed states involves careful management of the trade-offs between variance reduction and computational overhead. The work by Zheng et al. (2023) and the insights from \"Linear Attention Sequence Parallelism\" suggest that adaptive control variate coefficients and dynamic compression strategies can be combined to achieve efficient and stable attention mechanisms. This integration can be facilitated by designing hierarchical structures that leverage both control variates and compressed states efficiently[5,.\n\n## Stability Analysis Methods for Variance Reduction in Attention\n\n- Stability analysis for variance reduction in attention mechanisms is crucial to ensure that the hierarchical control variate structures and compression techniques do not compromise the model's stability. This can be approached by analyzing the impact of control variates on the variance of attention weights and ensuring that the compression techniques do not introduce significant errors. The stability analysis can be informed by the quantitative analysis of memory-computation trade-offs and the impact of hierarchical structures on variance reduction.\n\n### Additional References and Insights\n\n- **Gated Linear Attention Transformers**: The work by Yang et al. (2023) on \"Gated Linear Attention Transformers with Hardware-Efficient Training\" provides insights into hardware-efficient training methods that can be integrated with hierarchical control variate attention and memory compression techniques.\n\n- **Linear Attention Kernel Tricks**: The use of linear attention kernel tricks, as discussed in \"Lightning Attention-2,\" can be particularly beneficial in optimizing the computation of attention mechanisms, especially when combined with hierarchical control variate structures and dynamic memory compression.\n\nBy focusing on these areas and integrating the insights from the referenced papers, researchers can develop more efficient, scalable, and stable autoregressive language models that leverage hierarchical control variate attention, memory compression optimization, and hardware-efficient implementation techniques.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results provide valuable insights into three key aspects of the ControlVariateLinearAttention (CVLA) proposal:\n\na) Control Variate Integration:\n- Novel approaches to variance reduction in attention mechanisms\n- Implementation strategies for control variates in neural networks\n- Trade-offs between computational overhead and variance reduction\n\nb) Memory Compression:\n- Dynamic compression techniques for attention states\n- Information preservation strategies\n- Adaptive compression approaches\n\nc) Parallel Computation:\n- Block-based computation strategies\n- Hardware-aware optimization techniques\n- Memory access patterns optimization\n\n2. Useful References with Key Excerpts:\n\nFrom \"Efficient Attention via Control Variates\" (Zheng et al., 2023):\n\"Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence.\"\n\nFrom \"Lightning Attention-2\" (Qin et al., 2024):\n\"We leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks.\"\n\nFrom \"Linear Attention Sequence Parallelism\" (Sun et al., 2024):\n\"We design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP.\"\n\n3. Key Insights:\n\na) Control Variate Implementation:\n- Need for careful balance between variance reduction and computational overhead\n- Importance of adaptive control variate coefficients\n- Potential for hierarchical control variate structures\n\nb) Memory Management:\n- Trade-off between compression ratio and model performance\n- Importance of dynamic compression strategies\n- Need for efficient state management\n\nc) Hardware Optimization:\n- Critical role of memory access patterns\n- Benefits of block-based computation\n- Importance of hardware-aware design\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Specific implementations of hierarchical control variates\n2. Quantitative analysis of memory-computation trade-offs\n3. Hardware-specific optimization techniques\n4. Integration strategies for all components\n5. Stability analysis methods\n\n5. References:\n\n1. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n2. Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., & Zhong, Y. (2024). Linear Attention Sequence Parallelism. arXiv preprint.\n\n3. Zheng, L., Yuan, J., Wang, C., & Kong, L. (2023). Efficient Attention via Control Variates. International Conference on Learning Representations.\n\n4. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n5. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.997341,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.086288,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "AdaptiveLinearAttention",
    "proposal": "## Title: ControlVariateLinearAttention: Enhancing Stability and Efficiency in Linear Attention through Control Variates and Dynamic Memory Compression\n\n### Motivation\n\nModern autoregressive language models face significant challenges in maintaining stability during training and managing memory efficiently during inference, particularly for long sequences. While AdaptiveLinearAttention (ALA) provides a strong foundation through linear attention and meta-learning, there remain opportunities for improvement in:\n\n1. **Training Stability**: Linear attention approximations can introduce variance that affects training stability and model performance.\n2. **Memory Efficiency**: Current approaches to memory management in attention mechanisms often lead to inefficient resource utilization.\n3. **Scalability**: Existing designs struggle to maintain performance when scaling to longer sequences.\n4. **Hardware Utilization**: Memory access patterns and computation organization often fail to fully utilize modern hardware capabilities.\n\n### Problem Analysis\n\n#### Current Limitations\n\n1. **Variance Issues**:\n   - Linear attention approximations introduce variance that can destabilize training\n   - Meta-learning components may suffer from noisy gradients\n   - Lack of explicit variance control mechanisms\n\n2. **Memory Management**:\n   - Inefficient storage of attention states\n   - Static memory allocation regardless of content importance\n   - Limited ability to handle very long sequences\n\n3. **Computational Efficiency**:\n   - Suboptimal organization of computations\n   - Inefficient memory access patterns\n   - Limited hardware utilization\n\n#### Opportunities for Improvement\n\n1. **Control Variate Integration**:\n   - Reduce variance in attention approximations\n   - Stabilize training through principled variance reduction\n   - Enable more aggressive optimization\n\n2. **Dynamic Memory Compression**:\n   - Adaptive compression of attention states\n   - Content-aware memory management\n   - Efficient handling of long sequences\n\n3. **Hardware-Aware Design**:\n   - Optimize memory access patterns\n   - Improve computational organization\n   - Better utilize modern hardware capabilities\n\n### Core Idea and Philosophy\n\nControlVariateLinearAttention (CVLA) introduces three key innovations:\n\n1. **Control Variate Integration**: Incorporate control variates into linear attention to reduce variance and improve stability.\n2. **Dynamic Memory Compression**: Implement adaptive memory management for efficient resource utilization.\n3. **Hardware-Optimized Implementation**: Design computation patterns for modern hardware architectures.\n\n### Design Plan\n\n#### Selected GAU to Modify\n```selection\nAdaptiveLinearAttention\n```\n\n#### Modification Overview\n\n1. **Control Variate Integration**:\n   - Add control variate terms to linear attention computation\n   - Implement variance reduction through importance sampling\n   - Maintain theoretical guarantees for convergence\n\n2. **Dynamic Memory Management**:\n   - Implement adaptive compression of attention states\n   - Design content-aware memory allocation\n   - Enable efficient long sequence processing\n\n3. **Hardware Optimization**:\n   - Optimize memory access patterns\n   - Implement efficient computation organization\n   - Enable better hardware utilization\n\n### Mathematical Formulation\n\n1. **Control Variate Attention**:\n   Let Q, K, V be query, key, and value matrices. The control variate attention is computed as:\n\n   ```\n   CVAttention(Q, K, V) = LinearAttention(Q, K, V) + ControlVariate(Q, K, V)\n   ```\n\n   where:\n   ```\n   LinearAttention(Q, K, V) = (QK^T)V\n   ControlVariate(Q, K, V) = h(Q, K, V) - E[h(Q, K, V)]\n   ```\n\n   h(Q, K, V) is a control function designed to reduce variance.\n\n2. **Dynamic Memory Compression**:\n   Memory compression ratio R is computed as:\n\n   ```\n   R = \u03c3(f(S, I))\n   ```\n\n   where:\n   - S is the current memory state\n   - I is the importance score\n   - f is a learnable function\n   - \u03c3 is the sigmoid activation\n\n3. **Hardware-Optimized Implementation**:\n   Implement blocked computation:\n\n   ```\n   for each block b in sequence:\n       Q_b, K_b, V_b = get_block(b)\n       C_b = compute_control_variate(Q_b, K_b, V_b)\n       A_b = compute_attention(Q_b, K_b, V_b)\n       Y_b = A_b + C_b\n   ```\n\n### Implementation Guidelines\n\n```python\nclass ControlVariateLinearAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Linear projections\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        \n        # Control variate components\n        self.control_fn = nn.Linear(embed_dim, embed_dim)\n        self.control_gate = nn.Linear(embed_dim, 1, bias=False)\n        \n        # Memory compression\n        self.compress_fn = nn.Linear(2 * embed_dim, 1)\n        self.memory_norm = nn.LayerNorm(embed_dim)\n        \n        # Output projection\n        self.out_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def compute_control_variate(self, q, k, v):\n        # Compute control function\n        h_qk = self.control_fn(q @ k.transpose(-2, -1))\n        h_v = torch.matmul(h_qk, v)\n        \n        # Compute expected value (approximation)\n        h_expected = torch.mean(h_v, dim=-2, keepdim=True)\n        \n        # Control variate term\n        cv = h_v - h_expected\n        cv_weight = torch.sigmoid(self.control_gate(q))\n        \n        return cv * cv_weight\n    \n    def compress_memory(self, state, importance):\n        # Compute compression ratio\n        combined = torch.cat([state, importance], dim=-1)\n        ratio = torch.sigmoid(self.compress_fn(combined))\n        \n        # Apply compression\n        compressed = self.memory_norm(state * ratio)\n        return compressed\n    \n    def forward(self, x, memory_state=None):\n        batch_size, seq_len, _ = x.shape\n        \n        # Project inputs\n        q = self.q_proj(x).view(batch_size, seq_len, self.num_heads, -1)\n        k = self.k_proj(x).view(batch_size, seq_len, self.num_heads, -1)\n        v = self.v_proj(x).view(batch_size, seq_len, self.num_heads, -1)\n        \n        # Compute importance scores\n        importance = torch.norm(q, dim=-1, keepdim=True)\n        \n        # Compress memory if provided\n        if memory_state is not None:\n            memory_state = self.compress_memory(memory_state, importance)\n        \n        # Compute linear attention\n        attn = torch.matmul(q, k.transpose(-2, -1))\n        attn_output = torch.matmul(attn, v)\n        \n        # Add control variate\n        cv = self.compute_control_variate(q, k, v)\n        output = attn_output + cv\n        \n        # Project output\n        output = output.view(batch_size, seq_len, -1)\n        output = self.out_proj(output)\n        \n        return output, memory_state\n```\n\n### Integration Steps\n\n1. **Replace Attention Mechanism**:\n   - Implement control variate computation\n   - Add memory compression\n   - Maintain compatibility with existing architecture\n\n2. **Optimize Memory Management**:\n   - Implement efficient memory compression\n   - Design adaptive memory allocation\n   - Enable efficient state updates\n\n3. **Hardware Optimization**:\n   - Implement blocked computation\n   - Optimize memory access patterns\n   - Enable efficient parallelization\n\n### Theoretical Analysis\n\n1. **Variance Reduction**:\n   - Control variates provide unbiased estimation with reduced variance\n   - Theoretical guarantees for convergence under certain conditions\n   - Bounded memory requirements through compression\n\n2. **Memory Efficiency**:\n   - Adaptive compression enables efficient resource utilization\n   - Linear memory scaling with sequence length\n   - Bounded compression error\n\n3. **Computational Complexity**:\n   - Maintains linear complexity O(n) with sequence length\n   - Efficient parallel computation\n   - Optimal memory access patterns\n\n### Expected Benefits\n\n1. **Improved Stability**:\n   - Reduced variance in attention computation\n   - More stable training process\n   - Better convergence properties\n\n2. **Enhanced Efficiency**:\n   - Reduced memory requirements\n   - Better hardware utilization\n   - Faster inference\n\n3. **Better Scalability**:\n   - Efficient handling of long sequences\n   - Linear memory scaling\n   - Improved parallel processing\n\n### References\n\n1. Kong, L., Wang, C., & Zheng, L. (2022). Linear Complexity Randomized Self-attention Mechanism. International Conference on Machine Learning.\n\n2. Nawrot, P., La'ncucki, A., Chochowski, M., Tarjan, D., & Ponti, E. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. International Conference on Machine Learning.\n\n3. Peng, H., Kasai, J., Pappas, N., Yogatama, D., Wu, Z., Kong, L., Schwartz, R., & Smith, N.A. (2021). ABC: Attention with Bounded-memory Control. Annual Meeting of the Association for Computational Linguistics.\n\n4. Tian, Y., Wang, Y., Zhang, Z., Chen, B., & Du, S.S. (2023). JoMA: Demystifying Multilayer Transformers via Joint Dynamics of MLP and Attention. International Conference on Learning Representations.\n\n5. Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., & Kwok, J. (2021). SparseBERT: Rethinking the Importance Analysis in Self-attention. International Conference on Machine Learning.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "This proposal introduces ControlVariateLinearAttention (CVLA), an enhancement to AdaptiveLinearAttention that integrates control variates with dynamic memory compression for improved stability and efficiency. By combining variance reduction techniques with adaptive memory management, CVLA achieves better training stability and inference performance while maintaining linear complexity. The design leverages theoretical insights from stochastic optimization and memory compression to create a more robust and scalable attention mechanism.",
    "ideation": null,
    "modelname": "gpt_2_cvla",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of control variate integration with memory compression\n- Include concrete examples of dynamic compression strategies\n- Add benchmarking guidelines for different components\n- Specify initialization strategies for control variate coefficients\n\n2. Theoretical Analysis:\n- Expand on stability analysis for combined variance reduction and compression\n- Include formal proof of convergence under compression\n- Provide more detailed analysis of memory scaling\n- Add theoretical guarantees for variance reduction\n\n3. Memory Management:\n- Elaborate on compression strategy selection\n- Include failure mode analysis\n- Add guidelines for handling very long sequences\n- Specify recovery mechanisms for compressed states\n\n4. Hardware Optimization:\n- Add specific guidance for different GPU architectures\n- Include strategies for memory bandwidth optimization\n- Provide more detailed cache optimization guidelines\n- Specify requirements for different scales of deployment\n\n5. Integration Guidelines:\n- Add more detailed API specifications\n- Include example configurations\n- Provide debugging guidelines\n- Add performance optimization tips\n\n6. Evaluation Metrics:\n- Define specific metrics for measuring efficiency gains\n- Include guidelines for comparing with baseline models\n- Add ablation study recommendations\n- Specify benchmarking procedures",
    "user_input": ""
}