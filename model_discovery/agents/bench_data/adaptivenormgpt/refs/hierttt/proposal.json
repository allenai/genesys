{
    "variantname": null,
    "review": "The HierTTT proposal presents a novel approach to enhancing TTT by introducing a two-stream architecture that combines test-time training capabilities with efficient multi-stream processing. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.5/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n- Well-defined objectives and motivation\n- Detailed architectural specifications\n- Clear implementation guidelines\n- Comprehensive theoretical analysis\n\nINNOVATION (4.2/5):\nThe proposal demonstrates significant innovation through:\n- Novel integration of two-stream processing with test-time training\n- Unique approach to memory management through bounded contexts\n- Original stream fusion mechanism\nComparison with existing research shows this combination has not been previously explored in this context.\n\nFEASIBILITY (3.8/5):\nImplementation appears feasible but with some challenges:\n+ Clear implementation guidelines and pseudo-code provided\n+ Built on proven components from parent designs\n- Complex stream synchronization might require careful tuning\n- Stream fusion mechanism needs more detailed specification\n\nSCALABILITY (4.0/5):\nThe design shows promising scalability characteristics:\n+ Linear complexity maintained through bounded contexts\n+ Parallel processing potential through two-stream architecture\n+ Efficient memory management through chunk-based processing\n- Need for more detailed analysis of memory-computation trade-offs\n\nACCURACY AND ROBUSTNESS (4.2/5):\nExpected improvements in model performance:\n+ Enhanced ability to capture both local and global patterns\n+ Improved handling of long-range dependencies\n+ Adaptive processing through test-time training\n+ Redundancy through dual-stream architecture increases robustness\n\nEFFICIENCY (4.0/5):\nThe design offers several efficiency improvements:\n+ Reduced memory usage through bounded contexts\n+ Parallel processing capabilities\n+ Efficient stream fusion mechanism\n- Potential overhead from maintaining two streams\n\nSTRENGTHS:\n1. Novel combination of test-time training with multi-stream processing\n2. Well-thought-out memory management through bounded contexts\n3. Clear potential for improved handling of long-range dependencies\n4. Maintains linear complexity while enhancing model capabilities\n5. Strong theoretical foundation and justification\n\nCONCERNS:\n1. Stream synchronization complexity needs more detailed specification\n2. Memory-computation trade-offs require further analysis\n3. Training stability with dual streams needs careful consideration\n4. Implementation complexity might affect adoption",
    "search_stack": [
        {
            "ready": false,
            "query": "memory efficient attention mechanisms, adaptive neural networks, local global context processing",
            "detail": "Analyze recent developments in combining local and global context processing in language models, focusing on approaches that maintain linear complexity while maximizing expressiveness. Include specific attention to memory efficiency and adaptive mechanisms.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in combining local and global context processing in language models, focusing on approaches that maintain linear complexity while maximizing expressiveness. Include specific attention to memory efficiency and adaptive mechanisms.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Scaling Laws of RoPE-based Extrapolation (Avg. Score: 0.88)\n\n*Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 36  (*Influential: 4*)\n\n**TL;DR:** This work proposes a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length, and achieves extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n**Abstract:** The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.88)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/ P16-1144. URL https://doi.org/10.18653/v1/p16-1144. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URLhttps://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.\n```\n\n#### 2. Adapting Language Models to Compress Contexts (Avg. Score: 0.66)\n\n*Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 75  (*Influential: 11*)\n\n**TL;DR:** AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts and the benefits of pre-computing summary vectors for large corpora are explored.\n\n**Abstract:** Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.66)*\n\n```\n# Adapting Language Models to Compress Contexts \n\nAlexis Chevalier* Alexander Wettig* Anirudh Ajith Danqi Chen<br>Department of Computer Science \\& Princeton Language and Intelligence<br>Princeton University<br>\\{achevalier, anirudh.ajith\\}@princeton.edu<br>\\{awettig, danqic\\}@cs.princeton.edu\n\n\n#### Abstract\n\nTransformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.\n```\n\n#### 3. LongRoPE: Extending LLM Context Window Beyond 2 Million Tokens (Avg. Score: 0.60)\n\n*Yiran Ding, L. Zhang, Chengruidong Zhang, Yuanyuan Xu, Ning Shang, Jiahang Xu, Fan Yang, Mao Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 36  (*Influential: 1*)\n\n**TL;DR:** This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window.\n\n**Abstract:** Large context window is a desirable feature in large language models (LLMs). However, due to high fine-tuning costs, scarcity of long texts, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128k tokens. This paper introduces LongRoPE that, for the first time, extends the context window of pre-trained LLMs to an impressive 2048k tokens, with up to only 1k fine-tuning steps at within 256k training lengths, while maintaining performance at the original short context window. This is achieved by three key innovations: (i) we identify and exploit two forms of non-uniformities in positional interpolation through an efficient search, providing a better initialization for fine-tuning and enabling an 8x extension in non-fine-tuning scenarios; (ii) we introduce a progressive extension strategy that first fine-tunes a 256k length LLM and then conducts a second positional interpolation on the fine-tuned extended LLM to achieve a 2048k context window; (iii) we readjust LongRoPE on 8k length to recover the short context window performance. Extensive experiments on LLaMA2 and Mistral across various tasks demonstrate the effectiveness of our method. Models extended via LongRoPE retain the original architecture with minor modifications to the positional embedding, and can reuse most pre-existing optimizations.\n\n##### *Relevant Chunk: No. 26/32 (Score: 0.60)*\n\n```\nAdvances in Neural Information Processing Systems, 33: 7537-7547, 2020. Together, 2023. URL https://huggingface.co/ togethercomputer/LLaMA-2-7B-32K. Touvron, H., Martin, L., Stone, K., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S., Bhargava, P., Bhosale, S., Bikel, D., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu, D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A., Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez,\nV., Khabsa, M., Kloumann, I., Korenev, A., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X., Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K., Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X. E., Tang, B., Taylor, R., Williams, A., Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez, A., Stojnic, R., Edunov, S., and Scialom, T. Llama 2: Open foundation and fine-tuned chat models, 2023. Tworkowski, S., Staniszewski, K., Pacek, M., Wu, Y., Michalewski, H., and Mi\u0142o\u015b, P. Focused transformer: Contrastive training for context scaling. 2023. Wang, W., Dong, L., Cheng, H., Liu, X., Yan, X., Gao, J., and Wei, F. Augmenting language models with long-term memory. arXiv preprint arXiv:2306.07174, 2023. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. $\\operatorname{arXiv}, 2023$. Xiong, W., Liu, J., Molybog, I., Zhang, H., Bhargava, P., Hou, R., Martin, L., Rungta, R., Sankararaman, K. A., Oguz, B., Khabsa, M., Fang, H., Mehdad, Y., Narang, S., Malik, K., Fan, A., Bhosale, S., Edunov, S., Lewis, M., Wang, S., and Ma, H. Effective long-context scaling of foundation models, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., and Dou, Z. Soaring from 4 k to 400 k : Extending llm's context with activation beacon. arXiv preprint arXiv:2401.03462, 2024. Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., and Li, S. Pose: Efficient context window extension of llms via positional skip-wise training, 2023.\n```\n\n#### 4. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.49)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 16/46 (Score: 0.49)*\n\n```\nHakan Inan, Khashayar Khosravi, and Richard Socher. 2016. Tying word vectors and word classifiers: A loss framework for language modeling. arXiv preprint arXiv:1611.01462. Yangfeng Ji, Trevor Cohn, Lingpeng Kong, Chris Dyer, and Jacob Eisenstein. 2015. Document context language models. arXiv preprint arXiv:1511.03962. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. 2016. Exploring the limits of language modeling. arXiv preprint arXiv:1602.02410. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Koray Kavukcuoglu. 2016. Neural machine translation in linear time. arXiv preprint arXiv:1610.10099. Sekitoshi Kanai, Yasuhiro Fujiwara, Yuki Yamanaka, and Shuichi Adachi. 2018. Sigsoftmax: Reanalysis of the softmax bottleneck. arXiv preprint arXiv:1805.10829. Nan Rosemary Ke, Anirudh Goyal ALIAS PARTH GOYAL, Olexa Bilaniuk, Jonathan Binas, Michael C Mozer, Chris Pal, and Yoshua Bengio. 2018. Sparse attentive backtracking: Temporal credit assignment through reminding. In Advances in Neural Information Processing Systems, pages 7650-7661. Urvashi Khandelwal, He He, Peng Qi, and Dan Jurafsky. 2018. Sharp nearby, fuzzy far away: How neural language models use context. arXiv preprint arXiv:1805.04623.\n```\n\n#### 5. Extending Context Window of Large Language Models via Semantic Compression (Avg. Score: 0.39)\n\n*WeiZhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning.\n\n**Abstract:** Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.\n\n##### *Relevant Chunk: No. 8/21 (Score: 0.39)*\n\n```\nURL https://doi.org/10.5281/ zenodo. 5196577 . Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023a. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv, 2023b. Alexis Chevalier, Alexander Wettig, Anirudh Ajith, and Danqi Chen. Adapting language models to compress contexts. ArXiv, abs/2305.14788, 2023. URL https://api. semanticscholar.org/CorpusID:258865249. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhutdinov. Transformer-XL: Attentive language models beyond a fixed-length context. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 2978-2988, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1285. URL https://aclanthology.org/P19-1285. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: memory efficient attention mechanisms, adaptive neural networks, local global context processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: memory efficient attention mechanisms*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 2. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: memory efficient attention mechanisms*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 3. Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention\n\n*From Search Query: memory efficient attention mechanisms*\n\n*Huiyin Xue, Nikolaos Aletras*\n\n**TL;DR:** This work proposes an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head, and empirically demonstrates that this MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks.\n\n**Abstract:** Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 4. Adaptive Neural Networks for Efficient Inference\n\n*From Search Query: adaptive neural networks*\n\n*Tolga Bolukbasi, Joseph Wang, O. Dekel, Venkatesh Saligrama*\n\n**TL;DR:** It is shown that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples.\n\n**Abstract:** We present an approach to adaptively utilize deep neural networks in order to reduce the evaluation time on new examples without loss of accuracy. Rather than attempting to redesign or approximate existing networks, we propose two schemes that adaptively utilize networks. We first pose an adaptive network evaluation scheme, where we learn a system to adaptively choose the components of a deep network to be evaluated for each example. By allowing examples correctly classified using early layers of the system to exit, we avoid the computational time associated with full evaluation of the network. We extend this to learn a network selection system that adaptively selects the network to be evaluated for each example. We show that computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks and that complex, computationally costly networks are only necessary for a small fraction of examples. We pose a global objective for learning an adaptive early exit or network selection policy and solve it by reducing the policy learning problem to a layer-by-layer weighted binary classification problem. Empirically, these approaches yield dramatic reductions in computational cost, with up to a 2.8x speedup on state-of-the-art networks from the ImageNet image recognition challenge with minimal (<1%) loss of top5 accuracy.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 318  (*Influential: 26*)\n\n#### 5. Adaptive Smoothing Gradient Learning for Spiking Neural Networks\n\n*From Search Query: adaptive neural networks*\n\n*Ziming Wang, Runhao Jiang, S. Lian, Rui Yan, Huajin Tang*\n\n**TL;DR:** This work proposes a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise, in this way, the network learns adaptively the accurate gradients of loss landscape in SNNs.\n\n**Abstract:** Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is pro-hibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise. In this way, the network learns adaptively the accurate gradients of loss landscape in SNNs. The theoretical analysis further shows optimization on such a noisy network could be evolved into optimization on the embedded SNN with shared weights progressively. Moreover, The experiments on static images, dynamic event streams, speech, and instrumental sounds show the proposed method achieves state-of-the-art performance across all the datasets with remarkable robustness on different relaxation degrees.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 6. ASAM: Adaptive Sharpness-Aware Minimization for Scale-Invariant Learning of Deep Neural Networks\n\n*From Search Query: adaptive neural networks*\n\n*Jungmin Kwon, Jeongseop Kim, Hyunseong Park, I. Choi*\n\n**TL;DR:** A novel learning method, adaptive sharpness-aware minimization (ASAM), is suggested utilizing the proposed generalization bound, and experimental results show that ASAM contributes to significant improvement of model generalization performance.\n\n**Abstract:** Recently, learning algorithms motivated from sharpness of loss surface as an effective measure of generalization gap have shown state-of-the-art performances. Nevertheless, sharpness defined in a rigid region with a fixed radius, has a drawback in sensitivity to parameter re-scaling which leaves the loss unaffected, leading to weakening of the connection between sharpness and generalization gap. In this paper, we introduce the concept of adaptive sharpness which is scale-invariant and propose the corresponding generalization bound. We suggest a novel learning method, adaptive sharpness-aware minimization (ASAM), utilizing the proposed generalization bound. Experimental results in various benchmark datasets show that ASAM contributes to significant improvement of model generalization performance.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 235  (*Influential: 43*)\n\n#### 7. Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context\n\n*From Search Query: local global context processing*\n\n*Xinnian Liang, Shuangzhi Wu, Mu Li, Zhoujun Li*\n\n**TL;DR:** This paper proposes a novel method for UKE, where local and global contexts are jointly modeled, and proposes a new centrality computation method to capture local salient information based on the graph structure.\n\n**Abstract:** Embedding based methods are widely used for unsupervised keyphrase extraction (UKE) tasks. Generally, these methods simply calculate similarities between phrase embeddings and document embedding, which is insufficient to capture different context for a more effective UKE model. In this paper, we propose a novel method for UKE, where local and global contexts are jointly modeled. From a global view, we calculate the similarity between a certain phrase and the whole document in the vector space as transitional embedding based models do. In terms of the local view, we first build a graph structure based on the document where phrases are regarded as vertices and the edges are similarities between vertices. Then, we proposed a new centrality computation method to capture local salient information based on the graph structure. Finally, we further combine the modeling of global and local context for ranking. We evaluate our models on three public benchmarks (Inspec, DUC 2001, SemEval 2010) and compare with existing state-of-the-art models. The results show that our model outperforms most models while generalizing better on input documents with different domains and length. Additional ablation study shows that both the local and global information is crucial for unsupervised keyphrase extraction tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 56  (*Influential: 15*)\n\n#### 8. Extractive Summarization of Long Documents by Combining Global and Local Context\n\n*From Search Query: local global context processing*\n\n*Wen Xiao, G. Carenini*\n\n**TL;DR:** A novel neural single-document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic, where it outperforms previous work, both extractive and abstractive models.\n\n**Abstract:** In this paper, we propose a novel neural single-document extractive summarization model for long documents, incorporating both the global context of the whole document and the local context within the current topic. We evaluate the model on two datasets of scientific papers , Pubmed and arXiv, where it outperforms previous work, both extractive and abstractive models, on ROUGE-1, ROUGE-2 and METEOR scores. We also show that, consistently with our goal, the benefits of our method become stronger as we apply it to longer documents. Rather surprisingly, an ablation study indicates that the benefits of our model seem to come exclusively from modeling the local context, even for the longest documents.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 140  (*Influential: 20*)\n\n#### 9. Coupling Global and Local Context for Unsupervised Aspect Extraction\n\n*From Search Query: local global context processing*\n\n*Ming Liao, Jing Li, Haisong Zhang, Lingzhi Wang, Xixin Wu, Kam-Fai Wong*\n\n**TL;DR:** A novel neural model is proposed, capable of coupling global and local representation to discover aspect words, and it is found that aspect and non-aspect words do exhibit different context, interpreting the superiority of this model in unsupervised aspect extraction.\n\n**Abstract:** Aspect words, indicating opinion targets, are essential in expressing and understanding human opinions. To identify aspects, most previous efforts focus on using sequence tagging models trained on human-annotated data. This work studies unsupervised aspect extraction and explores how words appear in global context (on sentence level) and local context (conveyed by neighboring words). We propose a novel neural model, capable of coupling global and local representation to discover aspect words. Experimental results on two benchmarks, laptop and restaurant reviews, show that our model significantly outperforms the state-of-the-art models from previous studies evaluated with varying metrics. Analysis on model output show our ability to learn meaningful and coherent aspect representations. We further investigate how words distribute in global and local context, and find that aspect and non-aspect words do exhibit different context, interpreting our superiority in unsupervised aspect extraction.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 19  (*Influential: 2*)\n\n### 2 related papers from ArXiv\n\n#### 1. Private Language Model Adaptation for Speech Recognition\n\n*From Search Query: adaptive neural networks*\n\n*Zhe Liu, Ke Li, Shreyan Bakshi, Fuchun Peng*\n\n**Abstract:** Speech model adaptation is crucial to handle the discrepancy between\nserver-side proxy training data and actual data received on local devices of\nusers. With the use of federated learning (FL), we introduce an efficient\napproach on continuously adapting neural network language models (NNLMs) on\nprivate devices with applications on automatic speech recognition (ASR). To\naddress the potential speech transcription errors in the on-device training\ncorpus, we perform empirical studies on comparing various strategies of\nleveraging token-level confidence scores to improve the NNLM quality in the FL\nsettings. Experiments show that compared with no model adaptation, the proposed\nmethod achieves relative 2.6% and 10.8% word error rate (WER) reductions on two\nspeech evaluation datasets, respectively. We also provide analysis in\nevaluating privacy guarantees of our presented procedure.\n\n**Published:** 2021-09-28T00:15:43Z  (*Updated: 2022-06-15T04:00:47Z*)\n\n\n\n#### 2. Leveraging Deep Neural Network Activation Entropy to cope with Unseen\n  Data in Speech Recognition\n\n*From Search Query: adaptive neural networks*\n\n*Vikramjit Mitra, Horacio Franco*\n\n**Abstract:** Unseen data conditions can inflict serious performance degradation on systems\nrelying on supervised machine learning algorithms. Because data can often be\nunseen, and because traditional machine learning algorithms are trained in a\nsupervised manner, unsupervised adaptation techniques must be used to adapt the\nmodel to the unseen data conditions. However, unsupervised adaptation is often\nchallenging, as one must generate some hypothesis given a model and then use\nthat hypothesis to bootstrap the model to the unseen data conditions.\nUnfortunately, reliability of such hypotheses is often poor, given the mismatch\nbetween the training and testing datasets. In such cases, a model hypothesis\nconfidence measure enables performing data selection for the model adaptation.\nUnderlying this approach is the fact that for unseen data conditions, data\nvariability is introduced to the model, which the model propagates to its\noutput decision, impacting decision reliability. In a fully connected network,\nthis data variability is propagated as distortions from one layer to the next.\nThis work aims to estimate the propagation of such distortion in the form of\nnetwork activation entropy, which is measured over a short- time running window\non the activation from each neuron of a given hidden layer, and these\nmeasurements are then used to compute summary entropy. This work demonstrates\nthat such an entropy measure can help to select data for unsupervised model\nadaptation, resulting in performance gains in speech recognition tasks. Results\nfrom standard benchmark speech recognition tasks show that the proposed\napproach can alleviate the performance degradation experienced under unseen\ndata conditions by iteratively adapting the model to the unseen datas acoustic\ncondition.\n\n**Published:** 2017-08-31T01:00:19Z  (*Updated: 2017-08-31T01:00:19Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Neighborhood Attention Transformer\n\n*From Search Query: memory efficient attention mechanisms*\n\n*Humphrey Shi, Shen Li, Jiachen Li, Steven Walton, Ali Hassani*\n\n**Abstract:** We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2022-04-14\n\n\n\n#### 2. Linformer: Self-Attention with Linear Complexity\n\n*From Search Query: memory efficient attention mechanisms*\n\n*Madian Khabsa, Hao Ma, Sinong Wang, Belinda Z. Li, Han Fang*\n\n**Abstract:** Large transformer models have shown extraordinary success in achieving state-of-the-art results in many natural language processing applications. However, training and deploying these models can be prohibitively costly for long sequences, as the standard self-attention mechanism of the Transformer uses $O(n^2)$ time and space with respect to sequence length. In this paper, we demonstrate that the self-attention mechanism can be approximated by a low-rank matrix. We further exploit this finding to propose a new self-attention mechanism, which reduces the overall self-attention complexity from $O(n^2)$ to $O(n)$ in both time and space. The resulting linear transformer, the \\textit{Linformer}, performs on par with standard Transformer models, while being much more memory- and time-efficient.\n\n**Published:** 2020-06-08\n\n\n\n#### 3. Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: adaptive neural networks*\n\n*Alex Graves*\n\n**Abstract:** This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n\n**Published:** 2016-03-29\n\n\n\n#### 4. GeniePath: Graph Neural Networks with Adaptive Receptive Paths\n\n*From Search Query: adaptive neural networks*\n\n*Longfei Li, Jun Zhou, Chaochao Chen, Le Song, Ziqi Liu, Yuan Qi, Xiaolong Li*\n\n**Abstract:** We present, GeniePath, a scalable approach for learning adaptive receptive\nfields of neural networks defined on permutation invariant graph data. In\nGeniePath, we propose an adaptive path layer consists of two complementary\nfunctions designed for breadth and depth exploration respectively, where the\nformer learns the importance of different sized neighborhoods, while the latter\nextracts and filters signals aggregated from neighbors of different hops away.\nOur method works in both transductive and inductive settings, and extensive\nexperiments compared with competitive methods show that our approaches yield\nstate-of-the-art results on large graphs.\n\n**Published:** 2018-02-03\n\n\n\n#### 5. Spam Review Detection with Graph Convolutional Networks\n\n*From Search Query: local global context processing*\n\n*Runshi Liu, Zhou Qin, Yiqun Yang, Dong Li, Ao Li*\n\n**Abstract:** Customers make a lot of reviews on online shopping websites every day, e.g., Amazon and Taobao. Reviews affect the buying decisions of customers, meanwhile, attract lots of spammers aiming at misleading buyers. Xianyu, the largest second-hand goods app in China, suffering from spam reviews. The anti-spam system of Xianyu faces two major challenges: scalability of the data and adversarial actions taken by spammers. In this paper, we present our technical solutions to address these challenges. We propose a large-scale anti-spam method based on graph convolutional networks (GCN) for detecting spam advertisements at Xianyu, named GCN-based Anti-Spam (GAS) model. In this model, a heterogeneous graph and a homogeneous graph are integrated to capture the local context and global context of a comment. Offline experiments show that the proposed method is superior to our baseline model in which the information of reviews, features of users and items being reviewed are utilized. Furthermore, we deploy our system to process million-scale data daily at Xianyu. The online performance also demonstrates the effectiveness of the proposed method.\n\n**Published:** 2019-08-22\n\n\n\n#### 6. Memory Transformer\n\n*From Search Query: local global context processing*\n\n*Anton Peganov, Yuri Kuratov, Grigory V. Sapunov, Mikhail S. Burtsev*\n\n**Abstract:** Transformer-based models have achieved state-of-the-art results in many natural language processing tasks. The self-attention architecture allows transformer to combine information from all elements of a sequence into context-aware representations. However, information about the context is stored mostly in the same element-wise representations. This might limit the processing of properties related to the sequence as a whole more difficult. Adding trainable memory to selectively store local as well as global representations of a sequence is a promising direction to improve the Transformer model. Memory-augmented neural networks (MANNs) extend traditional neural architectures with general-purpose memory for representations. MANNs have demonstrated the capability to learn simple algorithms like Copy or Reverse and can be successfully trained via backpropagation on diverse tasks from question answering to language modeling outperforming RNNs and LSTMs of comparable complexity. In this work, we propose and study few extensions of the Transformer baseline (1) by adding memory tokens to store non-local representations, (2) creating memory bottleneck for the global information, (3) controlling memory update with dedicated layer. We evaluate these memory augmented Transformers and demonstrate that presence of memory positively correlates with the model performance for machine translation and language modelling tasks. Augmentation of pre-trained masked language model with memory tokens shows mixed results for tasks from GLUE benchmark. Visualization of attention patterns over the memory suggest that it improves the model's ability to process a global context.\n\n**Published:** 2020-06-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on combining local and global context processing while maintaining linear complexity and maximizing expressiveness, here are some key developments and approaches that can be considered:\n\n## Memory-Efficient Attention Mechanisms\n\n### Block-Attention\nThe Block-Attention mechanism introduced in the context of Retrieval-Augmented Generation (RAG) models shows promising results in reducing computational complexity and memory usage. By dividing the input sequence into blocks and computing attention over these blocks, Block-Attention can significantly reduce Time-To-First-Token (TTFT) and FLOPs, especially for longer input sequences. This approach maintains comparable or even slightly better performance than traditional self-attention models after fine-tuning.\n\n### FlashAttention\nFlashAttention is another efficient attention mechanism that uses tiling and recomputation techniques to reduce memory usage and computational complexity. It achieves a remarkable speedup of 2-4x compared to standard attention mechanisms by processing attention over smaller blocks and avoiding the storage of large intermediate matrices. This method is particularly beneficial for models with limited memory resources.\n\n### SwiftFormer\nSwiftFormer introduces an efficient additive attention mechanism that replaces quadratic matrix multiplication operations with linear element-wise multiplications. This approach enhances computational efficiency while maintaining high accuracy, making it suitable for real-time applications. SwiftFormer also employs a Shared Attention mechanism, which reduces both computational and memory overhead by sharing pre-computed attention weights across multiple layers.\n\n## Combining Local and Global Context Processing\n\n### Adaptive Neural Networks\nCombining the adaptive capabilities of TTT-Linear with the efficiency improvements of FastTTTLinear could be a viable approach. TTT-Linear's self-supervised learning for hidden state updates allows for continuous training on test sequences, which can be integrated with FastTTTLinear's Gated Linear Attention (GLA) and RWKV concepts. This hybrid approach can maintain linear complexity while enhancing expressiveness and adaptability.\n\n### Local and Global Context Integration\nThe integration of local and global context processing can be achieved through mechanisms like Block-Attention, which allows for the processing of local contexts within blocks while still considering global context through the interaction between blocks. Additionally, using data-dependent gates in GLA, as in FastTTTLinear, can help in dynamically focusing on both local and global contexts based on the input data.\n\n## State-of-the-Art Approaches to Adaptive Neural Networks\n\n### Dynamic and Adaptive Attention Mechanisms\nDynamic and adaptive attention mechanisms, inspired by human attentional focus, allow models to shift focus based on varying task demands and environmental cues. These mechanisms can be trained using reinforcement learning techniques to adaptively allocate attention based on rewards or task performance, enhancing the model's efficiency and effectiveness in handling diverse tasks.\n\n### Memory-Augmented Neural Networks\nMemory-augmented neural networks, such as Neural Turing Machines (NTMs) and MemoryBank, incorporate external memory components for dynamic read and write operations. These architectures can enhance the model's ability to store and retrieve information effectively, which is crucial for combining local and global context processing.\n\n## Conclusion\n\nTo achieve the goal of improving autoregressive language model design, the following strategies can be considered:\n\n- **Block-Attention and FlashAttention**: Implement these memory-efficient attention mechanisms to reduce computational complexity and memory usage.\n- **Hybrid Approach**: Combine the adaptive learning of TTT-Linear with the efficiency improvements of FastTTTLinear to maintain linear complexity while enhancing expressiveness and adaptability.\n- **Dynamic and Adaptive Attention**: Use reinforcement learning to train dynamic and adaptive attention mechanisms that can focus on both local and global contexts based on task demands.\n- **Memory-Augmented Architectures**: Incorporate external memory components to enhance the model's ability to store and retrieve information effectively.\n\nThese approaches can help in creating a more efficient, adaptive, and expressive autoregressive language model.\n",
            "analysis": "Initial Analysis of Parent Designs and Research Direction\n\n1. Summary of Analysis:\n\nThe two parent designs present interesting approaches to language modeling:\n\nParent 1 (TTT-Linear):\n- Introduces test-time training (TTT) layers with linear complexity\n- Uses self-supervised learning for hidden state updates\n- Shows strong performance in long contexts\n- Matches or exceeds Transformer and Mamba baselines\n- Demonstrates efficient scaling with sequence length\n\nParent 2 (FastTTTLinear):\n- Enhances TTTLinear with Gated Linear Attention (GLA)\n- Integrates RWKV concepts for improved efficiency\n- Focuses on hardware efficiency and vectorized operations\n- Uses data-dependent gates for enhanced expressiveness\n- Maintains linear complexity while improving performance\n\nKey Opportunities for Innovation:\n1. The combination of TTT's adaptive capabilities with FastTTTLinear's efficiency improvements suggests potential for a hybrid approach that maximizes both adaptability and performance\n2. Both designs show promise in handling long sequences, but there may be room for further optimization\n3. The integration of different attention mechanisms could be explored more deeply\n\n2. Useful References and Excerpts:\n\nFrom Parent 1:\n\"The key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\"\n\n\"TTT-Linear with the Mamba backbone outperforms both Mamba and Transformers with the Transformer backbone across various model sizes.\"\n\nFrom Parent 2:\n\"FastTTTLinear enhances the TTTLinear by integrating Gated Linear Attention and concepts from the RWKV architecture. This modification achieves linear computational and memory complexity with respect to sequence length.\"\n\n\"The use of data-dependent gates in GLA to maintain linear complexity while enhancing expressiveness is a novel approach.\"\n\n3. Key Insights and Analysis:\n\na) Complementary Strengths:\n- TTT-Linear excels in adaptive learning and test-time optimization\n- FastTTTLinear brings efficiency improvements and hardware optimization\n- Both maintain linear complexity while approaching problems differently\n\nb) Potential Areas for Enhancement:\n- The combination of adaptive learning with gated mechanisms could be optimized\n- Memory efficiency could be improved further\n- The integration of local and global context processing could be enhanced\n\nc) Design Considerations:\n- Need to preserve the test-time training capability while incorporating efficiency improvements\n- Should explore ways to combine different attention mechanisms effectively\n- Must maintain or improve upon the linear complexity of both parents\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Recent advances in memory-efficient attention mechanisms\n2. Innovations in combining local and global context processing\n3. State-of-the-art approaches to adaptive neural networks\n\n5. References:\n\n1. Sun, Y., et al. (2024). \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\"\n2. Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training\"\n3. Peng, B., et al. (2023). \"RWKV: Reinventing RNNs for the Transformer Era\""
        },
        {
            "ready": false,
            "query": "hierarchical neural architectures, memory management neural networks, adaptive computation",
            "detail": "Analyze recent developments in hierarchical neural architectures and memory management techniques, focusing on approaches that can effectively combine different levels of processing while maintaining computational efficiency. Include specific attention to adaptive computation mechanisms and their integration with memory management systems.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in hierarchical neural architectures and memory management techniques, focusing on approaches that can effectively combine different levels of processing while maintaining computational efficiency. Include specific attention to adaptive computation mechanisms and their integration with memory management systems.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.51)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.51)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 2. Towards mental time travel: a hierarchical memory for reinforcement learning agents (Avg. Score: 0.32)\n\n*Andrew Kyle Lampinen, Stephanie C. Y. Chan, Andrea Banino, Felix Hill*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** Hierarchical Chunk Attention Memory improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures), and is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n**Abstract:** Reinforcement learning agents often forget details of the past, especially after delays or distractor tasks. Agents with common memory architectures struggle to recall and integrate across multiple timesteps of a past event, or even to recall the details of a single timestep that is followed by distractor tasks. To address these limitations, we propose a Hierarchical Chunk Attention Memory (HCAM), which helps agents to remember the past in detail. HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks. An agent with HCAM can therefore\"mentally time-travel\"-- remember past events in detail without attending to all intervening events. We show that agents with HCAM substantially outperform agents with other memory architectures at tasks requiring long-term recall, retention, or reasoning over memory. These include recalling where an object is hidden in a 3D environment, rapidly learning to navigate efficiently in a new neighborhood, and rapidly learning and retaining new object names. Agents with HCAM can extrapolate to task sequences much longer than they were trained on, and can even generalize zero-shot from a meta-learning setting to maintaining knowledge across episodes. HCAM improves agent sample efficiency, generalization, and generality (by solving tasks that previously required specialized architectures). Our work is a step towards agents that can learn, interact, and adapt in complex and temporally-extended environments.\n\n##### *Relevant Chunk: No. 13/47 (Score: 0.41)*\n\n```\nOur agent achieves performance competitive with the specialized model proposed to solve each task. Top results (gold) and selected baselines (red) from each paper are indicated. (a) The Passive Visual Match task [21]. (a) The Paired Associative Inference task [2]. HCAM achieves near optimal performance, comparable to MEMO and above Universal Transformers [10]. (c) The One-Shot StreetLearn environment [49]. HCAM achieves comparable performance to EPN on this difficult navigation task, although it is slower to learn. Both substantially outperform strong baselines. (Some prior results were estimated from figures. HCAM results aggregate over: (a) 6 seeds. (b) 3 seeds, selected by validation accuracy. (c) 2 seeds.)\nlearning setting. The agent is placed in a neighborhood, and must navigate to as many goals as it can within a fixed time. The agent receives its current state and goal as visual inputs (Google StreetView images) and must learn about the neighborhood in early tasks within an episode, in order to navigate efficiently later. Ritter et al. designed a specialized Episodic Planning Network (EPN) to solve these tasks. HCAM-despite its more general purpose architecture-can achieve comparable performance to EPN (although HCAM is somewhat slower to reach this level of performance). Ritter et al. showed that EPN achieves near-optimal planning in later parts of each episode, and therefore HCAM must be planning close to optimally to match EPN's performance. Strong baselines perform much worse. Merlin [60]-a strong agent with a learned episodic key-value memory and auxiliary unsupervised training-only achieves around half the performance of HCAM and EPN. TrXL is also less effective. This highlights the value of more sophisticated memory architectures, and in particular emphasizes the value of sequences stored in memory, rather than single vectors: EPN stores transitions (sequences of length 2) and HCAM stores longer sequences, while Merlin has only single vector values for each key (as does $\\operatorname{TrXL}$ ). ## 4 Discussion\n\nIn this paper we have proposed the Hierarchical Chunk Attention Memory. HCAM allows agents to attend in detail to past events without attending to irrelevant intervening information. Thus, HCAM effectively implements a form of \"mental time-travel\" [55]. Our main insight is that this ability can be achieved using a hierarchically structured memory that sparsely distributes detailed attention. Specifically, recall uses attention over memory chunk summaries to allocate more detailed attention within only the most relevant chunks. We see this as a potentially important insight, because both mental time travel [52] and hierarchical structure [16] are essential to the power of human memory. Furthermore, recent work has suggested that human memory behaves more like a \"quantum wave function\" distribution over several past events [37], which matches our approach of a distribution over the top- $k$ relevant chunks. HCAM incorporates a number of important features of human memory. Correspondingly, we showed that agents with HCAM succeed at a wide range of environments and tasks that humans depend on memory to solve. HCAM allows agents to remember sequential events, such as a ballet, in detail. HCAM allows agents to rapidly learn to navigate near-optimally in a new neighborhood by planning over previous paths; to perform transitive inferences; and to maintain object permanence, despite delays, occlusion and looking away. HCAM allows agents to effectively learn new words from a single exposure, and maintain that knowledge across distractor tasks. The agents can even extrapolate far beyond the training distribution to recall words after subsequent learning episodes without ever being trained to do so-better memory systems can help with the challenge of bridging from meta-learning to continual learning, which is receiving increasing interest [13, 17, 49, 5, 41]. The tasks we considered are challenging because they rely on flexible use of memory. The agent did not know in advance which paths to remember in One-Shot StreetLearn, or which dancer would\nbe chosen in Ballet. Furthermore, we trained the agent simultaneously on procedurally generated episodes where task features varied-e.g. the agent did not know how many distractor phases would appear in an episode, or the length of delays, so it could not rely on a fixed policy that recalls information after a fixed time. HCAM might be especially useful in these difficult, variable settings. HCAM is robust to hyperparameters such as chunk size (App. D.4) and the number $k$ of memories selected at each layer and step (App.D.5_ it is even able to solve many tasks with $k=1$. It also outperforms TrXL models that are either twice as wide or twice as deep, and correspondingly have many more parameters and, in the deeper case, make many more attention computations (App.D.8). Our comparisons to other approaches (Sec. 3.4 show that our approach is competitive with state of the art, problem-specific memory architectures, and outperforms strong baselines like Merlin and Universal Transformers. Finally, in hyperparameter sweeps suggested by our reviewers to improve TrXL's performance, we found that HCAM was consistently more robust to hyperparameter variation (App.D.11). These observations suggest that our results should be relatively generalizable. Self-supervised learning As in prior memory work [60, 14, 19] we found that self-supervised learning was necessary to train the agent to store all task-relevant information in memory. Training the agent to reconstruct its input observations as outputs was sufficient in the tasks we explored, but more sophisticated forms of auxiliary learning [23, 3] might be useful in other settings. Memory in RL vs. supervised settings Our work has focused on improving the memory of a situated RL agent. Compared to supervised settings such as language modelling, where a vanilla transformer can achieve very impressive performance [48, 4], RL poses unique challenges to memory architectures. First, sparse rewards present a more impoverished learning signal than settings that provide detailed errors for each output. The ability of HCAM to restore a memory in detail may help ameliorate this problem. Second, the multimodal (language + vision) stimuli experienced by our agent contains much more information per step than the word-pieces alone that a language model experiences, and therefore our setting may place more demands on the memory architecture. Finally, our tasks require access to detailed, structural aspects of past memories, while many existing NLP tasks do not depend significantly on structure - for example, Pham et al. [45] show that BERT ignores word order when solving many language understanding benchmarks, and produces equivalent outputs for shuffled inputs. Detailed memory will be most beneficial in settings in which the sequential structure of the past is important. Nonetheless, we do not rule out possible applications of HCAM in supervised settings, such as language processing [48, 4], video understanding [53], or multimodal perception [24]. HCAM would likely be most beneficial in tasks that strongly rely on long-term context and structure, e.g. the full-length version of NarrativeQA [29]. Because videos often contain hierarchically-structured events, video models might especially benefit from HCAM-style attention. More broadly, the greater efficiency of sparse, hierarchical attention might allow detailed memory even in settings with limited computational resources, such as embedded systems. Episodic memories Nematzadeh et al. [42] suggested endowing transformers with external episodic memories. Several recent papers have proposed systems that can be interpreted as episodic memories from this perspective, specifically looking up related contexts from the training corpus using either nearest neighbors [26], or attention [62]. However, these approaches have generally only used this type of external memory to predict outputs, rather than allowing the model to perform further computations over the memories it recalls, as in HCAM. Thus, these memories would be inadequate for most RL tasks, which require planning or reasoning with memories. Various works have proposed other types of external/episodic memory, both in RL specifically [e.g. 60, 21, 14] and in deep learning more broadly [e.g. 50, 15]. These approaches have generally not stored memories with the hierarchical summary-chunk structure of HCAM. Ritter et al. [49] proposed an episodic memory for navigation tasks that stores state transitions; this architecture can be seen as a step towards our approach of storing sequences as chunks. Indeed, in the challenging city navigation domain that Ritter et al. explored, we showed that HCAM achieves comparable performance to their EPN architecture, despite HCAM being much more general. Furthermore, both HCAM and EPN substantially outperform Merlin [60], a strong baseline that learns to store vector memories, rather than the rich representations of sequences stored by HCAM (or transitions stored by EPN). This highlights the value of rich, sequential memories. We suggest that episodic memories could benefit from moving beyond the idea of keys and values as single vectors. It can be useful to store more general structures-such as a time-sequence of states-as a single \"value\" in memory. One benefit of episodic memory is that memory replay can support learning [30, 38], in particular by ameliorating catastrophic interference. It would therefore be interesting to explore whether HCAM's memory could be used for training. For example, could memory summaries be used to locate similar or contrasting memories that should be interleaved together [39] to improve continual learning? Transformer memories Many recent works have attempted to improve the computational efficiency of transformer attention over long sequences [e.g. 28, 58]. However, these approaches often perform poorly at even moderately long tasks [54]. Similarly, we found that on tasks like Ballet or One-Shot StreetLearn, Transformer-XL can perform suboptimally even when the entire task fits within a single, relatively short attention window. However, there could potentially be complementary benefits to combining approaches to obtain even more effective attention with hierarchical memory, which should be investigated in future work. In particular, some recent work has proposed a simple inductive bias which allows Transformers to benefit from evaluation lengths much longer than they were trained on [46]-while this strategy would likely not help with recalling specific instances in detail, it might be complementary to an approach like ours. Other works have used a hierarchy of transformers with different timescales for supervised tasks [34, 6461,32 , for example encoding sentences with one transformer, and then using these embeddings as higher-level tokens. This approach improves performance on tasks like document summarization, thus supporting the value of hierarchy. Luong et al. [36] also showed benefits of both local and global attention computation in LSTM-based models. However, we are not aware of prior transformers in which the coarse computations are used to select chunks for more detailed computation, as in HCAM (although recent work has demonstrated top- $k$ patch selection in CNNs [8]); nor are we aware of prior works that have demonstrated their approaches in RL, or beyond a single task domain. Memory and adaptation One major benefit of memory is that a model can flexibly use its memories in a goal- or context-dependent way in order to adapt to new tasks [55, 52, 30]. Adult humans use our recall of rich, contextual memories in order to generalize effectively [52, 43]. While our tasks required some forms of goal-dependent memory use-e.g. combining old paths to plan new ones-it would be interesting to evaluate more drastic forms of adaptation. Recent work shows that transforming prior task representations allows zero-shot adaptation to substantially altered tasks [31]. HCAM could potentially learn to transform and combine memories of prior tasks in order to adapt to radically different settings. Exploring these possibilities offers an exciting future direction. Limitations \\& future directions While we see our contribution as a step towards more effective mental time-travel for agent memory, many aspects could be further improved. Our implementation requires the ability to keep each previous step in (hardware) memory, even if the chunk containing that step is irrelevant. This approach would be challenging to scale to the entire lifetime of episodic memory that humans retain. Nonetheless, storing the past is feasible up to tens of thousands of steps on current accelerators. This could be extended further by using efficient $k \\mathrm{NN}$ implementations, which have recently been used to perform $k \\mathrm{NN}$ lookup over an entire language dataset [26]. The challenge of scaling to longer-term memory would be helped by deciding what to store, as in some episodic memory models [15], soft forgetting of rarely retrieved memories, and similar mechanisms. However, such mechanisms are simultaneously limiting, in that the model may be unable to recall knowledge that was not obviously useful at the time of encoding. Over longer timescales, HCAM might also benefit from including more layers of hierarchy-grouping memory chunks into higher-order chunks recursively to achieve logarithmic complexity for recall. Even over moderate timescales, more intelligent segmentation of memory into chunks would potentially be beneficial-human encoding and recall depends upon complex strategies for event segmentation [63, 51, 6, 35, 7]. HCAM might also benefit from improved chunk summaries; mean-pooling over the chunk is a relatively na\u00efve approach to summarization, so learning a compression mechanism might be beneficial [47]. These possibilities provide exciting directions for future work. Conclusions We have proposed a Hierarchical Chunk Attention Memory for RL agents. This architecture allows agents to recall their most relevant memories in detail, and to reason over those memories to achieve new goals. This approach outperforms (or matches the optimal performance of) a wide variety of baselines, across a wide variety of task domains. It allows agents to remember where objects were hidden, and to efficiently learn to navigate in a new neighborhood by planning from memory. It allows agents to recall words they have learned despite distracting intervening tasks, and even across episodes. These abilities to learn, adapt, and maintain new knowledge are critical to intelligent behavior, especially as the field progresses towards more complex environments. We\nhope that our work will motivate further exploration of hierarchically structured memories, in RL and beyond. Hierarchical memory may have many benefits for learning, reasoning, adaptation, and intermediate- or long-term recall. ## Acknowledgments and Disclosure of Funding\n\nWe would like to acknowledge Adam Santoro, Emilio Parisotto, Jay McClelland, David Raposo, Wilka Carvalho, Tim Scholtes, Tamara von Glehn, S\u00e9bastien Racani\u00e8re, and the anonymous reviewers for helpful comments and suggestions. ## References\n\n[1] Renee Baillargeon. Infants' reasoning about hidden objects: evidence for event-general and event-specific expectations. Developmental science, 7(4):391-414, 2004. [2] Andrea Banino, Adri\u00e0 Puigdom\u00e8nech Badia, Raphael K\u00f6ster, Martin J Chadwick, Vinicius Zambaldi, Demis Hassabis, Caswell Barry, Matthew Botvinick, Dharshan Kumaran, and Charles Blundell. Memo: A deep network for flexible combination of episodic memories.\n```\n\n##### *Relevant Chunk: No. 20/47 (Score: 0.22)*\n\n```\narXiv preprint arXiv:2101.03961, 2021. [13] Chelsea Finn, Aravind Rajeswaran, Sham Kakade, and Sergey Levine. Online meta-learning. In International Conference on Machine Learning, pages 1920-1930. PMLR, 2019. [14] Meire Fortunato, Melissa Tan, Ryan Faulkner, Steven Hansen, Adri\u00e0 Puigdom\u00e8nech Badia, Gavin Buttimore, Charlie Deck, Joel Z Leibo, and Charles Blundell. Generalization of reinforcement learners with working and episodic memory. arXiv preprint arXiv:1910.13406, 2019. [15] Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwi\u0144ska, Sergio G\u00f3mez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John Agapiou, et al. Hybrid computing using a neural network with dynamic external memory. Nature, 538 (7626):471-476, 2016. [16] Uri Hasson, Janice Chen, and Christopher J Honey. Hierarchical process memory: memory as an integral component of information processing.\n```\n\n#### 3. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.27)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 28/47 (Score: 0.27)*\n\n```\nIn Int. Conf. on Learning Representations (ICLR), Virtual only, May 2021. Yann Dubois, Gautier Dagan, Dieuwke Hupkes, and Elia Bruni. Location attention for extrapolation to longer sequences. In Proc. Association for Computational Linguistics (ACL), pp. 403-413, Virtual only, July 2020. Jerry Fodor and Brian P McLaughlin. Connectionism and the problem of systematicity: Why Smolensky's solution doesn't work. Cognition, 35(2):183-204, 1990. Jerry A Fodor, Zenon W Pylyshyn, et al. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. Daniel Furrer, Marc van Zee, Nathan Scales, and Nathanael Sch\u00e4rli. Compositional generalization in semantic parsing: Pre-training vs. specialized architectures. Preprint arXiv:2007.08970, 2020. Alex Graves. Adaptive computation time for recurrent neural networks. In Int. Conf. on Learning Representations (ICLR) Workshop Track, Vancouver, Canada, April 2016. Alex Graves, Greg Wayne, Malcolm Reynolds, Tim Harley, Ivo Danihelka, Agnieszka GrabskaBarwinska, Sergio Gomez Colmenarejo, Edward Grefenstette, Tiago Ramalho, John P. Agapiou, Adri\u00e0 Puigdom\u00e8nech Badia, Karl Moritz Hermann, Yori Zwols, Georg Ostrovski, Adam Cain, Helen King, Christopher Summerfield, Phil Blunsom, Koray Kavukcuoglu, and Demis Hassabis. Hybrid computing using a neural network with dynamic external memory. Nature, 538(7626): $471-476,2016$. Klaus Greff, Sjoerd van Steenkiste, and J\u00fcrgen Schmidhuber. On the binding problem in artificial neural networks. Preprint arXiv:2012.05208, 2020. Stephen Jos\u00e9 Hanson. A stochastic version of the delta rule. Physica D: Nonlinear Phenomena, 42 $(1-3): 265-272,1990$. Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proc. North American Chapter of the Association for Computational Linguistics on Human Language Technologies (NAACL-HLT), pp. 1118-1128, Minneapolis, USA, June 2019. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770-778, Las Vegas, NV, USA, June 2016. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, pp. $1735-1780,1997$. Dieuwke Hupkes, Anand Singh, Kris Korrel, German Kruszewski, and Elia Bruni. Learning compositionally through attentive guidance.\n```\n\n#### 4. Ordered Memory  (Avg. Score: 0.12)\n\n*Daniel Borisov, Matthew D\u2019Iorio, Jeffrey Hyacinthe*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\n\n**Abstract:** Natural language semantics can be modeled using the phrase-structured model, which can be represented using a tree-type architecture. As a result, recent advances in natural language processing have been made utilising recursive neural networks using memory models that allow them to infer tree-type representations of the input sentence sequence. These new tree models have allowed for improvements in sentiment analysis and semantic recognition. Here we review the Ordered Memory model proposed by Shen et al. (2019) at the NeurIPS 2019 conference, and try to either create baselines that can perform better or create simpler models that can perform equally as well. We found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\n\n##### *Relevant Chunk: No. 17/24 (Score: 0.12)*\n\n```\nIn Proceedings of The Fourteenth Annual Conference of Cognitive Science Society. Indiana University, page 14, 1992. Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. David Dowty. 4. Direct compositionality, 14:23-101, 2007. Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A Smith. Recurrent neural network grammars. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 199-209, 2016. Jeffrey L Elman. Finding structure in time. Cognitive science, 14(2):179-211, 1990. Jerry A Fodor and Zenon W Pylyshyn. Connectionism and cognitive architecture: A critical analysis. Cognition, 28(1-2):3-71, 1988. Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193-202, 1980. Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. Alex Graves. Adaptive computation time for recurrent neural networks. arXiv preprint arXiv:1603.08983, 2016. Alex Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. arXiv preprint arXiv: $1410.5401,2014$. Edward Grefenstette, Karl Moritz Hermann, Mustafa Suleyman, and Phil Blunsom. Learning to transduce with unbounded memory. In Advances in Neural Information Processing Systems, pages $1828-1836,2015$. Caglar Gulcehre, Sarath Chandar, and Yoshua Bengio. Memory augmented neural networks with wormhole connections.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical neural architectures, memory management neural networks, adaptive computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical neural architectures*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 2*)\n\n#### 2. Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars\n\n*From Search Query: hierarchical neural architectures*\n\n*Simon Schrodi, Daniel Stoll, Binxin Ru, R. Sukthanker, T. Brox, F. Hutter*\n\n**TL;DR:** This work introduces a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature.\n\n**Abstract:** The discovery of neural architectures from simple building blocks is a long-standing goal of Neural Architecture Search (NAS). Hierarchical search spaces are a promising step towards this goal but lack a unifying search space design framework and typically only search over some limited aspect of architectures. In this work, we introduce a unifying search space design framework based on context-free grammars that can naturally and compactly generate expressive hierarchical search spaces that are 100s of orders of magnitude larger than common spaces from the literature. By enhancing and using their properties, we effectively enable search over the complete architecture and can foster regularity. Further, we propose an efficient hierarchical kernel design for a Bayesian Optimization search strategy to efficiently search over such huge spaces. We demonstrate the versatility of our search space design framework and show that our search strategy can be superior to existing NAS approaches. Code is available at https://github.com/automl/hierarchical_nas_construction.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 3. Differentiable hierarchical and surrogate gradient search for spiking neural networks\n\n*From Search Query: hierarchical neural architectures*\n\n*Kaiwei Che, Luziwei Leng, Kaixuan Zhang, Jianguo Zhang, Qinghu Meng, Jie Cheng, Qinghai Guo, Jianxing Liao*\n\n**TL;DR:** A spike-based differentiable hierarchical search (SpikeDHS) framework, where spike- based computation is realized on both the cell and the layer level search space, which finds effective SNN architectures under limited computation cost and exceeds the accuracy of specially designed ANNs meanwhile.\n\n**Abstract:** Spiking neural network (SNN) has been viewed as a potential candidate for the next generation of artificial intelligence with appealing characteristics such as sparse computation and inherent temporal dynamics. By adopting architectures of deep artificial neural networks (ANNs), SNNs are achieving competitive performances in benchmark tasks such as image classification. However, successful architectures of ANNs are not necessary ideal for SNN and when tasks become more diverse effective architectural variations could be critical. To this end, we develop a spike-based differentiable hierarchical search (SpikeDHS) framework, where spike-based computation is realized on both the cell and the layer level search space. Based on this framework, we find effective SNN architectures under limited computation cost. During the training of SNN, a suboptimal surrogate gradient function could lead to poor approximations of true gradients, making the network enter certain local minima. To address this problem, we extend the differential approach to surrogate gradient search where the SG function is efficiently optimized locally. Our models achieve state-of-the-art performances on classification of CIFAR10/100 and ImageNet with accuracy of 95.50%, 76.25% and 68.64%. On event-based deep stereo, our method finds optimal layer variation and surpasses the accuracy of specially designed ANNs meanwhile with 26 \u00d7 lower energy cost ( 6 . 7mJ ), demonstrating the advantage of SNN in processing highly sparse and dynamic signals. Codes are available at https://github.com/Huawei-BIC/SpikeDHS .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 3*)\n\n#### 4. Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks\n\n*From Search Query: memory management neural networks*\n\n*Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin Gimpel*\n\n**TL;DR:** It is argued that keeping all entities in memory is unnecessary, and a memory-augmented neural network that tracks only a small bounded number of entities at a time is proposed, thus guaranteeing a linear runtime in length of document.\n\n**Abstract:** Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 45  (*Influential: 7*)\n\n#### 5. Single Model Ensemble using Pseudo-Tags and Distinct Vectors\n\n*From Search Query: memory management neural networks*\n\n*Ryosuke Kuwabara, Jun Suzuki, Hideki Nakayama*\n\n**TL;DR:** This study proposes a novel method that replicates the effects of a model ensemble with a single model that creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K- Distinct vectors.\n\n**Abstract:** Model ensemble techniques often increase task performance in neural networks; however, they require increased time, memory, and management effort. In this study, we propose a novel method that replicates the effects of a model ensemble with a single model. Our approach creates K-virtual models within a single parameter space using K-distinct pseudo-tags and K-distinct vectors. Experiments on text classification and sequence labeling tasks on several datasets demonstrate that our method emulates or outperforms a traditional model ensemble with 1/K-times fewer parameters.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. Memory-Consistent Neural Networks for Imitation Learning\n\n*From Search Query: memory management neural networks*\n\n*Kaustubh Sridhar, Souradeep Dutta, Dinesh Jayaraman, James Weimer, Insup Lee*\n\n**TL;DR:** This work revisits simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon, and provides a guaranteed upper bound for the sub-optimality gap induced by MCNN policies.\n\n**Abstract:** Imitation learning considerably simplifies policy synthesis compared to alternative approaches by exploiting access to expert demonstrations. For such imitation policies, errors away from the training samples are particularly critical. Even rare slip-ups in the policy action outputs can compound quickly over time, since they lead to unfamiliar future states where the policy is still more likely to err, eventually causing task failures. We revisit simple supervised ``behavior cloning'' for conveniently training the policy from nothing more than pre-recorded demonstrations, but carefully design the model class to counter the compounding error phenomenon. Our ``memory-consistent neural network'' (MCNN) outputs are hard-constrained to stay within clearly specified permissible regions anchored to prototypical ``memory'' training samples. We provide a guaranteed upper bound for the sub-optimality gap induced by MCNN policies. Using MCNNs on 10 imitation learning tasks, with MLP, Transformer, and Diffusion backbones, spanning dexterous robotic manipulation and driving, proprioceptive inputs and visual inputs, and varying sizes and types of demonstration data, we find large and consistent gains in performance, validating that MCNNs are better-suited than vanilla deep neural networks for imitation learning applications. Website: https://sites.google.com/view/mcnn-imitation\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 7. Scalable Adaptive Computation for Iterative Generation\n\n*From Search Query: adaptive computation*\n\n*A. Jabri, David J. Fleet, Ting Chen*\n\n**TL;DR:** The Recurrent Interface Networks (RINs) are proposed, an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data.\n\n**Abstract:** Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 84  (*Influential: 13*)\n\n#### 8. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation\n\n*From Search Query: adaptive computation*\n\n*Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, L. Lakshmanan, A. Awadallah, S\u00e9bastien Bubeck, Jianfeng Gao*\n\n**TL;DR:** AutoMoE is developed -- a framework for designing heterogeneous MoE's under computational constraints and leverages Neural Architecture Search to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, on aggregate over benchmark datasets for NMT.\n\n**Abstract:** Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n### 3 related papers from ArXiv\n\n#### 1. Language Model Pre-training for Hierarchical Document Representations\n\n*From Search Query: hierarchical neural architectures*\n\n*Ming-Wei Chang, Kristina Toutanova, Kenton Lee, Jacob Devlin*\n\n**Abstract:** Hierarchical neural architectures are often used to capture long-distance\ndependencies and have been applied to many document-level tasks such as\nsummarization, document segmentation, and sentiment analysis. However,\neffective usage of such a large context can be difficult to learn, especially\nin the case where there is limited labeled data available. Building on the\nrecent success of language model pretraining methods for learning flat\nrepresentations of text, we propose algorithms for pre-training hierarchical\ndocument representations from unlabeled data. Unlike prior work, which has\nfocused on pre-training contextual token representations or context-independent\n{sentence/paragraph} representations, our hierarchical document representations\ninclude fixed-length sentence/paragraph representations which integrate\ncontextual information from the entire documents. Experiments on document\nsegmentation, document-level question answering, and extractive document\nsummarization demonstrate the effectiveness of the proposed pre-training\nalgorithms.\n\n**Published:** 2019-01-26T00:35:35Z  (*Updated: 2019-01-26T00:35:35Z*)\n\n\n\n#### 2. Duo-LLM: A Framework for Studying Adaptive Computation in Large Language\n  Models\n\n*From Search Query: adaptive computation*\n\n*Keivan Alizadeh, Iman Mirzadeh, Hooman Shahrokhi, Dmitry Belenko, Frank Sun, Minsik Cho, Mohammad Hossein Sekhavat, Moin Nabi, Mehrdad Farajtabar*\n\n**Abstract:** Large Language Models (LLMs) typically generate outputs token by token using\na fixed compute budget, leading to inefficient resource utilization. To address\nthis shortcoming, recent advancements in mixture of expert (MoE) models,\nspeculative decoding, and early exit strategies leverage the insight that\ncomputational demands can vary significantly based on the complexity and nature\nof the input. However, identifying optimal routing patterns for dynamic\nexecution remains an open challenge, limiting the full potential of these\nadaptive methods. To address this need, we study adaptive computation in LLMs\nmore systematically. We propose a novel framework that integrates smaller\nauxiliary modules within each Feed-Forward Network layer of the LLM. This\ndesign enables dynamic routing of tokens based on task complexity: tokens can\nbe processed by either the small or big modules at each layer, or even bypass\ncertain layers entirely. This allows us to introduce a novel notion of a\ntoken's difficulty, defined by its potential to benefit from additional\ncomputational resources. Importantly, by employing oracles to identify optimal\npatterns of adaptive computations, we gain valuable insights into the internal\nworkings of LLMs and the routing processes in a simplified heterogeneous MoE\nsetup. We show that trained routers operate differently from oracles and often\nyield suboptimal solutions. Notably, activating a large module in just one\nlayer outperforms models that use large modules across all layers, underscoring\nthe gap between practical implementations of routing in MoE models and\ntheoretical optima for adaptive computation.\n\n**Published:** 2024-10-01T16:10:21Z  (*Updated: 2024-10-01T16:10:21Z*)\n\n\n\n#### 3. CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs\n\n*From Search Query: adaptive computation*\n\n*Weijie Lv, Xuan Xia, Sheng-Jun Huang*\n\n**Abstract:** Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n\n**Published:** 2024-08-05T02:38:48Z  (*Updated: 2024-08-05T02:38:48Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n\n*From Search Query: hierarchical neural architectures*\n\n*Li Fei-Fei, Alan Yuille, Florian Schroff, Liang-Chieh Chen, Wei Hua, Hartwig Adam, Chenxi Liu*\n\n**Abstract:** Recently, Neural Architecture Search (NAS) has successfully identified neural\nnetwork architectures that exceed human designed ones on large-scale image\nclassification. In this paper, we study NAS for semantic image segmentation.\nExisting works often focus on searching the repeatable cell structure, while\nhand-designing the outer network structure that controls the spatial resolution\nchanges. This choice simplifies the search space, but becomes increasingly\nproblematic for dense image prediction which exhibits a lot more network level\narchitectural variations. Therefore, we propose to search the network level\nstructure in addition to the cell level structure, which forms a hierarchical\narchitecture search space. We present a network level search space that\nincludes many popular designs, and develop a formulation that allows efficient\ngradient-based architecture search (3 P100 GPU days on Cityscapes images). We\ndemonstrate the effectiveness of the proposed method on the challenging\nCityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our\narchitecture searched specifically for semantic image segmentation, attains\nstate-of-the-art performance without any ImageNet pretraining.\n\n**Conference:** auto-deeplab-hierarchical-neural-architecture-1\n\n**Published:** 2019-01-10\n\n\n\n#### 2. Hierarchical Neural Architecture Search for Deep Stereo Matching\n\n*From Search Query: hierarchical neural architectures*\n\n*ZongYuan Ge, Hongdong Li, Tom Drummond, Xiaojun Chang, Yuchao Dai, Mehrtash Harandi, Yiran Zhong, Xuelian Cheng*\n\n**Abstract:** To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to enable the network the ability to choose among a set of operations (e.g., convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first end-to-end hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (i.e., feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. The code is available at https://github.com/XuelianCheng/LEAStereo.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-26\n\n\n\n#### 3. A Deep Reinforcement Learning Framework for the Financial Portfolio Management Problem\n\n*From Search Query: memory management neural networks*\n\n*Jinjun Liang, Dixing Xu, Zhengyao Jiang*\n\n**Abstract:** Financial portfolio management is the process of constant redistribution of a\nfund into different financial products. This paper presents a\nfinancial-model-free Reinforcement Learning framework to provide a deep machine\nlearning solution to the portfolio management problem. The framework consists\nof the Ensemble of Identical Independent Evaluators (EIIE) topology, a\nPortfolio-Vector Memory (PVM), an Online Stochastic Batch Learning (OSBL)\nscheme, and a fully exploiting and explicit reward function. This framework is\nrealized in three instants in this work with a Convolutional Neural Network\n(CNN), a basic Recurrent Neural Network (RNN), and a Long Short-Term Memory\n(LSTM). They are, along with a number of recently reviewed or published\nportfolio-selection strategies, examined in three back-test experiments with a\ntrading period of 30 minutes in a cryptocurrency market. Cryptocurrencies are\nelectronic and decentralized alternatives to government-issued money, with\nBitcoin as the best-known example of a cryptocurrency. All three instances of\nthe framework monopolize the top three positions in all experiments,\noutdistancing other compared trading algorithms. Although with a high\ncommission rate of 0.25% in the backtests, the framework is able to achieve at\nleast 4-fold returns in 50 days.\n\n**Published:** 2017-06-30\n\n\n\n#### 4. Graph-Partitioning-Based Diffusion Convolutional Recurrent Neural Network for Large-Scale Traffic Forecasting\n\n*From Search Query: memory management neural networks*\n\n*Tanwi Mallick, Jane Macfarlane, Eric Rask, Prasanna Balaprakash*\n\n**Abstract:** Traffic forecasting approaches are critical to developing adaptive strategies for mobility. Traffic patterns have complex spatial and temporal dependencies that make accurate forecasting on large highway networks a challenging task. Recently, diffusion convolutional recurrent neural networks (DCRNNs) have achieved state-of-the-art results in traffic forecasting by capturing the spatiotemporal dynamics of the traffic. Despite the promising results, however, applying DCRNNs for large highway networks still remains elusive because of computational and memory bottlenecks. We present an approach for implementing a DCRNN for a large highway network that overcomes these limitations. Our approach uses a graph-partitioning method to decompose a large highway network into smaller networks and trains them independently. We demonstrate the efficacy of the graph-partitioning-based DCRNN approach to model the traffic on a large California highway network with 11,160 sensor locations. We develop an overlapping nodes approach for the graph-partitioning-based DCRNN to include sensor locations from partitions that are geographically close to a given partition. Furthermore, we demonstrate that the DCRNN model can be used to forecast the speed and flow simultaneously and that the forecasted values preserve fundamental traffic flow dynamics. Our approach to developing DCRNN models that represent large highway networks can be a potential core capability in advanced highway traffic monitoring systems, where a trained DCRNN model forecasting traffic at all sensor locations can be used to adjust traffic management strategies proactively based on anticipated future conditions.\n\n**Published:** 2019-09-24\n\n\n\n#### 5. Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: adaptive computation*\n\n*Alex Graves*\n\n**Abstract:** This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n\n**Published:** 2016-03-29\n\n\n\n#### 6. Adaptive Computation with Elastic Input Sequence\n\n*From Search Query: adaptive computation*\n\n*Yang You, Mostafa Dehghani, Neil Houlsby, Anurag Arnab, Valerii Likhosherstov, Fuzhao Xue*\n\n**Abstract:** Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data. We examine the challenges and requirements to obtain dynamic sequence content and length, and propose the Adaptive Tape Reading (ATR) algorithm to achieve both goals. Through extensive experiments on image recognition tasks, we show that AdaTape can achieve better performance while maintaining the computational cost. To facilitate further research, we have released code at https://github.com/google-research/scenic.\n\n**Published:** 2023-01-30\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, focusing on hierarchical neural architectures, memory management, and adaptive computation, here are some key developments and techniques that can be particularly useful:\n\n## Hierarchical Neural Architectures\n\n### Hierarchical Recurrent Neural Networks (HRNN)\nHRNNs are designed to decompose hierarchical behavior into useful subprograms, similar to theories of memory presented by philosopher Henri Bergson. These networks connect neurons in various ways to handle hierarchical processing, which is beneficial for tasks like forecasting and natural language processing. HRNNs leverage information from higher levels to enhance lower-level predictions, demonstrating superior performance in tasks such as predicting inflation components of the consumer price index (CPI).\n\n### Multiple Timescales Recurrent Neural Networks (MTRNN)\nMTRNNs simulate the functional hierarchy of the brain by segmenting continuous sequences into reusable primitives, each with distinct time properties. This hierarchy allows for flexible integration of sequential behaviors and aligns with theories of memory and cognition, making it suitable for complex reasoning and extended interactions.\n\n## Memory Management Techniques\n\n### Long Short-Term Memory (LSTM)\nLSTMs are widely used for their ability to maintain long-term dependencies through a memory cell and gating mechanisms (input, forget, and output gates). This architecture is crucial for tasks requiring the retention of information over extended sequences, such as natural language processing and time series analysis. LSTMs effectively manage memory by controlling the flow of information into and out of the memory cell, addressing the vanishing gradient problem in traditional RNNs.\n\n### MemTree Algorithm\nThe MemTree algorithm uses a dynamic, tree-structured memory representation to optimize the organization, retrieval, and integration of information. It maintains a hierarchical structure where each node encapsulates aggregated textual content and semantic embeddings. This approach allows for efficient memory management, particularly in scenarios demanding structured memory, such as multi-turn dialogue understanding and document question answering. MemTree's hierarchical organization and adaptive updates ensure computational efficiency and effective memory clustering.\n\n## Adaptive Computation Mechanisms\n\n### Adaptive Computation Time for RNNs\nAdaptive computation time mechanisms allow RNNs to dynamically adjust the number of computational steps based on the input, which can significantly reduce computational time. This approach, as proposed by Graves (2016), enables the model to focus computational resources where needed, improving efficiency without compromising performance.\n\n### Gated Linear Attention and Test-Time Training\nIntegrating gated linear attention (GLA) and test-time training can enhance efficiency and expressiveness. For example, FastTTTLinear combines GLA with concepts from the RWKV architecture to achieve linear complexity while maintaining adaptive processing. This approach balances local and global context integration and optimizes state updates, which is crucial for scaling autoregressive models[Analysis Note - Round 1].\n\n## Integration of Hierarchical Processing and Adaptive Computation\n\n### Hierarchical Memory Networks\nCombining hierarchical neural architectures with memory networks can enhance the recall capabilities of AI models. For instance, using a hierarchical structure like MemTree within an LSTM framework can optimize memory management and integrate local and global contexts effectively. This integration allows the model to adaptively manage memory and computational resources, ensuring that the model can handle complex reasoning and extended interactions efficiently.\n\n### Block-Based Attention and Local-Global Context Integration\nImplementing block-based attention mechanisms within hierarchical architectures can help balance local and global processing. This approach, as suggested by Xiao & Carenini (2019) and Liang et al. (2021), benefits from jointly modeling local and global contexts, which is particularly advantageous for tasks involving long documents or sequences. Adaptive computation paths can further optimize resource usage by focusing on the most relevant contexts[Analysis Note - Round 1].\n\nBy incorporating these hierarchical neural architectures, advanced memory management techniques, and adaptive computation mechanisms, researchers can develop autoregressive language models that are more efficient, scalable, and capable of handling complex tasks with improved performance.\n",
            "analysis": "Analysis Note - Round 1\n\n1. Summary of Analysis:\n\nFrom analyzing the parent designs and search results, several key themes emerge:\n\na) Parent Design Analysis:\n- Parent 1 (TTT) focuses on test-time training with expressive hidden states\n- Parent 2 (FastTTTLinear) emphasizes efficiency through gated linear attention\n- Both maintain linear complexity but approach it differently\n\nb) Key Research Areas Identified:\n- Memory-efficient attention mechanisms\n- Adaptive neural architectures\n- Local-global context integration\n- Efficient state management\n\n2. Useful References with Excerpts:\n\nFrom Parent 1:\n\"The key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning.\"\n\nFrom Parent 2:\n\"FastTTTLinear enhances TTTLinear by integrating Gated Linear Attention (GLA) and concepts from RWKV architecture.\"\n\nFrom Search Results:\n\nXiao & Carenini (2019):\n\"incorporating both the global context of the whole document and the local context within the current topic [...] benefits become stronger as we apply it to longer documents.\"\n\nLiang et al. (2021):\n\"jointly modeling local and global contexts [...] where local and global contexts are jointly modeled [...] proposed a new centrality computation method to capture local salient information\"\n\nBolukbasi et al. (2017):\n\"computational time can be dramatically reduced by exploiting the fact that many examples can be correctly classified using relatively efficient networks\"\n\n3. Key Insights for Design:\n\na) Attention Mechanism:\n- Block-based attention could help balance local and global processing\n- Memory-efficient attention implementations are crucial for scaling\n- Adaptive mechanisms can help focus computational resources where needed\n\nb) State Management:\n- Combining test-time training with efficient state updates is promising\n- Need to balance expressiveness with computational efficiency\n- Local-global context integration could improve model performance\n\nc) Architecture Design:\n- Hierarchical processing could help manage different context levels\n- Memory augmentation might improve long-range dependencies\n- Adaptive computation paths could optimize resource usage\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Recent advances in hierarchical neural architectures\n2. State-of-the-art approaches to memory management in neural networks\n3. Efficient implementations of adaptive computation\n\n5. References:\n\n1. Bolukbasi, T., Wang, J., Dekel, O., & Saligrama, V. (2017). Adaptive Neural Networks for Efficient Inference. International Conference on Machine Learning.\n\n2. Liang, X., Wu, S., Li, M., & Li, Z. (2021). Unsupervised Keyphrase Extraction by Jointly Modeling Local and Global Context. Conference on Empirical Methods in Natural Language Processing.\n\n3. Xiao, W., & Carenini, G. (2019). Extractive Summarization of Long Documents by Combining Global and Local Context. Conference on Empirical Methods in Natural Language Processing.\n\n4. Liu, Z., Li, K., Bakshi, S., & Peng, F. (2021). Private Language Model Adaptation for Speech Recognition. arXiv preprint.\n\n5. Graves, A. (2016). Adaptive Computation Time for Recurrent Neural Networks. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hierarchical routing neural networks, efficient routing mechanisms, computational efficiency expressiveness balance",
            "detail": "Analyze recent developments in hierarchical routing mechanisms and techniques for balancing computational efficiency with model expressiveness in neural networks. Include specific attention to methods for combining adaptive and hierarchical processing while maintaining efficient routing between different levels.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in hierarchical routing mechanisms and techniques for balancing computational efficiency with model expressiveness in neural networks. Include specific attention to methods for combining adaptive and hierarchical processing while maintaining efficient routing between different levels.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.37)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 42/46 (Score: 0.37)*\n\n```\n[N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## Appendix\n\n## 6 Related Work\n\nHierarchical or Multiscale Recurrent neural networks. This work takes inspiration from a wide array of work on introducing multiple scales of processing into recurrent neural networks (Chung et al. 2016; Hihi \\& Bengio, 1995; Mozer, 1991, Schmidhuber, 1991, Koutn\u00edk et al., 2014). These works divide the processing into multiple streams each operating at a different temporal granularity. While these works mainly focus on recurrent neural networks and their application is mainly on natural language tasks, we focus on introducing multiple streams of processing and a hierarchical structure into Transformers while also focusing on a broader range of domains beyond natural language. Transformers. Some of the components we describe in the proposed model have been used previously in various Transformer models. Transformer XL (Dai et al., 2019) also divides the input into segments. Each segment considers the tokens from the current segment and the previous segment for attention without passing gradients into the previous segments. A number of previous works (Zhang et al., 2021; Liu et al., 2021b, Wu et al., 2021, Yuan et al., 2021, Wang et al., 2021; Yang et al., 2021) have worked on introducing a hierarchical structure in Transformers mainly in the domain of vision. The main goal of these works has been to introduce convolution-like hierarchies into Vision Transformers (Dosovitskiy et al. 2020). While these works progressively reduce the spatial resolution of the inputs in order to introduce hierarchies, we introduce hierarchies by adding another slow stream of information processing and without reducing the spatial resolution of the inputs. We also provision for the higher level of the hierarchy (i.e. the slow stream) to provide information to the lower levels as top-down conditioning which is not possible in any of the previous works. Top-Down Conditioning. Top-down information is information propagated from higher to lower levels of the network. It represents the models beliefs of the world and provides context for interpreting perceptual information.\n```\n\n#### 2. Efficient Beam Tree Recursion (Avg. Score: 0.34)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** These proposals standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n**Abstract:** Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n##### *Relevant Chunk: No. 15/50 (Score: 0.38)*\n\n```\nIn Sheila A. McIlraith and Kilian Q. Weinberger, editors, Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5094-5101. AAAI Press, 2018. URL https://www. aaai.org/ocs/index.php/AAAI/AAAI18/paper/ view/16682. [10] Jishnu Ray Chowdhury and Cornelia Caragea. Modeling hierarchical structures with continuous recursive neural networks. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1975-1988. PMLR, 18-24 Jul 2021. URL https://proceedings mlr.press/v139/chowdhury21a.html. [11] Caio Corro and Ivan Titov. Learning latent trees with stochastic perturbations and differentiable dynamic programming. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5508-5521, Florence, Italy, July 2019. Association for Computational Linguistics. doi: 10.18653/v1/P19-1551. URL https://aclanthology.org/ P19-1551. [12] R\u00f3bert Csord\u00e1s, Kazuki Irie, and J\u00fcrgen Schmidhuber. The neural data router: Adaptive control flow in transformers improves systematic generalization. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=KBQP4A_J1K\n[13] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and Lukasz Kaiser. Universal transformers. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=HyzdRiR9Y7. [14] Gr'egoire Del'etang, Anian Ruoss, Jordi Grau-Moya, Tim Genewein, Li Kevin Wenliang, Elliot Catt, Marcus Hutter, Shane Legg, and Pedro A. Ortega. Neural networks and the chomsky hierarchy. ArXiv, abs/2207.02098, 2022. [15] Andrew Drozdov, Patrick Verga, Mohit Yadav, Mohit Iyyer, and Andrew McCallum. Unsupervised latent tree induction with deep inside-outside recursive auto-encoders. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 11291141, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1116. URL https://aclanthology.org/N19-1116\n[16] Andrew Drozdov, Subendhu Rongali, Yi-Pei Chen, Tim O'Gorman, Mohit Iyyer, and Andrew McCallum. Unsupervised parsing with S-DIORA: Single tree encoding for deep inside-outside recursive autoencoders.\n```\n\n##### *Relevant Chunk: No. 19/50 (Score: 0.30)*\n\n```\nIn Proceedings of International Conference on Neural Networks (ICNN'96), volume 1, pages 347-352 vol.1, 1996. doi: 10.1109/ICNN.1996.548916. [25] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983\n[26] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [27] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022. [28] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-2017. URL https://aclanthology.org/N18-2017. [29] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11\n[30] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908-15919, 2021. [31] Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1118-1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1115. URLhttps://aclanthology org/N19-1115\n[32] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 908-921, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.74. URL https://aclanthology.org/2021 acl-long. 74\n[33] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735\n[34] Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling.\n```\n\n#### 3. Modeling Hierarchical Structures with Continuous Recursive Neural Networks  (Avg. Score: 0.07)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** International Conference on Machine Learning (2021)\t**Cited by** 11  (*Influential: 1*)\n\n**TL;DR:** This work proposes Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations of traditional RvNNs by incorporating a continuous relaxation to the induced structure.\n\n**Abstract:** Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference and ListOps. We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.07)*\n\n```\n# Modeling Hierarchical Structures with Continuous Recursive Neural Networks \n\nJishnu Ray Chowdhury ${ }^{1}$ Cornelia Caragea ${ }^{1}$\n\n\n#### Abstract\n\nRecursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases.\n```\n\n#### 4. The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization  (Avg. Score: 0.05)\n\n*R. Csord\u00e1s, Kazuki Irie, J. Schmidhuber*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 42  (*Influential: 7*)\n\n**TL;DR:** This novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect errors on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths.\n\n**Abstract:** Despite progress across a broad range of applications, Transformers have limited success in systematic generalization. The situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public.\n\n##### *Relevant Chunk: No. 2/47 (Score: 0.05)*\n\n```\nThe situation is especially frustrating in the case of algorithmic tasks, where they often fail to find intuitive solutions that route relevant information to the right node/operation at the right time in the grid represented by Transformer columns. To facilitate the learning of useful control flow, we propose two modifications to the Transformer architecture, copy gate and geometric attention. Our novel Neural Data Router (NDR) achieves 100\\% length generalization accuracy on the classic compositional table lookup task, as well as near-perfect accuracy on the simple arithmetic task and a new variant of ListOps testing for generalization across computational depths. NDR's attention and gating patterns tend to be interpretable as an intuitive form of neural routing. Our code is public $1^{1}$\n\n\n## 1 INTRODUCTION\n\nNeural networks (NNs) may easily learn certain training sets, but typically they do not generalize on systematically different test sets. Examples of systematic generalization (Fodor et al, 1988)\ninclude generalization to sequences longer than those seen during training-productivity, and algorithmic combinations of previously learned rules-systematicity. Despite recent efforts (Bahdanau et al., 2019, Korrel et al. 2019, Lake, 2019, Li et al., 2019, Russin et al., 2019, Csord\u00e1s et al., 2021), systematic generalization generally remains unsolved (Fodor \\& McLaughlin, 1990, Lake \\& Baroni, 2018, Liska et al., 2018, Greff et al., 2020, Hupkes et al., 2020). On some datasets, the best performing models are neuro-symbolic hybrids (Chen et al.| 2020, Liu et al. 2020) using task-specific symbolic functions. However, their applicability to other datasets remains limited (Furrer et al. 2020, Shaw et al., 2020). A big question is: which type of architectural inductive bias encourages the training process to select \"good\" solutions which generalize systematically? The popular Transformers (Vaswani et al. 2017) also often fail to generalize on algorithmic tasks (e.g. Liska et al. (2018); Dubois et al. (2020); Chaabouni et al. (2021); Csord\u00e1s et al. (2021); Onta\u00f1\u00f3n et al.(2021)), even on tasks with intuitive solutions that can be simply expressed in terms of Transformer attention patterns. Given an input sequence of length $N$ and a Transformer encoder of depth $T$, solving an algorithmic task is often all about routing the relevant information to the right node/operation at the right time in the $T$-by- $N$ grid represented by Transformer columns (illustrated in Figure 11(Left). Effectively the task is to learn to draw an adaptive control flow on the canvas of Transformer columns. In fact, recent work by Weiss et al. (2021) introduced a programming language called RASP, which is specifically designed to express solutions to sequence processing problems, and which has a direct equivalent to the operations in Transformer encoders. However, it is shown that Transformers learn solutions expressed in RASP only through intermediate supervision of attention patterns, and sometimes, even such supervision fails. Generally speaking, Transformers fail to find easily interpretable and/or symbolic solutions to algorithmic tasks. We conversely hypothesize that attention-based NNs that are able to find intuitive solutions (achieving interpretable attention patterns) could improve systematic generalization. [^0]Here we point out that regular Transformers lack some basic ingredients for learning such \"intuitive\" solutions to algorithmic problems. As a remedy, we propose simple architectural modifications to help them learn data routing. As a first step towards validating our model, we focus on the popular length generalization task of compositional table lookup (CTL; Liska et al. (2018); Hupkes et al. (2019); Dubois et al. (2020), as well as two more complex tasks: a simple arithmetic task and a variant of ListOps (Nangia \\& Bowman, 2018) designed to test the compositional generalization ability of NNs. Our novel Neural Data Router (NDR) achieves 100\\% generalization accuracy (never reported before; Dubois et al. (2020)) on the CTL task, and obtains nearly perfect accuracy on both the proposed simple arithmetic and ListOps tasks. We show that the attention and gating patterns of NDR tend to be interpretable as plausible control flows. ## 2 Improving Transformers FOR LEARning Adaptive Control Flow\n\nWe argue that the following components are needed to build Transformers capable of learning adaptive control flow. First, composing known operations in an arbitrary order requires that all operations are available at every computational step. This can be easily achieved by sharing the weights of the layers, as is done in Universal Transformers (Dehghani et al. 2019). Second, the network should be sufficiently deep, at least as deep as the deepest data dependency in the computational graph built from elementary operations (e.g., in the case of a parse tree, this is the depth of the tree). Otherwise, multiple operations must be fused into a single layer and hinder natural and elegant compositions. Third, inputs in some columns should be kept unchanged until it is their turn to be processed. The regular Transformer lacks a mechanism for skipping the whole transformation step by simply copying the input to the next step/layer. We propose a special gating function, copy gate, to implement such a mechanism (Sec.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hierarchical routing neural networks, efficient routing mechanisms, computational efficiency expressiveness balance\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. A Receptor Skeleton for Capsule Neural Networks\n\n*From Search Query: hierarchical routing neural networks*\n\n*Jintai Chen, Hongyun Yu, Chengde Qian, D. Chen, Jian Wu*\n\n**TL;DR:** A new capsule structure is presented, which contains a set of optimizable receptors and a transmitter is devised on the capsule\u2019s representation, to ensure the receptors in a CapsNet work cooperatively.\n\n**Abstract:** In previous Capsule Neural Networks (CapsNets), routing algorithms often performed clustering processes to assemble the child capsules\u2019 representations into parent capsules. Such routing algorithms were typically implemented with iterative processes and incurred high computing complexity. This paper presents a new capsule structure, which contains a set of optimizable receptors and a transmitter is devised on the capsule\u2019s representation. Speci\ufb01cally, child capsules\u2019 representations are sent to the parent capsules whose receptors match well the transmitters of the child cap-sules\u2019 representations, avoiding applying computationally complex routing algorithms. To ensure the receptors in a CapsNet work cooperatively, we build a skeleton to organize the receptors in different capsule layers in a CapsNet. The receptor skeleton assigns a share-out objective for each receptor, making the CapsNet perform as a hierarchical agglomerative clustering process. Comprehensive experiments verify that our approach facilitates ef\ufb01cient clustering processes, and Cap-sNets with our approach signi\ufb01cantly outperform CapsNets with previous routing algorithms on image classi\ufb01cation, af\ufb01ne transformation generalization, overlapped object recognition, and representation semantic decoupling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Learning Hierarchical Information Flow with Recurrent Neural Modules\n\n*From Search Query: hierarchical routing neural networks*\n\n*Danijar Hafner, A. Irpan, James Davidson, N. Heess*\n\n**TL;DR:** ThalNet, a deep learning model inspired by neocortical communication via the thalamus, is proposed, which consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps.\n\n**Abstract:** We propose ThalNet, a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. Our model outperforms standard recurrent neural networks on several sequential benchmarks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 3. Multigrid Neural Memory\n\n*From Search Query: efficient routing mechanisms*\n\n*T. Huynh, M. Maire, Matthew R. Walter*\n\n**TL;DR:** A novel approach to endowing neural networks with emergent, long-term, large-scale memory, which functions as a truly generic memory and yields excellent results on tasks decoupled from any notion of spatial geometry.\n\n**Abstract:** We introduce a novel approach to endowing neural networks with emergent, long-term, large-scale memory. Distinct from strategies that connect neural networks to external memory banks via intricately crafted controllers and hand-designed attentional mechanisms, our memory is internal, distributed, co-located alongside computation, and implicitly addressed, while being drastically simpler than prior efforts. Architecting networks with multigrid structure and connectivity, while distributing memory cells alongside computation throughout this topology, we observe the emergence of coherent memory subsystems. Our hierarchical spatial organization, parameterized convolutionally, permits efficient instantiation of large-capacity memories, while multigrid topology provides short internal routing pathways, allowing convolutional networks to efficiently approximate the behavior of fully connected networks. Such networks have an implicit capacity for internal attention; augmented with memory, they learn to read and write specific memory locations in a dynamic data-dependent manner. We demonstrate these capabilities on exploration and mapping tasks, where our network is able to self-organize and retain long-term memory for trajectories of thousands of time steps. On tasks decoupled from any notion of spatial geometry: sorting, associative recall, and question answering, our design functions as a truly generic memory and yields excellent results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 4. Routing to the Expert: Efficient Reward-guided Ensemble of Large Language Models\n\n*From Search Query: efficient routing mechanisms*\n\n*Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, Jingren Zhou*\n\n**TL;DR:** ZOOTER is proposed, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it, and shows computation efficiency in inference.\n\n**Abstract:** The complementary potential of Large Language Models (LLM) assumes off-the-shelf LLMs have heterogeneous expertise in a wide range of domains and tasks so that an ensemble of LLMs can achieve consistently better performance. Existing ensemble methods for LLMs mainly focus on reward model ranking of outputs, leading to significant computation overhead. To combat this issue, we revisit the complementary potential of LLMs and further elaborate on it by mining latent expertise with off-the-shelf reward models. We propose ZOOTER, a reward-guided routing method distilling rewards on training queries to train a routing function, which can precisely distribute each query to the LLM with expertise about it. We also integrate a tag-based label enhancement to mitigate noise from uncertainty when using rewards as silver supervision. ZOOTER shows computation efficiency in inference as it only introduces minor computation overhead of a routing function compared with reward model ranking methods. We evaluate ZOOTER on a comprehensive benchmark collection with 26 subsets in different domains and tasks. ZOOTER outperforms the best single model on average and ranks first on 44% of tasks, even surpassing multiple reward model ranking methods.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 29  (*Influential: 1*)\n\n#### 5. Merge, Then Compress: Demystify Efficient SMoE with Hints from Its Routing Policy\n\n*From Search Query: efficient routing mechanisms*\n\n*Pingzhi Li, Zhenyu (Allen) Zhang, Prateek Yadav, Yi-Lin Sung, Yu Cheng, Mohit Bansal, Tianlong Chen*\n\n**TL;DR:** M-SMoE is proposed, which leverages routing statistics to guide expert merging and further decomposes the merged experts into low-rank and structural sparse alternatives, thus diminishing the impact of insignificant experts.\n\n**Abstract:** Sparsely activated Mixture-of-Experts (SMoE) has shown promise to scale up the learning capacity of neural networks, however, they have issues like (a) High Memory Usage, due to duplication of the network layers into multiple copies as experts; and (b) Redundancy in Experts, as common learning-based routing policies suffer from representational collapse. Therefore, vanilla SMoE models are memory inefficient and non-scalable, especially for resource-constrained downstream scenarios. In this paper, we ask: Can we craft a compact SMoE model by consolidating expert information? What is the best recipe to merge multiple experts into fewer but more knowledgeable experts? Our pilot investigation reveals that conventional model merging methods fail to be effective in such expert merging for SMoE. The potential reasons are: (1) redundant information overshadows critical experts; (2) appropriate neuron permutation for each expert is missing to bring all of them in alignment. To address this, we propose M-SMoE, which leverages routing statistics to guide expert merging. Specifically, it starts with neuron permutation alignment for experts; then, dominant experts and their\"group members\"are formed; lastly, every expert group is merged into a single expert by utilizing each expert's activation frequency as their weight for merging, thus diminishing the impact of insignificant experts. Moreover, we observed that our proposed merging promotes a low dimensionality in the merged expert's weight space, naturally paving the way for additional compression. Hence, our final method, MC-SMoE (i.e., Merge, then Compress SMoE), further decomposes the merged experts into low-rank and structural sparse alternatives. Extensive experiments across 8 benchmarks validate the effectiveness of MC-SMoE. For instance, our MC-SMoE achieves up to 80% memory and a 20% FLOPs reduction, with virtually no loss in performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 6. A Non-commutative Bilinear Model for Answering Path Queries in Knowledge Graphs\n\n*From Search Query: computational efficiency expressiveness balance*\n\n*K. Hayashi, M. Shimbo*\n\n**TL;DR:** In this paper, a new bilinear KGE model is proposed, called BlockHolE, based on block circulant matrices, which can be non-commutative, allowing composite relations to be modeled by matrix product.\n\n**Abstract:** Bilinear diagonal models for knowledge graph embedding (KGE), such as DistMult and ComplEx, balance expressiveness and computational efficiency by representing relations as diagonal matrices. Although they perform well in predicting atomic relations, composite relations (relation paths) cannot be modeled naturally by the product of relation matrices, as the product of diagonal matrices is commutative and hence invariant with the order of relations. In this paper, we propose a new bilinear KGE model, called BlockHolE, based on block circulant matrices. In BlockHolE, relation matrices can be non-commutative, allowing composite relations to be modeled by matrix product. The model is parameterized in a way that covers a spectrum ranging from diagonal to full relation matrices. A fast computation technique can be developed on the basis of the duality of the Fourier transform of circulant matrices.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 7. JAWS-X: Addressing Efficiency Bottlenecks of Conformal Prediction Under Standard and Feedback Covariate Shift\n\n*From Search Query: computational efficiency expressiveness balance*\n\n*Drew Prinster, S. Saria, Anqi Liu*\n\n**TL;DR:** It is demonstrated that JAW-FCS and its computational relaxations outperform state-of-the-art baselines on a variety of real-world datasets under standard and feedback covariate shift, including for biomolecular design and active learning tasks.\n\n**Abstract:** We study the efficient estimation of predictive confidence intervals for black-box predictors when the common data exchangeability (e.g., i.i.d.) assumption is violated due to potentially feedback-induced shifts in the input data distribution. That is, we focus on standard and feedback covariate shift (FCS), where the latter allows for feedback dependencies between train and test data that occur in many decision-making scenarios like experimental design. Whereas prior conformal prediction methods for this problem are in general either extremely computationally demanding or make inefficient use of labeled data, we propose a collection of methods based on the jackknife+ that achieve a practical balance of computational and statistical efficiency. Theoretically, our proposed JAW-FCS method extends the rigorous, finite-sample coverage guarantee of the jackknife+ to FCS. We moreover propose two tunable relaxations to JAW-FCS\u2019s computation that maintain finite-sample guarantees: one using only K leave-one-out models (JAW-K LOO) and a second building on K -fold cross validation+ (WCV+). Practi-cally, we demonstrate that JAW-FCS and its computational relaxations outperform state-of-the-art baselines on a variety of real-world datasets under standard and feedback covariate shift, including for biomolecular design and active learning tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 8. Compressing Context to Enhance Inference Efficiency of Large Language Models\n\n*From Search Query: computational efficiency expressiveness balance*\n\n*Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin*\n\n**TL;DR:** Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used, indicating that it strikes a good balance between efficiency and performance.\n\n**Abstract:** Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 5*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Semantically Conditioned Dialog Response Generation via Hierarchical Disentangled Self-Attention\n\n*From Search Query: hierarchical routing neural networks*\n\n*William Yang Wang, Wenhu Chen, Pengda Qin, Xifeng Yan, Jianshu Chen*\n\n**Abstract:** Semantically controlled neural response generation on limited-domain has achieved great performance. However, moving towards multi-domain large-scale scenarios are shown to be difficult because the possible combinations of semantic inputs grow exponentially with the number of domains. To alleviate such scalability issue, we exploit the structure of dialog acts to build a multi-layer hierarchical graph, where each act is represented as a root-to-leaf route on the graph. Then, we incorporate such graph structure prior as an inductive bias to build a hierarchical disentangled self-attention network, where we disentangle attention heads to model designated nodes on the dialog act graph. By activating different (disentangled) heads at each layer, combinatorially many dialog act semantics can be modeled to control the neural response generation. On the large-scale Multi-Domain-WOZ dataset, our model can yield a significant improvement over the baselines on various automatic and human evaluation metrics.\n\n**Conference:** semantically-conditioned-dialog-response-1\n\n**Published:** 2019-05-30\n\n\n\n#### 2. Joint Slot Filling and Intent Detection via Capsule Neural Networks\n\n*From Search Query: hierarchical routing neural networks*\n\n*Yaliang Li, Philip S. Yu, Nan Du, Chenwei Zhang, Wei Fan*\n\n**Abstract:** Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.\n\n**Conference:** joint-slot-filling-and-intent-detection-via-1\n\n**Published:** 2018-12-22\n\n\n\n#### 3. Progressive Layered Extraction (PLE): A Novel Multi-Task Learning (MTL) Model for Personalized Recommendations\n\n*From Search Query: efficient routing mechanisms*\n\n*Xudong Gong, Ming Zhao, Junning Liu, Hongyan Tang*\n\n**Abstract:** Multi-task learning (MTL) has been successfully applied to many recommendation applications. However, MTL models often suffer from performance degeneration with negative transfer due to the complex and competing task correlation in real-world recommender systems. Moreover, through extensive experiments across SOTA MTL models, we have observed an interesting seesaw phenomenon that performance of one task is often improved by hurting the performance of some other tasks. To address these issues, we propose a Progressive Layered Extraction (PLE) model with a novel sharing structure design. PLE separates shared components and task-specific components explicitly and adopts a progressive routing mechanism to extract and separate deeper semantic knowledge gradually, improving efficiency of joint representation learning and information routing across tasks in a general setup. We apply PLE to both complicatedly correlated and normally correlated tasks, ranging from two-task cases to multi-task cases on a real-world Tencent video recommendation dataset with 1 billion samples, and results show that PLE outperforms state-of-the-art MTL models significantly under different task correlations and task-group size. Furthermore, online evaluation of PLE on a large-scale content recommendation platform at Tencent manifests 2.23% increase in view-count and 1.84% increase in watch time compared to SOTA MTL models, which is a significant improvement and demonstrates the effectiveness of PLE. Finally, extensive offline experiments on public benchmark datasets demonstrate that PLE can be applied to a variety of scenarios besides recommendations to eliminate the seesaw phenomenon. PLE now has been deployed to the online video recommender system in Tencent successfully.\n\n**Proceeding:** recsys-2020-2\n\n**Published:** 2020-02-22\n\n\n\n#### 4. Capsule Graph Neural Network\n\n*From Search Query: efficient routing mechanisms*\n\n*Zhang Xinyi, Lihui Chen*\n\n**Abstract:** The high-quality node embeddings learned from the Graph Neural Networks (GNNs) have been applied to a wide range of node-based applications and some of them have achieved state-of-the-art (SOTA) performance. However, when applying node embeddings learned from GNNs to generate graph embeddings, the scalar node representation may not suffice to preserve the node/graph properties efficiently, resulting in sub-optimal graph embeddings.\n\nInspired by the Capsule Neural Network (CapsNet), we propose the Capsule Graph Neural Network (CapsGNN), which adopts the concept of capsules to address the weakness in existing GNN-based graph embeddings algorithms. By extracting node features in the form of capsules, routing mechanism can be utilized to capture important information at the graph level. As a result, our model generates multiple embeddings for each graph to capture graph properties from different aspects. The attention module incorporated in CapsGNN is used to tackle graphs with various sizes which also enables the model to focus on critical parts of the graphs.\n\nOur extensive evaluations with 10 graph-structured datasets demonstrate that CapsGNN has a powerful mechanism that operates to capture macroscopic properties of the whole graph by data-driven. It outperforms other SOTA techniques on several graph classification tasks, by virtue of the new instrument.\n\n**Proceeding:** iclr-2019-5\n\n**Published:** 2019-05-01\n\n\n\n#### 5. HybridQ: A Hybrid Simulator for Quantum Circuits\n\n*From Search Query: computational efficiency expressiveness balance*\n\n*Anonymous*\n\n**Abstract:** Developing state-of-the-art classical simulators of quantum circuits is of utmost importance to test and evaluate early quantum technology and understand the true potential of full-blown error-corrected quantum computers. In the past few years, multiple theoretical and numerical advances have continuously pushed the boundary of what is classically simulable, hence the development of a plethora of tools which are often limited to a specific purpose or designed for a particular hardware (e.g. CPUs vs. GPUs). Moreover, such tools are typically developed using tailored languages and syntax, which makes it hard to compare results from, and create hybrid approaches using, different simulation techniques. To support unified and optimized use of these techniques across platforms, we developed HybridQ, a highly extensible platform designed to provide a common framework to integrate multiple state-of-the-art techniques to run on a variety of hardware. The philosophy behind its development has been driven by three main pillars: \"Easy to Use\", \"Easy to Extend\", and \"Use the Best Available Technology\". The powerful tools of HybridQ allow users to manipulate, develop, and extend noiseless and noisy circuits for different hardware architectures. HybridQ supports large-scale high-performance computing (HPC) simulations, automatically balancing workload among different processor nodes and enabling the use of multiple backends to maximize parallel efficiency. Everything is then glued together by a simple and expressive language that allows seamless switching from one technique to another as well as from one hardware to the next, without the need to write lengthy translations, thus greatly simplifying the development of new hybrid algorithms and techniques.\n\n**Published:** 2021-11-12\n\n\n\n#### 6. LivePortrait: Efficient Portrait Animation with Stitching and Retargeting Control\n\n*From Search Query: computational efficiency expressiveness balance*\n\n*Di Zhang, Pengfei Wan, Yuan Zhang, Zhizhou Zhong, Xiaoqiang Liu, Dingyun Zhang, Jianzhu Guo*\n\n**Abstract:** Portrait Animation aims to synthesize a lifelike video from a single source image, using it as an appearance reference, with motion (i.e., facial expressions and head pose) derived from a driving video, audio, text, or generation. Instead of following mainstream diffusion-based methods, we explore and extend the potential of the implicit-keypoint-based framework, which effectively balances computational efficiency and controllability. Building upon this, we develop a video-driven portrait animation framework named LivePortrait with a focus on better generalization, controllability, and efficiency for practical usage. To enhance the generation quality and generalization ability, we scale up the training data to about 69 million high-quality frames, adopt a mixed image-video training strategy, upgrade the network architecture, and design better motion transformation and optimization objectives. Additionally, we discover that compact implicit keypoints can effectively represent a kind of blendshapes and meticulously propose a stitching and two retargeting modules, which utilize a small MLP with negligible computational overhead, to enhance the controllability. Experimental results demonstrate the efficacy of our framework even compared to diffusion-based methods. The generation speed remarkably reaches 12.8ms on an RTX 4090 GPU with PyTorch. The inference code and models are available at https://github.com/KwaiVGI/LivePortrait\n\n**Published:** 2024-07-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models (LMs) by incorporating hierarchical routing mechanisms and balancing computational efficiency with model expressiveness, here are some key insights and techniques derived from the provided analysis and additional relevant information:\n\n## Hierarchical Processing\n\n### Hierarchical Architectures\nHierarchical architectures can decompose complex behaviors into manageable subprograms, which is crucial for autoregressive LMs. For instance, hierarchical document representations, as described by Chang et al. (2019), can be adapted for language models. This involves using fixed-length sentence or paragraph representations that integrate contextual information from the entire document, allowing for a more structured and efficient processing of long sequences[4,.\n\n### Multiple Timescales Processing\nUsing multiple timescales for different levels of the hierarchy can facilitate flexible integration of sequential behaviors. This approach is similar to the hierarchical associative memory networks described in the context of Hopfield networks, where neurons are organized in layers with different dynamic time scales. This can help in capturing both short-term and long-term dependencies in language models.\n\n### Block-Based Attention Mechanisms\nBlock-based attention mechanisms can balance local and global context processing. For example, hierarchical chunk attention, as mentioned in the analysis, can manage long-term dependencies efficiently by performing high-level attention over coarse summaries of chunks and then detailed attention within the most relevant chunks. This approach can be integrated into autoregressive LMs to enhance their ability to capture context at various scales.\n\n## Efficient Routing Mechanisms\n\n### Adaptive Computation\nDynamic adjustment of computational steps based on input complexity can significantly improve efficiency. Techniques like adaptive computation time for recurrent neural networks, as proposed by Graves (2016), can be adapted for autoregressive LMs. This involves integrating gated mechanisms with adaptive routing to allocate computational resources efficiently based on the input's complexity.\n\n### Hierarchical Memory Management\nTree-structured memory representations (MemTree) and hierarchical chunk attention can offer efficient organization and retrieval of context. Bounded memory approaches, such as those proposed by Toshniwal et al. (2020), can maintain performance while reducing resource requirements. These methods can be crucial in managing the memory footprint of autoregressive LMs, especially when dealing with long sequences.\n\n## Balancing Computational Efficiency and Model Expressiveness\n\n### Partial Convolutions and Frequency-Sparse Convolutions\nTechniques like partial convolutions and frequency-sparse convolutions, as described by Fu et al. (2023), can enable longer-sequence models while speeding up pretrained models without compromising quality. These methods can be adapted to autoregressive LMs to improve their efficiency and expressiveness, particularly in handling long-range dependencies.\n\n### Ladderpath Approach\nThe Ladderpath approach, which characterizes the hierarchical and nested relationships among repeating substructures in neural networks, can help in designing networks with optimal structural complexity. Networks with an order-rate \u03b7 in the middle range (around 0.5) tend to exhibit the richest hierarchical structure and superior performance. This approach can guide the design of autoregressive LMs to achieve a balance between computational efficiency and model expressiveness.\n\n## Integration Strategy\n\n### Combining Hierarchical and Adaptive Processing\nPreserving adaptive capabilities while adding hierarchical structure is key. Techniques like FastTTTLinear's gated mechanisms within a hierarchical framework can be integrated to ensure efficient routing between different processing levels. This combination allows for dynamic adjustment of computational resources based on input complexity while maintaining a structured and efficient processing hierarchy.\n\n### Efficient Routing Between Levels\nEfficient routing mechanisms, such as those used in graph neural networks (GNNs) for routing optimization, can be adapted for autoregressive LMs. For example, GNNs use link features and node aggregations to update hidden states efficiently, which can be applied to manage the flow of information between different levels of a hierarchical LM.\n\nBy incorporating these techniques, researchers can develop autoregressive language models that balance computational efficiency with model expressiveness, leveraging hierarchical processing, adaptive computation, and efficient memory management to achieve superior performance.\n",
            "analysis": "Analysis Note - Round 2\n\n1. Summary of Analysis:\n\nThe search results reveal several promising directions for combining and enhancing the parent designs:\n\na) Hierarchical Processing:\n- Hierarchical architectures can effectively decompose complex behaviors into manageable subprograms\n- Multiple timescales processing allows flexible integration of sequential behaviors\n- Block-based attention mechanisms can balance local and global context processing\n\nb) Memory Management:\n- Tree-structured memory representations (MemTree) offer efficient organization and retrieval\n- Hierarchical chunk attention shows promise for managing long-term dependencies\n- Bounded memory approaches can maintain performance while reducing resource requirements\n\nc) Adaptive Computation:\n- Dynamic adjustment of computational steps based on input complexity\n- Integration of gated mechanisms with adaptive routing\n- Efficient resource allocation through hierarchical processing\n\n2. Useful References with Excerpts:\n\nFrom Lampinen et al. (2021):\n\"HCAM stores memories by dividing the past into chunks, and recalls by first performing high-level attention over coarse summaries of the chunks, and then performing detailed attention within only the most relevant chunks.\"\n\nFrom Toshniwal et al. (2020):\n\"We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime.\"\n\nFrom Fu et al. (2023):\n\"Partial convolutions enable longer-sequence models [...] and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\"\n\nFrom Chang et al. (2019):\n\"Unlike prior work, which has focused on pre-training contextual token representations or context-independent sentence/paragraph representations, our hierarchical document representations include fixed-length sentence/paragraph representations which integrate contextual information from the entire documents.\"\n\n3. Key Insights for Design:\n\na) Memory Architecture:\n- Combine TTT's test-time training with hierarchical chunk-based memory\n- Use bounded memory tracking for efficiency while maintaining expressiveness\n- Implement tree-structured memory for better organization of context\n\nb) Processing Flow:\n- Adopt multiple timescales for different levels of the hierarchy\n- Use coarse-to-fine attention mechanisms\n- Implement adaptive computation paths based on input complexity\n\nc) Integration Strategy:\n- Preserve TTT's adaptive capabilities while adding hierarchical structure\n- Use FastTTTLinear's gated mechanisms within a hierarchical framework\n- Implement efficient routing between different processing levels\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific techniques for combining hierarchical and adaptive processing\n2. Methods for efficient routing in hierarchical neural networks\n3. Approaches to balance computational efficiency with model expressiveness\n\n5. References:\n\n1. Lampinen, A. K., Chan, S. C., Banino, A., & Hill, F. (2021). Towards mental time travel: a hierarchical memory for reinforcement learning agents. Neural Information Processing Systems.\n\n2. Toshniwal, S., Wiseman, S., Ettinger, A., Livescu, K., & Gimpel, K. (2020). Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks. Conference on Empirical Methods in Natural Language Processing.\n\n3. Fu, D. Y., Kumbong, H., Nguyen, E. N., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n4. Chang, M. W., Toutanova, K., Lee, K., & Devlin, J. (2019). Language Model Pre-training for Hierarchical Document Representations. arXiv preprint.\n\n5. Graves, A. (2016). Adaptive Computation Time for Recurrent Neural Networks. arXiv preprint.\n\n6. Jabri, A., Fleet, D. J., & Chen, T. (2022). Scalable Adaptive Computation for Iterative Generation. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "multi stream processing language models, adaptive hierarchical routing combination, efficient memory management language models",
            "detail": "Analyze recent developments in multi-stream processing architectures for language models, focusing on methods that combine adaptive and hierarchical processing streams while maintaining efficient memory usage. Include specific attention to techniques for managing information flow between different processing levels and approaches for balancing fast and slow processing paths.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in multi-stream processing architectures for language models, focusing on methods that combine adaptive and hierarchical processing streams while maintaining efficient memory usage. Include specific attention to techniques for managing information flow between different processing levels and approaches for balancing fast and slow processing paths.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.98)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.98)*\n\n```\n# EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SinKs \n\nGuangxuan Xiao $^{1 *}$ Yuandong Tian ${ }^{2} \\quad$ Beidi Chen $^{3} \\quad$ Song Han ${ }^{1,4} \\quad$ Mike Lewis $^{2}$<br>${ }^{1}$ Massachusetts Institute of Technology ${ }^{2}$ Meta AI<br>${ }^{3}$ Carnegie Mellon University ${ }^{4}$ NVIDIA<br>https://github.com/mit-han-lab/streaming-llm\n\n\n#### Abstract\n\nDeploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.\n```\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.56)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 16/49 (Score: 0.56)*\n\n```\nGovernment. Atri Rudra's research is supported by NSF grant CCF-1763481. ## References\n\n[1] Jimmy Ba, Geoffrey E Hinton, Volodymyr Mnih, Joel Z Leibo, and Catalin Ionescu. Using fast weights to attend to the recent past. Advances in neural information processing systems, 29, 2016. [2] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In International Conference on Learning Representations, 2018. [3] David H Bailey. FFTs in external or hierarchical memory. The journal of Supercomputing, 4(1):23-35, 1990 . [4] Sid Black, Leo Gao, Phil Wang, Connor Leahy, and Stella Biderman. GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow, March 2021. If you use this software, please cite it using these metadata. [5] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [6] Willian L Brogan. Modern control theory, 1974. [7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [8] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. On the properties of neural machine translation: Encoder-decoder approaches.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.43)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.43)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. MambaByte: Token-free Selective State Space Model (Avg. Score: 0.38)\n\n*Junxiong Wang, Tushaar Gangavarapu, Jing Nathan Yan, Alexander M. Rush*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 18  (*Influential: 1*)\n\n**TL;DR:** This work proposes MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences, and develops an adaptation of speculative decoding with tokenized drafting and byte-level verification, establishing the viability of SSMs in enabling token-free language modeling.\n\n**Abstract:** Token-free language models learn directly from raw bytes and remove the inductive bias of subword tokenization. Operating on bytes, however, results in significantly longer sequences. In this setting, standard autoregressive Transformers scale poorly as the effective memory required grows with sequence length. The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding. We propose MambaByte, a token-free adaptation of the Mamba SSM trained autoregressively on byte sequences. In terms of modeling, we show MambaByte to be competitive with, and even to outperform, state-of-the-art subword Transformers on language modeling tasks while maintaining the benefits of token-free language models, such as robustness to noise. In terms of efficiency, we develop an adaptation of speculative decoding with tokenized drafting and byte-level verification. This results in a $2.6\\times$ inference speedup to the standard MambaByte implementation, showing similar decoding efficiency as the subword Mamba. These findings establish the viability of SSMs in enabling token-free language modeling.\n\n##### *Relevant Chunk: No. 8/40 (Score: 0.38)*\n\n```\narXiv preprint arXiv:2402.11131, 2024. Guy E Blelloch. Prefix Sums and Their Applications. (CMU-CS-90-190), nov 1990. URL https://www.cs.cmu.edu/ guyb/papers/Ble93.pdf. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are Few-Shot Learners. Advances in neural information processing systems, 33:1877-1901, 2020. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating Large Language Model Decoding with Speculative Sampling, 2023a. Ziyi Chen, Xiaocong Yang, Jiacheng Lin, Chenkai Sun, Jie Huang, and Kevin Chen-Chuan Chang. Cascade speculative drafting for even faster llm inference. arXiv preprint arXiv:2312.11462, 2023b. Dokook Choe, Rami Al-Rfou, Mandy Guo, Heeyoung Lee, and Noah Constant. Bridging the gap for tokenizer-free language models. arXiv preprint arXiv:1908.10322, 2019. Jonathan H Clark, Dan Garrette, Iulia Turc, and John Wieting. Canine: Pre-training an Efficient Tokenization-Free Encoder for Language Representation. Transactions of the Association for Computational Linguistics, 10:73-91, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-Transformer: Filtering out Sequential Redundancy for Efficient Language Processing. Advances in neural information processing systems, 33:4271-4282, 2020. Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models.\n```\n\n#### 5. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.29)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 15/29 (Score: 0.29)*\n\n```\n2024. URL https://api semanticscholar.org/CorpusID:268681121. Simran Arora, Sabri Eyuboglu, Aman Timalsina, Isys Johnson, Michael Poli, James Zou, Atri Rudra, and Christopher R\u00e9. Zoology: Measuring and improving recall in efficient language models. arXiv:2312.04927, 2023. Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, and Christopher R\u00e9. Simple linear attention language models balance the recall-throughput tradeoff. CoRR, abs/2402.18668, 2024. doi: 10.48550/ARXIV. 2402.18668. URLhttps://doi.org/10.48550/arXiv.2402.18668\n\nMaximilian Beck, Korbinian Poppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G\u00fcnter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory. ArXiv, abs/2405.04517, 2024. URL https://api.semanticscholar.org/CorpusID:269614336\n\nStella Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373, 2023. URLhttps://api.semanticscholar.org/CorpusID:257921893\n\nDaniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, and Judy Hoffman. Hydra attention: Efficient attention with many heads.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: multi stream processing language models, adaptive hierarchical routing combination, efficient memory management language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Text2Topic: Multi-Label Text Classification System for Efficient Topic Detection in User Generated Content with Zero-Shot Capabilities\n\n*From Search Query: multi stream processing language models*\n\n*Fengjun Wang, Moran Beladev, Ofri Kleinfeld, Elina Frayerman, Tal Shachar, Eran Fainman, Karen Lastmann Assaraf, Sarai Mizrachi, Benjamin Wang*\n\n**TL;DR:** This work proposes Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that utilizes concatenation, subtraction, and multiplication of embeddings on both text and topic.\n\n**Abstract:** Multi-label text classification is a critical task in the industry. It helps to extract structured information from large amount of textual data. We propose Text to Topic (Text2Topic), which achieves high multi-label classification performance by employing a Bi-Encoder Transformer architecture that utilizes concatenation, subtraction, and multiplication of embeddings on both text and topic. Text2Topic also supports zero-shot predictions, produces domain-specific text embeddings, and enables production-scale batch-inference with high throughput. The final model achieves accurate and comprehensive results compared to state-of-the-art baselines, including large language models (LLMs). In this study, a total of 239 topics are defined, and around 1.6 million text-topic pairs annotations (in which 200K are positive) are collected on approximately 120K texts from 3 main data sources on Booking.com. The data is collected with optimized smart sampling and partial labeling. The final Text2Topic model is deployed on a real-world stream processing platform, and it outperforms other models with 92.9% micro mAP, as well as a 75.8% macro mAP score. We summarize the modeling choices which are extensively tested through ablation studies, and share detailed in-production decision-making steps.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 2. ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks\n\n*From Search Query: multi stream processing language models*\n\n*Jiasen Lu, Dhruv Batra, Devi Parikh, Stefan Lee*\n\n**TL;DR:** ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language, is presented, extending the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers.\n\n**Abstract:** We present ViLBERT (short for Vision-and-Language BERT), a model for learning task-agnostic joint representations of image content and natural language. We extend the popular BERT architecture to a multi-modal two-stream model, pro-cessing both visual and textual inputs in separate streams that interact through co-attentional transformer layers. We pretrain our model through two proxy tasks on the large, automatically collected Conceptual Captions dataset and then transfer it to multiple established vision-and-language tasks -- visual question answering, visual commonsense reasoning, referring expressions, and caption-based image retrieval -- by making only minor additions to the base architecture. We observe significant improvements across tasks compared to existing task-specific models -- achieving state-of-the-art on all four tasks. Our work represents a shift away from learning groundings between vision and language only as part of task training and towards treating visual grounding as a pretrainable and transferable capability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 3264  (*Influential: 411*)\n\n#### 3. Lifelong Language Knowledge Distillation\n\n*From Search Query: multi stream processing language models*\n\n*Yung-Sung Chuang, Shang-Yu Su, Yun-Nung Chen*\n\n**TL;DR:** The proposed Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation, is presented.\n\n**Abstract:** It is challenging to perform lifelong language learning (LLL) on a stream of different tasks without any performance degradation comparing to the multi-task counterparts. To address this issue, we present Lifelong Language Knowledge Distillation (L2KD), a simple but efficient method that can be easily applied to existing LLL architectures in order to mitigate the degradation. Specifically, when the LLL model is trained on a new task, we assign a teacher model to first learn the new task, and pass the knowledge to the LLL model via knowledge distillation. Therefore, the LLL model can better adapt to the new task while keeping the previously learned knowledge. Experiments show that the proposed L2KD consistently improves previous state-of-the-art models, and the degradation comparing to multi-task models in LLL tasks is well mitigated for both sequence generation and text classification tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 43  (*Influential: 6*)\n\n#### 4. Dynamic Routing Transformer Network for Multimodal Sarcasm Detection\n\n*From Search Query: adaptive hierarchical routing combination*\n\n*Yuan Tian, Nan Xu, Ruike Zhang, W. Mao*\n\n**TL;DR:** This work model the dynamic mechanism in multimodal sarcasm detection and proposes the Dynamic Routing Transformer Network (DynRT-Net), which utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity.\n\n**Abstract:** Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods. Our codes are available at https://github.com/TIAN-viola/DynRT.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n#### 5. Adaptive Neural Trees\n\n*From Search Query: adaptive hierarchical routing combination*\n\n*Ryutaro Tanno, Kai Arulkumaran, D. Alexander, A. Criminisi, A. Nori*\n\n**TL;DR:** Adapt neural trees via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers).\n\n**Abstract:** Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 156  (*Influential: 17*)\n\n#### 6. Coarse-to-Fine: a Hierarchical Diffusion Model for Molecule Generation in 3D\n\n*From Search Query: adaptive hierarchical routing combination*\n\n*Bo Qiang, Yuxuan Song, Minkai Xu, Jingjing Gong, B. Gao, Hao Zhou, Weiying Ma, Yanyan Lan*\n\n**TL;DR:** A Hierarchical Diffusion-based model (i.e.~HierDiff) is proposed to preserve the validity of local segments without relying on autoregressive modeling and consistently improves the quality of molecule generation over existing methods.\n\n**Abstract:** Generating desirable molecular structures in 3D is a fundamental problem for drug discovery. Despite the considerable progress we have achieved, existing methods usually generate molecules in atom resolution and ignore intrinsic local structures such as rings, which leads to poor quality in generated structures, especially when generating large molecules. Fragment-based molecule generation is a promising strategy, however, it is nontrivial to be adapted for 3D non-autoregressive generations because of the combinational optimization problems. In this paper, we utilize a coarse-to-fine strategy to tackle this problem, in which a Hierarchical Diffusion-based model (i.e.~HierDiff) is proposed to preserve the validity of local segments without relying on autoregressive modeling. Specifically, HierDiff first generates coarse-grained molecule geometries via an equivariant diffusion process, where each coarse-grained node reflects a fragment in a molecule. Then the coarse-grained nodes are decoded into fine-grained fragments by a message-passing process and a newly designed iterative refined sampling module. Lastly, the fine-grained fragments are then assembled to derive a complete atomic molecular structure. Extensive experiments demonstrate that HierDiff consistently improves the quality of molecule generation over existing methods\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 7. Learning to Ignore: Long Document Coreference with Bounded Memory Neural Networks\n\n*From Search Query: efficient memory management language models*\n\n*Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin Gimpel*\n\n**TL;DR:** It is argued that keeping all entities in memory is unnecessary, and a memory-augmented neural network that tracks only a small bounded number of entities at a time is proposed, thus guaranteeing a linear runtime in length of document.\n\n**Abstract:** Long document coreference resolution remains a challenging task due to the large memory and runtime requirements of current models. Recent work doing incremental coreference resolution using just the global representation of entities shows practical benefits but requires keeping all entities in memory, which can be impractical for long documents. We argue that keeping all entities in memory is unnecessary, and we propose a memory-augmented neural network that tracks only a small bounded number of entities at a time, thus guaranteeing a linear runtime in length of document. We show that (a) the model remains competitive with models with high memory and computational requirements on OntoNotes and LitBank, and (b) the model learns an efficient memory management strategy easily outperforming a rule-based strategy.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 45  (*Influential: 7*)\n\n#### 8. Memory-Efficient Fine-Tuning of Compressed Large Language Models via sub-4-bit Integer Quantization\n\n*From Search Query: efficient memory management language models*\n\n*Jeonghoon Kim, J. H. Lee, Sungdong Kim, Joonsuk Park, Kang Min Yoo, S. Kwon, Dongsoo Lee*\n\n**TL;DR:** Parameter-Efficient and Quantization-aware Adaptation (PEQA) is presented - a simple yet effective method that combines the advantages of PEFT with quantized LLMs and significantly reduces the memory overhead associated with the optimizer state.\n\n**Abstract:** Large language models (LLMs) face the challenges in fine-tuning and deployment due to their high memory demands and computational costs. While parameter-efficient fine-tuning (PEFT) methods aim to reduce the memory usage of the optimizer state during fine-tuning, the inherent size of pre-trained LLM weights continues to be a pressing concern. Even though quantization techniques are widely proposed to ease memory demands and accelerate LLM inference, most of these techniques are geared towards the deployment phase. To bridge this gap, this paper presents Parameter-Efficient and Quantization-aware Adaptation (PEQA) - a simple yet effective method that combines the advantages of PEFT with quantized LLMs. By updating solely the quantization scales, PEQA can be directly applied to quantized LLMs, ensuring seamless task transitions. Parallel to existing PEFT methods, PEQA significantly reduces the memory overhead associated with the optimizer state. Furthermore, it leverages the advantages of quantization to substantially reduce model sizes. Even after fine-tuning, the quantization structure of a PEQA-tuned LLM remains intact, allowing for accelerated inference on the deployment stage. We employ PEQA-tuning for task-specific adaptation on LLMs with up to 65 billion parameters. To assess the logical reasoning and language comprehension of PEQA-tuned LLMs, we fine-tune low-bit quantized LLMs using a instruction dataset. Our results show that even when LLMs are quantized to below 4-bit precision, their capabilities in language modeling, few-shot in-context learning, and comprehension can be resiliently restored to (or even improved over) their full-precision original performances with PEQA.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 65  (*Influential: 2*)\n\n#### 9. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: efficient memory management language models*\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 338  (*Influential: 59*)\n\n### 5 related papers from Papers with Code\n\n#### 1. Self-Supervised MultiModal Versatile Networks\n\n*From Search Query: multi stream processing language models*\n\n*Jeffrey De Fauw, Relja Arandjelovi\u0107, Adri\u00e0 Recasens, Jean-Baptiste Alayrac, Sander Dieleman, Rosalia Schneider, Lucas Smaira, Jason Ramapuram, Andrew Zisserman*\n\n**Abstract:** Videos are a rich source of multi-modal supervision. In this work, we learn representations using self-supervision by leveraging three modalities naturally present in videos: visual, audio and language streams. To this end, we introduce the notion of a multimodal versatile network -- a network that can ingest multiple modalities and whose representations enable downstream tasks in multiple modalities. In particular, we explore how best to combine the modalities, such that fine-grained representations of the visual and audio modalities can be maintained, whilst also integrating text into a common embedding. Driven by versatility, we also introduce a novel process of deflation, so that the networks can be effortlessly applied to the visual data in the form of video or a static image. We demonstrate how such networks trained on large collections of unlabelled video data can be applied on video, video-text, image and audio tasks. Equipped with these representations, we obtain state-of-the-art performance on multiple challenging benchmarks including UCF101, HMDB51, Kinetics600, AudioSet and ESC-50 when compared to previous self-supervised work. Our models are publicly available.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-29\n\n\n\n#### 2. A Hierarchical Multi-task Approach for Learning Embeddings from Semantic Tasks\n\n*From Search Query: multi stream processing language models*\n\n*Victor Sanh, Thomas Wolf, Sebastian Ruder*\n\n**Abstract:** Much effort has been devoted to evaluate whether multi-task learning can be\nleveraged to learn rich representations that can be used in various Natural\nLanguage Processing (NLP) down-stream applications. However, there is still a\nlack of understanding of the settings in which multi-task learning has a\nsignificant effect. In this work, we introduce a hierarchical model trained in\na multi-task learning setup on a set of carefully selected semantic tasks. The\nmodel is trained in a hierarchical fashion to introduce an inductive bias by\nsupervising a set of low level tasks at the bottom layers of the model and more\ncomplex tasks at the top layers of the model. This model achieves\nstate-of-the-art results on a number of tasks, namely Named Entity Recognition,\nEntity Mention Detection and Relation Extraction without hand-engineered\nfeatures or external NLP tools like syntactic parsers. The hierarchical\ntraining supervision induces a set of shared semantic representations at lower\nlayers of the model. We show that as we move from the bottom to the top layers\nof the model, the hidden states of the layers tend to represent more complex\nsemantic information.\n\n**Published:** 2018-11-14\n\n\n\n#### 3. AFter: Attention-based Fusion Router for RGBT Tracking\n\n*From Search Query: adaptive hierarchical routing combination*\n\n*Bin Luo, Jin Tang, Chenglong Li, Wanyu Wang, Andong Lu*\n\n**Abstract:** Multi-modal feature fusion as a core investigative component of RGBT tracking emerges numerous fusion studies in recent years. However, existing RGBT tracking methods widely adopt fixed fusion structures to integrate multi-modal feature, which are hard to handle various challenges in dynamic scenarios. To address this problem, this work presents a novel \\emph{A}ttention-based \\emph{F}usion rou\\emph{ter} called AFter, which optimizes the fusion structure to adapt to the dynamic challenging scenarios, for robust RGBT tracking. In particular, we design a fusion structure space based on the hierarchical attention network, each attention-based fusion unit corresponding to a fusion operation and a combination of these attention units corresponding to a fusion structure. Through optimizing the combination of attention-based fusion units, we can dynamically select the fusion structure to adapt to various challenging scenarios. Unlike complex search of different structures in neural architecture search algorithms, we develop a dynamic routing algorithm, which equips each attention-based fusion unit with a router, to predict the combination weights for efficient optimization of the fusion structure. Extensive experiments on five mainstream RGBT tracking datasets demonstrate the superior performance of the proposed AFter against state-of-the-art RGBT trackers. We release the code in https://github.com/Alexadlu/AFter.\n\n**Published:** 2024-05-04\n\n\n\n#### 4. Efficient Memory Management for Large Language Model Serving with PagedAttention\n\n*From Search Query: efficient memory management language models*\n\n*Ion Stoica, Hao Zhang, Joseph E. Gonzalez, Cody Hao Yu, Lianmin Zheng, Ying Sheng, Siyuan Zhuang, Zhuohan Li, Woosuk Kwon*\n\n**Abstract:** High throughput serving of large language models (LLMs) requires batching sufficiently many requests at a time. However, existing systems struggle because the key-value cache (KV cache) memory for each request is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. To address this problem, we propose PagedAttention, an attention algorithm inspired by the classical virtual memory and paging techniques in operating systems. On top of it, we build vLLM, an LLM serving system that achieves (1) near-zero waste in KV cache memory and (2) flexible sharing of KV cache within and across requests to further reduce memory usage. Our evaluations show that vLLM improves the throughput of popular LLMs by 2-4$\\times$ with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca. The improvement is more pronounced with longer sequences, larger models, and more complex decoding algorithms. vLLM's source code is publicly available at https://github.com/vllm-project/vllm\n\n**Published:** 2023-09-12\n\n\n\n#### 5. QLoRA: Efficient Finetuning of Quantized LLMs\n\n*From Search Query: efficient memory management language models*\n\n*Luke Zettlemoyer, Ari Holtzman, Artidoro Pagnoni, Tim Dettmers*\n\n**Abstract:** We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-23\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on multi-stream processing architectures that combine adaptive and hierarchical processing while maintaining efficient memory usage, here are some key insights and techniques from recent developments:\n\n## Multi-Stream Processing Architectures\n\n### Hierarchical Processing with Multiple Timescales\nThe concept of dividing computation into multiple streams, each operating at different timescales, is promising. For instance, the \"Temporal Latent Bottleneck\" approach divides computation into a slow stream for learning specialized and compressed representations and a fast stream parameterized as a Transformer to process chunks. This hierarchical processing can be adapted for language models to handle different context levels efficiently.\n\n### Adaptive and Hierarchical Routing\nThe \"Neural Data Router\" paper introduces adaptive control flow mechanisms using copy gates and geometric attention. These techniques can be integrated into language models to facilitate efficient routing between different processing levels, ensuring that the model can learn useful control flow and achieve high generalization accuracy.\n\n## Efficient Memory Management\n\n### Bounded and Structured Memory Approaches\nEfficient memory management is crucial for scalability. Techniques like bounded memory tracking and tree-structured memory representations can significantly reduce memory usage. For example, the \"Efficient Beam Tree Recursion\" strategy reduces memory usage by 10-16 times while maintaining similar performance. Similarly, \"Multigrid Neural Memory\" uses internal, distributed, and implicitly addressed memory with hierarchical spatial organization, allowing for efficient instantiation of large-capacity memories.\n\n## Balancing Fast and Slow Processing Paths\n\n### Block-Based Attention and Adaptive Computation\nImplementing block-based attention can help balance local and global context processing. This approach allows the model to focus on different context levels efficiently. Combining this with adaptive computation mechanisms, such as those in the \"Neural Data Router,\" can ensure that the model adapts its processing paths based on the input, optimizing both computational efficiency and expressiveness.\n\n## Specific Implementations in Language Models\n\n### Multi-Token Prediction and Speculative Decoding\nThe work on \"Faster Language Models with Better Multi-Token Prediction Using Tensor Decomposition\" introduces a model that predicts multiple tokens simultaneously, leveraging rank-canonical tensor decomposition. This approach can be integrated with speculative decoding techniques, which generate multiple tokens in parallel, reducing latency and improving inference speed. This multi-token prediction aligns well with the need for efficient multi-stream processing.\n\n### Hierarchical and Streaming Architectures\nThe \"Moshi\" model, which combines a text LLM with an audio language model, uses a hierarchical and streaming architecture. This architecture processes input and output audio streams jointly, allowing for real-time and full-duplex conversations. Similar hierarchical and streaming approaches can be adapted for text-only language models to improve their efficiency and scalability.\n\n## Integration Strategy\n\n### Combining Adaptive Learning with Hierarchical Processing\nCombining the adaptive learning capabilities of models like TTT with hierarchical processing can enhance the overall performance. This integration involves using efficient routing mechanisms between different processing levels and implementing bounded memory management to ensure scalability. The hierarchical processing can be structured to separate fast and slow processing streams, each handling different aspects of the input data efficiently.\n\nBy incorporating these techniques, researchers can design autoregressive language models that are not only efficient in terms of memory usage but also highly scalable and adaptive, balancing fast and slow processing paths to achieve better overall performance.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\n\nAfter analyzing the search results and parent designs, several promising directions have emerged for combining TTT and FastTTTLinear approaches:\n\na) Key Architectural Components:\n- Hierarchical processing with multiple timescales\n- Efficient routing mechanisms with adaptive computation\n- Memory management through bounded and structured approaches\n- Balance between computational efficiency and expressiveness\n\nb) Integration Opportunities:\n- Combining TTT's adaptive learning with hierarchical processing\n- Implementing efficient routing between different processing levels\n- Using bounded memory approaches for scalability\n- Leveraging block-based attention for local-global context balance\n\n2. Useful References with Excerpts:\n\nFrom Temporal Latent Bottleneck paper:\n\"divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation [...] while a fast stream is parameterized as a Transformer to process chunks\"\n\nFrom Efficient Beam Tree Recursion:\n\"strategies to remove this bottleneck and further simplify its memory usage [...] reduce the memory usage by 10-16 times while maintaining similar performance\"\n\nFrom Neural Data Router:\n\"To facilitate the learning of useful control flow, we propose [...] copy gate and geometric attention [...] achieves 100% length generalization accuracy\"\n\nFrom Multigrid Neural Memory:\n\"memory is internal, distributed, co-located alongside computation, and implicitly addressed [...] hierarchical spatial organization permits efficient instantiation of large-capacity memories\"\n\n3. Key Insights for Design:\n\na) Memory Architecture:\n- Use hierarchical memory organization with multiple timescales\n- Implement bounded memory tracking for efficiency\n- Integrate tree-structured memory representations\n- Maintain test-time training capabilities\n\nb) Processing Structure:\n- Separate fast and slow processing streams\n- Use block-based attention for different context levels\n- Implement adaptive routing mechanisms\n- Maintain linear complexity while preserving expressiveness\n\nc) Integration Strategy:\n- Combine TTT's adaptive capabilities with hierarchical processing\n- Use efficient routing between processing levels\n- Implement bounded memory management\n- Balance local and global context processing\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of multi-stream processing in language models\n2. Techniques for combining adaptive and hierarchical routing\n3. Methods for efficient memory management in large language models\n\n5. References:\n\n1. Didolkar, A., Gupta, K., Goyal, A., Lamb, A., Ke, N. R., & Bengio, Y. (2022). Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning. Neural Information Processing Systems.\n\n2. Chowdhury, J. R., & Caragea, C. (2023). Efficient Beam Tree Recursion. Neural Information Processing Systems.\n\n3. Csord\u00e1s, R., Irie, K., & Schmidhuber, J. (2021). The Neural Data Router: Adaptive Control Flow in Transformers Improves Systematic Generalization. International Conference on Learning Representations.\n\n4. Huynh, T., Maire, M., & Walter, M. R. (2019). Multigrid Neural Memory. International Conference on Machine Learning.\n\n5. Chen, J., Yu, H., Qian, C., Chen, D., & Wu, J. (2021). A Receptor Skeleton for Capsule Neural Networks. International Conference on Machine Learning.\n\n6. Hafner, D., Irpan, A., Davidson, J., & Heess, N. (2017). Learning Hierarchical Information Flow with Recurrent Neural Modules. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "adaptive neural stream fusion, efficient parallel state transfer, dynamic chunk processing",
            "detail": "Search for papers discussing:\n1. Adaptive fusion mechanisms in neural networks:\n   - Dynamic weighting strategies\n   - Cross-stream attention mechanisms\n   - Adaptive information routing\n2. Efficient state transfer in parallel neural systems:\n   - Low-latency state synchronization\n   - Memory-efficient state sharing\n   - Gradient flow optimization\n3. Dynamic chunk processing:\n   - Adaptive chunk size selection\n   - Context-aware chunking\n   - Efficient chunk boundary handling",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Adaptive fusion mechanisms in neural networks:\n   - Dynamic weighting strategies\n   - Cross-stream attention mechanisms\n   - Adaptive information routing\n2. Efficient state transfer in parallel neural systems:\n   - Low-latency state synchronization\n   - Memory-efficient state sharing\n   - Gradient flow optimization\n3. Dynamic chunk processing:\n   - Adaptive chunk size selection\n   - Context-aware chunking\n   - Efficient chunk boundary handling\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model (Avg. Score: 0.16)\n\n*Wenbing Li, Hang Zhou, Junqing Yu, Zikai Song, Wei Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Coupled SSM model is proposed, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes, and demonstrates that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n**Abstract:** The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n##### *Relevant Chunk: No. 17/22 (Score: 0.16)*\n\n```\nIn Proceedings of the conference. Association for computational linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019. [4] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. Cornell University - arXiv, Cornell University - arXiv, Jun 2014. [5] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. arXiv: Computation and Language, arXiv: Computation and Language, Jul 2017. [6] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jan 2018. [7] Guangnan Ye, Dong Liu, I-Hong Jhuo, and Shih-Fu Chang. Robust late fusion with rank minimization. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2012. [8] Dhanesh Ramachandram and Graham W. Taylor. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, page 96-108, Nov 2017. [9] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2014. [10] Valentin Vielzeuf, Alexis Lechervy, Stephane Pateux, and Frederic Jurie. Centralnet: a multilayer approach for multimodal fusion. Cornell University - arXiv,Cornell University - arXiv, Aug 2018. [11] Juan-Manuel Perez-Rua, Valentin Vielzeuf, Stephane Pateux, Moez Baccouche, and Frederic Jurie. Mfas: Multimodal fusion architecture search. [12] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, JohnR. Hershey, and TimK. Marks. Attention-based multimodal fusion for video description. Cornell University - arXiv, Cornell University - arXiv, Jan 2017. [13] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. Dec 2021. [14] Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Rong Yu, and Junzhou Huang. Deep multimodal fusion by channel exchanging. Cornell University - arXiv,Cornell University arXiv, Nov 2020. [15] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2020. [16] Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, and Jianhua Yao. Multimodal dynamics: Dynamical fusion for trustworthy multimodal classification. [17] Zihui Xue and Radu Marculescu. Dynamic multimodal fusion. [18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. [19] JimmyT.H. Smith, Andrew Warrington, and ScottW. Linderman. Simplified state space layers for sequence modeling. Aug 2022. [20] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. Jun 2022. [21] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.13)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.22)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.04)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 3. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.09)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 36/46 (Score: 0.09)*\n\n```\nURL https://arxiv.org/abs/2112.11446. J\u00fcrgen Schmidhuber. Neural sequence chunkers. Technical report, 1991. M. Schuster and K.K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681, 1997. doi: 10.1109/78.650093. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models. CoRR, abs/2005.00743, 2020a. URL https: //arxiv.org/abs/2005.00743\n\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. CoRR, abs/2002.11296, 2020b. URL https://arxiv.org/abs/2002.11296\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers.\n```\n\n#### 4. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.07)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.07)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: adaptive neural stream fusion, efficient parallel state transfer, dynamic chunk processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains\n\n*From Search Query: adaptive neural stream fusion*\n\n*Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu*\n\n**TL;DR:** It is speculated that the distinct responses and representations of the ventral and dorsal streams are more influenced by their distinct goals in visual attention and object recognition than by their specific bias or selectivity in retinal inputs.\n\n**Abstract:** The human visual system uses two parallel pathways for spatial processing and object recognition. In contrast, computer vision systems tend to use a single feedforward pathway, rendering them less robust, adaptive, or efficient than human vision. To bridge this gap, we developed a dual-stream vision model inspired by the human eyes and brain. At the input level, the model samples two complementary visual patterns to mimic how the human eyes use magnocellular and parvocellular retinal ganglion cells to separate retinal inputs to the brain. At the backend, the model processes the separate input patterns through two branches of convolutional neural networks (CNN) to mimic how the human brain uses the dorsal and ventral cortical pathways for parallel visual processing. The first branch (WhereCNN) samples a global view to learn spatial attention and control eye movements. The second branch (WhatCNN) samples a local view to represent the object around the fixation. Over time, the two branches interact recurrently to build a scene representation from moving fixations. We compared this model with the human brains processing the same movie and evaluated their functional alignment by linear transformation. The WhereCNN and WhatCNN branches were found to differentially match the dorsal and ventral pathways of the visual cortex, respectively, primarily due to their different learning objectives. These model-based results lead us to speculate that the distinct responses and representations of the ventral and dorsal streams are more influenced by their distinct goals in visual attention and object recognition than by their specific bias or selectivity in retinal inputs. This dual-stream model takes a further step in brain-inspired computer vision, enabling parallel neural networks to actively explore and understand the visual surroundings.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 2. Lookahead When It Matters: Adaptive Non-causal Transformers for Streaming Neural Transducers\n\n*From Search Query: adaptive neural stream fusion*\n\n*Grant P. Strimel, Yi Xie, Brian King, Martin H. Radfar, A. Rastrow, A. Mouchtaris*\n\n**TL;DR:** The Adaptive Non-Causal Attention Transducer (ANCAT) is presented, which is non-causal in the traditional sense, but executes in a low-latency, streaming manner by dynamically choosing when to rely on future context and to what degree within the audio stream.\n\n**Abstract:** Streaming speech recognition architectures are employed for low-latency, real-time applications. Such architectures are often characterized by their causality. Causal architectures emit tokens at each frame, relying only on current and past signal, while non-causal models are exposed to a window of future frames at each step to increase predictive accuracy. This dichotomy amounts to a trade-off for real-time Automatic Speech Recognition (ASR) system design: profit from the low-latency benefit of strictly-causal architectures while accepting predictive performance limitations, or realize the modeling benefits of future-context models accompanied by their higher latency penalty. In this work, we relax the constraints of this choice and present the Adaptive Non-Causal Attention Transducer (ANCAT). Our architecture is non-causal in the traditional sense, but executes in a low-latency, streaming manner by dynamically choosing when to rely on future context and to what degree within the audio stream. The resulting mechanism, when coupled with our novel regularization algorithms, delivers comparable accuracy to non-causal configurations while improving significantly upon latency, closing the gap with their causal counterparts. We showcase our design experimentally by reporting comparative ASR task results with measures of accuracy and latency on both publicly accessible and production-scale, voice-assistant datasets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 3. Adaptive Smoothing Gradient Learning for Spiking Neural Networks\n\n*From Search Query: adaptive neural stream fusion*\n\n*Ziming Wang, Runhao Jiang, S. Lian, Rui Yan, Huajin Tang*\n\n**TL;DR:** This work proposes a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise, in this way, the network learns adaptively the accurate gradients of loss landscape in SNNs.\n\n**Abstract:** Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is pro-hibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise. In this way, the network learns adaptively the accurate gradients of loss landscape in SNNs. The theoretical analysis further shows optimization on such a noisy network could be evolved into optimization on the embedded SNN with shared weights progressively. Moreover, The experiments on static images, dynamic event streams, speech, and instrumental sounds show the proposed method achieves state-of-the-art performance across all the datasets with remarkable robustness on different relaxation degrees.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 4. State-Free Inference of State-Space Models: The Transfer Function Approach\n\n*From Search Query: efficient parallel state transfer*\n\n*Rom N. Parnichkun, Stefano Massaroli, Alessandro Moro, Jimmy T.H. Smith, Ramin M. Hasani, Mathias Lechner, Qi An, Christopher R'e, Hajime Asama, Stefano Ermon, Taiji Suzuki, Atsushi Yamashita, Michael Poli*\n\n**TL;DR:** This work uncovers a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size.\n\n**Abstract:** We approach designing a state-space model for deep learning applications through its dual representation, the transfer function, and uncover a highly efficient sequence parallel inference algorithm that is state-free: unlike other proposed algorithms, state-free inference does not incur any significant memory or computational cost with an increase in state size. We achieve this using properties of the proposed frequency domain transfer function parametrization, which enables direct computation of its corresponding convolutional kernel's spectrum via a single Fast Fourier Transform. Our experimental results across multiple sequence lengths and state sizes illustrates, on average, a 35% training speed improvement over S4 layers -- parametrized in time-domain -- on the Long Range Arena benchmark, while delivering state-of-the-art downstream performances over other attention-free approaches. Moreover, we report improved perplexity in language modeling over a long convolutional Hyena baseline, by simply introducing our transfer function parametrization. Our code is available at https://github.com/ruke1ire/RTF.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 5. STEER: Unified Style Transfer with Expert Reinforcement\n\n*From Search Query: efficient parallel state transfer*\n\n*Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, S. Welleck, Yejin Choi*\n\n**TL;DR:** The proposed STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer, outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size.\n\n**Abstract:** While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 2*)\n\n#### 6. Cross-Lingual BERT Transformation for Zero-Shot Dependency Parsing\n\n*From Search Query: efficient parallel state transfer*\n\n*Yuxuan Wang, Wanxiang Che, Jiang Guo, Yijia Liu, Ting Liu*\n\n**TL;DR:** This paper proposes Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018).\n\n**Abstract:** This paper investigates the problem of learning cross-lingual representations in a contextual space. We propose Cross-Lingual BERT Transformation (CLBT), a simple and efficient approach to generate cross-lingual contextualized word embeddings based on publicly available pre-trained BERT models (Devlin et al., 2018). In this approach, a linear transformation is learned from contextual word alignments to align the contextualized embeddings independently trained in different languages. We demonstrate the effectiveness of this approach on zero-shot cross-lingual transfer parsing. Experiments show that our embeddings substantially outperform the previous state-of-the-art that uses static embeddings. We further compare our approach with XLM (Lample and Conneau, 2019), a recently proposed cross-lingual language model trained with massive parallel data, and achieve highly competitive results.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 112  (*Influential: 8*)\n\n#### 7. Exploiting Chunk-level Features to Improve Phrase Chunking\n\n*From Search Query: dynamic chunk processing*\n\n*Junsheng Zhou, Weiguang Qu, Fen Zhang*\n\n**TL;DR:** An efficient dynamic programming algorithm with pruning for decoding is proposed, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks.\n\n**Abstract:** Most existing systems solved the phrase chunking task with the sequence labeling approaches, in which the chunk candidates cannot be treated as a whole during parsing process so that the chunk-level features cannot be exploited in a natural way. In this paper, we formulate phrase chunking as a joint segmentation and labeling task. We propose an efficient dynamic programming algorithm with pruning for decoding, which allows the direct use of the features describing the internal characteristics of chunk and the features capturing the correlations between adjacent chunks. A relaxed, online maximum margin training algorithm is used for learning. Within this framework, we explored a variety of effective feature representations for Chinese phrase chunking. The experimental results show that the use of chunk-level features can lead to significant performance improvement, and that our approach achieves state-of-the-art performance. In particular, our approach is much better at recognizing long and complicated phrases.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2012\n\n**Citations:** 12  (*Influential: 2*)\n\n#### 8. Chunk, Align, Select: A Simple Long-sequence Processing Method for Transformers\n\n*From Search Query: dynamic chunk processing*\n\n*Jiawen Xie, Pengyu Cheng, Xiao Liang, Yong Dai, Nan Du*\n\n**TL;DR:** This work proposes a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths.\n\n**Abstract:** Although dominant in natural language processing, transformer-based models remain challenged by the task of long-sequence processing, because the computational cost of self-attention operations in transformers swells quadratically with the input sequence length. To alleviate the complexity of long-sequence processing, we propose a simple framework to enable the offthe-shelf pre-trained transformers to process much longer sequences, while the computation and memory costs remain growing linearly with the input sequence lengths. More specifically, our method divides each long-sequence input into a batch of chunks, then aligns the interchunk information during the encoding steps, and finally selects the most representative hidden states from the encoder for the decoding process. To extract inter-chunk semantic information, we align the start and end token embeddings among chunks in each encoding transformer block. To learn an effective hidden selection policy, we design a dual updating scheme inspired by reinforcement learning, which regards the decoders of transformers as environments, and the downstream performance metrics as the rewards to evaluate the hidden selection actions. Our empirical results on real-world long-text summarization and reading comprehension tasks demonstrate effective improvements compared to prior longsequence processing baselines.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 9. Chunk-based Nearest Neighbor Machine Translation\n\n*From Search Query: dynamic chunk processing*\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 Martins*\n\n**TL;DR:** Experiments on machine translation in two settings, static and \u201con-the-fly\u201d domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.\n\n**Abstract:** Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and \u201con-the-fly\u201d domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. VSR: A Unified Framework for Document Layout Analysis combining Vision, Semantics and Relations\n\n*From Search Query: adaptive neural stream fusion*\n\n*Fei Wu, Yi Niu, ShiLiang Pu, Zhanzhan Cheng, Liang Qiao, Can Li, Peng Zhang*\n\n**Abstract:** Document layout analysis is crucial for understanding document structures. On this task, vision and semantics of documents, and relations between layout components contribute to the understanding process. Though many works have been proposed to exploit the above information, they show unsatisfactory results. NLP-based methods model layout analysis as a sequence labeling task and show insufficient capabilities in layout modeling. CV-based methods model layout analysis as a detection or segmentation task, but bear limitations of inefficient modality fusion and lack of relation modeling between layout components. To address the above limitations, we propose a unified framework VSR for document layout analysis, combining vision, semantics and relations. VSR supports both NLP-based and CV-based methods. Specifically, we first introduce vision through document image and semantics through text embedding maps. Then, modality-specific visual and semantic features are extracted using a two-stream network, which are adaptively fused to make full use of complementary information. Finally, given component candidates, a relation module based on graph neural network is incorported to model relations between components and output final results. On three popular benchmarks, VSR outperforms previous models by large margins. Code will be released soon.\n\n**Published:** 2021-05-13\n\n\n\n#### 2. View Adaptive Neural Networks for High Performance Skeleton-based Human Action Recognition\n\n*From Search Query: adaptive neural stream fusion*\n\n*Wen-Jun Zeng, Cuiling Lan, Junliang Xing, Jianru Xue, Pengfei Zhang, Nanning Zheng*\n\n**Abstract:** Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in skeleton-based action recognition lies in the large view variations when capturing data. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints in a learning based data driven manner. We design two view adaptive neural networks, i.e., VA-RNN based on RNN, and VA-CNN based on CNN. For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various viewpoints to much more consistent virtual viewpoints which largely eliminates the viewpoint influence. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the fused prediction. Extensive experimental evaluations on five challenging benchmarks demonstrate that the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches. The source code is available at https://github.com/microsoft/View-Adaptive-Neural-Networks-for-Skeleton-based-Human-Action-Recognition.\n\n**Published:** 2018-04-20\n\n\n\n#### 3. Three things everyone should know about Vision Transformers\n\n*From Search Query: efficient parallel state transfer*\n\n*Herv\u00e9 J\u00e9gou, Jakob Verbeek, Alaaeldin El-Nouby, Matthieu Cord, Hugo Touvron*\n\n**Abstract:** After their initial success in natural language processing, transformer architectures have rapidly gained traction in computer vision, providing state-of-the-art results for tasks such as image classification, detection, segmentation, and video analysis. We offer three insights based on simple and easy to implement variants of vision transformers. (1) The residual layers of vision transformers, which are usually processed sequentially, can to some extent be processed efficiently in parallel without noticeably affecting the accuracy. (2) Fine-tuning the weights of the attention layers is sufficient to adapt vision transformers to a higher resolution and to other classification tasks. This saves compute, reduces the peak memory consumption at fine-tuning time, and allows sharing the majority of weights across tasks. (3) Adding MLP-based patch pre-processing layers improves Bert-like self-supervised training based on patch masking. We evaluate the impact of these design choices using the ImageNet-1k dataset, and confirm our findings on the ImageNet-v2 test set. Transfer performance is measured across six smaller datasets.\n\n**Published:** 2022-03-18\n\n\n\n#### 4. UniPT: Universal Parallel Tuning for Transfer Learning with Efficient Parameter and Memory\n\n*From Search Query: efficient parallel state transfer*\n\n*Long Chen, Huchuan Lu, Xu Jia, Ying Zhang, Bo Wan, Haiwen Diao*\n\n**Abstract:** Parameter-efficient transfer learning (PETL), i.e., fine-tuning a small portion of parameters, is an effective strategy for adapting pre-trained models to downstream domains. To further reduce the memory demand, recent PETL works focus on the more valuable memory-efficient characteristic. In this paper, we argue that the scalability, adaptability, and generalizability of state-of-the-art methods are hindered by structural dependency and pertinency on specific pre-trained backbones. To this end, we propose a new memory-efficient PETL strategy, Universal Parallel Tuning (UniPT), to mitigate these weaknesses. Specifically, we facilitate the transfer process via a lightweight and learnable parallel network, which consists of: 1) A parallel interaction module that decouples the sequential connections and processes the intermediate activations detachedly from the pre-trained network. 2) A confidence aggregation module that learns optimal strategies adaptively for integrating cross-layer features. We evaluate UniPT with different backbones (e.g., T5, VSE$\\infty$, CLIP4Clip, Clip-ViL, and MDETR) on various vision-and-language and pure NLP tasks. Extensive ablations on 18 datasets have validated that UniPT can not only dramatically reduce memory consumption and outperform the best competitor, but also achieve competitive performance over other plain PETL methods with lower training memory overhead. Our code is publicly available at: https://github.com/Paranioar/UniPT.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2023-08-28\n\n\n\n#### 5. Unified Streaming and Non-streaming Two-pass End-to-end Model for Speech Recognition\n\n*From Search Query: dynamic chunk processing*\n\n*Xin Lei, Lei Xie, Yaguang Hu, Liyong Guo, Chao Yang, Fan Yu, Xiong Wang, Zhuoyuan Yao, Di wu, BinBin Zhang*\n\n**Abstract:** In this paper, we present a novel two-pass approach to unify streaming and non-streaming end-to-end (E2E) speech recognition in a single model. Our model adopts the hybrid CTC/attention architecture, in which the conformer layers in the encoder are modified. We propose a dynamic chunk-based attention strategy to allow arbitrary right context length. At inference time, the CTC decoder generates n-best hypotheses in a streaming way. The inference latency could be easily controlled by only changing the chunk size. The CTC hypotheses are then rescored by the attention decoder to get the final result. This efficient rescoring process causes very little sentence-level latency. Our experiments on the open 170-hour AISHELL-1 dataset show that, the proposed method can unify the streaming and non-streaming model simply and efficiently. On the AISHELL-1 test set, our unified model achieves 5.60% relative character error rate (CER) reduction in non-streaming ASR compared to a standard non-streaming transformer. The same model achieves 5.42% CER with 640ms latency in a streaming ASR system.\n\n**Published:** 2020-12-10\n\n\n\n#### 6. Low-Latency Graph Streaming Using Compressed Purely-Functional Trees\n\n*From Search Query: dynamic chunk processing*\n\n*Anonymous*\n\n**Abstract:** Due to the dynamic nature of real-world graphs, there has been a growing\ninterest in the graph-streaming setting where a continuous stream of graph\nupdates is mixed with arbitrary graph queries. In principle, purely-functional\ntrees are an ideal choice for this setting due as they enable safe parallelism,\nlightweight snapshots, and strict serializability for queries. However,\ndirectly using them for graph processing would lead to significant space\noverhead and poor cache locality.\n  This paper presents C-trees, a compressed purely-functional search tree data\nstructure that significantly improves on the space usage and locality of\npurely-functional trees. The key idea is to use a chunking technique over trees\nin order to store multiple entries per tree-node. We design\ntheoretically-efficient and practical algorithms for performing batch updates\nto C-trees, and also show that we can store massive dynamic real-world graphs\nusing only a few bytes per edge, thereby achieving space usage close to that of\nthe best static graph processing frameworks.\n  To study the efficiency and applicability of our data structure, we designed\nAspen, a graph-streaming framework that extends the interface of Ligra with\noperations for updating graphs. We show that Aspen is faster than two\nstate-of-the-art graph-streaming systems, Stinger and LLAMA, while requiring\nless memory, and is competitive in performance with the state-of-the-art static\ngraph frameworks, Galois, GAP, and Ligra+. With Aspen, we are able to\nefficiently process the largest publicly-available graph with over two hundred\nbillion edges in the graph-streaming setting using a single commodity multicore\nserver with 1TB of memory.\n\n**Published:** 2019-04-17\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model (LM) with features such as adaptive neural stream fusion, efficient parallel state transfer, and dynamic chunk processing, here are some relevant insights and mechanisms from the provided sources and additional context:\n\n### Adaptive Fusion Mechanisms in Neural Networks\n\n1. **Dynamic Weighting Strategies and Cross-Stream Attention Mechanisms:**\n   - The concept of adaptive feature fusion, as seen in the multimodal and multiscale feature fusion method, can be applied to neural streams. For instance, the Adaptive Feature Fusion (AFF) module proposed in the video anomaly detection method dynamically assigns weights to different modalities based on their importance, which can be adapted for cross-stream attention mechanisms in LMs.\n   - This approach ensures that the model can adaptively fuse information from different streams, similar to how the AFF module fuses appearance and motion features.\n\n2. **Adaptive Information Routing:**\n   - The idea of multi-network and multi-scale integration from the multi-stream CNN framework can be translated to adaptive information routing in LMs. By integrating features from different streams or layers, the model can learn to route information adaptively, enhancing its ability to capture diverse contextual information.\n\n### Efficient State Transfer in Parallel Neural Systems\n\n1. **Low-Latency State Synchronization:**\n   - The synchronized parallel decoding mechanism from the Fast and Robust Early-Exiting framework can be applied here. This method synchronizes the decoding process of the current token with previously stacked early-exited tokens, which can help in low-latency state synchronization between parallel streams.\n   - Efficient state updates between streams can be achieved by using compact activation representations, such as those proposed by Activation Beacon, to reduce the overhead of state transfer.\n\n2. **Memory-Efficient State Sharing:**\n   - The attention sink mechanism from StreamingLLM can be used to manage memory efficiently. By keeping the key-value (KV) pairs of initial tokens, the model can recover performance without significant memory overhead, which is crucial for state sharing in parallel systems.\n\n3. **Gradient Flow Optimization:**\n   - The backpropagation algorithm and real-time learning mechanisms discussed in the AI-driven control of proton-boron fusion reactors can be adapted for optimizing gradient flow in parallel neural systems. This involves continuous learning and adaptation to ensure efficient and stable gradient updates across different streams.\n\n### Dynamic Chunk Processing\n\n1. **Adaptive Chunk Size Selection:**\n   - The trade-offs between chunk size, processing efficiency, and information preservation, as highlighted in the analysis note, suggest the need for adaptive chunk size selection. This can be achieved by using context-aware chunking strategies that dynamically adjust chunk sizes based on the complexity and context of the input data.\n   - The use of self-attention mechanisms, as in the multimodal and multiscale feature fusion method, can help in capturing long-term and short-term dependencies, which is essential for adaptive chunk size selection.\n\n2. **Context-Aware Chunking:**\n   - Maintaining context across chunk boundaries is crucial. The approach of using bounded contexts and efficient KV-cache management, as mentioned in the analysis note, can help in ensuring that context is preserved even when processing dynamic chunks.\n   - The dilated convolutional network with different dilation rates and self-attention mechanisms can be used to capture both short-term and long-term dependencies, making the chunking process context-aware.\n\n3. **Efficient Chunk Boundary Handling:**\n   - The sliding window approach from Activation Beacon can be used to streamingly process long contexts, which helps in handling chunk boundaries efficiently. This method ensures that the model can perceive a longer context with a limited context window, reducing the need for frequent chunk boundary adjustments.\n\nBy integrating these mechanisms, the researcher can develop an autoregressive LM that is more efficient, adaptive, and capable of handling complex contextual information, thereby improving its overall performance.\n",
            "analysis": "Analysis Note for HierTTT Proposal - Round 3\n\n1. Summary of Analysis\n\nThe third round of searches has revealed several critical insights about the HierTTT proposal's key components:\n\na) Memory Management and Streaming:\n- StreamingLLM's attention sink mechanism provides valuable insights for efficient memory management\n- Activation Beacon's approach to condensing activations offers potential solutions for the slow stream\n- The importance of bounded contexts and efficient KV-cache management\n\nb) Two-Stream Architecture:\n- Dual-stream approaches have shown success in other domains (visual processing, sign language)\n- Stream synchronization requires careful timing and coordination mechanisms\n- Need for efficient fusion mechanisms between streams\n\nc) Chunk Processing:\n- Various chunking strategies exist, from semantic to fixed-size approaches\n- Trade-offs between chunk size, processing efficiency, and information preservation\n- Importance of maintaining context across chunk boundaries\n\n2. Useful References with Excerpts\n\nFrom StreamingLLM paper:\n\"We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention [...] enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning.\"\n\nFrom Activation Beacon:\n\"condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window [...] works with the sliding window to streamingly process the long context\"\n\nFrom Fast and Robust Early-Exiting:\n\"incorporates a shallow-deep module and a synchronized parallel decoding [...] enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens\"\n\n3. Key Insights\n\na) Memory Efficiency:\n- Attention sink mechanism could be adapted for the slow stream's memory management\n- Compact activation representations could improve memory efficiency\n- Need for careful balance between memory usage and computational overhead\n\nb) Stream Synchronization:\n- Synchronized parallel decoding shows promise for coordinating streams\n- Need for adaptive mechanisms to balance stream processing speeds\n- Importance of efficient state updates between streams\n\nc) Architectural Considerations:\n- Two-stream processing requires careful coordination mechanisms\n- Need for efficient fusion strategies between streams\n- Importance of maintaining context across chunk boundaries\n\n4. Future Search Plan\n\nNeed additional information on:\n1. Adaptive fusion mechanisms between neural streams\n2. Efficient state transfer between parallel processors\n3. Dynamic chunk size adaptation methods\n\n5. References\n\n[1] Xiao, G., Tian, Y., Chen, B., Han, S., & Lewis, M. (2023). Efficient Streaming Language Models with Attention Sinks. arXiv preprint.\n\n[2] Zhang, P., Liu, Z., Xiao, S., Shao, N., Ye, Q., & Dou, Z. (2024). Soaring from 4K to 400K: Extending LLM's Context with Activation Beacon. arXiv preprint.\n\n[3] Bae, S., Ko, J., Song, H., & Yun, S. (2023). Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding. EMNLP 2023.\n\n[4] Choi, M., Han, K., Wang, X., Zhang, Y., & Liu, Z. (2023). A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains. NeurIPS 2023.\n\n[5] Kong, L., Zhou, C., Qiu, X., Gong, S., Zhang, J., & Huang, F. (2024). Training-Free Long-Context Scaling of Large Language Models. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "multi-stream language models, hierarchical neural networks, bounded context processing",
            "detail": "Find papers discussing:\n1. Multi-stream architectures in language models with focus on:\n   - Parallel processing of different timescales\n   - Memory management techniques\n   - Stream fusion methods\n2. Bounded context approaches and chunk-based processing\n3. Hierarchical state management in neural networks\n4. Integration of test-time training with efficient attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Multi-stream architectures in language models with focus on:\n   - Parallel processing of different timescales\n   - Memory management techniques\n   - Stream fusion methods\n2. Bounded context approaches and chunk-based processing\n3. Hierarchical state management in neural networks\n4. Integration of test-time training with efficient attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.91)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.91)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Scaling Laws of RoPE-based Extrapolation (Avg. Score: 0.69)\n\n*Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 36  (*Influential: 4*)\n\n**TL;DR:** This work proposes a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length, and achieves extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n**Abstract:** The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.69)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/ P16-1144. URL https://doi.org/10.18653/v1/p16-1144. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URLhttps://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.\n```\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.44)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 7/30 (Score: 0.44)*\n\n```\nV. N. Vishwanathan, and Roman Garnett, editors, Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 77-87, 2017. [6] Kyunghyun Cho, Bart van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using RNN encoderdecoder for statistical machine translation. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1724-1734, Doha, Qatar, Oct. 2014. Association for Computational Linguistics. [7] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. [8] Junyoung Chung, Sungjin Ahn, and Yoshua Bengio. Hierarchical multiscale recurrent neural networks. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. [9] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, KyungHyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [10] Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In Advances in Neural Information Processing Systems, 2022. [11] Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models. CoRR, abs/2212.14052, 2022. [12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding.\n```\n\n#### 4. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.40)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 45/64 (Score: 0.40)*\n\n```\nPlease refer to Tay et al. (2022) and Wan et al. (2023) for a comprehensive and in-depth survey of efficient transformers. Recurrent architectures Before the advent of transformers, recurrent neural networks, especially Long Short-Term Memory (LSTM) (Hochreiter \\& Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014), were the dominant architectures in NLP for sequence processing. However, traditional RNNs are hard, if not impossible, to parallelize across the time dimension, susceptible to gradient vanishing and explosion, and ineffective in capturing long-range dependencies, which are ubiquitous in natural language. These shortcomings contributed to the rapid decline of traditional RNNs in NLP. There has been a revival of RNNs in NLP research (Tiezzi et al., 2024) in recent years. Compared to transformers with quadratic complexity, RNNs are highly efficient in autoregressive inference with $\\mathrm{O}(1)$ time complexity per step, making them an attractive architecture for large language models. Many efforts have been devoted to parallelized recurrent models and improving their capability to capture long-range dependency, while maintaining the low inference complexity. The Legendre Memory Unit (LMU) (Voelker et al., 2019) was designed to efficiently handle longrange dependencies with a new type of memory cell for recurrent neural networks. Unlike LSTM units, which struggle with remembering information over very long sequences, LMU use Legendre polynomials to create a memory system that can maintain and process information over extended time periods more effectively. High-order polynomial projection operators (HiPPO) (Gu et al., 2020) generalizes LMU by providing a flexible framework for online compression of signals through polynomial projections, accommodating various polynomial bases beyond Legendre polynomials. It optimizes function approximation over time, adapting to different data timescales without needing predefined hyperparameters. SSMs have inspired a range of follow-up research to incorporate SSMs, or modified SSMs into end-to-end architectures for language modeling, including MEGA (Ma et al., 2022), DSS (Gupta et al., 2022), H3 (Fu et al., 2022), and Linear Recurrent Unit (LRU) (Orvieto et al., 2023). Mamba (Gu \\& Dao, 2023) is a selective SSM that introduces time-dependent selective mechanism to enhance the long-range modeling ability of SSMs. The selectivity removes the linear time-variance property of the SSM, making it no longer possible to parallelize Mamba as a long convolution kernel. Yet Mamba can still be effectively parallelized using parallel associative scan\n(Blelloch, 1990; Martin \\& Cundy, 2018; Smith et al., 2023) with a hardware-aware implementation. Recently proposed GateLoop (Katsch, 2023) also adopts a similar data-dependent state transitions. The data-dependent states, also concurrently proposed in GLA (Yang et al., 2023), are similar to the Weighted Key-Value State in Finch. A contemporary but independent work also proposes recurrent models named as Hawk and Griffin (De et al., 2024). Hawk is a recurrent model with the Real-Gated Linear Recurrent Unit (RG-LRU), whereas Griffin mixes the RG-LRU with local multi-query attention, thereby achieving long-context extrapolation efficiently. Please see Tiezzi et al.\n```\n\n#### 5. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.36)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 26/28 (Score: 0.36)*\n\n```\nAccessed: 2024-05-04. [39] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [40] Chenghao Yang, Zi Yang, and Nan Hua. Equipping transformer with random-access reading for long-context understanding. arXiv preprint arXiv:2405.13216, 2024. [41] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: multi-stream language models, hierarchical neural networks, bounded context processing\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Cross-Lingual Knowledge Editing in Large Language Models\n\n*From Search Query: multi-stream language models*\n\n*Jiaan Wang, Yunlong Liang, Zengkui Sun, Yu Cao, Jiarong Xu*\n\n**TL;DR:** This paper collects a large-scale cross-lingual synthetic dataset, conducts English editing on various knowledge editing methods covering different paradigms, and evaluates their performance in Chinese, and vice versa to figure out this cross-lingual effect in knowledge editing.\n\n**Abstract:** Knowledge editing aims to change language models' performance on several special cases (i.e., editing scope) by infusing the corresponding expected knowledge into them. With the recent advancements in large language models (LLMs), knowledge editing has been shown as a promising technique to adapt LLMs to new knowledge without retraining from scratch. However, most of the previous studies neglect the multi-lingual nature of some main-stream LLMs (e.g., LLaMA, ChatGPT and GPT-4), and typically focus on monolingual scenarios, where LLMs are edited and evaluated in the same language. As a result, it is still unknown the effect of source language editing on a different target language. In this paper, we aim to figure out this cross-lingual effect in knowledge editing. Specifically, we first collect a large-scale cross-lingual synthetic dataset by translating ZsRE from English to Chinese. Then, we conduct English editing on various knowledge editing methods covering different paradigms, and evaluate their performance in Chinese, and vice versa. To give deeper analyses of the cross-lingual effect, the evaluation includes four aspects, i.e., reliability, generality, locality and portability. Furthermore, we analyze the inconsistent behaviors of the edited models and discuss their specific challenges. Data and codes are available at https://github.com/krystalan/Bi_ZsRE\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 2. Look, Remember and Reason: Grounded Reasoning in Videos with Language Models\n\n*From Search Query: multi-stream language models*\n\n*Apratim Bhattacharyya, Sunny Panchal, Mingu Lee, Reza Pourreza, Pulkit Madan, Roland Memisevic*\n\n**TL;DR:** This work proposes training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities and demonstrates the effectiveness of this framework on diverse visual reasoning tasks from the ACRE, CATER, Something-Else and STAR datasets.\n\n**Abstract:** Multi-modal language models (LM) have recently shown promising performance in high-level reasoning tasks on videos. However, existing methods still fall short in tasks like causal or compositional spatiotemporal reasoning over actions, in which model predictions need to be grounded in fine-grained low-level details, such as object motions and object interactions. In this work, we propose training an LM end-to-end on low-level surrogate tasks, including object detection, re-identification, and tracking, to endow the model with the required low-level visual capabilities. We show that a two-stream video encoder with spatiotemporal attention is effective at capturing the required static and motion-based cues in the video. By leveraging the LM's ability to perform the low-level surrogate tasks, we can cast reasoning in videos as the three-step process of Look, Remember, Reason wherein visual information is extracted using low-level visual skills step-by-step and then integrated to arrive at a final answer. We demonstrate the effectiveness of our framework on diverse visual reasoning tasks from the ACRE, CATER, Something-Else and STAR datasets. Our approach is trainable end-to-end and surpasses state-of-the-art task-specific methods across these tasks by a large margin.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Robust CLIP: Unsupervised Adversarial Fine-Tuning of Vision Embeddings for Robust Large Vision-Language Models\n\n*From Search Query: multi-stream language models*\n\n*Christian Schlarmann, Naman D. Singh, Francesco Croce, Matthias Hein*\n\n**TL;DR:** It is shown that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with a robust one, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP.\n\n**Abstract:** Multi-modal foundation models like OpenFlamingo, LLaVA, and GPT-4 are increasingly used for various real-world tasks. Prior work has shown that these models are highly vulnerable to adversarial attacks on the vision modality. These attacks can be leveraged to spread fake information or defraud users, and thus pose a significant risk, which makes the robustness of large multi-modal foundation models a pressing problem. The CLIP model, or one of its variants, is used as a frozen vision encoder in many large vision-language models (LVLMs), e.g. LLaVA and OpenFlamingo. We propose an unsupervised adversarial fine-tuning scheme to obtain a robust CLIP vision encoder, which yields robustness on all vision down-stream tasks (LVLMs, zero-shot classification) that rely on CLIP. In particular, we show that stealth-attacks on users of LVLMs by a malicious third party providing manipulated images are no longer possible once one replaces the original CLIP model with our robust one. No retraining or fine-tuning of the down-stream LVLMs is required. The code and robust models are available at https://github.com/chs20/RobustVLM\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 4. Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts\n\n*From Search Query: hierarchical neural networks*\n\n*Di Jin, Peter Szolovits*\n\n**TL;DR:** This work presents a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence.\n\n**Abstract:** Prevalent models based on artificial neural network (ANN) for sentence classification often classify sentences in isolation without considering the context in which sentences appear. This hampers the traditional sentence classification approaches to the problem of sequential sentence classification, where structured prediction is needed for better overall classification performance. In this work, we present a hierarchical sequential labeling network to make use of the contextual information within surrounding sentences to help classify the current sentence. Our model outperforms the state-of-the-art results by 2%-3% on two benchmarking datasets for sequential sentence classification in medical scientific abstracts.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2018\n\n**Citations:** 71  (*Influential: 16*)\n\n#### 5. How poor is the stimulus? Evaluating hierarchical generalization in neural networks trained on child-directed speech\n\n*From Search Query: hierarchical neural networks*\n\n*Aditya Yedetore, Tal Linzen, R. Frank, R. Thomas McCoy*\n\n**TL;DR:** It is suggested that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.\n\n**Abstract:** When acquiring syntax, children consistently choose hierarchical rules over competing non-hierarchical possibilities. Is this preference due to a learning bias for hierarchical structure, or due to more general biases that interact with hierarchical cues in children\u2019s linguistic input? We explore these possibilities by training LSTMs and Transformers - two types of neural networks without a hierarchical bias - on data similar in quantity and content to children\u2019s linguistic input: text from the CHILDES corpus. We then evaluate what these models have learned about English yes/no questions, a phenomenon for which hierarchical structure is crucial. We find that, though they perform well at capturing the surface statistics of child-directed speech (as measured by perplexity), both model types generalize in a way more consistent with an incorrect linear rule than the correct hierarchical rule. These results suggest that human-like generalization from text alone requires stronger biases than the general sequence-processing biases of standard neural network architectures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 6. Discourse Parsing with Attention-based Hierarchical Neural Networks\n\n*From Search Query: hierarchical neural networks*\n\n*Qi Li, Tianshi Li, Baobao Chang*\n\n**TL;DR:** Experimental results show that the proposed attention-based hierarchical neural network model for discourse parsing obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.\n\n**Abstract:** RST-style document-level discourse parsing remains a dif\ufb01cult task and ef\ufb01cient deep learning models on this task have rarely been presented. In this paper, we propose an attention-based hierarchical neural network model for discourse parsing. We also incorporate tensor-based transformation function to model complicated feature interactions. Experimental results show that our approach obtains comparable performance to the contemporary state-of-the-art systems with little manual feature engineering.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2016\n\n**Citations:** 98  (*Influential: 14*)\n\n#### 7. Context-lumpable stochastic bandits\n\n*From Search Query: bounded context processing*\n\n*Chung-Wei Lee, Qinghua Liu, Yasin Abbasi-Yadkori, Chi Jin, Tor Lattimore, Csaba Szepesvari*\n\n**TL;DR:** The first to show the near-optimal sample complexity in the PAC setting and $\\widetilde O(\\sqrt{{poly}(r)(S+K)T})$ minimax regret in the online setting for this problem are shown.\n\n**Abstract:** We consider a contextual bandit problem with $S$ contexts and $K$ actions. In each round $t=1,2,\\dots$, the learner observes a random context and chooses an action based on its past experience. The learner then observes a random reward whose mean is a function of the context and the action for the round. Under the assumption that the contexts can be lumped into $r\\le \\min\\{S,K\\}$ groups such that the mean reward for the various actions is the same for any two contexts that are in the same group, we give an algorithm that outputs an $\\epsilon$-optimal policy after using at most $\\widetilde O(r (S +K )/\\epsilon^2)$ samples with high probability and provide a matching $\\Omega(r(S+K)/\\epsilon^2)$ lower bound. In the regret minimization setting, we give an algorithm whose cumulative regret up to time $T$ is bounded by $\\widetilde O(\\sqrt{r^3(S+K)T})$. To the best of our knowledge, we are the first to show the near-optimal sample complexity in the PAC setting and $\\widetilde O(\\sqrt{{poly}(r)(S+K)T})$ minimax regret in the online setting for this problem. We also show our algorithms can be applied to more general low-rank bandits and get improved regret bounds in some scenarios.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 1*)\n\n#### 8. Inverse Game Theory for Stackelberg Games: the Blessing of Bounded Rationality\n\n*From Search Query: bounded context processing*\n\n*Jibang Wu, Weiran Shen, Fei Fang, Haifeng Xu*\n\n**TL;DR:** This work relaxes the perfect rationality agent assumption to the classic quantal response model, a more realistic behavior model of bounded rationality, and shows that the smooth property brought by such bounded rationality model actually leads to provably more efficient learning of the follower utility parameters in general Stackelberg games.\n\n**Abstract:** Optimizing strategic decisions (a.k.a. computing equilibrium) is key to the success of many non-cooperative multi-agent applications. However, in many real-world situations, we may face the exact opposite of this game-theoretic problem -- instead of prescribing equilibrium of a given game, we may directly observe the agents' equilibrium behaviors but want to infer the underlying parameters of an unknown game. This research question, also known as inverse game theory, has been studied in multiple recent works in the context of Stackelberg games. Unfortunately, existing works exhibit quite negative results, showing statistical hardness and computational hardness, assuming follower's perfectly rational behaviors. Our work relaxes the perfect rationality agent assumption to the classic quantal response model, a more realistic behavior model of bounded rationality. Interestingly, we show that the smooth property brought by such bounded rationality model actually leads to provably more efficient learning of the follower utility parameters in general Stackelberg games. Systematic empirical experiments on synthesized games confirm our theoretical results and further suggest its robustness beyond the strict quantal response model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 9. Adaptive Data Debiasing through Bounded Exploration and Fairness\n\n*From Search Query: bounded context processing*\n\n*Yifan Yang, Yang Liu, Parinaz Naghizadeh*\n\n**TL;DR:** This work proposes an algorithm for sequentially debiasing such datasets through adaptive and bounded exploration in a classification problem with costly and censored feedback, and analytically shows that such exploration can help debias data in certain distributions.\n\n**Abstract:** Biases in existing datasets used to train algorithmic decision rules can raise ethical and economic concerns due to the resulting disparate treatment of different groups. We propose an algorithm for sequentially debiasing such datasets through adaptive and bounded exploration in a classification problem with costly and censored feedback. Exploration in this context means that at times, and to a judiciously-chosen extent, the decision maker deviates from its (current) loss-minimizing rule, and instead accepts some individuals that would otherwise be rejected, so as to reduce statistical data biases. Our proposed algorithm includes parameters that can be used to balance between the ultimate goal of removing data biases -- which will in turn lead to more accurate and fair decisions, and the exploration risks incurred to achieve this goal. We analytically show that such exploration can help debias data in certain distributions. We further investigate how fairness criteria can work in conjunction with our data debiasing algorithm. We illustrate the performance of our algorithm using experiments on synthetic and real-world datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 1*)\n\n### 2 related papers from ArXiv\n\n#### 1. Hierarchical Neural Network Architecture In Keyword Spotting\n\n*From Search Query: hierarchical neural networks*\n\n*Yixiao Qu, Sihao Xue, Zhenyi Ying, Hang Zhou, Jue Sun*\n\n**Abstract:** Keyword Spotting (KWS) provides the start signal of ASR problem, and thus it\nis essential to ensure a high recall rate. However, its real-time property\nrequires low computation complexity. This contradiction inspires people to find\na suitable model which is small enough to perform well in multi environments.\nTo deal with this contradiction, we implement the Hierarchical Neural\nNetwork(HNN), which is proved to be effective in many speech recognition\nproblems. HNN outperforms traditional DNN and CNN even though its model size\nand computation complexity are slightly less. Also, its simple topology\nstructure makes easy to deploy on any device.\n\n**Published:** 2018-11-06T12:32:27Z  (*Updated: 2018-11-06T12:32:27Z*)\n\n\n\n#### 2. Boilerplate Detection via Semantic Classification of TextBlocks\n\n*From Search Query: hierarchical neural networks*\n\n*Hao Zhang, Jie Wang*\n\n**Abstract:** We present a hierarchical neural network model called SemText to detect HTML\nboilerplate based on a novel semantic representation of HTML tags, class names,\nand text blocks. We train SemText on three published datasets of news webpages\nand fine-tune it using a small number of development data in CleanEval and\nGoogleTrends-2017. We show that SemText achieves the state-of-the-art accuracy\non these datasets. We then demonstrate the robustness of SemText by showing\nthat it also detects boilerplate effectively on out-of-domain community-based\nquestion-answer webpages.\n\n**Published:** 2022-03-09T01:01:49Z  (*Updated: 2022-03-09T01:01:49Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. SoCodec: A Semantic-Ordered Multi-Stream Speech Codec for Efficient Language Model Based Text-to-Speech Synthesis\n\n*From Search Query: multi-stream language models*\n\n*Anonymous*\n\n**Abstract:** The long speech sequence has been troubling language models (LM) based TTS approaches in terms of modeling complexity and efficiency. This work proposes SoCodec, a semantic-ordered multi-stream speech codec, to address this issue. It compresses speech into a shorter, multi-stream discrete semantic sequence with multiple tokens at each frame. Meanwhile, the ordered product quantization is proposed to constrain this sequence into an ordered representation. It can be applied with a multi-stream delayed LM to achieve better autoregressive generation along both time and stream axes in TTS. The experimental result strongly demonstrates the effectiveness of the proposed approach, achieving superior performance over baseline systems even if compressing the frameshift of speech from 20ms to 240ms (12x). The ablation studies further validate the importance of learning the proposed ordered multi-stream semantic representation in pursuing shorter speech sequences for efficient LM-based TTS.\n\n**Published:** 2024-09-02\n\n\n\n#### 2. LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding\n\n*From Search Query: multi-stream language models*\n\n*Lidong Zhou, Min Zhang, Wanxiang Che, Cha Zhang, Dinei Florencio, Yijuan Lu, Guoxin Wang, Furu Wei, Lei Cui, Tengchao Lv, Yiheng Xu, Yang Xu*\n\n**Abstract:** Pre-training of text and layout has proved effective in a variety of visually-rich document understanding tasks due to its effective model architecture and the advantage of large-scale unlabeled scanned/digital-born documents. We propose LayoutLMv2 architecture with new pre-training tasks to model the interaction among text, layout, and image in a single multi-modal framework. Specifically, with a two-stream multi-modal Transformer encoder, LayoutLMv2 uses not only the existing masked visual-language modeling task but also the new text-image alignment and text-image matching tasks, which make it better capture the cross-modality interaction in the pre-training stage. Meanwhile, it also integrates a spatial-aware self-attention mechanism into the Transformer architecture so that the model can fully understand the relative positional relationship among different text blocks. Experiment results show that LayoutLMv2 outperforms LayoutLM by a large margin and achieves new state-of-the-art results on a wide variety of downstream visually-rich document understanding tasks, including FUNSD (0.7895 $\\to$ 0.8420), CORD (0.9493 $\\to$ 0.9601), SROIE (0.9524 $\\to$ 0.9781), Kleister-NDA (0.8340 $\\to$ 0.8520), RVL-CDIP (0.9443 $\\to$ 0.9564), and DocVQA (0.7295 $\\to$ 0.8672). We made our model and code publicly available at \\url{https://aka.ms/layoutlmv2}.\n\n**Proceeding:** acl-2021-5\n\n**Published:** 2020-12-29\n\n\n\n#### 3. Learning Hierarchical Graph Neural Networks for Image Clustering\n\n*From Search Query: hierarchical neural networks*\n\n*David Wipf, Stefano Soatto, Zheng Zhang, Wei Xia, Yuanjun Xiong, Yongxin Wang, Tianjun Xiao, Tong He, Yifan Xing*\n\n**Abstract:** We propose a hierarchical graph neural network (GNN) model that learns how to cluster a set of images into an unknown number of identities using a training set of images annotated with labels belonging to a disjoint set of identities. Our hierarchical GNN uses a novel approach to merge connected components predicted at each level of the hierarchy to form a new graph at the next level. Unlike fully unsupervised hierarchical clustering, the choice of grouping and complexity criteria stems naturally from supervision in the training set. The resulting method, Hi-LANDER, achieves an average of 54% improvement in F-score and 8% increase in Normalized Mutual Information (NMI) relative to current GNN-based clustering algorithms. Additionally, state-of-the-art GNN-based methods rely on separate models to predict linkage probabilities and node densities as intermediate steps of the clustering process. In contrast, our unified framework achieves a seven-fold decrease in computational cost. We release our training and inference code at https://github.com/dmlc/dgl/tree/master/examples/pytorch/hilander.\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-07-03\n\n\n\n#### 4. HS-ResNet: Hierarchical-Split Block on Convolutional Neural Network\n\n*From Search Query: hierarchical neural networks*\n\n*Shumin Han, Errui Ding, Dongliang He, Ruoyu Guo, Yuning Du, Cheng Cui, Shufei Lin, Pengcheng Yuan*\n\n**Abstract:** This paper addresses representational block named Hierarchical-Split Block, which can be taken as a plug-and-play block to upgrade existing convolutional neural networks, improves model performance significantly in a network. Hierarchical-Split Block contains many hierarchical split and concatenate connections within one single residual block. We find multi-scale features is of great importance for numerous vision tasks. Moreover, Hierarchical-Split block is very flexible and efficient, which provides a large space of potential network architectures for different applications. In this work, we present a common backbone based on Hierarchical-Split block for tasks: image classification, object detection, instance segmentation and semantic image segmentation/parsing. Our approach shows significant improvements over all these core tasks in comparison with the baseline. As shown in Figure1, for image classification, our 50-layers network(HS-ResNet50) achieves 81.28% top-1 accuracy with competitive latency on ImageNet-1k dataset. It also outperforms most state-of-the-art models. The source code and models will be available on: https://github.com/PaddlePaddle/PaddleClas\n\n**Published:** 2020-10-15\n\n\n\n#### 5. SNIPER: Efficient Multi-Scale Training\n\n*From Search Query: bounded context processing*\n\n*Mahyar Najibi, Larry S. Davis, Bharat Singh*\n\n**Abstract:** We present SNIPER, an algorithm for performing efficient multi-scale training\nin instance level visual recognition tasks. Instead of processing every pixel\nin an image pyramid, SNIPER processes context regions around ground-truth\ninstances (referred to as chips) at the appropriate scale. For background\nsampling, these context-regions are generated using proposals extracted from a\nregion proposal network trained with a short learning schedule. Hence, the\nnumber of chips generated per image during training adaptively changes based on\nthe scene complexity. SNIPER only processes 30% more pixels compared to the\ncommonly used single scale training at 800x1333 pixels on the COCO dataset.\nBut, it also observes samples from extreme resolutions of the image pyramid,\nlike 1400x2000 pixels. As SNIPER operates on resampled low resolution chips\n(512x512 pixels), it can have a batch size as large as 20 on a single GPU even\nwith a ResNet-101 backbone. Therefore it can benefit from batch-normalization\nduring training without the need for synchronizing batch-normalization\nstatistics across GPUs. SNIPER brings training of instance level recognition\ntasks like object detection closer to the protocol for image classification and\nsuggests that the commonly accepted guideline that it is important to train on\nhigh resolution images for instance level visual recognition tasks might not be\ncorrect. Our implementation based on Faster-RCNN with a ResNet-101 backbone\nobtains an mAP of 47.6% on the COCO dataset for bounding box detection and can\nprocess 5 images per second during inference with a single GPU. Code is\navailable at https://github.com/MahyarNajibi/SNIPER/.\n\n**Conference:** sniper-efficient-multi-scale-training-1\n\n**Published:** 2018-05-23\n\n\n\n#### 6. Spatial Memory for Context Reasoning in Object Detection\n\n*From Search Query: bounded context processing*\n\n*Abhinav Gupta, Xinlei Chen*\n\n**Abstract:** Modeling instance-level context and object-object relationships is extremely\nchallenging. It requires reasoning about bounding boxes of different classes,\nlocations \\etc. Above all, instance-level spatial reasoning inherently requires\nmodeling conditional distributions on previous detections. Unfortunately, our\ncurrent object detection systems do not have any {\\bf memory} to remember what\nto condition on! The state-of-the-art object detectors still detect all object\nin parallel followed by non-maximal suppression (NMS). While memory has been\nused for tasks such as captioning, they mostly use image-level memory cells\nwithout capturing the spatial layout. On the other hand, modeling object-object\nrelationships requires {\\bf spatial} reasoning -- not only do we need a memory\nto store the spatial layout, but also a effective reasoning module to extract\nspatial patterns. This paper presents a conceptually simple yet powerful\nsolution -- Spatial Memory Network (SMN), to model the instance-level context\nefficiently and effectively. Our spatial memory essentially assembles object\ninstances back into a pseudo \"image\" representation that is easy to be fed into\nanother ConvNet for object-object context reasoning. This leads to a new\nsequential reasoning architecture where image and memory are processed in\nparallel to obtain detections which update the memory again. We show our SMN\ndirection is promising as it provides 2.2\\% improvement over baseline Faster\nRCNN on the COCO dataset so far.\n\n**Conference:** spatial-memory-for-context-reasoning-in-1\n\n**Published:** 2017-04-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on multi-stream architectures, bounded context processing, and hierarchical state management, here are some key points and relevant information:\n\n## Multi-Stream Architectures\n\n### Parallel Processing of Different Timescales\n- Multi-stream architectures can be designed to process different timescales in parallel, similar to the concept of slow and fast streams. This approach is reminiscent of models that handle multiple temporal scales, such as those using hierarchical or multi-resolution representations. However, the provided sources do not directly discuss multi-stream language models, but the idea can be inferred from hierarchical neural network designs and parallel processing techniques.\n\n### Memory Management Techniques\n- For memory management, techniques like **optimizer state sharding**, **activation checkpointing**, and **activation offloading** are crucial. These methods are discussed in the context of model parallelism in Amazon SageMaker, which can be applied to multi-stream architectures to manage memory efficiently.\n\n### Stream Fusion Methods\n- Stream fusion involves combining the outputs or states of different streams. While the sources do not explicitly discuss stream fusion in language models, the concept of **pipeline parallelism** and **tensor parallelism** can provide insights into how different components of a model can be combined efficiently. For example, using pipeline parallelism across multiple nodes and tensor parallelism within nodes can help in managing the fusion process efficiently.\n\n## Bounded Context Approaches and Chunk-Based Processing\n\n### Bounded Context Processing\n- Bounded context approaches are essential for managing memory and computational resources. **Sharded data parallelism** is a technique that splits the state of a model across GPUs, which can be seen as a form of bounded context processing. This method helps in reducing memory usage and can be applied to chunk-based processing where the model processes data in smaller chunks.\n\n### Chunk-Based Processing\n- Chunk-based processing is a common technique in language models to handle long sequences efficiently. By processing data in chunks, models can avoid running out of memory and improve computational efficiency. This approach aligns with the **pipeline parallelism** and **tensor parallelism** methods, where the model is partitioned and processed in smaller segments across multiple GPUs.\n\n## Hierarchical State Management in Neural Networks\n\n### Hierarchical State Management\n- Hierarchical state management involves organizing the state of the model in a hierarchical manner to manage complexity and memory usage. This can be achieved through techniques like **hierarchical neural networks**, where the model is structured in layers or modules that manage different aspects of the state. The SageMaker model parallel library discusses various types of model parallelism, including pipeline and tensor parallelism, which can be seen as hierarchical state management strategies.\n\n## Integration of Test-Time Training with Efficient Attention Mechanisms\n\n### Test-Time Training (TTT) and Efficient Attention\n- Integrating test-time training with efficient attention mechanisms, such as those described in \"FastTTTLinear: Enhancing TTTLinear with Gated Linear Attention,\" can enhance the adaptive capabilities of the model while optimizing memory usage. Efficient attention mechanisms like gated linear attention can help in preserving the adaptive capabilities of TTT while improving computational efficiency[Initial Analysis of HierTTT Proposal].\n\n### Summary of Relevant Techniques\n\n- **Model Parallelism**: Techniques such as pipeline parallelism, tensor parallelism, and sharded data parallelism can help in managing multi-stream architectures and bounded context processing efficiently.\n- **Memory Management**: Optimizer state sharding, activation checkpointing, and activation offloading are crucial for managing memory in large models.\n- **Hierarchical Designs**: Hierarchical neural networks and hierarchical state management strategies can help in organizing the model's state and processing data in a structured manner.\n- **Efficient Attention**: Gated linear attention and other efficient attention mechanisms can be integrated with test-time training to enhance the model's adaptive capabilities while optimizing computational resources[Initial Analysis of HierTTT Proposal].\n\nBy combining these techniques, researchers can design more efficient and scalable autoregressive language models that handle different timescales, manage memory effectively, and integrate adaptive training mechanisms.\n",
            "analysis": "Initial Analysis of HierTTT Proposal\n\n1. Key Aspects Requiring Investigation:\n\na) Two-Stream Architecture:\n- Novel combination of slow and fast streams for processing different timescales\n- Potential impact on computational efficiency and memory management\n- Comparison with existing multi-stream architectures in language models\n\nb) Memory Management:\n- Bounded context approach for memory efficiency\n- Hierarchical state management strategy\n- Effectiveness of chunk-based processing\n\nc) Integration of Parent Designs:\n- Preservation of TTT's adaptive capabilities\n- Incorporation of FastTTTLinear's efficiency improvements\n- Potential synergies between the two approaches\n\nd) Stream Fusion:\n- Effectiveness of adaptive weighting mechanism\n- Impact on gradient flow\n- Computational overhead of fusion process\n\n2. Investigation Plan:\n\nPhase 1: Multi-Stream Architectures\n- Search for existing multi-stream approaches in language models\n- Analyze effectiveness of similar hierarchical designs\n- Identify potential challenges and solutions\n\nPhase 2: Memory Management Techniques\n- Research bounded context approaches\n- Investigate chunk-based processing methods\n- Study hierarchical state management strategies\n\nPhase 3: Stream Fusion Methods\n- Examine existing fusion techniques\n- Analyze adaptive weighting mechanisms\n- Research gradient flow optimization\n\n3. Initial Assessment:\n\nStrengths:\n- Novel combination of TTT and FastTTTLinear capabilities\n- Structured approach to memory management\n- Potential for improved efficiency through parallel processing\n\nConcerns:\n- Complexity of stream fusion mechanism\n- Overhead of maintaining two separate streams\n- Potential challenges in training stability\n\n4. References Found So Far:\n\n[1] \"Learning to (Learn at Test Time): RNNs with Expressive Hidden States\" (Parent 1)\n- Demonstrates effectiveness of test-time training\n- Shows potential for linear complexity processing\n\n[2] \"FastTTTLinear: Enhancing TTTLinear with Gated Linear Attention\" (Parent 2)\n- Introduces efficient gated mechanisms\n- Provides framework for memory optimization\n\n5. Next Steps:\n\na) Search Focus:\n- Multi-stream architectures in language models\n- Bounded context and chunk processing techniques\n- Efficient stream fusion methods\n- Memory management in long-sequence processing\n\nb) Comparison Points:\n- Performance metrics with similar architectures\n- Memory efficiency measurements\n- Computational overhead analysis\n- Training stability indicators\n\nThis analysis suggests the proposal has potential merit but requires thorough investigation of its key components and comparison with existing approaches."
        },
        {
            "ready": false,
            "query": "efficient state management language models, test-time training neural networks, stream fusion techniques",
            "detail": "Search for papers discussing:\n1. State management techniques in language models:\n   - Memory-efficient architectures\n   - State compression methods\n   - Dynamic state updates\n2. Test-time training implementations:\n   - Adaptive computation\n   - Online learning mechanisms\n   - Integration with hierarchical structures\n3. Stream fusion methods:\n   - Information combination techniques\n   - Multi-stream architectures\n   - Adaptive fusion mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. State management techniques in language models:\n   - Memory-efficient architectures\n   - State compression methods\n   - Dynamic state updates\n2. Test-time training implementations:\n   - Adaptive computation\n   - Online learning mechanisms\n   - Integration with hierarchical structures\n3. Stream fusion methods:\n   - Information combination techniques\n   - Multi-stream architectures\n   - Adaptive fusion mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.19)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.19)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 2. Coupled Mamba: Enhanced Multi-modal Fusion with Coupled State Space Model (Avg. Score: 0.17)\n\n*Wenbing Li, Hang Zhou, Junqing Yu, Zikai Song, Wei Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Coupled SSM model is proposed, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes, and demonstrates that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n**Abstract:** The essence of multi-modal fusion lies in exploiting the complementary information inherent in diverse modalities. However, prevalent fusion methods rely on traditional neural architectures and are inadequately equipped to capture the dynamics of interactions across modalities, particularly in presence of complex intra- and inter-modality correlations. Recent advancements in State Space Models (SSMs), notably exemplified by the Mamba model, have emerged as promising contenders. Particularly, its state evolving process implies stronger modality fusion paradigm, making multi-modal fusion on SSMs an appealing direction. However, fusing multiple modalities is challenging for SSMs due to its hardware-aware parallelism designs. To this end, this paper proposes the Coupled SSM model, for coupling state chains of multiple modalities while maintaining independence of intra-modality state processes. Specifically, in our coupled scheme, we devise an inter-modal hidden states transition scheme, in which the current state is dependent on the states of its own chain and that of the neighbouring chains at the previous time-step. To fully comply with the hardware-aware parallelism, we devise an expedite coupled state transition scheme and derive its corresponding global convolution kernel for parallelism. Extensive experiments on CMU-MOSEI, CH-SIMS, CH-SIMSV2 through multi-domain input verify the effectiveness of our model compared to current state-of-the-art methods, improved F1-Score by 0.4\\%, 0.9\\%, and 2.3\\% on the three datasets respectively, 49\\% faster inference and 83.7\\% GPU memory save. The results demonstrate that Coupled Mamba model is capable of enhanced multi-modal fusion.\n\n##### *Relevant Chunk: No. 17/22 (Score: 0.17)*\n\n```\nIn Proceedings of the conference. Association for computational linguistics. Meeting, volume 2019, page 6558. NIH Public Access, 2019. [4] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. Cornell University - arXiv, Cornell University - arXiv, Jun 2014. [5] Amir Zadeh, Minghai Chen, Soujanya Poria, Erik Cambria, and Louis-Philippe Morency. Tensor fusion network for multimodal sentiment analysis. arXiv: Computation and Language, arXiv: Computation and Language, Jul 2017. [6] Zhun Liu, Ying Shen, Varun Bharadhwaj Lakshminarasimhan, Paul Pu Liang, AmirAli Bagher Zadeh, and Louis-Philippe Morency. Efficient low-rank multimodal fusion with modality-specific factors. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), Jan 2018. [7] Guangnan Ye, Dong Liu, I-Hong Jhuo, and Shih-Fu Chang. Robust late fusion with rank minimization. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2012. [8] Dhanesh Ramachandram and Graham W. Taylor. Deep multimodal learning: A survey on recent advances and trends. IEEE Signal Processing Magazine, page 96-108, Nov 2017. [9] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas Leung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video classification with convolutional neural networks. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, Jun 2014. [10] Valentin Vielzeuf, Alexis Lechervy, Stephane Pateux, and Frederic Jurie. Centralnet: a multilayer approach for multimodal fusion. Cornell University - arXiv,Cornell University - arXiv, Aug 2018. [11] Juan-Manuel Perez-Rua, Valentin Vielzeuf, Stephane Pateux, Moez Baccouche, and Frederic Jurie. Mfas: Multimodal fusion architecture search. [12] Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, JohnR. Hershey, and TimK. Marks. Attention-based multimodal fusion for video description. Cornell University - arXiv, Cornell University - arXiv, Jan 2017. [13] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen, Cordelia Schmid, and Chen Sun. Attention bottlenecks for multimodal fusion. Dec 2021. [14] Yikai Wang, Wenbing Huang, Fuchun Sun, Tingyang Xu, Rong Yu, and Junzhou Huang. Deep multimodal fusion by channel exchanging. Cornell University - arXiv,Cornell University arXiv, Nov 2020. [15] Yingwei Pan, Ting Yao, Yehao Li, and Tao Mei. X-linear attention networks for image captioning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), Jun 2020. [16] Zongbo Han, Fan Yang, Junzhou Huang, Changqing Zhang, and Jianhua Yao. Multimodal dynamics: Dynamical fusion for trustworthy multimodal classification. [17] Zihui Xue and Radu Marculescu. Dynamic multimodal fusion. [18] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. [19] JimmyT.H. Smith, Andrew Warrington, and ScottW. Linderman. Simplified state space layers for sequence modeling. Aug 2022. [20] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. Jun 2022. [21] Tri Dao, DanielY. Fu, KhaledK. Saab, ArminW. Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. Dec 2022. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.16)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.16)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. Scaling Laws of RoPE-based Extrapolation (Avg. Score: 0.03)\n\n*Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 36  (*Influential: 4*)\n\n**TL;DR:** This work proposes a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length, and achieves extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n**Abstract:** The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.03)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/ P16-1144. URL https://doi.org/10.18653/v1/p16-1144. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URLhttps://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.\n```\n\n#### 5. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.03)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.03)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient state management language models, test-time training neural networks, stream fusion techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models\n\n*From Search Query: efficient state management language models*\n\n*Junnan Li, Dongxu Li, S. Savarese, Steven C. H. Hoi*\n\n**TL;DR:** BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods, and is demonstrated's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.\n\n**Abstract:** The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 2852  (*Influential: 533*)\n\n#### 2. LLM-Adapters: An Adapter Family for Parameter-Efficient Fine-Tuning of Large Language Models\n\n*From Search Query: efficient state management language models*\n\n*Zhiqiang Hu, Yihuai Lan, Lei Wang, Wanyu Xu, Ee-Peng Lim, R. Lee, Lidong Bing, Soujanya Poria*\n\n**TL;DR:** LLM-Adapters is presented, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks, demonstrating that using adapter- based PEFT in smaller-scale LLMs with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs in zero-shot inference on both reasoning tasks.\n\n**Abstract:** The success of large language models (LLMs), like GPT-4 and ChatGPT, has led to the development of numerous cost-effective and accessible alternatives that are created by finetuning open-access LLMs with task-specific data (e.g., ChatDoctor) or instruction data (e.g., Alpaca). Among the various fine-tuning methods, adapter-based parameter-efficient fine-tuning (PEFT) is undoubtedly one of the most attractive topics, as it only requires fine-tuning a few external parameters instead of the entire LLMs while achieving comparable or even better performance. To enable further research on PEFT methods of LLMs, this paper presents LLM-Adapters, an easy-to-use framework that integrates various adapters into LLMs and can execute these adapter-based PEFT methods of LLMs for different tasks. The framework includes state-of-the-art open-access LLMs such as LLaMA, BLOOM, and GPT-J, as well as widely used adapters such as Series adapters, Parallel adapter, Prompt-based learning and Reparametrization-based methods. Moreover, we conduct extensive empirical studies on the impact of adapter types, placement locations, and hyper-parameters to the best design for each adapter-based methods. We evaluate the effectiveness of the adapters on fourteen datasets from two different reasoning tasks, Arithmetic Reasoning and Commonsense Reasoning. The results demonstrate that using adapter-based PEFT in smaller-scale LLMs (7B) with few extra trainable parameters yields comparable, and in some cases superior, performance to powerful LLMs (175B) in zero-shot inference on both reasoning tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 142  (*Influential: 15*)\n\n#### 3. YaRN: Efficient Context Window Extension of Large Language Models\n\n*From Search Query: efficient state management language models*\n\n*Bowen Peng, Jeffrey Quesnelle, Honglu Fan, Enrico Shippole*\n\n**TL;DR:** YaRN is presented, a compute-efficient method to extend the context window of RoPE extensioN models, requiring 10x less tokens and 2.5x less training steps than previous methods, and it is shown that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow.\n\n**Abstract:** Rotary Position Embeddings (RoPE) have been shown to effectively encode positional information in transformer-based language models. However, these models fail to generalize past the sequence length they were trained on. We present YaRN (Yet another RoPE extensioN method), a compute-efficient method to extend the context window of such models, requiring 10x less tokens and 2.5x less training steps than previous methods. Using YaRN, we show that LLaMA models can effectively utilize and extrapolate to context lengths much longer than their original pre-training would allow, while also surpassing previous the state-of-the-art at context window extension. In addition, we demonstrate that YaRN exhibits the capability to extrapolate beyond the limited context of a fine-tuning dataset. The models fine-tuned using YaRN has been made available and reproduced online up to 128k context length at https://github.com/jquesnelle/yarn\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 147  (*Influential: 20*)\n\n#### 4. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: test-time training neural networks*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 5. GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation\n\n*From Search Query: test-time training neural networks*\n\n*Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye*\n\n**TL;DR:** GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes, and can also mitigate the degree bias for either self-supervised or supervised GNNs.\n\n**Abstract:** Recent studies have shown that graph neural networks (GNNs) exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes. Existing works tackle this problem by deriving either designated GNN architectures or training strategies specifically for low-degree nodes. Though effective, these approaches unintentionally create an artificial out-of-distribution scenario, where models mainly or even only observe low-degree nodes during the training, leading to a downgraded performance for high-degree nodes that GNNs originally perform well at. In light of this, we propose a test-time augmentation framework, namely GraphPatcher, to enhance test-time generalization of any GNNs on low-degree nodes. Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs. Comprehensive experiments are conducted over seven benchmark datasets and GraphPatcher consistently enhances common GNNs' overall performance by up to 3.6% and low-degree performance by up to 6.5%, significantly outperforming state-of-the-art baselines. The source code is publicly available at https://github.com/jumxglhf/GraphPatcher.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 6. BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework\n\n*From Search Query: stream fusion techniques*\n\n*Tingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, T. Tang, Bing Wang, Zhi Tang*\n\n**TL;DR:** This work proposes a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods and is the first to handle realistic LiDar malfunction and can be deployed to realistic scenarios without any post-processing procedure.\n\n**Abstract:** Fusing the camera and LiDAR information has become a de-facto standard for 3D object detection tasks. Current methods rely on point clouds from the LiDAR sensor as queries to leverage the feature from the image space. However, people discovered that this underlying assumption makes the current fusion framework infeasible to produce any prediction when there is a LiDAR malfunction, regardless of minor or major. This fundamentally limits the deployment capability to realistic autonomous driving scenarios. In contrast, we propose a surprisingly simple yet novel fusion framework, dubbed BEVFusion, whose camera stream does not depend on the input of LiDAR data, thus addressing the downside of previous methods. We empirically show that our framework surpasses the state-of-the-art methods under the normal training settings. Under the robustness training settings that simulate various LiDAR malfunctions, our framework significantly surpasses the state-of-the-art methods by 15.7% to 28.9% mAP. To the best of our knowledge, we are the first to handle realistic LiDAR malfunction and can be deployed to realistic scenarios without any post-processing procedure. The code is available at https://github.com/ADLab-AutoDrive/BEVFusion.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 275  (*Influential: 48*)\n\n#### 7. Two-Stream Network for Sign Language Recognition and Translation\n\n*From Search Query: stream fusion techniques*\n\n*Yutong Chen, Ronglai Zuo, Fangyun Wei, Yu Wu, Shujie Liu, B. Mak*\n\n**TL;DR:** A dual visual encoder containing two separate streams to model both the raw videos and the keypoint sequences generated by an off-the-shelf keypoint estimator for sign language recognition (SLR) and sign language translation (SLT).\n\n**Abstract:** Sign languages are visual languages using manual articulations and non-manual elements to convey information. For sign language recognition and translation, the majority of existing approaches directly encode RGB videos into hidden representations. RGB videos, however, are raw signals with substantial visual redundancy, leading the encoder to overlook the key information for sign language understanding. To mitigate this problem and better incorporate domain knowledge, such as handshape and body movement, we introduce a dual visual encoder containing two separate streams to model both the raw videos and the keypoint sequences generated by an off-the-shelf keypoint estimator. To make the two streams interact with each other, we explore a variety of techniques, including bidirectional lateral connection, sign pyramid network with auxiliary supervision, and frame-level self-distillation. The resulting model is called TwoStream-SLR, which is competent for sign language recognition (SLR). TwoStream-SLR is extended to a sign language translation (SLT) model, TwoStream-SLT, by simply attaching an extra translation network. Experimentally, our TwoStream-SLR and TwoStream-SLT achieve state-of-the-art performance on SLR and SLT tasks across a series of datasets including Phoenix-2014, Phoenix-2014T, and CSL-Daily. Code and models are available at: https://github.com/FangyunWei/SLRT.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 76  (*Influential: 12*)\n\n#### 8. Parameter Efficient Multi-task Model Fusion with Partial Linearization\n\n*From Search Query: stream fusion techniques*\n\n*A. Tang, Li Shen, Yong Luo, Yibing Zhan, Han Hu, Bo Du, Yixin Chen, Dacheng Tao*\n\n**TL;DR:** This work proposes a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning, which partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters.\n\n**Abstract:** Large pre-trained models have enabled significant advances in machine learning and served as foundation components. Model fusion methods, such as task arithmetic, have been proven to be powerful and scalable to incorporate fine-tuned weights from different tasks into a multi-task model. However, efficiently fine-tuning large pre-trained models on multiple downstream tasks remains challenging, leading to inefficient multi-task model fusion. In this work, we propose a novel method to improve multi-task fusion for parameter-efficient fine-tuning techniques like LoRA fine-tuning. Specifically, our approach partially linearizes only the adapter modules and applies task arithmetic over the linearized adapters. This allows us to leverage the the advantages of model fusion over linearized fine-tuning, while still performing fine-tuning and inference efficiently. We demonstrate that our partial linearization technique enables a more effective fusion of multiple tasks into a single model, outperforming standard adapter tuning and task arithmetic alone. Experimental results demonstrate the capabilities of our proposed partial linearization technique to effectively construct unified multi-task models via the fusion of fine-tuned task vectors. We evaluate performance over an increasing number of tasks and find that our approach outperforms standard parameter-efficient fine-tuning techniques. The results highlight the benefits of partial linearization for scalable and efficient multi-task model fusion. The code is available at https://github.com/tanganke/peta\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 3*)\n\n### 6 related papers from Papers with Code\n\n#### 1. QLoRA: Efficient Finetuning of Quantized LLMs\n\n*From Search Query: efficient state management language models*\n\n*Luke Zettlemoyer, Ari Holtzman, Artidoro Pagnoni, Tim Dettmers*\n\n**Abstract:** We present QLoRA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance. QLoRA backpropagates gradients through a frozen, 4-bit quantized pretrained language model into Low Rank Adapters~(LoRA). Our best model family, which we name Guanaco, outperforms all previous openly released models on the Vicuna benchmark, reaching 99.3% of the performance level of ChatGPT while only requiring 24 hours of finetuning on a single GPU. QLoRA introduces a number of innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce the average memory footprint by quantizing the quantization constants, and (c) paged optimziers to manage memory spikes. We use QLoRA to finetune more than 1,000 models, providing a detailed analysis of instruction following and chatbot performance across 8 instruction datasets, multiple model types (LLaMA, T5), and model scales that would be infeasible to run with regular finetuning (e.g. 33B and 65B parameter models). Our results show that QLoRA finetuning on a small high-quality dataset leads to state-of-the-art results, even when using smaller models than the previous SoTA. We provide a detailed analysis of chatbot performance based on both human and GPT-4 evaluations showing that GPT-4 evaluations are a cheap and reasonable alternative to human evaluation. Furthermore, we find that current chatbot benchmarks are not trustworthy to accurately evaluate the performance levels of chatbots. A lemon-picked analysis demonstrates where Guanaco fails compared to ChatGPT. We release all of our models and code, including CUDA kernels for 4-bit training.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-23\n\n\n\n#### 2. SMART: Robust and Efficient Fine-Tuning for Pre-trained Natural Language Models through Principled Regularized Optimization\n\n*From Search Query: efficient state management language models*\n\n*Tuo Zhao, Xiaodong Liu, Weizhu Chen, Jianfeng Gao, Haoming Jiang, Pengcheng He*\n\n**Abstract:** Transfer learning has fundamentally changed the landscape of natural language processing (NLP) research. Many existing state-of-the-art models are first pre-trained on a large text corpus and then fine-tuned on downstream tasks. However, due to limited data resources from downstream tasks and the extremely large capacity of pre-trained models, aggressive fine-tuning often causes the adapted model to overfit the data of downstream tasks and forget the knowledge of the pre-trained model. To address the above issue in a more principled manner, we propose a new computational framework for robust and efficient fine-tuning for pre-trained language models. Specifically, our proposed framework contains two important ingredients: 1. Smoothness-inducing regularization, which effectively manages the capacity of the model; 2. Bregman proximal point optimization, which is a class of trust-region methods and can prevent knowledge forgetting. Our experiments demonstrate that our proposed method achieves the state-of-the-art performance on multiple NLP benchmarks.\n\n**Conference:** smart-robust-and-efficient-fine-tuning-for-1\n\n**Published:** 2019-11-08\n\n\n\n#### 3. The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention\n\n*From Search Query: test-time training neural networks*\n\n*J\u00fcrgen Schmidhuber, R\u00f3bert Csord\u00e1s, Kazuki Irie*\n\n**Abstract:** Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.\n\n**Published:** 2022-02-11\n\n\n\n#### 4. Layer Normalization\n\n*From Search Query: test-time training neural networks*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 5. Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers\n\n*From Search Query: stream fusion techniques*\n\n*Dapeng Oliver Wu, Junhui Hou, Zhiyu Zhu*\n\n**Abstract:** This paper addresses the problem of cross-modal object tracking from RGB videos and event data. Rather than constructing a complex cross-modal fusion network, we explore the great potential of a pre-trained vision Transformer (ViT). Particularly, we delicately investigate plug-and-play training augmentations that encourage the ViT to bridge the vast distribution gap between the two modalities, enabling comprehensive cross-modal information interaction and thus enhancing its ability. Specifically, we propose a mask modeling strategy that randomly masks a specific modality of some tokens to enforce the interaction between tokens from different modalities interacting proactively. To mitigate network oscillations resulting from the masking strategy and further amplify its positive effect, we then theoretically propose an orthogonal high-rank loss to regularize the attention matrix. Extensive experiments demonstrate that our plug-and-play training augmentation techniques can significantly boost state-of-the-art one-stream and twostream trackers to a large extent in terms of both tracking precision and success rate. Our new perspective and findings will potentially bring insights to the field of leveraging powerful pre-trained ViTs to model cross-modal data. The code will be publicly available.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-07-09\n\n\n\n#### 6. DeepCD: Learning Deep Complementary Descriptors for Patch Representations\n\n*From Search Query: stream fusion techniques*\n\n*Yung-Yu Chuang, Yen-Yu Lin, Jo-Han Hsu, Tsun-Yi Yang*\n\n**Abstract:** This paper presents the DeepCD framework which learns a pair of complementary descriptors jointly for a patch by employing deep learning techniques. It can be achieved by taking any descriptor learning architecture for learning a leading descriptor and augmenting the architecture with an additional network stream for learning a complementary descriptor. To enforce the complementary property, a new network layer, called data-dependent modulation (DDM) layer, is introduced for adaptively learning the augmented network stream with the emphasis on the training data that are not well handled by the leading stream. By optimizing the proposed joint loss function with late fusion, the obtained descriptors are complementary to each other and their fusion improves performance. Experiments on several problems and datasets show that the proposed method is simple yet effective, outperforming state-of-the-art methods. \r\n\n**Proceeding:** iccv-2017-10\n\n**Published:** 2017-10-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient state management, test-time training, and stream fusion methods, here are some key insights and relevant information from the provided sources and additional context:\n\n### State Management Techniques in Language Models\n\n#### Memory-Efficient Architectures\n- The concept of using modular, distributed approaches to language models can be beneficial. For instance, the article on \"Rethinking LLMs: A Modular, Distributed Approach to AI\" suggests breaking down large models into smaller, specialized models that can be more efficient in terms of computational resources. This approach can help in managing memory more effectively by distributing the workload across multiple smaller models.\n\n#### State Compression Methods\n- The paper on \"Stateful Large Language Model Serving with Pensieve\" introduces a system that caches previously processed conversation history to avoid redundant computation. This stateful approach can be seen as a form of state compression, where the model maintains relevant context across requests, reducing the need for repeated processing of the same history.\n\n#### Dynamic State Updates\n- The study on the o1-preview model highlights its self-evaluation mechanism, which allows the model to check and correct its actions during plan generation. This mechanism involves dynamic state updates, where the model manages multiple states effectively within a plan, although it faces challenges in more complex environments.\n\n### Test-Time Training Implementations\n\n#### Adaptive Computation\n- The idea of adaptive computation can be linked to the modular and distributed approach mentioned earlier. By using smaller, specialized models, the system can adaptively engage only the relevant models for each task, optimizing computational resources and reducing unnecessary computations.\n\n#### Online Learning Mechanisms\n- While the provided sources do not directly discuss online learning mechanisms in the context of language models, the concept of test-time training suggests the need for models that can learn and adapt during inference. This could involve integrating hierarchical structures that allow for continuous learning and adaptation based on the input received during the test phase.\n\n#### Integration with Hierarchical Structures\n- The Hierarchically Gated Recurrent Neural Network (HGRN) mentioned in the analysis note provides insights into how hierarchical gating mechanisms can be valuable for managing different timescales and integrating test-time training. This architecture includes forget gates that are lower bounded by a learnable value, which increases monotonically when moving up layers, facilitating more efficient state management and adaptation (Analysis Note).\n\n### Stream Fusion Methods\n\n#### Information Combination Techniques\n- The concept of stream fusion can be related to the dense connections between streams in the DenseMamba approach. By selectively integrating shallow-layer hidden states into deeper layers, the model retains fine-grained information crucial for the final output, enhancing information flow between layers (Analysis Note).\n\n#### Multi-Stream Architectures\n- The two-stream architecture mentioned in the analysis note shows promise for managing different streams of information. This can be particularly useful in language models where multiple streams might represent different aspects of the input data, such as context, semantics, and syntax (Analysis Note).\n\n#### Adaptive Fusion Mechanisms\n- The modular and distributed approach proposed in \"Rethinking LLMs\" can also be seen as a form of adaptive fusion. Here, multiple specialized models work together, and the system orchestrates which models to engage based on the task, allowing for adaptive fusion of information from different models.\n\n### Additional Insights\n\n- **Stateful Serving Systems**: The Pensieve system, which maintains conversation state across requests by caching previously processed history, can be a model for efficient state management in multi-turn conversations. This approach reduces redundant computation and improves serving throughput and latency.\n\n- **Hierarchical State Management**: The Hierarchically Gated Recurrent Neural Network (HGRN) and similar architectures can provide valuable insights into managing states across different layers and timescales, which is crucial for efficient and adaptive language models (Analysis Note).\n\nBy integrating these concepts\u2014modular and distributed architectures, stateful serving systems, hierarchical state management, and adaptive fusion mechanisms\u2014researchers can develop more efficient, scalable, and adaptive autoregressive language models.\n",
            "analysis": "Analysis Note for HierTTT Proposal\n\n1. Summary of Analysis\nThe initial search results reveal several important aspects relevant to the HierTTT proposal:\n\na) Multi-Stream Processing:\n- DenseMamba's approach to enhancing information flow between layers provides insights for stream interaction\n- Hierarchical state management in neural networks shows promise for managing different timescales\n- Limited direct research on multi-stream language models suggests novelty potential\n\nb) Memory Management:\n- Bounded context approaches are being explored in various forms\n- Chunk-based processing appears as a common solution for long sequences\n- Need more information on efficient state management techniques\n\nc) Integration Challenges:\n- Limited research on combining test-time training with hierarchical structures\n- Gap in literature regarding stream fusion methods\n- Need more exploration of adaptive computation mechanisms\n\n2. Useful References with Excerpts\n\nFrom DenseMamba paper:\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom HGRN paper:\n\"Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers.\"\n\n3. Key Insights\n\na) Architectural Design:\n- Two-stream architecture appears novel in language modeling context\n- Hierarchical gating mechanisms could be valuable for stream management\n- Dense connections between streams might improve information flow\n\nb) Memory Efficiency:\n- Need to explore more efficient state management techniques\n- Chunk-based processing shows promise but requires careful implementation\n- Bounded context approaches need more investigation\n\nc) Research Gaps:\n- Limited work on multi-stream language models\n- Few studies on test-time training in hierarchical contexts\n- Stream fusion methods need more exploration\n\n4. Future Search Plan\n\nRound 2 Search Focus:\n1. State management techniques in language models\n2. Test-time training implementations\n3. Stream fusion methods in neural networks\n\nSpecific queries needed:\n- Efficient state management in language models\n- Test-time training architectures\n- Neural network stream fusion techniques\n\n5. References\n\n[1] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[2] Qin, Z., Yang, S., & Zhong, Y. (2023). Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Neural Information Processing Systems.\n\n[3] Jin, D., & Szolovits, P. (2018). Hierarchical Neural Networks for Sequential Sentence Classification in Medical Scientific Abstracts. Conference on Empirical Methods in Natural Language Processing.\n\n[4] Lee, C. W., Liu, Q., Abbasi-Yadkori, Y., Jin, C., Lattimore, T., & Szepesvari, C. (2023). Context-lumpable stochastic bandits. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "chunk-based language processing, parallel stream synchronization, memory efficient state updates",
            "detail": "Search for papers discussing:\n1. Chunk-based processing in language models:\n   - Efficient chunking strategies\n   - Memory management across chunks\n   - Information preservation between chunks\n2. Stream synchronization methods:\n   - Parallel processing coordination\n   - State synchronization techniques\n   - Timing mechanisms\n3. Memory-efficient state updates:\n   - Incremental state updates\n   - Sparse state representations\n   - Compression techniques",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Chunk-based processing in language models:\n   - Efficient chunking strategies\n   - Memory management across chunks\n   - Information preservation between chunks\n2. Stream synchronization methods:\n   - Parallel processing coordination\n   - State synchronization techniques\n   - Timing mechanisms\n3. Memory-efficient state updates:\n   - Incremental state updates\n   - Sparse state representations\n   - Compression techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.82)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.82)*\n\n```\n# EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SinKs \n\nGuangxuan Xiao $^{1 *}$ Yuandong Tian ${ }^{2} \\quad$ Beidi Chen $^{3} \\quad$ Song Han ${ }^{1,4} \\quad$ Mike Lewis $^{2}$<br>${ }^{1}$ Massachusetts Institute of Technology ${ }^{2}$ Meta AI<br>${ }^{3}$ Carnegie Mellon University ${ }^{4}$ NVIDIA<br>https://github.com/mit-han-lab/streaming-llm\n\n\n#### Abstract\n\nDeploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.\n```\n\n#### 2. Weighted Grouped Query Attention in Transformers (Avg. Score: 0.45)\n\n*Sai Sena Chinnakonduru, Astarag Mohapatra*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA), is proposed, introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning.\n\n**Abstract:** The attention mechanism forms the foundational blocks for transformer language models. Recent approaches show that scaling the model achieves human-level performance. However, with increasing demands for scaling and constraints on hardware memory, the inference costs of these models remain high. To reduce the inference time, Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) were proposed in (Shazeer, 2019) and (Ainslieet al., 2023) respectively. In this paper, we propose a variation of Grouped-Query Attention, termed Weighted Grouped-Query Attention (WGQA). We introduced new learnable parameters for each key and value head in the T5 decoder attention blocks, enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA, and the performance converges to traditional Multi-head attention (MHA) with no additional overhead during inference. We evaluated the introduction of these parameters and subsequent finetuning informs the model about the grouping mechanism during training, thereby enhancing performance. Additionally, we demonstrate the scaling laws in our analysis by comparing the results between T5-small and T5-base architecture.\n\n##### *Relevant Chunk: No. 6/10 (Score: 0.45)*\n\n```\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam search strategies for neural machine translation. In Proceedings of the First Workshop on Neural Machine Translation. Association for Computational Linguistics. Kavita Ganesan. 2018. Rouge 2.0: Updated and improved measures for evaluation of summarization tasks. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, and Hannaneh Hajishirzi. 2024. Olmo: Accelerating the science of language models. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L\u00e9lio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. 2023. Efficient memory management for large language model serving with pagedattention. Kai Lv, Yuqing Yang, Tengxiao Liu, Qinghui Gao, Qipeng Guo, and Xipeng Qiu. 2024. Full parameter fine-tuning for large language models with limited resources. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. 2024. Openelm: An efficient language model family with open training and inference framework. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Anselm Levskaya, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. 2022. Efficiently scaling transformer inference. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\n```\n\n#### 3. Soaring from 4K to 400K: Extending LLM\u2019s Context with Activation Beacon (Avg. Score: 0.18)\n\n*Peitian Zhang, Zheng Liu, Shitao Xiao, Ninglu Shao, Qiwei Ye, Zhicheng Dou*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 30  (*Influential: 1*)\n\n**TL;DR:** Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts and works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference.\n\n**Abstract:** The utilization of long contexts poses a big challenge for LLMs due to their limited context window size. Although the context window can be extended through fine-tuning, it will result in a considerable cost at both training and inference time, and exert an unfavorable impact to the LLM's original capabilities. In this work, we propose a new method called Activation Beacon, which condenses LLM's raw activations into compact forms such that the LLM can perceive a longer context with a limited context window. Activation Beacon is introduced as a plug-in module, which fully preserves the LLM's original capability in short contexts. It works with the sliding window to streamingly process the long context, which leads to a competitive memory and time efficiency in both training and inference. Activation Beacon is trained with short-sequence data of diversified condensing ratios. Thanks to such a treatment, it can be effectively learned to support different context lengths with a small training cost. Our experiment verifies Activation Beacon's effectiveness of context extension: it can remarkably accomplish high-quality extension of Llama-2-7B's context by $\\times100$ times (from 4K to 400K); meanwhile, it can also achieve superior performances across a variety of long-context language modeling and understanding tasks. The source code and model checkpoint are available at \\url{https://github.com/FlagOpen/FlagEmbedding}.\n\n##### *Relevant Chunk: No. 7/27 (Score: 0.18)*\n\n```\ndoi: 10.48550/ARXIV.2304.11062. URL https: //doi.org/10.48550/arXiv.2304.11062\n[5] Chen, S., Wong, S., Chen, L., and Tian, Y. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595, 2023. [6] Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. [7] Chevalier, A., Wettig, A., Ajith, A., and Chen, D. Adapting language models to compress contexts. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 3829-3846. Association for Computational Linguistics, 2023. URL https: //aclanthology.org/2023.emnlp-main. 232 . [8] Child, R., Gray, S., Radford, A., and Sutskever, I. Generating long sequences with sparse transformers.\n```\n\n#### 4. Scaling Laws of RoPE-based Extrapolation (Avg. Score: 0.16)\n\n*Xiaoran Liu, Hang Yan, Shuo Zhang, Chen An, Xipeng Qiu, Dahua Lin*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 36  (*Influential: 4*)\n\n**TL;DR:** This work proposes a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length, and achieves extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n**Abstract:** The extrapolation capability of Large Language Models (LLMs) based on Rotary Position Embedding is currently a topic of considerable interest. The mainstream approach to addressing extrapolation with LLMs involves modifying RoPE by replacing 10000, the rotary base of $\\theta_n={10000}^{-2n/d}$ in the original RoPE, with a larger value and providing longer fine-tuning text. In this work, we first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance. After that, we propose \\textbf{\\textit{Scaling Laws of RoPE-based Extrapolation}}, a unified framework from the periodic perspective, to describe the relationship between the extrapolation performance and base value as well as tuning context length. In this process, we also explain the origin of the RoPE-based extrapolation issue by \\textbf{\\textit{critical dimension for extrapolation}}. Besides these observations and analyses, we achieve extrapolation up to 1 million context length within only 16K training length on LLaMA2 7B and 13B.\n\n##### *Relevant Chunk: No. 20/39 (Score: 0.16)*\n\n```\nIn Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers. The Association for Computer Linguistics, 2016. doi: 10.18653/V1/ P16-1144. URL https://doi.org/10.18653/v1/p16-1144. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. CoRR, abs/2309.00071, 2023. doi: 10.48550/ARXIV.2309. 00071. URLhttps://doi.org/10.48550/arXiv.2309.00071. Ofir Press, Noah A. Smith, and Mike Lewis. Train short, test long: Attention with linear biases enables input length extrapolation. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. URL https://openreview.net/forum?id=R8sQPpGCv0. Shawn Presser. Books3, 2020. URL https://twitter.com/theshawwn/status/ 1320282149329784833\n\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T. Kramer (eds.), Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November 9-19, 2020, pp. 20. IEEE/ACM, 2020. doi: 10.1109/SC41405.2020.00024. URL https: //doi.org/10.1109/SC41405.2020.00024. Nir Ratner, Yoav Levine, Yonatan Belinkov, Ori Ram, Omri Abend, Ehud Karpas, Amnon Shashua, Kevin Leyton-Brown, and Yoav Shoham. Parallel context windows improve in-context learning of large language models.\n```\n\n#### 5. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.12)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 36/46 (Score: 0.12)*\n\n```\nURL https://arxiv.org/abs/2112.11446. J\u00fcrgen Schmidhuber. Neural sequence chunkers. Technical report, 1991. M. Schuster and K.K. Paliwal. Bidirectional recurrent neural networks. IEEE Transactions on Signal Processing, 45(11):2673-2681, 1997. doi: 10.1109/78.650093. Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer: Rethinking self-attention in transformer models. CoRR, abs/2005.00743, 2020a. URL https: //arxiv.org/abs/2005.00743\n\nYi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-Cheng Juan. Sparse sinkhorn attention. CoRR, abs/2002.11296, 2020b. URL https://arxiv.org/abs/2002.11296\n\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: chunk-based language processing, parallel stream synchronization, memory efficient state updates\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Chunk-based Nearest Neighbor Machine Translation\n\n*From Search Query: chunk-based language processing*\n\n*Pedro Henrique Martins, Zita Marinho, Andr\u00e9 Martins*\n\n**TL;DR:** Experiments on machine translation in two settings, static and \u201con-the-fly\u201d domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.\n\n**Abstract:** Semi-parametric models, which augment generation with retrieval, have led to impressive results in language modeling and machine translation, due to their ability to retrieve fine-grained information from a datastore of examples. One of the most prominent approaches, kNN-MT, exhibits strong domain adaptation capabilities by retrieving tokens from domain-specific datastores (Khandelwal et al., 2021). However, kNN-MT requires an expensive retrieval operation for every single generated token, leading to a very low decoding speed (around 8 times slower than a parametric model). In this paper, we introduce a chunk-based kNN-MT model which retrieves chunks of tokens from the datastore, instead of a single token. We propose several strategies for incorporating the retrieved chunks into the generation process, and for selecting the steps at which the model needs to search for neighbors in the datastore. Experiments on machine translation in two settings, static and \u201con-the-fly\u201d domain adaptation, show that the chunk-based kNN-MT model leads to significant speed-ups (up to 4 times) with only a small drop in translation quality.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 0*)\n\n#### 2. Non-autoregressive Streaming Transformer for Simultaneous Translation\n\n*From Search Query: chunk-based language processing*\n\n*Zhengrui Ma, Shaolei Zhang, Shoutao Guo, Chenze Shao, Min Zhang, Yang Feng*\n\n**TL;DR:** NAST is proposed which comprises a unidirectional encoder and a non-autoregressive decoder with intra-chunk parallelism that enables NAST to generate the blank token or repetitive tokens to adjust its READ/WRITE strategy flexibly, and train it to maximize the non-monotonic latent alignment with an alignment-based latency loss.\n\n**Abstract:** Simultaneous machine translation (SiMT) models are trained to strike a balance between latency and translation quality. However, training these models to achieve high quality while maintaining low latency often leads to a tendency for aggressive anticipation. We argue that such issue stems from the autoregressive architecture upon which most existing SiMT models are built. To address those issues, we propose non-autoregressive streaming Transformer (NAST) which comprises a unidirectional encoder and a non-autoregressive decoder with intra-chunk parallelism. We enable NAST to generate the blank token or repetitive tokens to adjust its READ/WRITE strategy flexibly, and train it to maximize the non-monotonic latent alignment with an alignment-based latency loss. Experiments on various SiMT benchmarks demonstrate that NAST outperforms previous strong autoregressive SiMT baselines.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 3. Transition-Based Disfluency Detection using LSTMs\n\n*From Search Query: chunk-based language processing*\n\n*Shaolei Wang, Wanxiang Che, Yue Zhang, Meishan Zhang, Ting Liu*\n\n**TL;DR:** This model incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information, which can capture non-local chunk-level features and is free for noise in syntax.\n\n**Abstract:** In this paper, we model the problem of disfluency detection using a transition-based framework, which incrementally constructs and labels the disfluency chunk of input sentences using a new transition system without syntax information. Compared with sequence labeling methods, it can capture non-local chunk-level features; compared with joint parsing and disfluency detection methods, it is free for noise in syntax. Experiments show that our model achieves state-of-the-art f-score of 87.5% on the commonly used English Switchboard test set, and a set of in-house annotated Chinese data.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2017\n\n**Citations:** 50  (*Influential: 7*)\n\n#### 4. A Dual-Stream Neural Network Explains the Functional Segregation of Dorsal and Ventral Visual Pathways in Human Brains\n\n*From Search Query: parallel stream synchronization*\n\n*Minkyu Choi, Kuan Han, Xiaokai Wang, Yizhen Zhang, Zhongming Liu*\n\n**TL;DR:** It is speculated that the distinct responses and representations of the ventral and dorsal streams are more influenced by their distinct goals in visual attention and object recognition than by their specific bias or selectivity in retinal inputs.\n\n**Abstract:** The human visual system uses two parallel pathways for spatial processing and object recognition. In contrast, computer vision systems tend to use a single feedforward pathway, rendering them less robust, adaptive, or efficient than human vision. To bridge this gap, we developed a dual-stream vision model inspired by the human eyes and brain. At the input level, the model samples two complementary visual patterns to mimic how the human eyes use magnocellular and parvocellular retinal ganglion cells to separate retinal inputs to the brain. At the backend, the model processes the separate input patterns through two branches of convolutional neural networks (CNN) to mimic how the human brain uses the dorsal and ventral cortical pathways for parallel visual processing. The first branch (WhereCNN) samples a global view to learn spatial attention and control eye movements. The second branch (WhatCNN) samples a local view to represent the object around the fixation. Over time, the two branches interact recurrently to build a scene representation from moving fixations. We compared this model with the human brains processing the same movie and evaluated their functional alignment by linear transformation. The WhereCNN and WhatCNN branches were found to differentially match the dorsal and ventral pathways of the visual cortex, respectively, primarily due to their different learning objectives. These model-based results lead us to speculate that the distinct responses and representations of the ventral and dorsal streams are more influenced by their distinct goals in visual attention and object recognition than by their specific bias or selectivity in retinal inputs. This dual-stream model takes a further step in brain-inspired computer vision, enabling parallel neural networks to actively explore and understand the visual surroundings.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 5. Fast and Robust Early-Exiting Framework for Autoregressive Language Models with Synchronized Parallel Decoding\n\n*From Search Query: parallel stream synchronization*\n\n*Sangmin Bae, Jongwoo Ko, Hwanjun Song, SeYoung Yun*\n\n**TL;DR:** This work proposes a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding that enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens.\n\n**Abstract:** To tackle the high inference latency exhibited by autoregressive language models, previous studies have proposed an early-exiting framework that allocates adaptive computation paths for each token based on the complexity of generating the subsequent token. However, we observed several shortcomings, including performance degradation caused by a state copying mechanism or numerous exit paths, and sensitivity to exit confidence thresholds. Consequently, we propose a Fast and Robust Early-Exiting (FREE) framework, which incorporates a shallow-deep module and a synchronized parallel decoding. Our framework enables faster inference by synchronizing the decoding process of the current token with previously stacked early-exited tokens. Furthermore, as parallel decoding allows us to observe predictions from both shallow and deep models, we present a novel adaptive threshold estimator that exploits a Beta mixture model to determine suitable confidence thresholds. We empirically demonstrated the superiority of our proposed framework on extensive generation tasks.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 33  (*Influential: 5*)\n\n#### 6. A Quadratic Synchronization Rule for Distributed Deep Learning\n\n*From Search Query: parallel stream synchronization*\n\n*Xinran Gu, Kaifeng Lyu, Sanjeev Arora, Jingzhao Zhang, Longbo Huang*\n\n**TL;DR:** This work proposes a theory-grounded method for determining H, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the learning rate $\\eta$ decays over time.\n\n**Abstract:** In distributed deep learning with data parallelism, synchronizing gradients at each training step can cause a huge communication overhead, especially when many nodes work together to train large models. Local gradient methods, such as Local SGD, address this issue by allowing workers to compute locally for $H$ steps without synchronizing with others, hence reducing communication frequency. While $H$ has been viewed as a hyperparameter to trade optimization efficiency for communication cost, recent research indicates that setting a proper $H$ value can lead to generalization improvement. Yet, selecting a proper $H$ is elusive. This work proposes a theory-grounded method for determining $H$, named the Quadratic Synchronization Rule (QSR), which recommends dynamically setting $H$ in proportion to $\\frac{1}{\\eta^2}$ as the learning rate $\\eta$ decays over time. Extensive ImageNet experiments on ResNet and ViT show that local gradient methods with QSR consistently improve the test accuracy over other synchronization strategies. Compared with the standard data parallel training, QSR enables Local AdamW on ViT-B to cut the training time on 16 or 64 GPUs down from 26.7 to 20.2 hours or from 8.6 to 5.5 hours and, at the same time, achieves $1.16\\%$ or $0.84\\%$ higher top-1 validation accuracy.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 7. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: memory efficient state updates*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 8. REST: Efficient and Accelerated EEG Seizure Analysis through Residual State Updates\n\n*From Search Query: memory efficient state updates*\n\n*Arshia Afzal, Grigorios G. Chrysos, V. Cevher, Mahsa Shoaran*\n\n**TL;DR:** This paper introduces a novel graph-based residual state update mechanism (REST) for real-time EEG signal analysis in applications such as epileptic seizure detection, and achieves a remarkable 9-fold acceleration in inference speed compared to state-of-the-art models.\n\n**Abstract:** EEG-based seizure detection models face challenges in terms of inference speed and memory efficiency, limiting their real-time implementation in clinical devices. This paper introduces a novel graph-based residual state update mechanism (REST) for real-time EEG signal analysis in applications such as epileptic seizure detection. By leveraging a combination of graph neural networks and recurrent structures, REST efficiently captures both non-Euclidean geometry and temporal dependencies within EEG data. Our model demonstrates high accuracy in both seizure detection and classification tasks. Notably, REST achieves a remarkable 9-fold acceleration in inference speed compared to state-of-the-art models, while simultaneously demanding substantially less memory than the smallest model employed for this task. These attributes position REST as a promising candidate for real-time implementation in clinical devices, such as Responsive Neurostimulation or seizure alert systems.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 9. Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control\n\n*From Search Query: memory efficient state updates*\n\n*Giorgos Mamakoukas, Orest Xherija, T. Murphey*\n\n**TL;DR:** This work presents an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived, and achieves an orders-of-magnitude improvement in reconstruction error and superior results in terms of control performance.\n\n**Abstract:** Learning a stable Linear Dynamical System (LDS) from data involves creating models that both minimize reconstruction error and enforce stability of the learned representation. We propose a novel algorithm for learning stable LDSs. Using a recent characterization of stable matrices, we present an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived in this paper. When applied to LDSs with inputs, our approach---in contrast to current methods for learning stable LDSs---updates both the state and control matrices, expanding the solution space and allowing for models with lower reconstruction error. We apply our algorithm in simulations and experiments to a variety of problems, including learning dynamic textures from image sequences and controlling a robotic manipulator. Compared to existing approaches, our proposed method achieves an orders-of-magnitude improvement in reconstruction error and superior results in terms of control performance. In addition, it is provably more memory-efficient, with an O(n^2) space complexity compared to O(n^4) of competing alternatives, thus scaling to higher-dimensional systems when the other methods fail.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 18  (*Influential: 1*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Biomedical Named Entity Recognition at Scale\n\n*From Search Query: chunk-based language processing*\n\n*David Talby, Veysel Kocaman*\n\n**Abstract:** Named entity recognition (NER) is a widely applicable natural language processing task and building block of question answering, topic modeling, information retrieval, etc. In the medical domain, NER plays a crucial role by extracting meaningful chunks from clinical notes and reports, which are then fed to downstream tasks like assertion status detection, entity resolution, relation extraction, and de-identification. Reimplementing a Bi-LSTM-CNN-Char deep learning architecture on top of Apache Spark, we present a single trainable NER model that obtains new state-of-the-art results on seven public biomedical benchmarks without using heavy contextual embeddings like BERT. This includes improving BC4CHEMD to 93.72% (4.1% gain), Species800 to 80.91% (4.6% gain), and JNLPBA to 81.29% (5.2% gain). In addition, this model is freely available within a production-grade code base as part of the open-source Spark NLP library; can scale up for training and inference in any Spark cluster; has GPU support and libraries for popular programming languages such as Python, R, Scala and Java; and can be extended to support other human languages with no code changes.\n\n**Published:** 2020-11-12\n\n\n\n#### 2. Training-Free Long-Context Scaling of Large Language Models\n\n*From Search Query: chunk-based language processing*\n\n*Lingpeng Kong, Chang Zhou, Xipeng Qiu, Shansan Gong, Jun Zhang, Fei Huang, Chenxin An*\n\n**Abstract:** The ability of Large Language Models (LLMs) to process and generate coherent text is markedly weakened when the number of input tokens exceeds their pretraining length. Given the expensive overhead of finetuning large-scale models with longer sequences, we propose Dual Chunk Attention (DCA), which enables Llama2 70B to support context windows of more than 100k tokens without continual training. By decomposing the attention computation for long sequences into chunk-based modules, DCA manages to effectively capture the relative positional information of tokens within the same chunk (Intra-Chunk) and across distinct chunks (Inter-Chunk), as well as integrates seamlessly with Flash Attention. In addition to its impressive extrapolation capability, DCA achieves performance on practical long-context tasks that is comparable to or even better than that of finetuned models. When compared with proprietary models, our training-free 70B model attains 94% of the performance of gpt-3.5-16k, indicating it is a viable open-source alternative. All code and data used in this work are released at \\url{https://github.com/HKUNLP/ChunkLlama}.\n\n**Published:** 2024-02-27\n\n\n\n#### 3. Stream Processing With Dependency-Guided Synchronization (Extended Version)\n\n*From Search Query: parallel stream synchronization*\n\n*Anonymous*\n\n**Abstract:** Real-time data processing applications with low latency requirements have led to the increasing popularity of stream processing systems. While such systems offer convenient APIs that can be used to achieve data parallelism automatically, they offer limited support for computations that require synchronization between parallel nodes. In this paper, we propose *dependency-guided synchronization (DGS)*, an alternative programming model for stateful streaming computations with complex synchronization requirements. In the proposed model, the input is viewed as partially ordered, and the program consists of a set of parallelization constructs which are applied to decompose the partial order and process events independently. Our programming model maps to an execution model called *synchronization plans* which supports synchronization between parallel nodes. Our evaluation shows that APIs offered by two widely used systems -- Flink and Timely Dataflow -- cannot suitably expose parallelism in some representative applications. In contrast, DGS enables implementations with scalable performance, the resulting synchronization plans offer throughput improvements when implemented manually in existing systems, and the programming overhead is small compared to writing sequential code.\n\n**Published:** 2021-04-09\n\n\n\n#### 4. Distributed Graph Embedding with Information-Oriented Random Walks\n\n*From Search Query: parallel stream synchronization*\n\n*Yuchao Cao, Wei Yin, Zhenli Li, Dan Feng, Fang Wang, Siqiang Luo, Arijit Khan, Peng Fang*\n\n**Abstract:** Graph embedding maps graph nodes to low-dimensional vectors, and is widely adopted in machine learning tasks. The increasing availability of billion-edge graphs underscores the importance of learning efficient and effective embeddings on large graphs, such as link prediction on Twitter with over one billion edges. Most existing graph embedding methods fall short of reaching high data scalability. In this paper, we present a general-purpose, distributed, information-centric random walk-based graph embedding framework, DistGER, which can scale to embed billion-edge graphs. DistGER incrementally computes information-centric random walks. It further leverages a multi-proximity-aware, streaming, parallel graph partitioning strategy, simultaneously achieving high local partition quality and excellent workload balancing across machines. DistGER also improves the distributed Skip-Gram learning model to generate node embeddings by optimizing the access locality, CPU throughput, and synchronization efficiency. Experiments on real-world graphs demonstrate that compared to state-of-the-art distributed graph embedding frameworks, including KnightKing, DistDGL, and Pytorch-BigGraph, DistGER exhibits 2.33x-129x acceleration, 45% reduction in cross-machines communication, and > 10% effectiveness improvement in downstream tasks.\n\n**Published:** 2023-03-28\n\n\n\n#### 5. Killing Two Birds with One Stone:Efficient and Robust Training of Face Recognition CNNs by Partial FC\n\n*From Search Query: memory efficient state updates*\n\n*Tongliang Liu, Jing Yang, Xuhan Zhu, Ziyong Feng, Jia Guo, Jiankang Deng, Xiang An*\n\n**Abstract:** Learning discriminative deep feature embeddings by using million-scale in-the-wild datasets and margin-based softmax loss is the current state-of-the-art approach for face recognition. However, the memory and computing cost of the Fully Connected (FC) layer linearly scales up to the number of identities in the training set. Besides, the large-scale training data inevitably suffers from inter-class conflict and long-tailed distribution. In this paper, we propose a sparsely updating variant of the FC layer, named Partial FC (PFC). In each iteration, positive class centers and a random subset of negative class centers are selected to compute the margin-based softmax loss. All class centers are still maintained throughout the whole training process, but only a subset is selected and updated in each iteration. Therefore, the computing requirement, the probability of inter-class conflict, and the frequency of passive update on tail class centers, are dramatically reduced. Extensive experiments across different training data and backbones (e.g. CNN and ViT) confirm the effectiveness, robustness and efficiency of the proposed PFC. The source code is available at \\https://github.com/deepinsight/insightface/tree/master/recognition.\n\n**Published:** 2022-03-28\n\n\n\n#### 6. Efficient Training of Retrieval Models using Negative Cache\n\n*From Search Query: memory efficient state updates*\n\n*Sanjiv Kumar, Ruiqi Guo, Sashank Reddi, Erik Lindgren*\n\n**Abstract:** Factorized models, such as two tower neural network models, are widely used for scoring (query, document) pairs in information retrieval tasks. These models are typically trained by optimizing the model parameters to score relevant positive\" pairs higher than the irrelevantnegative\" ones. While a large set of negatives typically improves the model performance, limited computation and memory budgets place constraints on the number of negatives used during training. In this paper, we develop a novel negative sampling technique for accelerating training with softmax cross-entropy loss. By using cached (possibly stale) item embeddings, our technique enables training with a large pool of negatives with reduced memory and computation. We also develop a streaming variant of our algorithm geared towards very large datasets. Furthermore, we establish a theoretical basis for our approach by showing that updating a very small fraction of the cache at each iteration can still ensure fast convergence. Finally, we experimentally validate our approach and show that it is efficient and compares favorably with more complex, state-of-the-art approaches.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-12-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on chunk-based processing, stream synchronization, and memory-efficient state updates, here are some key insights and strategies from the provided sources and additional context:\n\n## Chunk-Based Processing in Language Models\n\n### Efficient Chunking Strategies\n- **Semantic Chunking**: This approach involves dividing text based on semantic meaning rather than fixed sizes. It ensures that each chunk represents a coherent piece of information, improving retrieval accuracy and context preservation. This method is highlighted in the discussions on semantic chunking, which calculates cosine distance between sentence embeddings to group semantically similar sentences.\n- **Hierarchy-Based Chunking**: This strategy retains the structural nuances of documents, ensuring that the context remains intact during the extraction process. It is particularly useful for handling complex documents like PDFs, Word files, and PowerPoint presentations.\n- **Late Chunking**: This method involves generating contextual chunk embeddings after the initial processing, which helps in preserving context across chunks. It is beneficial for applications like Retrieval-Augmented Generation (RAG) where maintaining long-context dependencies is crucial.\n\n### Memory Management Across Chunks\n- **Chunk Representation Methods**: Techniques like ChuLo, which uses unsupervised keyphrase extraction to group tokens into chunks, help in reducing input length while minimizing information loss. This approach ensures that each chunk retains the core content of the document, making it memory-efficient and scalable.\n- **Context-Enriched Chunks**: This strategy involves appending summaries of documents to each segment, enriching the information available to the language model without excessively increasing the computational load. This method helps in maintaining context and reducing memory usage.\n\n### Information Preservation Between Chunks\n- **Overlapping Chunks**: Implementing a rolling window approach where chunks overlap slightly can help maintain context between chunks. This ensures that the language model has a more comprehensive view of the text, reducing the risk of information loss.\n- **Keyphrase-Based Chunking**: By identifying and emphasizing semantically important tokens, methods like ChuLo ensure that each chunk retains the core content of the document, preserving global and local features.\n\n## Stream Synchronization Methods\n\n### Parallel Processing Coordination\n- **Two-Stream Architectures**: While not directly discussed in the provided sources, the concept of two-stream architectures from other domains can be applied here. These architectures can process different streams independently, improving robustness and efficiency. However, they require careful synchronization to ensure coherence[Analysis Note, section 2].\n\n### State Synchronization Techniques\n- **Hierarchical State Organization**: Organizing states hierarchically can improve efficiency and facilitate better synchronization across parallel streams. This approach is mentioned in the context of state management and compression techniques[Analysis Note, section 1a].\n\n### Timing Mechanisms\n- **Stream Fusion Mechanisms**: Efficient fusion mechanisms are crucial for synchronizing and combining information from different streams. This involves balancing parallelism with memory usage and ensuring that the fusion process does not introduce significant delays or information loss[Analysis Note, section 3b].\n\n## Memory-Efficient State Updates\n\n### Incremental State Updates\n- **Selective State Integration**: Techniques like those used in DenseMamba, which selectively integrate shallow-layer hidden states into deeper layers, can retain fine-grained information while maintaining training parallelizability and inference efficiency. This approach is incremental and helps in preserving important information without significant memory overhead[Analysis Note, section 2].\n\n### Sparse State Representations\n- **Quantization Techniques**: Methods like QLoRA, which introduce innovations such as 4-bit NormalFloat (NF4) and double quantization, can significantly reduce the memory footprint without sacrificing performance. These techniques are particularly useful for sparse state representations[Analysis Note, section 2].\n\n### Compression Techniques\n- **Hierarchical State Compression**: Hierarchical organization and compression of states can improve memory efficiency. This involves organizing states in a way that allows for efficient compression and retrieval, reducing the overall memory footprint[Analysis Note, section 1a].\n\nBy integrating these strategies, researchers can develop more efficient and scalable autoregressive language models that handle long documents effectively, preserve context across chunks, synchronize parallel streams efficiently, and manage memory resources optimally.\n",
            "analysis": "Analysis Note for HierTTT Proposal - Round 2\n\n1. Summary of Analysis\n\nThe second round of searches revealed several important aspects relevant to the HierTTT proposal:\n\na) Memory Management and State Compression:\n- Novel approaches to state management through modular architectures\n- Importance of efficient caching and state compression\n- Hierarchical state organization techniques\n\nb) Stream Processing:\n- Effectiveness of two-stream architectures in various domains\n- Methods for efficient stream fusion and information combination\n- Trade-offs between parallel processing and memory efficiency\n\nc) Test-Time Training:\n- Integration with parameter-efficient fine-tuning methods\n- Approaches to maintain model adaptability during inference\n- Techniques for efficient state updates\n\n2. Useful References with Excerpts\n\nFrom QLoRA paper:\n\"QLoRA introduces innovations to save memory without sacrificing performance: (a) 4-bit NormalFloat (NF4), a new data type that is information theoretically optimal for normally distributed weights (b) double quantization to reduce average memory footprint...\"\n\nFrom DenseMamba:\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output while maintaining training parallelizability and inference efficiency.\"\n\nFrom BEVFusion:\n\"A surprisingly simple yet novel fusion framework whose camera stream does not depend on the input of LiDAR data, addressing the downside of previous methods.\"\n\n3. Key Insights\n\na) State Management:\n- Quantization techniques can significantly reduce memory footprint\n- Selective state integration can preserve important information\n- Hierarchical organization improves efficiency\n\nb) Stream Processing:\n- Independent stream processing can improve robustness\n- Efficient fusion mechanisms are crucial for performance\n- Need to balance parallelism with memory usage\n\nc) Areas Needing Further Investigation:\n- Specific mechanisms for stream synchronization\n- Impact of chunk size on performance\n- Memory-computation trade-offs in multi-stream processing\n\n4. Future Search Plan\n\nNeed additional information on:\n1. Chunk-based processing in language models\n2. Synchronization mechanisms for parallel streams\n3. Memory-efficient state update techniques\n\n5. References\n\n[1] Dettmers, T., et al. (2023). \"QLoRA: Efficient Finetuning of Quantized LLMs.\" NeurIPS 2023.\n\n[2] He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection.\" arXiv preprint.\n\n[3] Liang, T., et al. (2022). \"BEVFusion: A Simple and Robust LiDAR-Camera Fusion Framework.\" NeurIPS 2022.\n\n[4] Chen, Y., et al. (2022). \"Two-Stream Network for Sign Language Recognition and Translation.\" NeurIPS 2022.\n\n[5] Tang, A., et al. (2023). \"Parameter Efficient Multi-task Model Fusion with Partial Linearization.\" ICLR 2023."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.692349,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.7503120000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "# HierTTT: Hierarchical Test-Time Training with Multi-Stream Processing\n\n## Motivation\n\nThe parent designs (TTT and FastTTTLinear) present complementary strengths:\n- TTT excels in adaptive learning and test-time optimization\n- FastTTTLinear brings efficiency improvements through gated mechanisms\n- Both maintain linear complexity but approach it differently\n\nThe key opportunity lies in combining TTT's adaptive capabilities with efficient multi-stream processing while addressing the memory management challenges in long-sequence processing.\n\n## Problem Analysis\n\n### Parent Design Analysis\n\n1. TTT Strengths:\n- Expressive hidden states through test-time training\n- Strong performance on long contexts\n- Adaptive learning during inference\n\n2. FastTTTLinear Strengths:\n- Efficient gated linear attention\n- Hardware-optimized operations\n- Improved memory management\n\n### Challenges to Address:\n1. Memory efficiency in long sequences\n2. Balancing adaptive learning with computational efficiency\n3. Maintaining expressiveness while reducing complexity\n\n## Core Idea and Philosophy\n\nHierTTT introduces a hierarchical two-stream architecture that separates processing into:\n1. Slow Stream: Handles long-term dependencies using test-time training\n2. Fast Stream: Processes local context efficiently using gated mechanisms\n\nThis separation allows for:\n- Efficient memory management through bounded contexts\n- Parallel processing of different timescales\n- Preserved adaptability from TTT\n- Enhanced efficiency from FastTTTLinear\n\n## Design Plan\n\n### 1. Hierarchical Architecture\n\n```python\nclass HierTTT(GAUBase):\n    def __init__(self, embed_dim, block_loc, kwarg_all, **kwargs):\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \n        # Slow stream for long-term dependencies\n        self.slow_stream = SlowStreamTTT(\n            embed_dim=embed_dim,\n            block_loc=block_loc,\n            kwarg_all=kwarg_all\n        )\n        \n        # Fast stream for local context\n        self.fast_stream = FastStreamGated(\n            embed_dim=embed_dim,\n            block_loc=block_loc,\n            kwarg_all=kwarg_all\n        )\n        \n        # Stream fusion module\n        self.fusion = StreamFusion(\n            embed_dim=embed_dim,\n            block_loc=block_loc,\n            kwarg_all=kwarg_all\n        )\n```\n\n### 2. Slow Stream (From TTT)\n\nMaintains test-time training capabilities with modifications:\n- Bounded memory through chunk-based processing\n- Hierarchical state management\n- Adaptive computation paths\n\n```python\nclass SlowStreamTTT(nn.Module):\n    def __init__(self, embed_dim, block_loc, kwarg_all):\n        super().__init__()\n        self.chunk_size = kwarg_all.get('chunk_size', 1024)\n        self.ttt_linear = TTTLinear(\n            embed_dim=embed_dim,\n            block_loc=block_loc,\n            kwarg_all=kwarg_all\n        )\n        self.state_manager = HierarchicalStateManager(\n            embed_dim=embed_dim,\n            num_chunks=kwarg_all.get('num_chunks', 8)\n        )\n```\n\n### 3. Fast Stream (From FastTTTLinear)\n\nFocuses on efficient local processing:\n- Gated linear attention mechanisms\n- Local convolutional augmentation\n- Efficient routing between layers\n\n```python\nclass FastStreamGated(nn.Module):\n    def __init__(self, embed_dim, block_loc, kwarg_all):\n        super().__init__()\n        self.gated_attention = GatedLinearAttention(\n            embed_dim=embed_dim,\n            num_heads=kwarg_all.get('num_heads', 8)\n        )\n        self.local_conv = LocalConvolution(\n            embed_dim=embed_dim,\n            kernel_size=kwarg_all.get('kernel_size', 3)\n        )\n```\n\n### 4. Stream Fusion\n\nCombines outputs from both streams:\n- Adaptive weighting based on input characteristics\n- Efficient information routing\n- Gradient flow management\n\n```python\nclass StreamFusion(nn.Module):\n    def __init__(self, embed_dim, block_loc, kwarg_all):\n        super().__init__()\n        self.fusion_gate = nn.Linear(embed_dim * 2, embed_dim)\n        self.norm = RMSNorm(embed_dim)\n```\n\n## Implementation Guidelines\n\n### 1. Memory Management\n\n```python\nclass HierarchicalStateManager:\n    def __init__(self, embed_dim, num_chunks):\n        self.chunk_states = []\n        self.max_chunks = num_chunks\n        \n    def update_state(self, new_state, chunk_idx):\n        if chunk_idx < self.max_chunks:\n            self.chunk_states[chunk_idx] = new_state\n        else:\n            # Merge older states\n            self._merge_states()\n            \n    def _merge_states(self):\n        # Implement state merging strategy\n        pass\n```\n\n### 2. Stream Processing\n\n```python\ndef forward(self, X, **Z):\n    # Split input into chunks\n    chunks = self._split_chunks(X)\n    \n    # Process through streams\n    slow_outputs = []\n    fast_outputs = []\n    \n    for chunk in chunks:\n        # Slow stream processing\n        slow_out = self.slow_stream(chunk, **Z)\n        slow_outputs.append(slow_out)\n        \n        # Fast stream processing\n        fast_out = self.fast_stream(chunk, **Z)\n        fast_outputs.append(fast_out)\n    \n    # Fuse outputs\n    return self.fusion(slow_outputs, fast_outputs)\n```\n\n### 3. Training Process\n\n1. Initialize both streams with pretrained weights from parents\n2. Train fusion module while freezing streams\n3. Fine-tune entire model end-to-end\n4. Implement gradient checkpointing for memory efficiency\n\n## Theoretical Analysis\n\n### 1. Complexity Analysis\n\n- Time Complexity: O(n) maintained through bounded contexts\n- Space Complexity: O(k) where k is the chunk size\n- Memory Usage: Reduced by factor of chunk_size/sequence_length\n\n### 2. Information Flow\n\nThe hierarchical structure ensures:\n- Efficient gradient flow through both streams\n- Preserved test-time training capabilities\n- Balanced local and global context processing\n\n## Expected Benefits\n\n1. Improved Efficiency:\n- Reduced memory usage through bounded contexts\n- Parallel processing of different timescales\n- Efficient routing between streams\n\n2. Enhanced Performance:\n- Better handling of long-term dependencies\n- Preserved adaptability from TTT\n- Improved local context processing\n\n3. Scalability:\n- Linear complexity maintained\n- Efficient memory management\n- Hardware-friendly operations\n\n## References\n\n1. Parent Designs:\n   - TTT (Original implementation)\n   - FastTTTLinear (Efficiency improvements)\n\n2. Key Papers:\n   - \"Efficient Streaming Language Models with Attention Sinks\"\n   - \"Learning to Ignore: Long Document Coreference with Bounded Memory\"\n   - \"DenseMamba: State Space Models with Dense Hidden Connection\"",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "gpt4o_0806",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A hierarchical enhancement of TTT that combines the adaptive capabilities of test-time training with efficient multi-stream processing. The design introduces a two-stream architecture with bounded memory management, where a slow stream handles long-term dependencies through test-time training while a fast stream processes local context efficiently. This approach preserves the benefits of both parent designs while improving scalability and computational efficiency.",
    "ideation": null,
    "modelname": "hierttt",
    "suggestions": "1. Stream Synchronization:\n- Provide more detailed specifications for stream synchronization mechanisms\n- Consider adding adaptive timing mechanisms for stream coordination\n- Include failure recovery strategies for stream desynchronization\n\n2. Memory Management:\n- Develop more detailed analysis of memory-computation trade-offs\n- Consider adaptive chunk sizing based on input characteristics\n- Add mechanisms for efficient state compression between chunks\n\n3. Implementation Details:\n- Expand on training procedure specifics\n- Include ablation study designs for evaluating component contributions\n- Add more detail on initialization strategies for both streams\n\n4. Theoretical Analysis:\n- Provide formal bounds on memory usage\n- Include analysis of worst-case computational complexity\n- Add theoretical guarantees for stream fusion stability\n\n5. Practical Considerations:\n- Add guidelines for hyperparameter selection\n- Include strategies for handling edge cases\n- Provide more detailed performance monitoring recommendations",
    "user_input": ""
}