{
    "variantname": "AdaptiveRMSNorm",
    "review": "The AdaptiveNormGPT proposal presents an innovative approach to enhancing RMSNorm through context-aware parameter adaptation and hardware-efficient implementation. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel integration of context-dependent parameter adaptation with RMSNorm\n- Well-grounded mathematical formulation of adaptive mechanisms\n- Clear differentiation from existing adaptive normalization approaches\n- Thoughtful consideration of hardware efficiency through IO-aware design\n\n2. Technical Design:\n- Detailed mathematical formulation of parameter adaptation\n- Clear implementation guidelines with practical considerations\n- Careful attention to memory management and computational overhead\n- Integration of progressive re-parameterization for stability\n\n3. Efficiency Considerations:\n- Strategic use of parameter sharing to reduce memory overhead\n- Efficient implementation of adaptive computations\n- Hardware-aware design with specific optimization strategies\n- Clear focus on practical implementation efficiency\n\n4. Scalability:\n- Linear computational complexity maintained through careful design\n- Efficient memory management through progressive updates\n- Clear strategy for parallel processing\n- Consideration of cache hierarchy in design\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex coordination between adaptive mechanisms and base normalization\n- Potential challenges in maintaining numerical stability\n- Need for careful tuning of progressive re-parameterization\n- Complex memory access patterns requiring optimization\n\n2. Training Stability:\n- Progressive parameter updates may affect convergence\n- Complex gradient flow through adaptive components\n- Need for careful initialization strategies\n- Possible instability in early training stages\n\n3. Memory Management:\n- Additional overhead from context-dependent computations\n- Complex state management for adaptive parameters\n- Need for efficient caching strategies\n- Trade-offs between adaptivity and memory efficiency\n\n4. Hardware Utilization:\n- Complex memory access patterns may impact efficiency\n- Need for hardware-specific optimizations\n- Potential challenges in parallel implementation\n- Trade-offs between computation and memory access\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than traditional adaptive normalization\n2. Novel integration of context-awareness with RMSNorm\n3. Unique approach to progressive re-parameterization\n4. Thoughtful integration with hardware optimization\n\nHowever, it shares some concepts with:\n1. AdaNorm's adaptive parameter computation\n2. SALN's context-dependent normalization\n3. Progressive re-parameterization in SLAB",
    "search_stack": [
        {
            "ready": false,
            "query": "sparse selective attention mechanisms memory optimization",
            "detail": "Find specific implementations and technical details of:\n1. Sparse and selective attention mechanisms that maintain efficiency while preserving expressiveness\n2. Memory optimization techniques for handling long sequences and state management\n3. Advanced normalization methods that improve stability and gradient flow",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementations and technical details of:\n1. Sparse and selective attention mechanisms that maintain efficiency while preserving expressiveness\n2. Memory optimization techniques for handling long sequences and state management\n3. Advanced normalization methods that improve stability and gradient flow\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Big Bird: Transformers for Longer Sequences (Avg. Score: 0.97)\n\n*M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 1631  (*Influential: 238*)\n\n**TL;DR:** It is shown that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.\n\n**Abstract:** Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n\n##### *Relevant Chunk: No. 5/94 (Score: 0.97)*\n\n```\nWe provide implementation details in App. D. ## 3 Theoretical Results about Sparse Attention Mechanism\n\nIn this section, we will show that that sparse attention mechanisms are as powerful and expressive as full-attention mechanisms in two respects. First, we show that when sparse attention mechanisms are used in a standalone encoder (such as BERT), they are Universal Approximators of sequence to sequence functions in the style of Yun et al.\n```\n\n#### 2. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.54)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 2/41 (Score: 0.92)*\n\n```\nIn this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. ## 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges $[1,20,19]$, owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods $[39,61]$ allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually $\\operatorname{cost} O(n \\log n)$ to maintain clusters. Ainslie et al. [1]\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_81cbe08ae077927ce965g-02.jpg?height=549&width=1261&top_left_y=254&top_left_x=432)\n\nFigure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query ( $\\tau(\\mathbf{u})$ ) such that the sum of normalized scores is $k$, which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency. incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SofTTOPK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows. ## Incremental KV Selection. The SPARSEK operator (\u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (\u00a73.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments\non language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. ## 2 Related Work\n\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence $[17,35,36,12]$ or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions $[15,27,42]$. Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns $[34,13]$, or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].\n```\n\n##### *Relevant Chunk: No. 40/41 (Score: 0.17)*\n\n```\nHowever, our method is not dependent on the input modality. Future research involving vision or speech could further substantiate the robustness of our method. ## E Impact Statement\n\nThis paper presents SPARSEK Attention, whose goal is to address both computational and memory efficiency challenges in long-range Transformer computing. We believe our innovative attention mechanism can benefit both NLP and machine learning communities in constructing long-range foundation models. Specifically, we highlight the potential impacts of SPARSEK as follows:\n\n- Efficient Long-Range Modeling. First and foremost, the SPARSEK attention mechanism significantly reduces computational requirements compared to traditional self-attention mechanisms. By prioritizing a subset of key-value pairs, SPARSEK attention effectively reduces the memory footprint without sacrificing model performance.\n```\n\n#### 3. Sparse and continuous attention mechanisms (Avg. Score: 0.23)\n\n*Andr\u00e9 F. T. Martins, Marcos Vin\u00edcius Treviso, Ant\u00f3nio Farinhas, Vlad Niculae, M\u00e1rio A. T. Figueiredo, P. Aguiar*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 28  (*Influential: 2*)\n\n**TL;DR:** This paper extends alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families, and introduces continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}.\n\n**Abstract:** Exponential families are widely used in machine learning; they include many distributions in continuous and discrete domains (e.g., Gaussian, Dirichlet, Poisson, and categorical distributions via the softmax transformation). Distributions in each of these families have fixed support. In contrast, for finite domains, there has been recent work on sparse alternatives to softmax (e.g. sparsemax and alpha-entmax), which have varying support, being able to assign zero probability to irrelevant categories. This paper expands that work in two directions: first, we extend alpha-entmax to continuous domains, revealing a link with Tsallis statistics and deformed exponential families. Second, we introduce continuous-domain attention mechanisms, deriving efficient gradient backpropagation algorithms for alpha in {1,2}. Experiments on attention-based text classification, machine translation, and visual question answering illustrate the use of continuous attention in 1D and 2D, showing that it allows attending to time intervals and compact regions.\n\n##### *Relevant Chunk: No. 1/36 (Score: 0.23)*\n\n```\n# Sparse and Continuous Attention Mechanisms \n\nAndr\u00e9 F.\n```\n\n#### 4. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.16)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.16)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: sparse selective attention mechanisms memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Prototype memory and attention mechanisms for few shot image generation\n\n*From Search Query: sparse selective attention mechanisms memory optimization*\n\n*Tianqin Li, Zijie Li, Andrew Luo, Harold Rockwell, A. Farimani, T. Lee*\n\n**TL;DR:** The results demonstrate the feasibility of the idea that these super-sparse complex feature detectors of macaque monkeys can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Abstract:** Recent discoveries indicate that the neural codes in the super\ufb01cial layers of the primary visual cortex (V1) of macaque monkeys are complex, diverse and super-sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing during the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized through a memory-based attention operation. Integrating this mechanism, we propose Memory Concept Attention ( MoCA ) to improve few shot image generation quality. We show that having a prototype memory with attention mechanisms can improve image synthesis quality, learn interpretable visual concept clusters, and improve the robustness of the model. Our results demonstrate the feasibility of the idea that these super-sparse complex feature detectors can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 2. Attention Approximates Sparse Distributed Memory\n\n*From Search Query: sparse selective attention mechanisms memory optimization*\n\n*Trenton Bricken, C. Pehlevan*\n\n**TL;DR:** It is shown that Transformer Attention can be closely related under certain data conditions to Kanerva\u2019s Sparse Distributed Memory (SDM), a biologically plausible associative memory model.\n\n**Abstract:** While Attention has come to be an important mechanism in deep learning, there remains limited intuition for why it works so well. Here, we show that Transformer Attention can be closely related under certain data conditions to Kanerva\u2019s Sparse Distributed Memory (SDM), a biologically plausible associative memory model. We con\ufb01rm that these conditions are satis\ufb01ed in pre-trained GPT2 Transformer models. We discuss the implications of the Attention-SDM map and provide new computational and biological interpretations of Attention.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 32  (*Influential: 3*)\n\n#### 3. Long-range Sequence Modeling with Predictable Sparse Attention\n\n*From Search Query: sparse selective attention mechanisms memory optimization*\n\n*Yimeng Zhuang, Jing Zhang, Mei Tu*\n\n**TL;DR:** An efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling, which remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.\n\n**Abstract:** Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling. We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable. Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and pools L^2 semantic combinations in \\mathcal{O}(L\\log L) time complexity. (2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module. By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements. The overall complexity about the sequence length is reduced from \\mathcal{O}(L^2) to \\mathcal{O}(L\\log L). Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 4. Visual Attention Emerges from Recurrent Sparse Reconstruction\n\n*From Search Query: sparse selective attention mechanisms memory optimization*\n\n*Baifeng Shi, Ya-heng Song, Neel Joshi, Trevor Darrell, Xin Wang*\n\n**TL;DR:** VARS, Visual Attention from Recurrent Sparse reconstruction, a new attention formulation built on two prominent features of the human visual attention mechanism: recurrency and sparsity, can be readily used as a replacement for self-attention in popular vision transformers, consistently improving their robustness across various benchmarks.\n\n**Abstract:** Visual attention helps achieve robust perception under noise, corruption, and distribution shifts in human vision, which are areas where modern neural networks still fall short. We present VARS, Visual Attention from Recurrent Sparse reconstruction, a new attention formulation built on two prominent features of the human visual attention mechanism: recurrency and sparsity. Related features are grouped together via recurrent connections between neurons, with salient objects emerging via sparse regularization. VARS adopts an attractor network with recurrent connections that converges toward a stable pattern over time. Network layers are represented as ordinary differential equations (ODEs), formulating attention as a recurrent attractor network that equivalently optimizes the sparse reconstruction of input using a dictionary of\"templates\"encoding underlying patterns of data. We show that self-attention is a special case of VARS with a single-step optimization and no sparsity constraint. VARS can be readily used as a replacement for self-attention in popular vision transformers, consistently improving their robustness across various benchmarks. Code is released on GitHub (https://github.com/bfshi/VARS).\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 2*)\n\n#### 5. Combiner: Full Attention Transformer with Sparse Computation Cost\n\n*From Search Query: sparse selective attention mechanisms memory optimization*\n\n*Hongyu Ren, H. Dai, Zihang Dai, Mengjiao Yang, J. Leskovec, D. Schuurmans, Bo Dai*\n\n**TL;DR:** Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Abstract:** Transformers provide a class of expressive architectures that are extremely effective for sequence modeling. However, the key limitation of transformers is their quadratic memory and time complexity $\\mathcal{O}(L^2)$ with respect to the sequence length in attention layers, which restricts application in extremely long sequences. Most existing approaches leverage sparsity or low-rank assumptions in the attention matrix to reduce cost, but sacrifice expressiveness. Instead, we propose Combiner, which provides full attention capability in each attention head while maintaining low computation and memory complexity. The key idea is to treat the self-attention mechanism as a conditional expectation over embeddings at each location, and approximate the conditional distribution with a structured factorization. Each location can attend to all other locations, either via direct attention, or through indirect attention to abstractions, which are again conditional expectations of embeddings from corresponding local regions. We show that most sparse attention patterns used in existing sparse transformers are able to inspire the design of such factorization for full attention, resulting in the same sub-quadratic cost ($\\mathcal{O}(L\\log(L))$ or $\\mathcal{O}(L\\sqrt{L})$). Combiner is a drop-in replacement for attention layers in existing transformers and can be easily implemented in common frameworks. An experimental evaluation on both autoregressive and bidirectional sequence tasks demonstrates the effectiveness of this approach, yielding state-of-the-art results on several image and text modeling tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 69  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, focusing on sparse and selective attention mechanisms, memory optimization, and advanced normalization methods, here are some specific implementations and technical details that can be beneficial:\n\n### Sparse and Selective Attention Mechanisms\n\n1. **SeerAttention**:\n   - This mechanism learns and leverages intrinsic sparsity in attention maps, adapting to different context lengths and sparsity ratios. It uses a learnable gate to identify significant blocks in the attention map, reducing computational costs while maintaining accuracy. SeerAttention achieves up to a 5.67\u00d7 speedup over dense baselines with minimal perplexity loss, especially in long-context fine-tuning.\n\n2. **Selective Attention**:\n   - Introduced as a parameter-free modification to the standard attention mechanism, selective attention reduces attention to unneeded elements. This approach decreases the size of the attention context buffer, leading to significant reductions in memory and compute requirements during inference. It maintains or improves perplexity while reducing memory needs by factors of 16X, 25X, and 47X for different context sizes.\n\n3. **Sparse Transformers**:\n   - These models apply sparse factorizations to self-attention matrices, reducing complexity from \\(O(n^2)\\) to \\(O(n\\sqrt{n})\\). This allows for faster processing times and better handling of larger input sizes, making them efficient for real-time applications and scalable across various tasks.\n\n### Memory Optimization Techniques\n\n1. **Sparse Storage Formats**:\n   - Using sparse storage formats can minimize memory consumption by storing only non-zero elements. This optimization is particularly beneficial in high-dimensional data and large-scale neural networks, leading to faster computation by skipping zero elements.\n\n2. **Context Buffer Reduction**:\n   - Selective attention mechanisms can reduce the size of the attention context buffer, resulting in meaningful reductions in memory and compute requirements. For example, transformers with selective attention can reduce memory needs by factors of 5X to 47X depending on the context size.\n\n3. **Memory-Augmented Neural Networks**:\n   - Models like Neural Turing Machines (NTMs) and Differentiable Neural Computers (DNCs) incorporate external memory components for dynamic read and write operations. These architectures help manage information more flexibly and effectively, especially in tasks requiring extensive context.\n\n### Advanced Normalization Methods\n\n1. **Complex Exponential Moving Average (CEMA)**:\n   - Introduced in the MEGALODON architecture, CEMA is a variant of exponential moving average that improves state handling. This method, combined with timestep normalization and pre-norm with two-hop residual configuration, enhances stability and gradient flow.\n\n2. **Timestep Normalization**:\n   - This method, also part of the MEGALODON architecture, provides better stability by normalizing the inputs based on the timestep. This helps in managing the state across hierarchical levels more effectively.\n\n3. **Dynamic Layer Normalization**:\n   - The HieraNorm-AttnGPT design already uses dynamic layer normalization. However, further enhancements can be made by incorporating techniques like pre-norm with two-hop residual configurations to improve gradient flow and stability.\n\n### Additional Insights\n\n1. **Positional Skip-wise Training (PoSE)**:\n   - This method simulates long inputs using a fixed context window, potentially supporting infinite length limited only by memory usage in inference. This can help in optimizing position handling and context window extension.\n\n2. **Attention as an RNN**:\n   - The Aaren module, which updates attention efficiently with new tokens using constant memory, provides insights into efficient attention computation and memory management. This can be integrated to improve the overall efficiency of the model.\n\nBy incorporating these sparse and selective attention mechanisms, optimizing memory usage through advanced storage formats and context buffer reduction, and enhancing normalization methods with complex exponential moving averages and timestep normalization, the autoregressive language model can achieve better efficiency, scalability, and performance.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design HieraNorm-AttnGPT introduces hierarchical adaptive multi-head attention with dynamic layer normalization. From analyzing the seed design and references, I've identified several key areas for potential improvement:\n\n1. Memory Efficiency & Long Sequences:\n- Current designs struggle with long sequence handling\n- Need for more efficient memory management strategies\n- Opportunity to improve scaling with sequence length\n\n2. Attention Mechanism:\n- Current hierarchical attention still has room for optimization\n- Potential to incorporate sparse/selective attention patterns\n- Need for better balance between efficiency and expressiveness\n\n3. Normalization & State Management:\n- Current dynamic normalization could be enhanced\n- Opportunity to improve state handling across hierarchical levels\n- Need for more robust gradient flow mechanisms\n\nKey References & Excerpts:\n\n1. From \"Sparse Modular Activation for Efficient Sequence Modeling\":\n\"SMA enables neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages.\"\n\nThis suggests potential for incorporating selective activation mechanisms to improve efficiency.\n\n2. From \"MEGALODON\":\n\"Megalodon inherits the architecture of Mega (exponential moving average with gated attention), and further introduces multiple technical components including complex exponential moving average (CEMA), timestep normalization layer, normalized attention mechanism and pre-norm with two-hop residual configuration.\"\n\nThe CEMA and timestep normalization concepts could be valuable for improving our normalization approach.\n\n3. From \"PoSE\":\n\"Positional Skip-wisE (PoSE) training that smartly simulates long inputs using a fixed context window... can potentially support infinite length, limited only by memory usage in inference.\"\n\nThis suggests possibilities for improving position handling and context window extension.\n\n4. From \"Attention as an RNN\":\n\"Aaren is introduced, an attention-based module that can not only be trained in parallel but also be updated efficiently with new tokens, requiring only constant memory for inferences.\"\n\nThis provides insights into efficient attention computation and memory management.\n\nKey Insights:\n\n1. Memory & Computation Efficiency:\n- Selective activation mechanisms could significantly reduce computation and memory needs\n- Constant memory attention updates are possible through careful design\n- Position handling can be optimized for better efficiency\n\n2. Architectural Innovations:\n- Complex exponential moving averages offer improved state handling\n- Timestep normalization provides better stability\n- Two-hop residual configurations improve gradient flow\n\n3. Scaling Capabilities:\n- Position-wise training can enable better length generalization\n- Selective mechanisms can improve efficiency without sacrificing performance\n- Memory usage can be optimized through careful state management\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Specific implementations of selective attention mechanisms\n- Details of exponential moving average variants\n- Memory-efficient state management techniques\n- Position encoding optimization methods\n\n2. Search focus:\n- Technical details of sparse attention implementations\n- Memory optimization techniques in large language models\n- State compression and management strategies\n- Position encoding innovations\n\nReferences:\n\n1. Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., & Zhai, C. (2023). Sparse Modular Activation for Efficient Sequence Modeling. Neural Information Processing Systems.\n\n2. Ma, X., Yang, X., Xiong, W., Chen, B., Yu, L., Zhang, H., ... & Zhou, C. (2024). MEGALODON: Efficient LLM Pretraining and Inference with Unlimited Context Length. arXiv.org.\n\n3. Zhu, D., Yang, N., Wang, L., Song, Y., Wu, W., Wei, F., & Li, S. (2023). PoSE: Efficient Context Window Extension of LLMs via Positional Skip-wise Training. arXiv.org.\n\n4. Feng, L., Tung, F., Hajimirsadeghi, H., Ahmed, M. O., Bengio, Y., & Mori, G. (2024). Attention as an RNN. arXiv.org."
        },
        {
            "ready": false,
            "query": "memory efficient backpropagation kv pair selection",
            "detail": "Find technical details about:\n1. Memory-efficient backpropagation techniques in attention mechanisms\n2. KV pair selection and management strategies\n3. Integration methods between sparse attention and normalization layers",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about:\n1. Memory-efficient backpropagation techniques in attention mechanisms\n2. KV pair selection and management strategies\n3. Integration methods between sparse attention and normalization layers\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.90)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 40/53 (Score: 0.90)*\n\n```\nIn Proceedings of the AAAI Conference on Artificial Intelligence. AAAI Conference on Artificial Intelligence, volume 35, page 14138,2021 . [91] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch on imagenet. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 558-567, 2021. [92] Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in Neural Information Processing Systems, 33, 2020. [93] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. [94] Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, Tom Goldstein, Anima Anandkumar, and Bryan Catanzaro. Long-short transformer: Efficient transformers for language and vision. Advances in Neural Information Processing Systems, 34, 2021. ## A Related Work\n\nIO-Aware Runtime Optimization. The broad concept of optimizing for reading and writing to fast/slow memory has a long history in computer science and has been known by many names. We draw the most direct connection to the literature of analyzing I/O complexity in this work [1], but concepts of memory hierarchies are fundamental and has appeared in many forms, from the working set model [21], to data locality [86], to the Roofline model of arithmetic intensity [85], to analyses of scalability [59], to standard textbook treatments of computer architecture [40]. We hope that this work encourages the community to adopt these ideas in more parts of the deep learning stack. Efficient ML Models with Structured Matrices. Matrix multiply is the core computational bottleneck of most machine learning models. To reduce the computational complexity, there have been numerous approaches to learn over a more efficient set of matrices. These matrices are called structured matrices, which have subquadratic ( $o\\left(n^{2}\\right)$ for dimension $n \\times n$ ) number of parameters and runtime. Most common examples of structured matrices are sparse and low-rank matrices, along with fast transforms commonly encountered in signal processing (Fourier, Chebyshev, sine/cosine, orthogonal polynomials). There have been several more general classes of structured matrices proposed in machine learning: Toeplitz-like [78, low-displacement rank [49], quasi-separable [25]). The butterfly pattern we use for our block-sparse attention is motivated by the fact that butterfly matrices [15, 64] and their products have been shown to be able to express any structured matrices with almost optimal runtime and number of parameters 16, 20. However, even though structured matrices are efficient in theory, they have not seen wide adoption since it is hard to translate their efficiency to wall-clock speedup since dense unconstrained matrix multiply has very optimize implementation, a phenomenon known as the hardware lottery [41]. Extensions of butterfly matrices [17, 18] aimed to make butterfly matrices more hardware-friendly. Sparse Training. Our block-sparse FlashAttention can be seen as a step towards making sparse model training more efficient. Sparse models have seen success in compressing models for inference (pruning) by sparsifying the weight matrices [23, 38, 39, 55, 76. For model training, the lottery tickets hypothesis [28, 29, 30] suggests that there are a set of small sub-networks derived from a larger dense network that performs as well as the original dense network. Out block-sparse FlashAttention can also be seen as a fixed lottery ticket in the context of attention: we fix the sparsity pattern to be the butterfly pattern through training, and observe that it performs almost as well as the (dense) FlashAttention on the Long-range Arena tasks. Efficient Transformer. Transformer-based models have become the most widely-used architecture in natural language processing [22] and computer vision [24, 91]. However, one of their computational bottlenecks is that their time and memory scales quadratic in the sequence length. There are numerous approaches to overcome this bottleneck, including approximation with hashing (i.e., sparse) such as Reformer 51 and Smyrf [19] and with low-rank approximation such as Performer [12, 54. One can even combine sparse and low-rank approximation for better accuracy (e.g., Longformer [3, BigBird 92, Scatterbrain 9], Long-short transformer [94], Combiner [73]). Other approaches include compressing along the sequence dimension to attend to multiple tokens at once [52, 57, 79, 89]. One can also attend over the states from previous sequences to help lengthen the context (e.g., Transformer-XL 14 and Compressive Transformer 69]). We recommend the survey 81 for more details. There are several lines of work on developing other modules instead of attention to model longer context. HiPPO [35] and its extensions, most notably S4 [31, 36, 37] projects the history on a polynomial basis, allowing accurate reconstruction of the history through state-space models. They combine the strengths of CNNs (efficient training), RNNs (efficient inference), and continuous models (robust to change in sampling rates). LambdaNetworks [2], AFT [93] and FLASH 42] are other attempts at replacing attention in the context of image classification and language modeling. ## B Algorithm Details\n\nWe first derive the forward and backward passes of attention and show that they can be computed in a memory-efficient manner (requiring extra memory linear instead of quadratic in the sequence length). Though they reduce the amount of extra memory required, naively they still incur quadratic HBM accesses, resulting in slower execution speed. We describe the FlashAtTEntion algorithm to implement both the forward\nand the backward passes on GPUs that reduces HBM accesses, leading to both faster runtime and smaller memory footprint. ## B. 1 Memory-efficient forward pass\n\nThe main challenge in making attention memory-efficient is the softmax that couples the columns of $\\mathbf{K}$ (and columns of $\\mathbf{V}$ ). Our approach is to compute the softmax normalization constant separately to decouple the columns. This technique [60] has been used in the literature [51, 66] to show that attention computation does not need quadratic extra memory (though the number of HBM accesses is still quadratic, resulting in slow run-time). For simplicity, we omit here the max-shifting step during softmax. The full algorithm in Appendix B. 3 contains all the steps. Recall that given input sequences $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}$, we want to compute the attention output $\\mathbf{O} \\in \\mathbb{R}^{N \\times d}$ :\n\n$$\n\\mathbf{S}=\\mathbf{Q K}^{\\top} \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{P}=\\operatorname{softmax}(\\mathbf{S}) \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{O}=\\mathbf{P V} \\in \\mathbb{R}^{N \\times d}\n$$\n\nWe have that $S_{i j}=q_{i}^{T} k_{j}$ where $q_{i}$ and $k_{j}$ are the $i$-th and $j$-th columns of $\\mathbf{Q}$ and $\\mathbf{K}$ respectively. Define the normalization constants of softmax:\n\n$$\nL_{i}=\\sum_{j} e^{q_{i}^{T} k_{j}}\n$$\n\nLet $v_{j}$ be the $j$-th column of $\\mathbf{V}$, then the $i$-th columns of the output is\n\n$$\no_{i}=P_{i:} \\mathbf{V}=\\sum_{j} P_{i j} v_{j}=\\sum_{j} \\frac{e^{q_{i}^{T} k_{j}}}{L_{i}} v_{j}\n$$\n\nWe see that once $L_{i}$ is computed, we can compute $o_{i}$ without extra memory by repeatedly summing $\\frac{q^{q_{i}^{T} k_{j}}}{L_{i}} v_{j}$.\n```\n\n#### 2. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.88)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 2/41 (Score: 1.00)*\n\n```\nIn this work, we introduce SparseK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SparseK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications. Our code will be publicly available. ## 1 Introduction\n\nTransformer models [72] have been considered as a de facto backbone of modeling arbitrary sequences, pretraining foundation models [8, 21], and more recently, constructing large language models (LLMs) [9, 69]. Despite the inspiring success of their wide applications on both Natural Language Processing (NLP) and Machine Learning (ML) downstream tasks, extending the context window size to long sequences with computation and memory efficiently poses significant challenges $[1,20,19]$, owing to the quadratic computation complexity and large amounts of key/value vectors associated with self-attention, especially on resource-constrained devices. Many recent studies resort to developing learnable sparse and memory-efficient forms of attention to scale to large sequence lengths. However, applying traditional learnable sparse attention methods to long-range Transformer decoders suffers from two major bottlenecks: (i) Previous studies usually overlook the memory cost of fully memorizing Key-Value (KV) pairs. Clustering-based methods $[39,61]$ allow queries to attend to different sets of KV pairs. In such methods, KV embeddings are required to be fully stored in memory to avoid repetitive computation, which leads to huge memory redundancy and inefficiency when it comes to long-range inference [81, 42, 78]. (ii) Previous learnable sparse attention often has super-linear complexity, especially during training. For example, clustering-based methods usually $\\operatorname{cost} O(n \\log n)$ to maintain clusters. Ainslie et al. [1]\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_81cbe08ae077927ce965g-02.jpg?height=549&width=1261&top_left_y=254&top_left_x=432)\n\nFigure 1: Left: SPARSEK operation in the attention module. KV pairs are scored by u. SPARSEK computes a threshold for each query ( $\\tau(\\mathbf{u})$ ) such that the sum of normalized scores is $k$, which is 3 in this example. We select top- $k$ KV pairs (orange cells) to perform attention. Right: the SPARSEK attention module. We fuse selection and attention in one kernel for efficiency. incorporates a SOFTTOPK operator [41] to compute soft masks in Transformer encoders. Meanwhile, migrating SOFTTOPK to Transformer decoders is less advantageous because solving SofTTOPK for variable-length context associated with different queries requires quadratic time in total. To tackle the aforementioned barriers, we propose SparseK Attention, an innovative technique that achieves both computational and memory efficiency for training and inference-time attention computing in Transformer decoders, as depicted in Figure 1. Within a self-attention module, our method incorporates (1) a scoring network evaluating the importance of each KV pair without accessing the queries that possibly attend to it, and (2) a novel differentiable top- $k$ mask operator SPARSEK, which normalizes scores to a soft mask (or gates) in linear time. It is worth noting that our method draws inspiration from the concept of top- $k$ attention [32, 1]. Unfortunately, conventional top- $k$ attention is non-differentiable and therefore cannot be used to train the scoring network. With thorough comparisons with prior sparse attention learning approaches, we highlight the main advantages of SPARSEK attention as follows. ## Incremental KV Selection. The SPARSEK operator (\u00a7 3.3) supports incremental evaluation and thus has a linear complexity in the decoder. Besides, compared with SOFTTOPK that performs iterative approximation as in CoLT5 [1], our operator computes the exact operation results. Computational and Memory Efficiency. SPARSEK reduces the quadratic training-time complexity of previous learnable sparse attention methods [65,32, 2, 47] to linear time and achieves constant memory cost in inference. This improvement of training-time complexity is achieved by the efficiency of KV selection and applying the same level of sparsity in training as in inference. Additionally, the query-independence of our scoring network guarantees the irreversibility of masking out key-value pairs. This ensures memory efficiency at inference time, allowing for the safe removal of masked key-value pairs from memory immediately (\u00a73.2). Extension with IO-awareness. FlashAttention [20] is a widely adopted optimization for accelerating LLMs with IO-awareness. However, the sparsity learned through our method presents a complex memory access pattern, hindering its direct application. To address this, we develop a Triton kernel that fuses the computation of attention and the selection of proper key-value pairs. Our implementation exhibits linear complexity and surpasses FlashAttention in performance when handling 4096 input tokens, of which 1024 key-value pairs are selected for each query. Additionally, we offer a kernel for the backward pass, which fuses the computation of the gradient of SPARSEK and others, resulting in increased speed and improved memory efficiency. We verify the advantages of SPARSEK attention by replacing full attention in various models (such as GPT2 [57] and Pythia [6]) with it and other efficient attention methods. We consider a wide range of settings, including training from scratch and fine-tuning pretrained models. Experiments\non language modeling and downstream tasks demonstrate that, when matching the context size, our method outperforms other efficient attention methods consistently while providing promising speed-up at training compared to full attention. ## 2 Related Work\n\nLong-range Transformers Self-attention is a cornerstone of Transformer success, but its quadratic complexity concerning input length poses challenges for tasks requiring long context. Numerous efficient approaches have emerged, spanning state-space models [30, 62], recurrent neural networks [45, 52, 49], linear attention [55, 38] and low-rank approximations of self-attention [75, 14, 53], which replace the self-attention with novel linear blocks for long-context modeling. Nonetheless, these approaches historically underperformed compared to modern Transformer models [70] in language modeling tasks until recent efforts [29, 77]. Besides, a few studies combine the Transformer with block-wise recurrence $[17,35,36,12]$ or key-value compression [60, 59, 18]. In contrast, our approach falls under sparse attention, reducing complexity by pruning the attention matrix. This approach is motivated by observations that the attention matrix in dense models naturally becomes sparse, and the performance of language models remains robust under reasonably sparse conditions $[15,27,42]$. Sparse attention Some sparse attention utilized fixed patterns to restrict the number of tokens involved, such as sliding windows [56, 51], dilated sliding windows [4, 22], combination of patterns $[34,13]$, or domain-specific patterns [31]. Recent studies have aimed at achieving constant memory costs during inference through predefined heuristic cache eviction policies [81, 42, 27].\n```\n\n##### *Relevant Chunk: No. 3/41 (Score: 0.76)*\n\n```\nHowever, these static methods often prove suboptimal in various scenarios [66, 2]. Alternatively, sparse patterns can be learned in a data-driven manner. For example, Reformer [39] employs locality-sensitive hashing for token clustering and do attention within a cluster, while Routing Transformers [61], Cluster-Former [74] and Clustered Attention [73] use K-Means clustering on tokens. Besides, Sparse Sinkhorn Attention [68] establishes sparsity by sorting blocks of inputs. Despite achieving sub-quadratic complexity, these methods still remain above linear complexity and face challenges when handling extremely long sequences or failing to offer constant memory cost during inference. A recent approach by Anagnostidis et al. [2] introduces a learnable, irreversible key-value pair pruning for inference-time memory efficiency with the concept of relaxing pruning actions to accumulated gating. However, this method still suffers from quadratic complexity during training, hindering its ability to expedite the training process. In this paper, we present a novel, efficient sparse attention mechanism with learnable patterns, addressing all the aforementioned challenges. ## 3 SparseK Attention\n\n### 3.1 Background\n\nSelf-Attention Given a sequence of vectors $\\boldsymbol{X} \\in \\mathbb{R}^{n \\times d}$ where $n$ is the sequence length and $d$ is the hidden dimension, an attention head first projects $\\boldsymbol{X}$ into query, key and value vectors with $\\boldsymbol{W}_{Q}, \\boldsymbol{W}_{K}, \\boldsymbol{W}_{V} \\in \\mathbb{R}^{d \\times p}$ where $p=\\frac{d}{h}$ and $h$ is the number of attention heads:\n\n$$\n\\boldsymbol{Q}=\\boldsymbol{X} \\boldsymbol{W}_{Q} \\quad \\boldsymbol{K}=\\boldsymbol{X} \\boldsymbol{W}_{K} \\quad \\boldsymbol{V}=\\boldsymbol{X} \\boldsymbol{W}_{V}\n$$\n\nIn the decoder-only architecture [72], a causal attention mask $\\boldsymbol{M}$ guarantees each query $q_{i}$ only attends to positions $\\leq i$. Consequently, the output $\\boldsymbol{O}$ of single-head dot-product attention is defined as\n\n$$\n\\boldsymbol{S}=\\boldsymbol{Q} \\boldsymbol{K}^{\\top} \\quad \\boldsymbol{P}=\\operatorname{SoftMAx}(\\boldsymbol{S}+\\boldsymbol{M}) \\quad \\boldsymbol{O}=\\boldsymbol{P} \\boldsymbol{V}\n$$\n\nThe multi-head self-attention concatenates the outputs of multiple heads (indexed by subscripts) and applies a linear projection with $\\boldsymbol{W}_{O} \\in \\mathbb{R}^{d \\times d}$ :\n\n$$\n\\operatorname{MHA}(\\boldsymbol{X})=\\text { Concatenate }\\left(\\boldsymbol{O}_{1}, \\boldsymbol{O}_{2}, \\ldots, \\boldsymbol{O}_{h}\\right) \\boldsymbol{W}_{O}\n$$\n\nThe quadratic complexity of self-attention is contributed by the quadratically sized attention weight $\\boldsymbol{S}$. Inspired by Ainslie et al. [1], we propose to select a constant number of key-value pairs for each query in an irreversible way (defined formally in the following subsections 3.2 and 3.3), leading to linear training complexity and a constant inference-time memory cost. For simplicity, here we omit the RoPE position embedding [64] and focus on single-head attention to illustrate our methodology. The multi-head case is briefly discussed in Appendix C.7. SparseMax operator There are many popular technical choices that relax ArgMAX operation, such as SoftMax and SparseMax [46]. Especially, SparseMax uses the Euclidean projection onto the probabilistic simplex and tends to yield sparse solutions:\n\n$$\n\\operatorname{SPARSEMAX}(\\boldsymbol{z}):=\\underset{\\boldsymbol{p} \\in \\triangle^{m-1}}{\\arg \\min }\\|\\boldsymbol{p}-\\boldsymbol{z}\\|^{2}\n$$\n\nwhere $\\triangle^{m-1}=\\left\\{\\boldsymbol{p} \\in \\mathbb{R}^{m} \\mid \\mathbf{1}^{\\top} \\boldsymbol{p}=1, \\boldsymbol{p} \\geq 0\\right\\}$. Building on this, we introduce SPARSEK, an extension of SparseMAX for the case where $k=\\mathbf{1}^{\\top} \\boldsymbol{p} \\geq 1$\n\n### 3.2 Learnable Key-Value Pair Selection\n\nKey-value pair selection We use $\\Delta \\in\\{0,1\\}^{k \\times m}$ to represent the selection of $k$ key-value pairs out of $m$ entries, where $\\Delta(i, j)=1$ indicates that the $j$-th key-value pair is the $i$-th selected entry ( i.e., the $j$-th key-value pair is positioned in the $i$-th slot after sorting), and $\\Delta(i, j)=0$ otherwise.\n```\n\n#### 3. Big Bird: Transformers for Longer Sequences (Avg. Score: 0.49)\n\n*M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 1631  (*Influential: 238*)\n\n**TL;DR:** It is shown that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.\n\n**Abstract:** Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n\n##### *Relevant Chunk: No. 5/94 (Score: 0.49)*\n\n```\nWe provide implementation details in App. D. ## 3 Theoretical Results about Sparse Attention Mechanism\n\nIn this section, we will show that that sparse attention mechanisms are as powerful and expressive as full-attention mechanisms in two respects. First, we show that when sparse attention mechanisms are used in a standalone encoder (such as BERT), they are Universal Approximators of sequence to sequence functions in the style of Yun et al.\n```\n\n#### 4. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.43)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 2/16 (Score: 0.43)*\n\n```\n## 2. Related Works\n\n### 2.1. Sparse Attention\n\nIn recent times, the computational burden of long contexts has been effectively alleviated with the introduction of various sparsified attention mechanisms. (Zaheer et al., 2021) integrating random attention, windowed attention, and global attention achieved commendable results. (Zhao et al., 2019), (Gupta et al., 2021) posits that the plethora of irrelevant information within the attention mechanism can be distracting for the model, and thus zeroes out the less significant positions within the attention matrix to focus the model's attention. Subsequently, (Zhang et al., 2023) proposed a method to filter tokens of importance by summing up attention scores. Going a step further, (Ribar et al., 2023) estimated attention scores in the embedding dimension using the top-r values to then select the top- k largest KV pairs. The recently prominent Mistral architecture(Jiang et al., 2023a), employs windowed attention akin to the receptive fields of CNNs(O'Shea \\& Nash, 2015), theoretically enabling the effortless handling of text sequences up to the length of $32 \\times 4096$. However, none of these works can achieve lossless compression of context.\n```\n\n\n\n---\n## Found 2 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: memory efficient backpropagation kv pair selection\n\nConsidering refining your search by improving the query keywords input.\n\n### 2 related papers from Semantic Scholar\n\n#### 1. Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference\n\n*From Search Query: memory efficient backpropagation kv pair selection*\n\n*Harry Dong, Xinyu Yang, Zhenyu (Allen) Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen*\n\n**TL;DR:** LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps, shows merit on a variety of tasks where it can help reduce the performance gap from caching everything.\n\n**Abstract:** Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 25  (*Influential: 4*)\n\n#### 2. Non-adversarial training of Neural SDEs with signature kernel scores\n\n*From Search Query: memory efficient backpropagation kv pair selection*\n\n*Zacharia Issa, Blanka Horvath, M. Lemercier, C. Salvi*\n\n**TL;DR:** This paper introduces a novel class of scoring rules on pathspace based on signature kernels and uses them as objective for training Neural SDEs non-adversarially and provides existence and uniqueness guarantees for the minimiser.\n\n**Abstract:** Neural SDEs are continuous-time generative models for sequential data. State-of-the-art performance for irregular time series generation has been previously obtained by training these models adversarially as GANs. However, as typical for GAN architectures, training is notoriously unstable, often suffers from mode collapse, and requires specialised techniques such as weight clipping and gradient penalty to mitigate these issues. In this paper, we introduce a novel class of scoring rules on pathspace based on signature kernels and use them as objective for training Neural SDEs non-adversarially. By showing strict properness of such kernel scores and consistency of the corresponding estimators, we provide existence and uniqueness guarantees for the minimiser. With this formulation, evaluating the generator-discriminator pair amounts to solving a system of linear path-dependent PDEs which allows for memory-efficient adjoint-based backpropagation. Moreover, because the proposed kernel scores are well-defined for paths with values in infinite dimensional spaces of functions, our framework can be easily extended to generate spatiotemporal data. Our procedure permits conditioning on a rich variety of market conditions and significantly outperforms alternative ways of training Neural SDEs on a variety of tasks including the simulation of rough volatility models, the conditional probabilistic forecasts of real-world forex pairs where the conditioning variable is an observed past trajectory, and the mesh-free generation of limit order book dynamics.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly in the areas of memory-efficient backpropagation, KV pair selection and management, and integration with sparse attention and normalization layers, here are some key findings and techniques:\n\n## Memory-Efficient Backpropagation Techniques\n\n1. **CompAct: Compressed Activations**\n   - This method involves compressing activations, gradients, and optimizer states of linear layers to reduce peak memory utilization during training. By using low-rank, compressed activations stored for the backward pass, CompAct significantly reduces the required memory without severe performance impact. This technique is particularly effective for larger models, achieving memory savings of 25-30% for pretraining and 50% for fine-tuning.\n\n2. **Activation Checkpointing (CKPT)**\n   - Also known as gradient checkpointing, this method reduces the memory footprint by saving activations only at specific layers (checkpoints) and recomputing the forward pass between checkpoints during backpropagation. While this incurs a 20-30% compute cost overhead, it significantly reduces the memory footprint of the entire computation graph.\n\n## KV Pair Selection and Management Strategies\n\n1. **KV-Cache Eviction via Attention-Gate**\n   - This approach uses a lightweight, trainable module called the Attention-Gate (AG) to determine which tokens in the KV-Cache should be retained or discarded. AG generates binary flags for each token, guiding both the computation of attention scores and the management of the KV-Cache. This method is adaptive and can be trained to optimize KV pair selection, reducing both computational and memory costs.\n\n2. **SPARSEK Attention**\n   - This method integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query. This selective KV pair mechanism reduces complexity to linear time and maintains a constant memory footprint during generation[Key References & Excerpts].\n\n## Integration Methods for Sparse Attention and Normalization Layers\n\n1. **Hybrid Attention Mechanisms**\n   - Combining different attention types, such as sparse attention mechanisms (e.g., SPARSEK Attention, BigBird) with full attention capabilities, can maintain low computation and memory complexity while preserving the properties of full attention models. For example, Combiner treats self-attention as a conditional expectation over embeddings, allowing for full attention capability in each attention head while keeping complexity low[Key References & Excerpts].\n\n2. **Normalization Integration**\n   - Advanced normalization strategies, such as complex exponential moving averages and timestep normalization, can be integrated with sparse attention mechanisms to improve stability and performance. For instance, timestep-aware normalization can be beneficial when combined with sparse attention to handle long-range dependencies effectively[Key Insights].\n\n3. **Stability Analysis**\n   - Approaches like Visual Attention from Recurrent Sparse Reconstruction (VARS) formulate attention as a recurrent attractor network, which can improve stability and convergence. Integrating such stability-enhancing mechanisms with sparse attention and normalization layers can further optimize the model's performance[Key References & Excerpts].\n\nBy incorporating these techniques, researchers can develop more efficient, scalable, and robust autoregressive language models that balance memory efficiency, computational complexity, and performance.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for improving the HieraNorm-AttnGPT design, particularly in three key areas:\n\n1. Sparse Attention Mechanisms:\n- SPARSEK Attention offers linear complexity and constant memory footprint\n- Routing Transformers demonstrate effective content-based sparse attention\n- BigBird shows theoretical guarantees for sparse attention mechanisms\n\n2. Memory Management:\n- Efficient KV pair selection and storage techniques\n- Incremental evaluation methods for reduced complexity\n- Memory-efficient backpropagation strategies\n\n3. Advanced Normalization:\n- Complex exponential moving averages for improved state handling\n- Timestep normalization for better stability\n- Integration with sparse attention mechanisms\n\nKey References & Excerpts:\n\n1. From \"Sparser is Faster and Less is More\":\n\"SPARSEK Attention offers linear time complexity and constant memory footprint during generation... integrates a scoring network and a differentiable top-k mask operator to select a constant number of KV pairs for each query.\"\n\nThis suggests potential for incorporating selective KV pair mechanisms.\n\n2. From \"Big Bird\":\n\"BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.\"\n\nThis provides theoretical backing for sparse attention approaches.\n\n3. From \"Combiner\":\n\"Combiner provides full attention capability in each attention head while maintaining low computation and memory complexity... treating self-attention mechanism as a conditional expectation over embeddings.\"\n\nThis offers insights into efficient attention computation.\n\n4. From \"Visual Attention from Recurrent Sparse Reconstruction\":\n\"VARS adopts an attractor network with recurrent connections that converges toward a stable pattern over time... formulating attention as a recurrent attractor network.\"\n\nThis suggests potential improvements in stability and convergence.\n\nKey Insights:\n\n1. Attention Mechanisms:\n- Selective KV pair attention can significantly reduce complexity\n- Theoretical guarantees possible with careful sparse attention design\n- Hybrid approaches combining different attention types show promise\n\n2. Memory Optimization:\n- Incremental evaluation crucial for efficiency\n- Careful KV pair management can reduce memory requirements\n- Memory-efficient backpropagation important for training\n\n3. Normalization Strategies:\n- Complex state handling improves stability\n- Timestep-aware normalization beneficial\n- Integration with attention mechanisms critical\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific implementations of KV pair selection mechanisms\n2. Details of memory-efficient backpropagation techniques\n3. Integration methods for sparse attention and normalization\n4. Stability analysis of different attention-normalization combinations\n\nReferences:\n\n1. Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv.org.\n\n2. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Onta\u00f1\u00f3n, S., ... & Ahmed, A. (2020). Big Bird: Transformers for Longer Sequences. Neural Information Processing Systems.\n\n3. Ren, H., Dai, H., Dai, Z., Yang, M., Leskovec, J., Schuurmans, D., & Dai, B. (2021). Combiner: Full Attention Transformer with Sparse Computation Cost. Neural Information Processing Systems.\n\n4. Shi, B., Song, Y., Joshi, N., Darrell, T., & Wang, X. (2022). Visual Attention Emerges from Recurrent Sparse Reconstruction. International Conference on Machine Learning.\n\n5. Bricken, T., & Pehlevan, C. (2021). Attention Approximates Sparse Distributed Memory. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "memory efficient normalization hardware optimization",
            "detail": "Find technical details about:\n1. Memory-efficient normalization techniques and their implementation\n2. Hardware optimization strategies for attention mechanisms\n3. Integration methods between sparse computation and normalization",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about:\n1. Memory-efficient normalization techniques and their implementation\n2. Hardware optimization strategies for attention mechanisms\n3. Integration methods between sparse computation and normalization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.44)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.44)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n#### 2. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.31)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.54)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n##### *Relevant Chunk: No. 2/12 (Score: 0.07)*\n\n```\n(2019, equation 4). Unfortunately their paper went in a different direction and did not discuss the memory complexity implications and other innovations we present in the remainder of this paper. For more details see Section 6\nThis can be computed with constant memory: The memory overhead of this algorithm consists of a vector $v^{*} \\in \\mathbb{R}^{d}$ and a scalar $s^{*} \\in \\mathbb{R}$, both initialized with 0 . Given the query $q$, keys $k_{1}, \\ldots, k_{n}$ and values $v_{1}, \\ldots, v_{n}$, we process the keys and values in sequence. Given a key value pair $k_{i}$, $v_{i}$, we compute $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$ and update $v^{*} \\leftarrow v^{*}+v_{i} e^{s_{i}}$ and $s^{*} \\leftarrow s^{*}+e^{s_{i}}$. After processing all keys and values, we divide $\\frac{v^{*}}{s^{*}}$ to get the final result. The analysis of space complexity assumes that inputs are given in a particular order: we first read the query, and then a list of pairs of keys and values. If the inputs are provided in a different order, we have to additionally store an index into the sequence, requiring $O(\\log n)$ memory instead. To extend this algorithm to self-attention, we compute the results to all queries sequentially. This requires just one additional index into the list of queries, giving rise to the $O(\\log n)$ memory complexity. Note that the operation produces outputs that are linear in the size of the number of queries, i.e., $O(n)$, which is not counted towards the space complexity. ## 3 Numerical Stability\n\nThe formulation of standard attention that we presented in the Introduction, as well as our memory-efficient algorithm, are not numerically stable when using floating point arithmetic, because the softmax exponentiates the scores. For scores $\\geq 89$ the exponentiation results in inf (for bfloat16 and float32), which will be carried through to the final result of the attention operation. In practice, the softmax is implemented by subtracting the maximum score from all scores. This does not change the result of the softmax, but avoids this numerical problem. Our incremental computation of the sum of exponentiated scores (and the values times the scores) does not immediately allow for the same trick, as the maximum may depend on the last score in the sequence. But the subtraction cannot be delayed either, since the scores must be exponentiated before they can be added to the cumulative sum. To resolve this problem, we introduce an additional scalar, which keeps track of the maximum score that the incremental algorithm has seen so far, and we renormalize the sums of exponentiated values as needed: We initialize the vector $v^{*} \\in \\mathbb{R}^{d}$ and scalar $s^{*} \\in \\mathbb{R}$ with 0 , and $m^{*}$ with -inf. As before, given a key value pair $k_{i}, v_{i}$, we compute $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$, but then the algorithm differs slightly from Section 2. We first compute $m_{i}=\\max \\left(m^{*}, s_{i}\\right)$ and update $v^{*} \\leftarrow v^{*} e^{m^{*}-m_{i}}+v_{i} e^{s_{i}-m_{i}}$ and $s^{*} \\leftarrow s^{*} e^{m^{*}-m_{i}}+e^{s_{i}-m_{i}}$ and $m^{*} \\leftarrow m_{i}$. After processing all keys and queries, we divide $\\frac{v^{*}}{s^{*}}$ to get the final result. ## 4 An Implementation For TPUs\n\nIn this section, we provide a version of the algorithm above that exploits the massive parallelism of modern hardware, such as GPUs or TPUs. The naive algorithm above is is not trivial to parallelize for a compiler, as the incremental sum introduces a dependency across all keys and values. We present the entire implementation, including the support for multiple attention heads and memory-efficient differentiation in Figure 1 The implementation does not optimize strictly for memory efficiency, but instead aims to strike a balance between simplicity, computational efficiency, and memory requirements. ```\nimport functools, jax, math\nfrom jax import numpy as jnp\ndef _query_chunk_attention(query, key, value, precision, key_chunk_size=4096):\n    \"\"\"Multi-head dot product attention with a limited number of queries.\"\"\"\n    num_kv, num_heads, k_features = key.shape\n    v_features = value.shape[-1]\n    key_chunk_size = min(key_chunk_size, num_kv)\n    query = query / jnp.sqrt(k_features)\n    @functools.partial(jax.checkpoint, prevent_cse=False)\n    def summarize_chunk(query, key, value):\n        attn_weights = jnp.einsum('qhd,khd->qhk', query, key, precision=precision)\n        max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n        max_score = jax.lax.stop_gradient(max_score)\n        exp_weights = jnp.exp(attn_weights - max_score)\n        exp_values = jnp.einsum('vhf,qhv->qhf', value, exp_weights, precision=precision)\n        return (exp_values, exp_weights.sum(axis=-1),\n            max_score.reshape((query.shape[0], num_heads)))\n    def chunk_scanner(chunk_idx):\n        key_chunk = jax.lax.dynamic_slice(\n            key, (chunk_idx, 0, 0),\n            slice_sizes=(key_chunk_size, num_heads, k_features))\n        value_chunk = jax.lax.dynamic_slice(\n            value, (chunk_idx, 0, 0),\n            slice_sizes=(key_chunk_size, num_heads, v_features))\n        return summarize_chunk(query, key_chunk, value_chunk)\n    chunk_values, chunk_weights, chunk_max = jax.lax.map(\n        chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size))\n    global_max = jnp.max(chunk_max, axis=0, keepdims=True)\n    max_diffs = jnp.exp(chunk_max - global_max)\n    chunk_values *= jnp.expand_dims(max_diffs, axis=-1)\n    chunk_weights *= max_diffs\n    all_values = chunk_values.sum(axis=0)\n    all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)\n    return all_values / all_weights\ndef attention(query, key, value, precision=jax.lax.Precision.HIGHEST,\n                query_chunk_size=1024):\n    \"\"\"Memory-efficient multi-head dot product attention.\"\"\"\n    num_q, num_heads, q_features = query.shape\n    def chunk_scanner(chunk_idx, _):\n        query_chunk = lax.dynamic_slice(\n            query, (chunk_idx, 0, 0),\n            slice_sizes=(min(query_chunk_size, num_q), num_heads, q_features))\n        return (chunk_idx + query_chunk_size,\n                _query_chunk_attention(query_chunk, key, value, precision=precision))\n    _, res = jax.lax.scan(\n        chunk_scanner, init=0, xs=None, length=math.ceil(num_q / query_chunk_size))\n    return res.reshape(num_q, num_heads, value.shape[-1])\n```\n\nFigure 1: Implementation of memory-efficient attention suited for TPUs.\n```\n\n#### 3. Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers (Avg. Score: 0.02)\n\n*Chao Lou, Zixia Jia, Zilong Zheng, Kewei Tu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** SPARSEK Attention is introduced, a novel sparse attention mechanism designed to overcome computational and memory obstacles while maintaining performance and can be seamlessly integrated into pre-trained Large Language Models with minimal fine-tuning.\n\n**Abstract:** Accommodating long sequences efficiently in autoregressive Transformers, especially within an extended context window, poses significant challenges due to the quadratic computational complexity and substantial KV memory requirements inherent in self-attention mechanisms. In this work, we introduce SPARSEK Attention, a novel sparse attention mechanism designed to overcome these computational and memory obstacles while maintaining performance. Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization. As a result, SPARSEK Attention offers linear time complexity and constant memory footprint during generation. Experimental results reveal that SPARSEK Attention outperforms previous sparse attention methods and provides significant speed improvements during both training and inference, particularly in language modeling and downstream tasks. Furthermore, our method can be seamlessly integrated into pre-trained Large Language Models (LLMs) with minimal fine-tuning, offering a practical solution for effectively managing long-range dependencies in diverse applications.\n\n##### *Relevant Chunk: No. 5/41 (Score: 0.02)*\n\n```\nThe idea is that we can compute step $t$ based on the results from step $t-1$ instead of starting from scratch. Algorithm 2 illustrates the algorithm, where highlighted lines are the main difference from Algorithm 1. We introduce two min-heaps (and maintain the sum of elements for each heap) for tracking the search progress of $(u, w)$ and achieving $O(1)$ evaluation of line 9 in Algorithm 2. Note that each insertion into a min-heap costs logarithmic time in the heap size and each $z_{t}$ introduces at most two more possible ( $u, w$ ) pairs (lines 2-6 in Algorithm 2). Therefore, executing Algorithm 2 over $m$ incremental sets (i.e., $m$ steps) costs $O(m \\log m)$ in total. As Peters et al. [54] have noted, the solution $\\boldsymbol{p}^{*}$ tends to contain only a few nonzeros, leading to small $u^{*}$ and $w^{*}$. Therefore, in practice, we can use partial sort on the $k^{\\prime}=O(k)$ largest values instead of full sort in Algorithm 1, thereby achieving a complexity of $O(m \\log k)$. With respect to Algorithm 2, this change is equivalent to restricting the size of the min-heap $\\mathcal{S}$ to an upper bound for achieving the same reduction in complexity. ### 3.4 Extensions\n\nTraining with fixed-size truncation-free cache Our selection method enables training on extremely long documents that need to be segmented into smaller chunks for recurrent processing. Algorithm 3 illustrates the process. Without introducing any additional truncation strategies or parameters, the algorithm maintains a fixed-size cache benefit by recurrent calculations and produces exactly the same results as calculating without chunking, which is guaranteed by the irreversibility of our selection method. To minimize the memory footprint, we stop the gradients of the cache, thereby pruning the computation graph, as in Transformer-XL [17]. With this algorithm, we can extend the training context length to hundreds of thousands of tokens. Combine with other efficient attention mechanism Our SPARSEK attention can be combined with other sparse attention as long as they have irreversible selection patterns. In this work, we integrate SPARSEK attention with sliding window (SW) attention by default, motivated by the well-known experience that sliding windows are simple yet incredibly strong for language modeling [58, 37]. Specifically, given a sliding window size $w$, we replace $\\hat{\\boldsymbol{K}}_{i}, \\hat{\\boldsymbol{V}}_{i}$ in (5) with\n\n$$\n\\hat{\\boldsymbol{K}}_{i}=\\left[\\begin{array}{c}\n\\Delta_{i-w} \\boldsymbol{K} \\\\\n\\boldsymbol{K}_{i-w+1: i}\n\\end{array}\\right] \\quad \\hat{\\boldsymbol{V}}_{i}=\\left[\\begin{array}{c}\n\\Delta_{i-w} \\boldsymbol{V} \\\\\n\\boldsymbol{V}_{i-w+1: i}\n\\end{array}\\right]\n$$\n\nThis combination does not introduce any overhead thanks to our fused Triton kernel. In this combination, SPARSEK attention attention aims at efficiently global (long-range) dependencies modeling, while SW attention is used for modeling local dependencies. Besides, SPARSEK attention can also be combined with linear attention methods, which hypothesize the existence of low-rank structures in attention scores rather than sparsity. From a theoretical perspective, Chen et al. [10] reveal that linear attention and sparse attention capture different attention patterns, and their combination provides a closer approximation to full attention. In this work, we extend their results to SparseK attention and recent attention optimizations [19]. For technical details, please refer to Appendix B.1. Straight-through estimator From TOPK to SpArseK, we employ relaxation techniques to facilitate gradient-based training. Alternatively, the straight-through estimator (ST) [5] can be utilized, i.e., $\\Delta^{\\text {st }}=\\Delta^{\\text {soft }}-\\operatorname{stop} \\_\\operatorname{grad}\\left(\\Delta^{\\text {soft }}\\right)+\\Delta^{\\text {hard }}$, allowing the model to perform true selection. By utilizing the ST method, the model achieves slightly improved efficiency since it bypasses the multiplication of selection scores during the forward pass. Our experimental results indicate that employing ST results in negligible performance degradation. Consequently, this technique shows promise in balancing performance and computational efficiency. ### 3.5 Techniques for Faster and More Stable Training\n\nWe introduce three beneficial modeling tricks discovered in our experiments. We also develop an optimized implementation based on FlashAttention- $2^{2}$ for obtaining practically efficient sparse attention.\n```\n\n#### 4. Sparse Sinkhorn Attention (Avg. Score: 0.02)\n\n*Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, Da-Cheng Juan*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 285  (*Influential: 36*)\n\n**TL;DR:** This work introduces a meta sorting network that learns to generate latent permutations over sequences and is able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module.\n\n**Abstract:** We propose Sparse Sinkhorn Attention, a new efficient and sparse method for learning to attend. Our method is based on differentiable sorting of internal representations. Concretely, we introduce a meta sorting network that learns to generate latent permutations over sequences. Given sorted sequences, we are then able to compute quasi-global attention with only local windows, improving the memory efficiency of the attention module. To this end, we propose new algorithmic innovations such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image generation, document classification and natural language inference, we demonstrate that our memory efficient Sinkhorn Attention method is competitive with vanilla attention and consistently outperforms recently proposed efficient Transformer models such as Sparse Transformers.\n\n##### *Relevant Chunk: No. 23/23 (Score: 0.02)*\n\n```\nIn International conference on machine learning, pp. 2048-2057, 2015. [^0]:    ${ }^{1}$ Google AI. Correspondence to: Yi Tay $<$ yitay@google.com $>$. [^1]:    ${ }^{1}$ As an illustration, when $\\ell=1024$ and $N_{B}=64$, this results in a memory saving factor of 240 times. [^2]:    ${ }^{2}$ That said, Sparse Attention requires highly specialized GPU kernels for efficient computation. This generally makes the approach less appealing, e.g., for portability purposes such as running on TPU pods. [^3]:    ${ }^{3}$ tensor2tensor/models/research/1m_ experiments.py\n\n[^4]:    ${ }^{4}$ To the best of our knowledge, (Shazeer et al., 2018) is the best performing model on per-word perplexity. (Baevski \\& Auli, 2018) and (Dai et al., 2019) report per-token perplexity\n\n\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: memory efficient normalization hardware optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 2. Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*Zirui Liu, Guanchu Wang, Shaochen Zhong, Zhaozhuo Xu, D. Zha, Ruixiang Tang, Zhimeng Jiang, Kaixiong Zhou, V. Chaudhary, Shuai Xu, Xia Hu*\n\n**TL;DR:** This work proposes a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient in a stochastic manner.\n\n**Abstract:** With the rapid growth in model size, fine-tuning the large pre-trained language model has become increasingly difficult due to its extensive memory usage. Previous works usually focus on reducing the number of trainable parameters in the network. While the model parameters do contribute to memory usage, the primary memory bottleneck during training arises from storing feature maps, also known as activations, as they are crucial for gradient calculation. Notably, neural networks are usually trained using stochastic gradient descent. We argue that in stochastic optimization, models can handle noisy gradients as long as the gradient estimator is unbiased with reasonable variance. Following this motivation, we propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient. Our work provides both theoretical and experimental evidence that, in the context of tuning transformers, our proposed estimators exhibit lower variance compared to existing ones. By replacing the linear operation with our approximated one in transformers, we can achieve up to 2.7$\\times$ peak memory reduction with almost no accuracy drop and enables up to $6.4\\times$ larger batch size. Under the same hardware, WTA-CRS enables better down-streaming task performance by applying larger models and/or faster training speed with larger batch sizes.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. LLM in a flash: Efficient Large Language Model Inference with Limited Memory\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*Keivan Alizadeh-Vahid, Iman Mirzadeh, Dmitry Belenko, Karen Khatamifard, Minsik Cho, C. C. D. Mundo, Mohammad Rastegari, Mehrdad Farajtabar*\n\n**TL;DR:** The integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.\n\n**Abstract:** Large language models (LLMs) are central to modern natural language processing, delivering exceptional performance in various tasks. However, their substantial computational and memory requirements present challenges, especially for devices with limited DRAM capacity. This paper tackles the challenge of efficiently running LLMs that exceed the available DRAM capacity by storing the model parameters in flash memory, but bringing them on demand to DRAM. Our method involves constructing an inference cost model that takes into account the characteristics of flash memory, guiding us to optimize in two critical areas: reducing the volume of data transferred from flash and reading data in larger, more contiguous chunks. Within this hardware-informed framework, we introduce two principal techniques. First,\"windowing\"strategically reduces data transfer by reusing previously activated neurons, and second,\"row-column bundling\", tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory. These methods collectively enable running models up to twice the size of the available DRAM, with a 4-5x and 20-25x increase in inference speed compared to naive loading approaches in CPU and GPU, respectively. Our integration of sparsity awareness, context-adaptive loading, and a hardware-oriented design paves the way for effective inference of LLMs on devices with limited memory.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 60  (*Influential: 8*)\n\n#### 4. Memory-Efficient Pipeline-Parallel DNN Training\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*D. Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, M. Zaharia*\n\n**TL;DR:** This work proposes PipeDream-2BW, a system that performs memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining, able to accelerate the training of large language models with up to 2.5 billion parameters by up to 6.9x compared to optimized baselines.\n\n**Abstract:** Many state-of-the-art results in domains such as NLP and computer vision have been obtained by scaling up the number of parameters in existing models. However, the weight parameters and intermediate outputs of these large models often do not fit in the main memory of a single accelerator device; this means that it is necessary to use multiple accelerators to train large models, which is challenging to do in a time-efficient way. In this work, we propose PipeDream-2BW, a system that performs memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining. Our system uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream-2BW automatically partitions the model over the available hardware resources, while being cognizant of constraints such as compute capabilities, memory capacities, and interconnect topologies, and determines when to employ existing memory-savings techniques, such as activation recomputation, that trade off extra computation for lower memory footprint. PipeDream-2BW is able to accelerate the training of large language models with up to 2.5 billion parameters by up to 6.9x compared to optimized baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 169  (*Influential: 34*)\n\n### 3 related papers from Papers with Code\n\n#### 1. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*Xipeng Qiu, Haijun Lv, Qipeng Guo, Hang Yan, Kai Lv*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive parameter size necessitates substantial memory for training, thereby setting a high threshold. While the recently proposed low-memory optimization (LOMO) reduces memory footprint, its optimization technique, akin to stochastic gradient descent, is sensitive to hyper-parameters and exhibits suboptimal convergence, failing to match the performance of the prevailing optimizer for large language models, AdamW. Through empirical analysis of the Adam optimizer, we found that, compared to momentum, the adaptive learning rate is more critical for bridging the gap. Building on this insight, we introduce the low-memory optimization with adaptive learning rate (AdaLomo), which offers an adaptive learning rate for each parameter. To maintain memory efficiency, we employ non-negative matrix factorization for the second-order moment estimation in the optimizer state. Additionally, we suggest the use of a grouped update normalization to stabilize convergence. Our experiments with instruction-tuning and further pre-training demonstrate that AdaLomo achieves results on par with AdamW, while significantly reducing memory requirements, thereby lowering the hardware barrier to training large language models. The code is accessible at https://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16\n\n\n\n#### 2. Semantic Word Clouds with Background Corpus Normalization and t-distributed Stochastic Neighbor Embedding\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*Johanna Gei\u00df, Andreas Spitz, Michael Weiler, Erich Schubert, Michael Gertz*\n\n**Abstract:** Many word clouds provide no semantics to the word placement, but use a random\nlayout optimized solely for aesthetic purposes. We propose a novel approach to\nmodel word significance and word affinity within a document, and in comparison\nto a large background corpus. We demonstrate its usefulness for generating more\nmeaningful word clouds as a visual summary of a given document. We then select\nkeywords based on their significance and construct the word cloud based on the\nderived affinity. Based on a modified t-distributed stochastic neighbor\nembedding (t-SNE), we generate a semantic word placement. For words that\ncooccur significantly, we include edges, and cluster the words according to\ntheir cooccurrence. For this we designed a scalable and memory-efficient\nsketch-based approach usable on commodity hardware to aggregate the required\ncorpus statistics needed for normalization, and for identifying keywords as\nwell as significant cooccurences. We empirically validate our approch using a\nlarge Wikipedia corpus.\n\n**Published:** 2017-08-11\n\n\n\n#### 3. Real-Time Super-Resolution System of 4K-Video Based on Deep Learning\n\n*From Search Query: memory efficient normalization hardware optimization*\n\n*He Li, Yongming Tang, Changjun Song, Chengcheng Wang, Yanpeng Cao*\n\n**Abstract:** Video super-resolution (VSR) technology excels in reconstructing low-quality video, avoiding unpleasant blur effect caused by interpolation-based algorithms. However, vast computation complexity and memory occupation hampers the edge of deplorability and the runtime inference in real-life applications, especially for large-scale VSR task. This paper explores the possibility of real-time VSR system and designs an efficient and generic VSR network, termed EGVSR. The proposed EGVSR is based on spatio-temporal adversarial learning for temporal coherence. In order to pursue faster VSR processing ability up to 4K resolution, this paper tries to choose lightweight network structure and efficient upsampling method to reduce the computation required by EGVSR network under the guarantee of high visual quality. Besides, we implement the batch normalization computation fusion, convolutional acceleration algorithm and other neural network acceleration techniques on the actual hardware platform to optimize the inference process of EGVSR network. Finally, our EGVSR achieves the real-time processing capacity of 4K@29.61FPS. Compared with TecoGAN, the most advanced VSR network at present, we achieve 85.04% reduction of computation density and 7.92x performance speedups. In terms of visual quality, the proposed EGVSR tops the list of most metrics (such as LPIPS, tOF, tLP, etc.) on the public test dataset Vid4 and surpasses other state-of-the-art methods in overall performance score. The source code of this project can be found on https://github.com/Thmen/EGVSR.\n\n**Published:** 2021-07-12\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design with a focus on memory efficiency, normalization techniques, and hardware optimization, here are some key points and techniques from the provided sources and additional insights:\n\n## Memory-Efficient Normalization Techniques\n\n### Dynamic Normalization\nMemory-efficient bidirectional transformers often employ dynamic normalization techniques to stabilize the training process. For instance, normalization techniques proposed by Gilmer et al. (2023) involve normalizing queries and keys, which have proven effective in stabilizing the training process of large models. This approach helps in maintaining the stability of the model during scaling, which is crucial for efficient training.\n\n### Omission of Biases\nSome models, like ViT-22B, eliminate biases in the QKV projections and LayerNorms. While this adjustment slightly increases resource utilization, it optimizes the model's efficiency by reducing the number of parameters that need to be stored and computed.\n\n## Hardware Optimization Strategies for Attention Mechanisms\n\n### IO-Aware Computation\nFlashAttention's IO-aware computation strategy is highly relevant. This method uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, significantly reducing memory bandwidth requirements and optimizing memory access patterns.\n\n### Sharding Techniques\nSharding model parameters and activations across multiple compute devices is crucial for managing the large memory requirements of transformer models. Techniques like Fully Sharded Data Parallel (FSDP) extend this concept by sharding model parameters, optimizer states, and gradients, eliminating redundancy and reducing the memory footprint.\n\n### Memory Offloading\nStrategies like ZeRO-Offload move optimizer states and gradients to CPU memory when not actively in use, freeing up GPU memory for other operations. This approach helps in managing the memory constraints during training large language models.\n\n## Integration Methods Between Sparse Computation and Normalization\n\n### Sparse Attention with Dynamic Normalization\nIntegrating sparse attention mechanisms, such as SPARSEK, with dynamic normalization techniques can be highly effective. SPARSEK selects a constant number of KV pairs for each query, enabling gradient-based optimization while maintaining performance. Combining this with dynamic normalization can help in stabilizing the training process and reducing computational demands.\n\n### IO-Aware Implementation Strategies\nIO-aware computation can be integrated with hierarchical structures by optimizing memory access patterns. For example, using tiling strategies to reduce memory reads/writes can be combined with sparse attention mechanisms to ensure efficient state management and gradient flow in sparse architectures.\n\n### State Compression Methods\nTechniques like Lossless Compressed Memory Attention (LoMA) enable lossless compression of the KV cache, reducing memory and computational demands during autoregressive generation. Integrating such methods with sparse attention and dynamic normalization can further optimize the model's efficiency (though not directly mentioned in the provided sources, it is a relevant concept from the key insights).\n\n## Additional Insights\n\n### Parameter-Efficient Fine-Tuning (PEFT)\nTechniques like Low-Rank Adaptation (LoRA) and GaLore can be used to reduce the memory footprint during fine-tuning. These methods approximate the optimizer states or model parameters, allowing for full-parameter learning while being more memory-efficient. This is particularly useful when fine-tuning large language models on limited hardware resources.\n\n### Automated Optimization and Hardware Design\nAutomating optimization algorithms and designing efficient hardware architectures can significantly enhance performance. For example, using flexible internal memory, programmable data control, and memory-allocation algorithms can maximize the performance of DNN accelerators, which can be applied to optimize the hardware for autoregressive language models.\n\nBy integrating these techniques, researchers can develop more memory-efficient, hardware-optimized, and scalable autoregressive language models.\n",
            "analysis": "Summary of Analysis:\n\nAfter analyzing the search results, I've identified several promising directions for improving the HieraNorm-AttnGPT design, particularly in memory efficiency and attention mechanisms:\n\n1. Memory-Efficient Computation:\n- FlashAttention's IO-aware computation strategy\n- Memory-efficient backpropagation techniques\n- KV cache optimization methods\n\n2. Sparse Attention Mechanisms:\n- SPARSEK's selective KV pair approach\n- Efficient integration with normalization layers\n- Linear complexity implementations\n\n3. Integration Opportunities:\n- Combining sparse attention with dynamic normalization\n- IO-aware implementation strategies\n- Memory-efficient state management\n\nKey References & Excerpts:\n\n1. From FlashAttention:\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.\"\n\nThis suggests potential for incorporating IO-aware computation in our design.\n\n2. From SPARSEK:\n\"Our approach integrates a scoring network and a differentiable top-k mask operator, SPARSEK, to select a constant number of KV pairs for each query, thereby enabling gradient-based optimization.\"\n\nThis offers insights into efficient KV pair selection.\n\n3. From LoMA:\n\"Lossless Compressed Memory Attention (LoMA) enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\"\n\nThis provides ideas for memory optimization.\n\nKey Insights:\n\n1. Memory Efficiency:\n- IO-aware computation can significantly reduce memory bandwidth requirements\n- Tiling strategies can optimize memory access patterns\n- Careful state management crucial for efficiency\n\n2. Attention Mechanisms:\n- Selective KV pair attention can maintain performance while reducing complexity\n- Differentiable top-k operations enable efficient training\n- Integration with normalization layers needs careful consideration\n\n3. Implementation Strategies:\n- Hardware-aware design crucial for real performance gains\n- Memory access patterns significantly impact efficiency\n- Balance needed between complexity and expressiveness\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific implementations of:\n   - Memory-efficient normalization techniques\n   - Hardware-optimized attention mechanisms\n   - State compression methods\n\n2. Integration methods for:\n   - Sparse attention with dynamic normalization\n   - IO-aware computation with hierarchical structures\n   - Efficient gradient flow in sparse architectures\n\nReferences:\n\n1. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n2. Lou, C., Jia, Z., Zheng, Z., & Tu, K. (2024). Sparser is Faster and Less is More: Efficient Sparse Attention for Long-Range Transformers. arXiv.org.\n\n3. Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv.org.\n\n4. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Onta\u00f1\u00f3n, S., ... & Ahmed, A. (2020). Big Bird: Transformers for Longer Sequences. Neural Information Processing Systems.\n\n5. Dong, H., Yang, X., Zhang, Z., Wang, Z., Chi, Y., & Chen, B. (2024). Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "advanced normalization techniques memory efficient gradients",
            "detail": "Find technical details about:\n1. Advanced normalization techniques for improved stability and efficiency\n2. Memory-efficient gradient computation and backpropagation methods\n3. Integration strategies between normalization and attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about:\n1. Advanced normalization techniques for improved stability and efficiency\n2. Memory-efficient gradient computation and backpropagation methods\n3. Integration strategies between normalization and attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization (Avg. Score: 0.13)\n\n*Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A novel method named PRepBN is proposed to progressively replace LayerNorm with re-parameterized BatchNorm in training and a simplified linear attention (SLA) module is proposed that is simple yet effective to achieve strong performance.\n\n**Abstract:** Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency.Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.\n\n##### *Relevant Chunk: No. 2/25 (Score: 0.13)*\n\n```\nHowever, the high computational cost makes it quite challenging to deploy on resourceconstraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6 \\%$ top-1 accuracy on ImageNet-1K with 16.2 ms latency, which is 2.4 ms less than that of Flatten-Swin with $0.1 \\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency. Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/. ## 1. Introduction\n\nIntroduced initially for tasks in natural language processing (Vaswani et al., 2017), transformer architecture has\n\n[^0]rapidly emerged as a preeminent model in the landscape of language models. Its influence has significantly expanded with the introduction of Vision Transformer (ViT) (Dosovitskiy et al., 2020), illustrating the efficacy and versatility of transformer-based architectures. These architectures have demonstrated their capability to achieve competitive performance benchmarks in comparison to convolutional neural networks (CNNs) across diverse vision tasks (Han et al., 2022; Wang et al., 2022; Zheng et al., 2023; Tang et al., 2023a; Carion et al., 2020; Xu et al., 2023). Due to its powerful performance, transformer has become the mainstream architecture in deep learning. However, the computational demands of transformer architecture pose a significant challenge, which is predominantly due to the quadratic computational complexity of its attention mechanism and the necessity for online statistic computation of LayerNorm component. Numerous efforts have been directed towards enhancing the efficiency of transformer architecture (Tang et al., 2024; Wu et al., 2023; Tang et al., 2023b). Several approaches have sought to mitigate computational complexity by limiting the scope of token interactions within self-attention mechanisms, such as downsampling the key and value matrices (Wang et al., 2021), implementing sparse global attention patterns (Child et al., 2019), and computing selfattention within smaller windows (Tu et al., 2022; Liu et al., 2021; Dong et al., 2022). Meanwhile, linear attention emerges as an alternative strategy to enhance computational efficiency by breaking down the attention mechanism into linear computational cost (Katharopoulos et al., 2020; Cai et al., 2022; Han et al., 2023; You et al., 2023), yet it is still a challenging task to obtain a good balance between efficiency and accuracy. Moreover, there are some explorations into substituting LayerNorm (LN) with BatchNorm (BN) within transformers, motivated by the additional computational overhead LayerNorm incurs during inference. Yang et al. (2022) propose to add a BatchNorm layer in-between the two linear layers in the feed forward network to stabilize the training. However, there still exists a performance gap between the LayerNorm-based and BatchNorm-based transformers. In this paper, we focus on obtaining efficient transformer\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_d98ef7f3d09ffeb83a3fg-02.jpg?height=508&width=770&top_left_y=234&top_left_x=209)\n\nFigure 1. Comparisons of different methods on ImageNet. architectures by digging deep into the computational inefficient modules, i.e., normalization layers and attention modules. We first explore to replace LayerNorm with BatchNorm to accelerate inference for transformer. BatchNorm leads to lower inference latency but may cause training collapse and inferior performance, while LayerNorm could stabilize the training yet has extra computational cost during inference. To this end, we first propose a progressive strategy to gradually replace LayerNorm with BatchNorm by using a hyper-parameter to control the proportion of both normalization layers. Initially the transformer architecture is dominated by the LayerNorm and gradually transits to pure BatchNorm at the end of training. This strategy effectively mitigates the risk of training collapse and also eliminating the need for calculating statistics during inference. In addition to the progressive strategy, we also propose a novel re-parameterization formula for BatchNorm (RepBN), to enhance training stability and overall performance. Furthermore, the computational cost of attention is critical for efficient transformer and prior methods struggle to obtain good balance of efficiency and accuracy. To this end, we propose a simplified linear attention (SLA) module which utilizes ReLU as the kernel function and incorporate a depthwise convolution to perform local feature enhancement. The proposed attention mechanism is more efficient than prior linear attention but still attains comparable performance. We extensively evaluate our proposed method for various architectures on various benchmarks. Our progressive reparameterized BatchNorm shows strong performance for image classification and object detection tasks, obtaining similar accuracy with lower inference latency. Moreover, coupled with the progressive RepBN and simplified linear attention module, our SLAB transformer achieves competitive accuracy compared to Flatten transformer with improved computational efficiency. For example, SLAB-SwinS achieves $83.6 \\%$ Top-1 accuracy on ImageNet-1K with 16.2 ms latency, which is 2.4 ms less than that of FlattenSwin-S with $0.1 \\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower inference latency.\n```\n\n#### 2. Normalized Attention Without Probability Cage (Avg. Score: 0.06)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 17/28 (Score: 0.06)*\n\n```\nIEEE, 2011. [21] Toan Q Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of self-attention. arXiv preprint arXiv:1910.05895, 2019. [22] Emilio Parisotto, H Francis Song, Jack W Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant M Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing transformers for reinforcement learning.\n```\n\n#### 3. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.02)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.02)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 4. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.02)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.02)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 5. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.02)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 2/12 (Score: 0.02)*\n\n```\n(2019, equation 4). Unfortunately their paper went in a different direction and did not discuss the memory complexity implications and other innovations we present in the remainder of this paper. For more details see Section 6\nThis can be computed with constant memory: The memory overhead of this algorithm consists of a vector $v^{*} \\in \\mathbb{R}^{d}$ and a scalar $s^{*} \\in \\mathbb{R}$, both initialized with 0 . Given the query $q$, keys $k_{1}, \\ldots, k_{n}$ and values $v_{1}, \\ldots, v_{n}$, we process the keys and values in sequence. Given a key value pair $k_{i}$, $v_{i}$, we compute $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$ and update $v^{*} \\leftarrow v^{*}+v_{i} e^{s_{i}}$ and $s^{*} \\leftarrow s^{*}+e^{s_{i}}$. After processing all keys and values, we divide $\\frac{v^{*}}{s^{*}}$ to get the final result. The analysis of space complexity assumes that inputs are given in a particular order: we first read the query, and then a list of pairs of keys and values. If the inputs are provided in a different order, we have to additionally store an index into the sequence, requiring $O(\\log n)$ memory instead. To extend this algorithm to self-attention, we compute the results to all queries sequentially. This requires just one additional index into the list of queries, giving rise to the $O(\\log n)$ memory complexity. Note that the operation produces outputs that are linear in the size of the number of queries, i.e., $O(n)$, which is not counted towards the space complexity. ## 3 Numerical Stability\n\nThe formulation of standard attention that we presented in the Introduction, as well as our memory-efficient algorithm, are not numerically stable when using floating point arithmetic, because the softmax exponentiates the scores. For scores $\\geq 89$ the exponentiation results in inf (for bfloat16 and float32), which will be carried through to the final result of the attention operation. In practice, the softmax is implemented by subtracting the maximum score from all scores. This does not change the result of the softmax, but avoids this numerical problem. Our incremental computation of the sum of exponentiated scores (and the values times the scores) does not immediately allow for the same trick, as the maximum may depend on the last score in the sequence. But the subtraction cannot be delayed either, since the scores must be exponentiated before they can be added to the cumulative sum. To resolve this problem, we introduce an additional scalar, which keeps track of the maximum score that the incremental algorithm has seen so far, and we renormalize the sums of exponentiated values as needed: We initialize the vector $v^{*} \\in \\mathbb{R}^{d}$ and scalar $s^{*} \\in \\mathbb{R}$ with 0 , and $m^{*}$ with -inf. As before, given a key value pair $k_{i}, v_{i}$, we compute $s_{i}=\\operatorname{dot}\\left(q, k_{i}\\right)$, but then the algorithm differs slightly from Section 2. We first compute $m_{i}=\\max \\left(m^{*}, s_{i}\\right)$ and update $v^{*} \\leftarrow v^{*} e^{m^{*}-m_{i}}+v_{i} e^{s_{i}-m_{i}}$ and $s^{*} \\leftarrow s^{*} e^{m^{*}-m_{i}}+e^{s_{i}-m_{i}}$ and $m^{*} \\leftarrow m_{i}$. After processing all keys and queries, we divide $\\frac{v^{*}}{s^{*}}$ to get the final result. ## 4 An Implementation For TPUs\n\nIn this section, we provide a version of the algorithm above that exploits the massive parallelism of modern hardware, such as GPUs or TPUs. The naive algorithm above is is not trivial to parallelize for a compiler, as the incremental sum introduces a dependency across all keys and values. We present the entire implementation, including the support for multiple attention heads and memory-efficient differentiation in Figure 1 The implementation does not optimize strictly for memory efficiency, but instead aims to strike a balance between simplicity, computational efficiency, and memory requirements. ```\nimport functools, jax, math\nfrom jax import numpy as jnp\ndef _query_chunk_attention(query, key, value, precision, key_chunk_size=4096):\n    \"\"\"Multi-head dot product attention with a limited number of queries.\"\"\"\n    num_kv, num_heads, k_features = key.shape\n    v_features = value.shape[-1]\n    key_chunk_size = min(key_chunk_size, num_kv)\n    query = query / jnp.sqrt(k_features)\n    @functools.partial(jax.checkpoint, prevent_cse=False)\n    def summarize_chunk(query, key, value):\n        attn_weights = jnp.einsum('qhd,khd->qhk', query, key, precision=precision)\n        max_score = jnp.max(attn_weights, axis=-1, keepdims=True)\n        max_score = jax.lax.stop_gradient(max_score)\n        exp_weights = jnp.exp(attn_weights - max_score)\n        exp_values = jnp.einsum('vhf,qhv->qhf', value, exp_weights, precision=precision)\n        return (exp_values, exp_weights.sum(axis=-1),\n            max_score.reshape((query.shape[0], num_heads)))\n    def chunk_scanner(chunk_idx):\n        key_chunk = jax.lax.dynamic_slice(\n            key, (chunk_idx, 0, 0),\n            slice_sizes=(key_chunk_size, num_heads, k_features))\n        value_chunk = jax.lax.dynamic_slice(\n            value, (chunk_idx, 0, 0),\n            slice_sizes=(key_chunk_size, num_heads, v_features))\n        return summarize_chunk(query, key_chunk, value_chunk)\n    chunk_values, chunk_weights, chunk_max = jax.lax.map(\n        chunk_scanner, xs=jnp.arange(0, num_kv, key_chunk_size))\n    global_max = jnp.max(chunk_max, axis=0, keepdims=True)\n    max_diffs = jnp.exp(chunk_max - global_max)\n    chunk_values *= jnp.expand_dims(max_diffs, axis=-1)\n    chunk_weights *= max_diffs\n    all_values = chunk_values.sum(axis=0)\n    all_weights = jnp.expand_dims(chunk_weights, -1).sum(axis=0)\n    return all_values / all_weights\ndef attention(query, key, value, precision=jax.lax.Precision.HIGHEST,\n                query_chunk_size=1024):\n    \"\"\"Memory-efficient multi-head dot product attention.\"\"\"\n    num_q, num_heads, q_features = query.shape\n    def chunk_scanner(chunk_idx, _):\n        query_chunk = lax.dynamic_slice(\n            query, (chunk_idx, 0, 0),\n            slice_sizes=(min(query_chunk_size, num_q), num_heads, q_features))\n        return (chunk_idx + query_chunk_size,\n                _query_chunk_attention(query_chunk, key, value, precision=precision))\n    _, res = jax.lax.scan(\n        chunk_scanner, init=0, xs=None, length=math.ceil(num_q / query_chunk_size))\n    return res.reshape(num_q, num_heads, value.shape[-1])\n```\n\nFigure 1: Implementation of memory-efficient attention suited for TPUs.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: advanced normalization techniques memory efficient gradients\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Fast and Memory Efficient Differentially Private-SGD via JL Projections\n\n*From Search Query: advanced normalization techniques memory efficient gradients*\n\n*Zhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Y. Lee, Judy Hanwen Shen, U. Tantipongpipat*\n\n**TL;DR:** This paper proposes an algorithmic solution which works for any network in a black-box manner and trains a Recurrent Neural Network to achieve good privacy-vs-accuracy tradeoff, while being significantly faster than DP-SGD and with a similar memory footprint as non-private SGD.\n\n**Abstract:** Differentially Private-SGD (DP-SGD) of Abadi et al. (2016) and its variations are the only known algorithms for private training of large scale neural networks. This algorithm requires computation of per-sample gradients norms which is extremely slow and memory intensive in practice. In this paper, we present a new framework to design differentially private optimizers called DP-SGD-JL and DP-Adam-JL. Our approach uses Johnson-Lindenstrauss (JL) projections to quickly approximate the per-sample gradient norms without exactly computing them, thus making the training time and memory requirements of our optimizers closer to that of their non-DP versions. Unlike previous attempts to make DP-SGD faster which work only on a subset of network architectures or use compiler techniques, we propose an algorithmic solution which works for any network in a black-box manner which is the main contribution of this paper. To illustrate this, on IMDb dataset, we train a Recurrent Neural Network (RNN) to achieve good privacy-vs-accuracy tradeoff, while being significantly faster than DP-SGD and with a similar memory footprint as non-private SGD. The privacy analysis of our algorithms is more involved than DP-SGD, we use the recently proposed f-DP framework of Dong et al. (2019) to prove privacy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 39  (*Influential: 2*)\n\n#### 2. LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning\n\n*From Search Query: advanced normalization techniques memory efficient gradients*\n\n*Yi-Lin Sung, Jaemin Cho, Mohit Bansal*\n\n**TL;DR:** LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections, and achieves higher accuracy than Adapter and LoRA in a low-memory regime.\n\n**Abstract:** Fine-tuning large pre-trained models on downstream tasks has been adopted in a variety of domains recently. However, it is costly to update the entire parameter set of large pre-trained models. Although recently proposed parameter-efficient transfer learning (PETL) techniques allow updating a small subset of parameters (e.g. only using 2% of parameters) inside a pre-trained backbone network for a new task, they only reduce the training memory requirement by up to 30%. This is because the gradient computation for the trainable parameters still requires backpropagation through the large pre-trained backbone model. To address this, we propose Ladder Side-Tuning (LST), a new PETL technique that can reduce training memory requirements by more substantial amounts. Unlike existing parameter-efficient methods that insert additional parameters inside backbone networks, we train a ladder side network, a small and separate network that takes intermediate activations as input via shortcut connections (called ladders) from backbone networks and makes predictions. LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections. We evaluate our method with various models (T5 and CLIP-T5) on both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the whole network, while other methods only save 26% of that in similar parameter usages (hence, 2.7x more memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA in a low-memory regime. To further show the advantage of this better memory efficiency, we also apply LST to larger T5 models, attaining better GLUE performance than full fine-tuning and other PETL methods. The accuracy-efficiency trade-off also holds on VL tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 179  (*Influential: 26*)\n\n#### 3. Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark\n\n*From Search Query: advanced normalization techniques memory efficient gradients*\n\n*Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen*\n\n**TL;DR:** This study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance, and introduces novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity.\n\n**Abstract:** In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 4. Efficient and Accurate Gradients for Neural SDEs\n\n*From Search Query: advanced normalization techniques memory efficient gradients*\n\n*Patrick Kidger, James Foster, Xuechen Li, Terry Lyons*\n\n**TL;DR:** A new SDE solver that is algebraically reversible, eliminating numerical gradient errors, and the first such solver of which the author is aware is introduced, which outperform the state-of-the-art by substantial margins with respect to training speed, and withrespect to classification, prediction, and MMD test metrics.\n\n**Abstract:** Neural SDEs combine many of the best qualities of both RNNs and SDEs: memory efficient training, high-capacity function approximation, and strong priors on model space. This makes them a natural choice for modelling many types of temporal dynamics. Training a Neural SDE (either as a VAE or as a GAN) requires backpropagating through an SDE solve. This may be done by solving a backwards-in-time SDE whose solution is the desired parameter gradients. However, this has previously suffered from severe speed and accuracy issues, due to high computational cost and numerical truncation errors. Here, we overcome these issues through several technical innovations. First, we introduce the \\textit{reversible Heun method}. This is a new SDE solver that is \\textit{algebraically reversible}: eliminating numerical gradient errors, and the first such solver of which we are aware. Moreover it requires half as many function evaluations as comparable solvers, giving up to a $1.98\\times$ speedup. Second, we introduce the \\textit{Brownian Interval}: a new, fast, memory efficient, and exact way of sampling \\textit{and reconstructing} Brownian motion. With this we obtain up to a $10.6\\times$ speed improvement over previous techniques, which in contrast are both approximate and relatively slow. Third, when specifically training Neural SDEs as GANs (Kidger et al. 2021), we demonstrate how SDE-GANs may be trained through careful weight clipping and choice of activation function. This reduces computational cost (giving up to a $1.87\\times$ speedup) and removes the numerical truncation errors associated with gradient penalty. Altogether, we outperform the state-of-the-art by substantial margins, with respect to training speed, and with respect to classification, prediction, and MMD test metrics. We have contributed implementations of all of our techniques to the torchsde library to help facilitate their adoption.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 46  (*Influential: 6*)\n\n#### 5. Memory-Efficient Pipeline-Parallel DNN Training\n\n*From Search Query: advanced normalization techniques memory efficient gradients*\n\n*D. Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, M. Zaharia*\n\n**TL;DR:** This work proposes PipeDream-2BW, a system that performs memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining, able to accelerate the training of large language models with up to 2.5 billion parameters by up to 6.9x compared to optimized baselines.\n\n**Abstract:** Many state-of-the-art results in domains such as NLP and computer vision have been obtained by scaling up the number of parameters in existing models. However, the weight parameters and intermediate outputs of these large models often do not fit in the main memory of a single accelerator device; this means that it is necessary to use multiple accelerators to train large models, which is challenging to do in a time-efficient way. In this work, we propose PipeDream-2BW, a system that performs memory-efficient pipeline parallelism, a hybrid form of parallelism that combines data and model parallelism with input pipelining. Our system uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint, and weight update semantics similar to data parallelism. In addition, PipeDream-2BW automatically partitions the model over the available hardware resources, while being cognizant of constraints such as compute capabilities, memory capacities, and interconnect topologies, and determines when to employ existing memory-savings techniques, such as activation recomputation, that trade off extra computation for lower memory footprint. PipeDream-2BW is able to accelerate the training of large language models with up to 2.5 billion parameters by up to 6.9x compared to optimized baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 169  (*Influential: 34*)\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on advanced normalization techniques, memory-efficient gradient computation, and integration strategies between normalization and attention mechanisms, here are some key points and techniques from the available sources and additional insights:\n\n## Advanced Normalization Techniques\n\n### Batch Normalization and Variants\nBatch Normalization (BatchNorm) is a well-established technique to improve the training of deep neural networks by normalizing the inputs to each layer. This helps in stabilizing the training process and improving the flow of gradients.\n\n### Dynamic Normalization\nFor autoregressive models, dynamic normalization techniques can be particularly useful. For instance, combining sparse attention with dynamic normalization can help in adapting the normalization process based on the input sequence, which can enhance stability and efficiency. This approach is hinted at in the analysis of integrating sparse attention with dynamic normalization.\n\n### Layer Normalization\nLayer Normalization is another technique that normalizes the activations across all features for a single sample, which is particularly useful in models like transformers where the input sequence length can vary. This can help in maintaining stability and efficiency across different layers and sequence lengths.\n\n## Memory-Efficient Gradient Computation and Backpropagation\n\n### Gradient Checkpointing\nGradient checkpointing is a method that reduces memory usage by selectively storing only a subset of activations during the forward pass. This allows for the reconstruction of the necessary gradients during the backward pass, significantly reducing memory footprint.\n\n### Low-Rank Gradient Approximation\nTechniques like GaLore and CompAct utilize the low-rank structure of gradients to reduce memory usage. GaLore projects the gradient matrix into a low-rank form, reducing the memory footprint without severely limiting the model\u2019s expressiveness. CompAct compresses activations, gradients, and optimizer states, leading to significant memory savings during training.\n\n### Winner-Take-All Column Row Sampling\nThis method proposes unbiased estimators for matrix products that only require storing sub-sampled activations for calculating the gradient, which reduces memory usage and variance in gradient computation[Analysis Reference 2].\n\n## Integration Strategies Between Normalization and Attention Mechanisms\n\n### Combining Sparse Attention with Dynamic Normalization\nIntegrating sparse attention mechanisms with dynamic normalization can enhance the efficiency and stability of the model. Sparse attention reduces the computational cost by focusing on relevant parts of the input sequence, while dynamic normalization adapts to the input sequence, ensuring that the normalization process is optimized for the current context[Analysis].\n\n### Efficient State Management\nEfficient state management is crucial for integrating normalization and attention mechanisms. Techniques like windowing and row-column bundling can strategically reduce data transfer and increase the size of data chunks read from memory, optimizing memory access patterns[Analysis Reference 3].\n\n### Hardware-Optimized Implementation\nHardware-optimized sparse computation and efficient parallel processing strategies are essential for real-world performance. Tiling strategies for memory access optimization and hardware-aware sparse computation can be integrated with advanced normalization and attention mechanisms to ensure high throughput and low memory footprint[Analysis Reference 4].\n\n## Additional Insights\n\n### IO-Aware Computation\nIO-aware computation strategies, such as those from FlashAttention, can significantly reduce memory bandwidth requirements. This can be combined with advanced normalization techniques to ensure efficient and stable training[Analysis].\n\n### Memory-Efficient Backpropagation\nTechniques like PipeDream-2BW, which use pipelining and weight gradient coalescing, can ensure high throughput and low memory footprint during backpropagation. These can be integrated with low-rank gradient approximation methods to further optimize memory usage[Analysis Reference 4].\n\nBy incorporating these advanced normalization techniques, memory-efficient gradient computation methods, and integration strategies, researchers can significantly improve the efficiency, stability, and performance of autoregressive language models.\n",
            "analysis": "Summary of Analysis:\n\nAfter analyzing the search results across multiple rounds, I've identified several promising directions for improving the HieraNorm-AttnGPT design, particularly in three key areas:\n\n1. Memory-Efficient Computation:\n- IO-aware computation strategies from FlashAttention\n- Memory-efficient backpropagation techniques\n- Efficient state management and compression methods\n\n2. Hardware-Optimized Implementation:\n- Tiling strategies for memory access optimization\n- Hardware-aware sparse computation\n- Efficient parallel processing techniques\n\n3. Integration Opportunities:\n- Combining sparse attention with dynamic normalization\n- Memory-efficient state management\n- Hardware-optimized implementation strategies\n\nKey References & Excerpts:\n\n1. From Yang & Zhang (2024) \"Gated Linear Attention\":\n\"FLASHLINEARATTENTION is faster than FLASHATTENTION-2 as a standalone layer even on short sequence lengths... GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\nThis suggests potential for incorporating linear attention mechanisms with hardware optimization.\n\n2. From Liu et al. (2023) \"Winner-Take-All Column Row Sampling\":\n\"We propose a new family of unbiased estimators called WTA-CRS, for matrix production with reduced variance, which only requires storing the sub-sampled activations for calculating the gradient.\"\n\nThis offers insights into memory-efficient gradient computation.\n\n3. From Alizadeh-Vahid et al. (2023) \"LLM in a flash\":\n\"Windowing strategically reduces data transfer by reusing previously activated neurons, and row-column bundling, tailored to the sequential data access strengths of flash memory, increases the size of data chunks read from flash memory.\"\n\nThis provides ideas for efficient memory access patterns.\n\n4. From Narayanan et al. (2020) \"Memory-Efficient Pipeline-Parallel\":\n\"PipeDream-2BW uses a novel pipelining and weight gradient coalescing strategy, combined with the double buffering of weights, to ensure high throughput, low memory footprint.\"\n\nThis suggests strategies for efficient parallel processing.\n\nKey Insights:\n\n1. Memory Efficiency:\n- IO-aware computation can significantly reduce memory bandwidth requirements\n- Selective activation and gradient computation can reduce memory usage\n- Efficient state management crucial for performance\n\n2. Hardware Optimization:\n- Tiling strategies can optimize memory access patterns\n- Hardware-aware sparse computation important for real performance\n- Efficient parallel processing requires careful coordination\n\n3. Integration Strategies:\n- Combining sparse attention with dynamic normalization shows promise\n- Memory-efficient state management needs careful design\n- Hardware optimization crucial for real-world performance\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific implementations of:\n   - Advanced normalization techniques\n   - Memory-efficient gradient computation\n   - Hardware-optimized sparse computation\n\n2. Integration methods for:\n   - Combining different attention mechanisms\n   - Efficient state management across layers\n   - Hardware-aware implementation strategies\n\nReferences:\n\n1. Yang, S., & Zhang, Y. (2024). FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism. arXiv preprint.\n\n2. Liu, Z., Wang, G., Zhong, S., Xu, Z., Zha, D., Tang, R., ... & Hu, X. (2023). Winner-Take-All Column Row Sampling for Memory Efficient Adaptation of Language Model. Neural Information Processing Systems.\n\n3. Alizadeh-Vahid, K., Mirzadeh, I., Belenko, D., Khatamifard, K., Cho, M., Mundo, C. C. D., ... & Farajtabar, M. (2023). LLM in a flash: Efficient Large Language Model Inference with Limited Memory. Annual Meeting of the Association for Computational Linguistics.\n\n4. Narayanan, D., Phanishayee, A., Shi, K., Chen, X., & Zaharia, M. (2020). Memory-Efficient Pipeline-Parallel DNN Training. International Conference on Machine Learning.\n\n5. Qiu, X., Lv, H., Guo, Q., Yan, H., & Lv, K. (2023). AdaLomo: Low-memory Optimization with Adaptive Learning Rate. arXiv preprint."
        },
        {
            "ready": false,
            "query": "\"adaptive normalization production\", \"normalization empirical comparison\", \"hardware optimization normalization\"",
            "detail": "Search for papers discussing:\n\n1. Production implementations of adaptive normalization:\n- Real-world deployment examples\n- Performance benchmarks\n- Implementation challenges and solutions\n\n2. Empirical studies of normalization techniques:\n- Comparative analysis of different approaches\n- Ablation studies\n- Performance metrics across different scales\n\n3. Hardware-specific optimizations:\n- Implementation details for different hardware\n- Memory and computation trade-offs\n- Optimization strategies for specific architectures\n\nFocus on:\n- Concrete implementation details\n- Performance comparisons\n- Hardware utilization metrics\n- Production deployment considerations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n\n1. Production implementations of adaptive normalization:\n- Real-world deployment examples\n- Performance benchmarks\n- Implementation challenges and solutions\n\n2. Empirical studies of normalization techniques:\n- Comparative analysis of different approaches\n- Ablation studies\n- Performance metrics across different scales\n\n3. Hardware-specific optimizations:\n- Implementation details for different hardware\n- Memory and computation trade-offs\n- Optimization strategies for specific architectures\n\nFocus on:\n- Concrete implementation details\n- Performance comparisons\n- Hardware utilization metrics\n- Production deployment considerations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization (Avg. Score: 0.04)\n\n*Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A novel method named PRepBN is proposed to progressively replace LayerNorm with re-parameterized BatchNorm in training and a simplified linear attention (SLA) module is proposed that is simple yet effective to achieve strong performance.\n\n**Abstract:** Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency.Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.\n\n##### *Relevant Chunk: No. 2/25 (Score: 0.04)*\n\n```\nHowever, the high computational cost makes it quite challenging to deploy on resourceconstraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6 \\%$ top-1 accuracy on ImageNet-1K with 16.2 ms latency, which is 2.4 ms less than that of Flatten-Swin with $0.1 \\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency. Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/. ## 1. Introduction\n\nIntroduced initially for tasks in natural language processing (Vaswani et al., 2017), transformer architecture has\n\n[^0]rapidly emerged as a preeminent model in the landscape of language models. Its influence has significantly expanded with the introduction of Vision Transformer (ViT) (Dosovitskiy et al., 2020), illustrating the efficacy and versatility of transformer-based architectures. These architectures have demonstrated their capability to achieve competitive performance benchmarks in comparison to convolutional neural networks (CNNs) across diverse vision tasks (Han et al., 2022; Wang et al., 2022; Zheng et al., 2023; Tang et al., 2023a; Carion et al., 2020; Xu et al., 2023). Due to its powerful performance, transformer has become the mainstream architecture in deep learning. However, the computational demands of transformer architecture pose a significant challenge, which is predominantly due to the quadratic computational complexity of its attention mechanism and the necessity for online statistic computation of LayerNorm component. Numerous efforts have been directed towards enhancing the efficiency of transformer architecture (Tang et al., 2024; Wu et al., 2023; Tang et al., 2023b). Several approaches have sought to mitigate computational complexity by limiting the scope of token interactions within self-attention mechanisms, such as downsampling the key and value matrices (Wang et al., 2021), implementing sparse global attention patterns (Child et al., 2019), and computing selfattention within smaller windows (Tu et al., 2022; Liu et al., 2021; Dong et al., 2022). Meanwhile, linear attention emerges as an alternative strategy to enhance computational efficiency by breaking down the attention mechanism into linear computational cost (Katharopoulos et al., 2020; Cai et al., 2022; Han et al., 2023; You et al., 2023), yet it is still a challenging task to obtain a good balance between efficiency and accuracy. Moreover, there are some explorations into substituting LayerNorm (LN) with BatchNorm (BN) within transformers, motivated by the additional computational overhead LayerNorm incurs during inference. Yang et al. (2022) propose to add a BatchNorm layer in-between the two linear layers in the feed forward network to stabilize the training. However, there still exists a performance gap between the LayerNorm-based and BatchNorm-based transformers. In this paper, we focus on obtaining efficient transformer\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_d98ef7f3d09ffeb83a3fg-02.jpg?height=508&width=770&top_left_y=234&top_left_x=209)\n\nFigure 1. Comparisons of different methods on ImageNet. architectures by digging deep into the computational inefficient modules, i.e., normalization layers and attention modules. We first explore to replace LayerNorm with BatchNorm to accelerate inference for transformer. BatchNorm leads to lower inference latency but may cause training collapse and inferior performance, while LayerNorm could stabilize the training yet has extra computational cost during inference. To this end, we first propose a progressive strategy to gradually replace LayerNorm with BatchNorm by using a hyper-parameter to control the proportion of both normalization layers. Initially the transformer architecture is dominated by the LayerNorm and gradually transits to pure BatchNorm at the end of training. This strategy effectively mitigates the risk of training collapse and also eliminating the need for calculating statistics during inference. In addition to the progressive strategy, we also propose a novel re-parameterization formula for BatchNorm (RepBN), to enhance training stability and overall performance. Furthermore, the computational cost of attention is critical for efficient transformer and prior methods struggle to obtain good balance of efficiency and accuracy. To this end, we propose a simplified linear attention (SLA) module which utilizes ReLU as the kernel function and incorporate a depthwise convolution to perform local feature enhancement. The proposed attention mechanism is more efficient than prior linear attention but still attains comparable performance. We extensively evaluate our proposed method for various architectures on various benchmarks. Our progressive reparameterized BatchNorm shows strong performance for image classification and object detection tasks, obtaining similar accuracy with lower inference latency. Moreover, coupled with the progressive RepBN and simplified linear attention module, our SLAB transformer achieves competitive accuracy compared to Flatten transformer with improved computational efficiency. For example, SLAB-SwinS achieves $83.6 \\%$ Top-1 accuracy on ImageNet-1K with 16.2 ms latency, which is 2.4 ms less than that of FlattenSwin-S with $0.1 \\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower inference latency.\n```\n\n#### 2. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.01)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.01)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 3. Retentive network: a successor to transformer for large language models (Avg. Score: 0.01)\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 143  (*Influential: 18*)\n\n**TL;DR:** This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n##### *Relevant Chunk: No. 17/21 (Score: 0.01)*\n\n```\narXiv preprint arXiv:1909.08053, 2019. [SSI ${ }^{+}$22] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. Scrolls: Standardized comparison over long language sequences. arXiv preprint arXiv:2201.03533, 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, 4-9 December 2017, Long Beach, CA, USA, pages 60006010, 2017. [WH18] Yuxin Wu and Kaiming He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pages 3-19, 2018. $\\left[\\mathrm{WMD}^{+}\\right.$22] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. DeepNet: Scaling Transformers to 1,000 layers. ArXiv, abs/2203.00555, 2022. [WMH ${ }^{+}$22] Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal Bajaj, Saksham Singhal, Alon Benhaim, et al. Foundation transformers. arXiv preprint arXiv:2210.06423, 2022. $\\left[\\mathrm{WPN}^{+}\\right.$19] Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. SuperGLUE: A stickier benchmark for general-purpose language understanding systems.\n```\n\n#### 4. Linearizing Large Language Models (Avg. Score: 0.01)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 5. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.00)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.00)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive normalization production\", \"normalization empirical comparison\", \"hardware optimization normalization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 2. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Junho Kim, Minjae Kim, Hyeonwoo Kang, Kwanghee Lee*\n\n**TL;DR:** A novel method for unsupervised image- to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner, which can translate both images requiring holistic changes and images requiring large shape changes.\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at this https URL or this https URL.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 507  (*Influential: 97*)\n\n#### 3. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1088  (*Influential: 144*)\n\n#### 4. Global Normalization for Streaming Speech Recognition in a Modular Framework\n\n*From Search Query: \"normalization empirical comparison\"*\n\n*Ehsan Variani, Ke Wu, M. Riley, David Rybach, Matt Shannon, Cyril Allauzen*\n\n**TL;DR:** By switching to a globally normalized model, the word error rate gap between streaming and non-streaming speech-recognition models can be greatly reduced, and the label bias problem in streaming speech recognition is addressed.\n\n**Abstract:** We introduce the Globally Normalized Autoregressive Transducer (GNAT) for addressing the label bias problem in streaming speech recognition. Our solution admits a tractable exact computation of the denominator for the sequence-level normalization. Through theoretical and empirical results, we demonstrate that by switching to a globally normalized model, the word error rate gap between streaming and non-streaming speech-recognition models can be greatly reduced (by more than 50\\% on the Librispeech dataset). This model is developed in a modular framework which encompasses all the common neural speech recognition models. The modularity of this framework enables controlled comparison of modelling choices and creation of new models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 5. CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark\n\n*From Search Query: \"normalization empirical comparison\"*\n\n*Ningyu Zhang, Zhen Bi, Xiaozhuan Liang, Lei Li, Xiang Chen, Shumin Deng, Luoqiu Li, Xin Xie, Hongbin Ye, Xin Shang, Kangping Yin, Chuanqi Tan, Jian Xu, Mosha Chen, Fei Huang, Luo Si, Yuan Ni, G. Xie, Zhifang Sui, Baobao Chang, Hui Zong, Zheng Yuan, Linfeng Li, Jun Yan, Hongying Zan, Kunli Zhang, Huajun Chen, Buzhou Tang, Qingcai Chen*\n\n**TL;DR:** The first Chinese Biomedical Language Understanding Evaluation Evaluation (CBLUE) benchmark is presented: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis.\n\n**Abstract:** Artificial Intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most benchmarks are limited to English, which makes it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, clinical diagnosis normalization, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11 pre-trained Chinese models, and experimental results show that state-of-the-art neural models perform by far worse than the human ceiling.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 150  (*Influential: 12*)\n\n#### 6. Towards Defending against Adversarial Examples via Attack-Invariant Features\n\n*From Search Query: \"normalization empirical comparison\"*\n\n*Dawei Zhou, Tongliang Liu, Bo Han, N. Wang, Chunlei Peng, Xinbo Gao*\n\n**TL;DR:** This paper introduces an adversarial feature learning mechanism to disentangle invariant features from adversarial noise and proposes a normalization term in the encoded space of the attack-invariant features to address the bias issue between the seen and unseen types of attacks.\n\n**Abstract:** Deep neural networks (DNNs) are vulnerable to adversarial noise. Their adversarial robustness can be improved by exploiting adversarial examples. However, given the continuously evolving attacks, models trained on seen types of adversarial examples generally cannot generalize well to unseen types of adversarial examples. To solve this problem, in this paper, we propose to remove adversarial noise by learning generalizable invariant features across attacks which maintain semantic classification information. Specifically, we introduce an adversarial feature learning mechanism to disentangle invariant features from adversarial noise. A normalization term has been proposed in the encoded space of the attack-invariant features to address the bias issue between the seen and unseen types of attacks. Empirical evaluations demonstrate that our method could provide better protection in comparison to previous state-of-the-art approaches, especially against unseen types of attacks and adaptive attacks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 2*)\n\n#### 7. Temporal Effective Batch Normalization in Spiking Neural Networks\n\n*From Search Query: \"hardware optimization normalization\"*\n\n*Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, Tiejun Huang*\n\n**TL;DR:** Experimental results show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Abstract:** Spiking Neural Networks (SNNs) are promising in neuromorphic hardware owing to utilizing spatio-temporal information and sparse event-driven signal processing. However, it is challenging to train SNNs due to the non-differentiable nature of the binary firing function. The surrogate gradients alleviate the training problem and make SNNs obtain comparable performance as Artificial Neural Networks (ANNs) with the same structure. Unfortunately, batch normalization, contributing to the success of ANNs, does not play a prominent role in SNNs because of the additional temporal dimension. To this end, we propose an effective normalization method called temporal effective batch normalization (TEBN). By rescaling the presynaptic inputs with different weights at every time-step, temporal distributions become smoother and uniform. Theoretical analysis shows that TEBN can be viewed as a smoother of SNN\u2019s optimization landscape and could help stabilize the gradient norm. Experimental results on both static and neuromorphic datasets show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 64  (*Influential: 7*)\n\n#### 8. On-Device Training Under 256KB Memory\n\n*From Search Query: \"hardware optimization normalization\"*\n\n*Ji Lin, Ligeng Zhu, Wei-Ming Chen, Wei-Chen Wang, Chuang Gan, Song Han*\n\n**TL;DR:** This framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW.\n\n**Abstract:** On-device training enables the model to adapt to new data collected from the sensors by fine-tuning a pre-trained model. Users can benefit from customized AI models without having to transfer the data to the cloud, protecting the privacy. However, the training memory consumption is prohibitive for IoT devices that have tiny memory resources. We propose an algorithm-system co-design framework to make on-device training possible with only 256KB of memory. On-device training faces two unique challenges: (1) the quantized graphs of neural networks are hard to optimize due to low bit-precision and the lack of normalization; (2) the limited hardware resource does not allow full back-propagation. To cope with the optimization difficulty, we propose Quantization-Aware Scaling to calibrate the gradient scales and stabilize 8-bit quantized training. To reduce the memory footprint, we propose Sparse Update to skip the gradient computation of less important layers and sub-tensors. The algorithm innovation is implemented by a lightweight training system, Tiny Training Engine, which prunes the backward computation graph to support sparse updates and offload the runtime auto-differentiation to compile time. Our framework is the first solution to enable tiny on-device training of convolutional neural networks under 256KB SRAM and 1MB Flash without auxiliary memory, using less than 1/1000 of the memory of PyTorch and TensorFlow while matching the accuracy on tinyML application VWW. Our study enables IoT devices not only to perform inference but also to continuously adapt to new data for on-device lifelong learning. A video demo can be found here: https://youtu.be/0pUFZYdoMY8.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 141  (*Influential: 17*)\n\n#### 9. Demystifying Batch Normalization in ReLU Networks: Equivalent Convex Optimization Models and Implicit Regularization\n\n*From Search Query: \"hardware optimization normalization\"*\n\n*Tolga Ergen, Arda Sahiner, Batu Mehmet Ozturkler, J. Pauly, M. Mardani, Mert Pilanci*\n\n**TL;DR:** An analytic framework based on convex duality is introduced to obtain exact convex representations of weight-decay regularized ReLU networks with BN, which can be trained in polynomial-time, and it is found that Gradient Descent provides an algorithmic bias effect on the standard non-convex BN network.\n\n**Abstract:** Batch Normalization (BN) is a commonly used technique to accelerate and stabilize training of deep neural networks. Despite its empirical success, a full theoretical understanding of BN is yet to be developed. In this work, we analyze BN through the lens of convex optimization. We introduce an analytic framework based on convex duality to obtain exact convex representations of weight-decay regularized ReLU networks with BN, which can be trained in polynomial-time. Our analyses also show that optimal layer weights can be obtained as simple closed-form formulas in the high-dimensional and/or overparameterized regimes. Furthermore, we find that Gradient Descent provides an algorithmic bias effect on the standard non-convex BN network, and we design an approach to explicitly encode this implicit regularization into the convex objective. Experiments with CIFAR image classification highlight the effectiveness of this explicit regularization for mimicking and substantially improving the performance of standard BN networks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 26  (*Influential: 1*)\n\n### 4 related papers from ArXiv\n\n#### 1. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 2. Adaptive Multi-view Rule Discovery for Weakly-Supervised Compatible\n  Products Prediction\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Rongzhi Zhang, Rebecca West, Xiquan Cui, Chao Zhang*\n\n**Abstract:** On e-commerce platforms, predicting if two products are compatible with each\nother is an important functionality to achieve trustworthy product\nrecommendation and search experience for consumers. However, accurately\npredicting product compatibility is difficult due to the heterogeneous product\ndata and the lack of manually curated training data. We study the problem of\ndiscovering effective labeling rules that can enable weakly-supervised product\ncompatibility prediction. We develop AMRule, a multi-view rule discovery\nframework that can (1) adaptively and iteratively discover novel rulers that\ncan complement the current weakly-supervised model to improve compatibility\nprediction; (2) discover interpretable rules from both structured attribute\ntables and unstructured product descriptions. AMRule adaptively discovers\nlabeling rules from large-error instances via a boosting-style strategy, the\nhigh-quality rules can remedy the current model's weak spots and refine the\nmodel iteratively. For rule discovery from structured product attributes, we\ngenerate composable high-order rules from decision trees; and for rule\ndiscovery from unstructured product descriptions, we generate prompt-based\nrules from a pre-trained language model. Experiments on 4 real-world datasets\nshow that AMRule outperforms the baselines by 5.98% on average and improves\nrule quality and rule proposal efficiency.\n\n**Published:** 2022-06-28T04:11:58Z  (*Updated: 2022-06-28T04:11:58Z*)\n\n\n\n#### 3. An Empirical Comparison of SVM and Some Supervised Learning Algorithms\n  for Vowel recognition\n\n*From Search Query: \"normalization empirical comparison\"*\n\n*Rimah Amami, Dorra Ben Ayed, Noureddine Ellouze*\n\n**Abstract:** In this article, we conduct a study on the performance of some supervised\nlearning algorithms for vowel recognition. This study aims to compare the\naccuracy of each algorithm. Thus, we present an empirical comparison between\nfive supervised learning classifiers and two combined classifiers: SVM, KNN,\nNaive Bayes, Quadratic Bayes Normal (QDC) and Nearst Mean. Those algorithms\nwere tested for vowel recognition using TIMIT Corpus and Mel-frequency cepstral\ncoefficients (MFCCs).\n\n**Published:** 2015-07-22T00:34:15Z  (*Updated: 2015-07-22T00:34:15Z*)\n\n\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"hardware optimization normalization\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. NeMo Inverse Text Normalization: From Development To Production\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Boris Ginsburg, Kyle Gorman, Evelina Bakhturina, Yang Zhang*\n\n**Abstract:** Inverse text normalization (ITN) converts spoken-domain automatic speech recognition (ASR) output into written-domain text to improve the readability of the ASR output. Many state-of-the-art ITN systems use hand-written weighted finite-state transducer(WFST) grammars since this task has extremely low tolerance to unrecoverable errors. We introduce an open-source Python WFST-based library for ITN which enables a seamless path from development to production. We describe the specification of ITN grammar rules for English, but the library can be adapted for other languages. It can also be used for written-to-spoken text normalization. We evaluate the NeMo ITN library using a modified version of the Google Text normalization dataset.\n\n**Published:** 2021-04-11\n\n\n\n#### 2. Fast Direct Methods for Gaussian Processes\n\n*From Search Query: \"adaptive normalization production\"*\n\n*Michael O'Neil, Daniel Foreman-Mackey, Sivaram Ambikasaran, David W. Hogg, Leslie Greengard*\n\n**Abstract:** A number of problems in probability and statistics can be addressed using the\nmultivariate normal (Gaussian) distribution. In the one-dimensional case,\ncomputing the probability for a given mean and variance simply requires the\nevaluation of the corresponding Gaussian density. In the $n$-dimensional\nsetting, however, it requires the inversion of an $n \\times n$ covariance\nmatrix, $C$, as well as the evaluation of its determinant, $\\det(C)$. In many\ncases, such as regression using Gaussian processes, the covariance matrix is of\nthe form $C = \\sigma^2 I + K$, where $K$ is computed using a specified\ncovariance kernel which depends on the data and additional parameters\n(hyperparameters). The matrix $C$ is typically dense, causing standard direct\nmethods for inversion and determinant evaluation to require $\\mathcal O(n^3)$\nwork. This cost is prohibitive for large-scale modeling. Here, we show that for\nthe most commonly used covariance functions, the matrix $C$ can be\nhierarchically factored into a product of block low-rank updates of the\nidentity matrix, yielding an $\\mathcal O (n\\log^2 n) $ algorithm for inversion.\nMore importantly, we show that this factorization enables the evaluation of the\ndeterminant $\\det(C)$, permitting the direct calculation of probabilities in\nhigh dimensions under fairly broad assumptions on the kernel defining $K$. Our\nfast algorithm brings many problems in marginalization and the adaptation of\nhyperparameters within practical reach using a single CPU core. The combination\nof nearly optimal scaling in terms of problem size with high-performance\ncomputing resources will permit the modeling of previously intractable\nproblems. We illustrate the performance of the scheme on standard covariance\nkernels.\n\n**Published:** 2014-03-24\n\n\n\n#### 3. An Empirical Comparison of GANs and Normalizing Flows for Density Estimation\n\n*From Search Query: \"normalization empirical comparison\"*\n\n*Tianci Liu, Jeffrey Regier*\n\n**Abstract:** Generative adversarial networks (GANs) and normalizing flows are both approaches to density estimation that use deep neural networks to transform samples from an uninformative prior distribution to an approximation of the data distribution. There is great interest in both for general-purpose statistical modeling, but the two approaches have seldom been compared to each other for modeling non-image data. The difficulty of computing likelihoods with GANs, which are implicit models, makes conducting such a comparison challenging. We work around this difficulty by considering several low-dimensional synthetic datasets. An extensive grid search over GAN architectures, hyperparameters, and training procedures suggests that no GAN is capable of modeling our simple low-dimensional data well, a task we view as a prerequisite for an approach to be considered suitable for general-purpose statistical modeling. Several normalizing flows, on the other hand, excelled at these tasks, even substantially outperforming WGAN in terms of Wasserstein distance -- the metric that WGAN alone targets. Scientists and other practitioners should be wary of relying on WGAN for applications that require accurate density estimation.\n\n**Published:** 2020-06-17\n\n\n\n#### 4. Semantic Word Clouds with Background Corpus Normalization and t-distributed Stochastic Neighbor Embedding\n\n*From Search Query: \"normalization empirical comparison\"*\n\n*Johanna Gei\u00df, Andreas Spitz, Michael Weiler, Erich Schubert, Michael Gertz*\n\n**Abstract:** Many word clouds provide no semantics to the word placement, but use a random\nlayout optimized solely for aesthetic purposes. We propose a novel approach to\nmodel word significance and word affinity within a document, and in comparison\nto a large background corpus. We demonstrate its usefulness for generating more\nmeaningful word clouds as a visual summary of a given document. We then select\nkeywords based on their significance and construct the word cloud based on the\nderived affinity. Based on a modified t-distributed stochastic neighbor\nembedding (t-SNE), we generate a semantic word placement. For words that\ncooccur significantly, we include edges, and cluster the words according to\ntheir cooccurrence. For this we designed a scalable and memory-efficient\nsketch-based approach usable on commodity hardware to aggregate the required\ncorpus statistics needed for normalization, and for identifying keywords as\nwell as significant cooccurences. We empirically validate our approch using a\nlarge Wikipedia corpus.\n\n**Published:** 2017-08-11\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, focusing on adaptive normalization, empirical comparisons, and hardware-specific optimizations, here are some key points and references that align with the given details:\n\n## Production Implementations of Adaptive Normalization\n\n### Real-world Deployment Examples and Performance Benchmarks\n- The paper on \"Enhancing Neural Network Representations with Prior Knowledge\" introduces Adaptive Context Normalization (ACN), which dynamically builds contexts in the latent space during training. This method shows improvements in convergence and performance across various tasks, including image classification and domain adaptation, which can be analogous to the needs in language models.\n- For language models, adaptive normalization techniques like Dynamic Layer Normalization can be crucial. A study on \"Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling\" highlights the benefits of dynamic layer normalization in adapting to context-dependent inputs, which is relevant for autoregressive language models.\n\n### Implementation Challenges and Solutions\n- The implementation of adaptive normalization can be challenging due to the need for dynamic mode estimation and additional parameters. The paper on \"Enhancing Neural Network Representations with Prior Knowledge\" addresses these challenges by proposing methods like Context Normalization (CN) and Adaptive Context Normalization (ACN), which reduce computational overhead and improve performance.\n\n## Empirical Studies of Normalization Techniques\n\n### Comparative Analysis of Different Approaches\n- The comparison between different normalization techniques such as Batch Normalization (BN), Layer Normalization (LN), and Switchable Normalization (SwitchNorm) is crucial. SwitchNorm, for instance, dynamically combines multiple normalization techniques, which can be beneficial in scenarios where BN's assumptions about uniform data distribution are violated. This flexibility can improve performance across various tasks, including those in language models.\n- Empirical studies often involve ablation studies to understand the impact of different normalization layers. For example, the \"SLAB\" paper proposes a method to progressively replace LayerNorm with re-parameterized BatchNorm, showing improvements in training stability and memory efficiency.\n\n### Performance Metrics Across Different Scales\n- Performance metrics such as convergence speed, training stability, and validation loss are critical. The \"Adaptive Data Optimization\" paper, while focused on data distribution optimization, highlights the importance of dynamic adjustments during training, which can be applied to normalization techniques to improve performance across different scales.\n\n## Hardware-Specific Optimizations\n\n### Implementation Details for Different Hardware\n- Hardware-aware design is essential for optimizing normalization layers. The \"Full Stack Optimization of Transformer Inference\" survey discusses the implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization. It emphasizes the need for IO-aware implementations and cache-friendly computation patterns to achieve significant performance gains.\n\n### Memory and Computation Trade-offs\n- Memory efficiency is a key consideration. Techniques like progressive re-parameterization can reduce memory overhead while maintaining performance. The \"SLAB\" paper demonstrates this by progressively replacing LayerNorm with re-parameterized BatchNorm, which is beneficial for hardware with limited memory resources.\n\n### Optimization Strategies for Specific Architectures\n- For specific architectures, parallel processing optimization is crucial. The \"MA-BERT\" paper discusses the elimination of non-linear functions in Transformer-based models, which can simplify the hardware requirements and improve efficiency. This aligns with the need for optimizing normalization layers in autoregressive language models.\n\nIn summary, the following points are particularly relevant:\n\n- **Adaptive Normalization**: Techniques like Dynamic Layer Normalization and Adaptive Context Normalization can be adapted for autoregressive language models to improve performance and stability.\n- **Empirical Comparisons**: Studies comparing different normalization techniques (e.g., BN, LN, SwitchNorm) and ablation studies on the impact of normalization layers can provide valuable insights.\n- **Hardware Optimizations**: IO-aware implementations, cache-friendly computation patterns, and memory-efficient techniques like progressive re-parameterization are essential for optimizing normalization layers in autoregressive language models.\n\nThese insights and techniques can help in designing and optimizing autoregressive language models with better performance, efficiency, and scalability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several key insights around adaptive normalization and hardware efficiency:\n\na) Adaptive Normalization Approaches:\n- Dynamic Layer Normalization shows promise for context-dependent adaptation\n- Progressive re-parameterization methods can reduce memory overhead\n- Position of normalization layers critically affects gradient flow and training stability\n\nb) Hardware Optimization:\n- IO-aware implementations demonstrate significant performance gains\n- Cache-friendly computation patterns are crucial for efficiency\n- Memory access patterns strongly impact performance\n\nc) Novel Elements in Current Proposal:\n- Context-dependent parameter adaptation appears novel\n- Integration with RMSNorm is unique\n- Hardware-aware design aligns with current trends\n\n2. Useful References with Key Excerpts:\n\nFrom \"MA-BERT\":\n\"...eliminating the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy.\"\n\nFrom \"Full Stack Optimization of Transformer Inference\":\n\"...implications of Transformer architecture on hardware, including the impact of non-linear operations such as Layer Normalization, Softmax, and GELU...\"\n\nFrom \"SLAB\":\n\"...propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training.\"\n\n3. Key Insights:\n\na) Normalization Stability:\n- Pre-normalization architectures show better training stability\n- Position of normalization layers affects gradient flow\n- Progressive parameter updates can improve convergence\n\nb) Memory Efficiency:\n- Structured parameter sharing maintains performance while reducing memory\n- Cache-friendly computation patterns crucial for efficiency\n- Progressive re-parameterization reduces memory overhead\n\nc) Hardware Considerations:\n- IO-aware implementations show significant gains\n- Memory access patterns affect performance substantially\n- Parallel processing optimization opportunities exist\n\n4. Future Search Needed:\n\nNeed to investigate:\n1. Specific implementations of adaptive normalization in production systems\n2. Empirical comparisons of different normalization strategies\n3. Hardware-specific optimization techniques for normalization layers\n\n5. References:\n\n[1] Kim, S., et al. (2023). \"Full Stack Optimization of Transformer Inference: a Survey.\" arXiv preprint.\n\n[2] Wei Ming, N., et al. (2023). \"MA-BERT: Towards Matrix Arithmetic-only BERT Inference.\" ICLR.\n\n[3] Guo, J., et al. (2024). \"SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization.\" arXiv preprint.\n\n[4] Kim, T., et al. (2017). \"Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling.\" arXiv preprint.\n\n[5] Huang, X., et al. (2020). \"Improving Transformer Optimization Through Better Initialization.\" ICML."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"adaptive layer normalization\", \"efficient normalization neural networks\", \"context-dependent normalization\"",
            "detail": "Search for papers discussing:\n\n1. Adaptive normalization techniques:\n- Context-dependent parameter adaptation\n- Position-aware normalization\n- Dynamic parameter adjustment in neural networks\n\n2. Memory-efficient normalization:\n- Parameter re-parameterization methods\n- Gradient computation optimization\n- State compression techniques\n\n3. Hardware-optimized normalization:\n- IO-aware implementation\n- Cache-friendly normalization\n- Parallel processing optimization\n\nFocus on:\n- Mathematical formulations of adaptive normalization\n- Memory efficiency techniques\n- Hardware optimization strategies\n- Empirical comparisons of different approaches",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n\n1. Adaptive normalization techniques:\n- Context-dependent parameter adaptation\n- Position-aware normalization\n- Dynamic parameter adjustment in neural networks\n\n2. Memory-efficient normalization:\n- Parameter re-parameterization methods\n- Gradient computation optimization\n- State compression techniques\n\n3. Hardware-optimized normalization:\n- IO-aware implementation\n- Cache-friendly normalization\n- Parallel processing optimization\n\nFocus on:\n- Mathematical formulations of adaptive normalization\n- Memory efficiency techniques\n- Hardware optimization strategies\n- Empirical comparisons of different approaches\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.05)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.05)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 2. Linearizing Large Language Models (Avg. Score: 0.05)\n\n*Jean-Pierre Mercat, Igor Vasiljevic, Sedrick Scott Keh, Kushal Arora, Achal Dave, Adrien Gaidon, Thomas Kollar*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work presents Scalable UPtraining for Recurrent Attention (SUPRA), a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget, and finds that the linearization technique leads to competitive performance on standard benchmarks, but it is identified persistent in-context learning and long-context modeling shortfalls for even the largest linear models.\n\n**Abstract:** Linear transformers have emerged as a subquadratic-time alternative to softmax attention and have garnered significant interest due to their fixed-size recurrent state that lowers inference cost. However, their original formulation suffers from poor scaling and underperforms compute-matched transformers. Recent linear models such as RWKV and Mamba have attempted to address these shortcomings by proposing novel time-mixing and gating architectures, but pre-training large language models requires significant data and compute investments. Thus, the search for subquadratic architectures is limited by the availability of compute and quality pre-training datasets. As a cost-effective alternative to pre-training linear transformers, we propose Scalable UPtraining for Recurrent Attention (SUPRA). We present a method to uptrain existing large pre-trained transformers into Recurrent Neural Networks (RNNs) with a modest compute budget. This allows us to leverage the strong pre-training data and performance of existing transformer LLMs, while requiring 5% of the training cost. We find that our linearization technique leads to competitive performance on standard benchmarks, but we identify persistent in-context learning and long-context modeling shortfalls for even the largest linear models. Our code and models can be found at https://github.com/TRI-ML/linear_open_lm.\n\n##### *Relevant Chunk: No. 6/22 (Score: 0.05)*\n\n```\narXiv preprint arXiv:2401.12973, 2024. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 3. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.05)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.05)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 4. Simple linear attention language models balance the recall-throughput tradeoff (Avg. Score: 0.05)\n\n*Simran Arora, Sabri Eyuboglu, Michael Zhang, Aman Timalsina, Silas Alberti, Dylan Zinsley, James Zou, Atri Rudra, Christopher R'e*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 17  (*Influential: 4*)\n\n**TL;DR:** To make BASED competitive, IO-aware algorithms are developed that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameters and show that BASED matches the strongest sub-quadratic models and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points.\n\n**Abstract:** Recent work has shown that attention-based language models excel at recall, the ability to ground generations in tokens previously seen in context. However, the efficiency of attention-based models is bottle-necked during inference by the KV-cache's aggressive memory consumption. In this work, we explore whether we can improve language model efficiency (e.g. by reducing memory consumption) without compromising on recall. By applying experiments and theory to a broad set of architectures, we identify a key tradeoff between a model's state size and recall ability. We show that efficient alternatives to attention (e.g. H3, Mamba, RWKV) maintain a fixed-size recurrent state, but struggle at recall. We propose BASED a simple architecture combining linear and sliding window attention. By varying BASED window size and linear attention feature dimension, we can dial the state size and traverse the pareto frontier of the recall-memory tradeoff curve, recovering the full quality of attention on one end and the small state size of attention-alternatives on the other. We train language models up to 1.3b parameters and show that BASED matches the strongest sub-quadratic models (e.g. Mamba) in perplexity and outperforms them on real-world recall-intensive tasks by 6.22 accuracy points. Implementations of linear attention are often less efficient than optimized standard attention implementations. To make BASED competitive, we develop IO-aware algorithms that enable 24x higher throughput on language generation than FlashAttention-2, when generating 1024 tokens using 1.3b parameter models. Code for this work is provided at: https://github.com/HazyResearch/based.\n\n##### *Relevant Chunk: No. 35/72 (Score: 0.05)*\n\n```\narXiv preprint arXiv:2110.15343, 2021. [53] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025-7041, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: $10.18653 / v 1 / 2022 . e m n l p-m a i n .473$. URL https: //aclanthology.org/2022.emnlp-main. 473 . [54] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. [55] Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [56] James W Cooley and John W Tukey. An algorithm for the machine calculation of complex fourier series. Mathematics of computation, 19(90):297-301, 1965. [57] David W. Romero, Anna Kuzina, Erik J. Bekkers, Jakub M. Tomczak, and Mark Hoogendoorn. Ckconv: Continuous kernel convolution for sequential data.\n```\n\n#### 5. PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation (Avg. Score: 0.05)\n\n*Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yunhe Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work presents a new efficient model architecture for establishing modern language models, namely, PanGu-$\\pi$, and develops an LLM named YunShan for practical application, which can surpass other models with similar scales on benchmarks.\n\n**Abstract:** The recent trend of large language models (LLMs) is to increase the scale of both model size (\\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices. However, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art language model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the nonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The series informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further used to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the model nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using the same dataset and training strategy to compare PanGu-$\\pi$ with state-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a comparable performance to that of benchmarks with about 10\\% inference speed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical application. The results show that YunShan can surpass other models with similar scales on benchmarks.\n\n##### *Relevant Chunk: No. 39/62 (Score: 0.05)*\n\n```\narXiv preprint arXiv:1808.06226, 2018. [107] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [108] I. Loshchilov and F. Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint arXiv:1608.03983, 2016. [109] B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. [110] J. Su et al. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, pp. 127063, 2023. [111] O. Contributors. Opencompass: A universal evaluation platform for foundation models. https://github.com/open-compass/opencompass 2023.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive layer normalization\", \"efficient normalization neural networks\", \"context-dependent normalization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Understanding and Improving Layer Normalization\n\n*From Search Query: \"adaptive layer normalization\"*\n\n*Jingjing Xu, Xu Sun, Zhiyuan Zhang, Guangxiang Zhao, Junyang Lin*\n\n**TL;DR:** A new normalization method, Adaptive Normalization (AdaNorm), is proposed, by replacing the bias and gain with a new transformation function, and Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.\n\n**Abstract:** Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 263  (*Influential: 9*)\n\n#### 2. Meta-StyleSpeech : Multi-Speaker Adaptive Text-to-Speech Generation\n\n*From Search Query: \"adaptive layer normalization\"*\n\n*Dong Min, Dong Bok Lee, Eunho Yang, S. Hwang*\n\n**TL;DR:** Style-Adaptive Layer Normalization (SALN) is proposed, a new TTS model which aligns gain and bias of the text input according to the style extracted from a reference speech audio which effectively synthesizes speech in the style of the target speaker even from single speech audio.\n\n**Abstract:** With rapid progress in neural text-to-speech (TTS) models, personalized speech generation is now in high demand for many applications. For practical applicability, a TTS model should generate high-quality speech with only a few audio samples from the given speaker, that are also short in length. However, existing methods either require to fine-tune the model or achieve low adaptation quality without fine-tuning. In this work, we propose StyleSpeech, a new TTS model which not only synthesizes high-quality speech but also effectively adapts to new speakers. Specifically, we propose Style-Adaptive Layer Normalization (SALN) which aligns gain and bias of the text input according to the style extracted from a reference speech audio. With SALN, our model effectively synthesizes speech in the style of the target speaker even from single speech audio. Furthermore, to enhance StyleSpeech's adaptation to speech from new speakers, we extend it to Meta-StyleSpeech by introducing two discriminators trained with style prototypes, and performing episodic training. The experimental results show that our models generate high-quality speech which accurately follows the speaker's voice with single short-duration (1-3 sec) speech audio, significantly outperforming baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 131  (*Influential: 25*)\n\n#### 3. Additive Powers-of-Two Quantization: An Efficient Non-uniform Discretization for Neural Networks\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Yuhang Li, Xin Dong, Wei Wang*\n\n**TL;DR:** Experimental results show that the proposed Additive Powers-of-Two~(APoT) quantization method outperforms state-of theart methods, and is even competitive with the full-precision models, demonstrating the effectiveness of the proposed APoT quantization.\n\n**Abstract:** We propose Additive Powers-of-Two~(APoT) quantization, an efficient non-uniform quantization scheme for the bell-shaped and long-tailed distribution of weights and activations in neural networks. By constraining all quantization levels as the sum of Powers-of-Two terms, APoT quantization enjoys high computational efficiency and a good match with the distribution of weights. A simple reparameterization of the clipping function is applied to generate a better-defined gradient for learning the clipping threshold. Moreover, weight normalization is presented to refine the distribution of weights to make the training more stable and consistent. Experimental results show that our proposed method outperforms state-of-the-art methods, and is even competitive with the full-precision models, demonstrating the effectiveness of our proposed APoT quantization. For example, our 4-bit quantized ResNet-50 on ImageNet achieves 76.6% top-1 accuracy without bells and whistles; meanwhile, our model reduces 22% computational cost compared with the uniformly quantized counterpart.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 225  (*Influential: 46*)\n\n#### 4. Stable Rank Normalization for Improved Generalization in Neural Networks and GANs\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Amartya Sanyal, Philip H. S. Torr, P. Dokania*\n\n**TL;DR:** Stable rank normalization (SRN) is proposed, a novel, optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator and can be shown to have a unique optimal solution.\n\n**Abstract:** Exciting new work on the generalization bounds for neural networks (NN) given by Neyshabur et al. , Bartlett et al. closely depend on two parameter-depenedent quantities: the Lipschitz constant upper-bound and the stable rank (a softer version of the rank operator). This leads to an interesting question of whether controlling these quantities might improve the generalization behaviour of NNs. To this end, we propose stable rank normalization (SRN), a novel, optimal, and computationally efficient weight-normalization scheme which minimizes the stable rank of a linear operator. Surprisingly we find that SRN, inspite of being non-convex problem, can be shown to have a unique optimal solution. Moreover, we show that SRN allows control of the data-dependent empirical Lipschitz constant, which in contrast to the Lipschitz upper-bound, reflects the true behaviour of a model on a given dataset. We provide thorough analyses to show that SRN, when applied to the linear layers of a NN for classification, provides striking improvements-11.3% on the generalization gap compared to the standard NN along with significant reduction in memorization. When applied to the discriminator of GANs (called SRN-GAN) it improves Inception, FID, and Neural divergence scores on the CIFAR 10/100 and CelebA datasets, while learning mappings with low empirical Lipschitz constants.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 39  (*Influential: 3*)\n\n#### 5. Norm matters: efficient and accurate normalization schemes in deep networks\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Elad Hoffer, Ron Banner, Itay Golan, Daniel Soudry*\n\n**TL;DR:** A novel view is presented on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective, and a modification to weight-normalization, which improves its performance on large-scale tasks.\n\n**Abstract:** Over the past few years, Batch-Normalization has been commonly used in deep networks, allowing faster training and high performance for a wide variety of applications. However, the reasons behind its merits remained unanswered, with several shortcomings that hindered its use for certain tasks. In this work, we present a novel view on the purpose and function of normalization methods and weight-decay, as tools to decouple weights' norm from the underlying optimized objective. This property highlights the connection between practices such as normalization, weight decay and learning-rate adjustments. We suggest several alternatives to the widely used $L^2$ batch-norm, using normalization in $L^1$ and $L^\\infty$ spaces that can substantially improve numerical stability in low-precision implementations as well as provide computational and memory benefits. We demonstrate that such methods enable the first batch-norm alternative to work for half-precision implementations. Finally, we suggest a modification to weight-normalization, which improves its performance on large-scale tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 174  (*Influential: 15*)\n\n#### 6. Normalizing Non-canonical Turkish Texts Using Machine Translation Approaches\n\n*From Search Query: \"context-dependent normalization\"*\n\n*Talha \u00c7olako\u011flu, U. Sulubacak, A. C. Tantug*\n\n**TL;DR:** This work proposes a fully automated, context-aware machine translation approach with fewer stages of processing in Turkish text normalization, able to surpass the current best-performing system by a large margin.\n\n**Abstract:** With the growth of the social web, user-generated text data has reached unprecedented sizes. Non-canonical text normalization provides a way to exploit this as a practical source of training data for language processing systems. The state of the art in Turkish text normalization is composed of a token level pipeline of modules, heavily dependent on external linguistic resources and manually defined rules. Instead, we propose a fully automated, context-aware machine translation approach with fewer stages of processing. Experiments with various implementations of our approach show that we are able to surpass the current best-performing system by a large margin.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 12  (*Influential: 0*)\n\n#### 7. Artificial Neuronal Ensembles with Learned Context Dependent Gating\n\n*From Search Query: \"context-dependent normalization\"*\n\n*M. Tilley, Michelle Miller, David J. Freedman*\n\n**TL;DR:** Learned Context Dependent Gating (LXDG) is introduced, a method to flexibly allocate and recall `artificial neuronal ensembles', using a particular network structure and a new set of regularization terms to alleviate catastrophic forgetting on continual learning benchmarks.\n\n**Abstract:** Biological neural networks are capable of recruiting different sets of neurons to encode different memories. However, when training artificial neural networks on a set of tasks, typically, no mechanism is employed for selectively producing anything analogous to these neuronal ensembles. Further, artificial neural networks suffer from catastrophic forgetting, where the network's performance rapidly deteriorates as tasks are learned sequentially. By contrast, sequential learning is possible for a range of biological organisms. We introduce Learned Context Dependent Gating (LXDG), a method to flexibly allocate and recall `artificial neuronal ensembles', using a particular network structure and a new set of regularization terms. Activities in the hidden layers of the network are modulated by gates, which are dynamically produced during training. The gates are outputs of networks themselves, trained with a sigmoid output activation. The regularization terms we have introduced correspond to properties exhibited by biological neuronal ensembles. The first term penalizes low gate sparsity, ensuring that only a specified fraction of the network is used. The second term ensures that previously learned gates are recalled when the network is presented with input from previously learned tasks. Finally, there is a regularization term responsible for ensuring that new tasks are encoded in gates that are as orthogonal as possible from previously used ones. We demonstrate the ability of this method to alleviate catastrophic forgetting on continual learning benchmarks. When the new regularization terms are included in the model along with Elastic Weight Consolidation (EWC) it achieves better performance on the benchmark `permuted MNIST' than with EWC alone. The benchmark `rotated MNIST' demonstrates how similar tasks recruit similar neurons to the artificial neuronal ensemble.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Exploring the Compositional Generalization in Context Dependent Text-to-SQL Parsing\n\n*From Search Query: \"context-dependent normalization\"*\n\n*Aiwei Liu, W. Liu, Xuming Hu, Shuang Li, Fukun Ma, Yawen Yang, Lijie Wen*\n\n**TL;DR:** This work proposes a method named \\texttt{p-align} to improve the compositional generalization of Text-to-SQL models and finds that better aligning the previous SQL statements with the input utterance could give models better compositionalgeneralization ability.\n\n**Abstract:** In the context-dependent Text-to-SQL task, the generated SQL statements are refined iteratively based on the user input utterance from each interaction. The input text from each interaction can be viewed as component modifications to the previous SQL statements, which could be further extracted as the modification patterns. Since these modification patterns could also be combined with other SQL statements, the models are supposed to have the compositional generalization to these novel combinations. This work is the first exploration of compositional generalization in context-dependent Text-to-SQL scenarios. To facilitate related studies, we constructed two challenging benchmarks named \\textsc{CoSQL-CG} and \\textsc{SParC-CG} by recombining the modification patterns and existing SQL statements. The following experiments show that all current models struggle on our proposed benchmarks. Furthermore, we found that better aligning the previous SQL statements with the input utterance could give models better compositional generalization ability. Based on these observations, we propose a method named \\texttt{p-align} to improve the compositional generalization of Text-to-SQL models. Further experiments validate the effectiveness of our method. Source code and data are available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 5 related papers from ArXiv\n\n#### 1. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive layer normalization\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 2. Training Integer-Only Deep Recurrent Neural Networks\n\n*From Search Query: \"adaptive layer normalization\"*\n\n*Vahid Partovi Nia, Eyy\u00fcb Sari, Vanessa Courville, Masoud Asgharian*\n\n**Abstract:** Recurrent neural networks (RNN) are the backbone of many text and speech\napplications. These architectures are typically made up of several\ncomputationally complex components such as; non-linear activation functions,\nnormalization, bi-directional dependence and attention. In order to maintain\ngood accuracy, these components are frequently run using full-precision\nfloating-point computation, making them slow, inefficient and difficult to\ndeploy on edge devices. In addition, the complex nature of these operations\nmakes them challenging to quantize using standard quantization methods without\na significant performance drop. We present a quantization-aware training method\nfor obtaining a highly accurate integer-only recurrent neural network (iRNN).\nOur approach supports layer normalization, attention, and an adaptive piecewise\nlinear (PWL) approximation of activation functions, to serve a wide range of\nstate-of-the-art RNNs. The proposed method enables RNN-based language models to\nrun on edge devices with $2\\times$ improvement in runtime, and $4\\times$\nreduction in model size while maintaining similar accuracy as its\nfull-precision counterpart.\n\n**Published:** 2022-12-22T15:22:36Z  (*Updated: 2022-12-22T15:22:36Z*)\n\n\n\n#### 3. Capitalization Normalization for Language Modeling with an Accurate and\n  Efficient Hierarchical RNN Model\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Hao Zhang, You-Chi Cheng, Shankar Kumar, W. Ronny Huang, Mingqing Chen, Rajiv Mathews*\n\n**Abstract:** Capitalization normalization (truecasing) is the task of restoring the\ncorrect case (uppercase or lowercase) of noisy text. We propose a fast,\naccurate and compact two-level hierarchical word-and-character-based recurrent\nneural network model. We use the truecaser to normalize user-generated text in\na Federated Learning framework for language modeling. A case-aware language\nmodel trained on this normalized text achieves the same perplexity as a model\ntrained on text with gold capitalization. In a real user A/B experiment, we\ndemonstrate that the improvement translates to reduced prediction error rates\nin a virtual keyboard application. Similarly, in an ASR language model fusion\nexperiment, we show reduction in uppercase character error rate and word error\nrate.\n\n**Published:** 2022-02-16T16:21:53Z  (*Updated: 2022-02-16T16:21:53Z*)\n\n\n\n#### 4. Strategies for Training Large Vocabulary Neural Language Models\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Welin Chen, David Grangier, Michael Auli*\n\n**Abstract:** Training neural network language models over large vocabularies is still\ncomputationally very costly compared to count-based models such as Kneser-Ney.\nAt the same time, neural language models are gaining popularity for many\napplications such as speech recognition and machine translation whose success\ndepends on scalability. We present a systematic comparison of strategies to\nrepresent and train large vocabularies, including softmax, hierarchical\nsoftmax, target sampling, noise contrastive estimation and self normalization.\nWe further extend self normalization to be a proper estimator of likelihood and\nintroduce an efficient variant of softmax. We evaluate each method on three\npopular benchmarks, examining performance on rare words, the speed/accuracy\ntrade-off and complementarity to Kneser-Ney.\n\n**Published:** 2015-12-15T19:29:01Z  (*Updated: 2015-12-15T19:29:01Z*)\n\n\n\n#### 5. Multilingual Normalization of Temporal Expressions with Masked Language\n  Models\n\n*From Search Query: \"context-dependent normalization\"*\n\n*Lukas Lange, Jannik Str\u00f6tgen, Heike Adel, Dietrich Klakow*\n\n**Abstract:** The detection and normalization of temporal expressions is an important task\nand preprocessing step for many applications. However, prior work on\nnormalization is rule-based, which severely limits the applicability in\nreal-world multilingual settings, due to the costly creation of new rules. We\npropose a novel neural method for normalizing temporal expressions based on\nmasked language modeling. Our multilingual method outperforms prior rule-based\nsystems in many languages, and in particular, for low-resource languages with\nperformance improvements of up to 33 F1 on average compared to the state of the\nart.\n\n**Published:** 2022-05-20T18:34:23Z  (*Updated: 2023-02-10T13:47:56Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive layer normalization\"*\n\n*Junho Kim, Kwanghee Lee, Hyeonwoo Kang, Minjae Kim*\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-25\n\n\n\n#### 2. LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization\n\n*From Search Query: \"adaptive layer normalization\"*\n\n*Jie Chen, Xiangyang Ji, Haoyi Duan, Chang Liu, Kehan Li, Yinhuai Wang, Zhennan Wang, Runyi Yu*\n\n**Abstract:**     Position information is critical for Vision Transformers (VTs) due to the permutation-invariance of self-attention operations. A typical way to introduce position information is adding the absolute Position Embedding (PE) to patch embedding before entering VTs. However, this approach operates the same Layer Normalization (LN) to token embedding and PE, and delivers the same PE to each layer. This results in restricted and monotonic PE across layers, as the shared LN affine parameters are not dedicated to PE, and the PE cannot be adjusted on a per-layer basis. To overcome these limitations, we propose using two independent LNs for token embeddings and PE in each layer, and progressively delivering PE across layers. By implementing this approach, VTs will receive layer-adaptive and hierarchical PE. We name our method as Layer-adaptive Position Embedding, abbreviated as LaPE, which is simple, effective, and robust. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that LaPE significantly outperforms the default PE method. For example, LaPE improves +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. This is remarkable considering LaPE only increases negligible parameters, memory, and computational cost.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 3. CondConv: Conditionally Parameterized Convolutions for Efficient Inference\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Gabriel Bender, Quoc V. Le, Brandon Yang, Jiquan Ngiam*\n\n**Abstract:** Convolutional layers are one of the basic building blocks of modern deep neural networks. One fundamental assumption is that convolutional kernels should be shared for all examples in a dataset. We propose conditionally parameterized convolutions (CondConv), which learn specialized convolutional kernels for each example. Replacing normal convolutions with CondConv enables us to increase the size and capacity of a network, while maintaining efficient inference. We demonstrate that scaling networks with CondConv improves the performance and inference cost trade-off of several existing convolutional neural network architectures on both classification and detection tasks. On ImageNet classification, our CondConv approach applied to EfficientNet-B0 achieves state-of-the-art performance of 78.3% accuracy with only 413M multiply-adds. Code and checkpoints for the CondConv Tensorflow layer and CondConv-EfficientNet models are available at: https://github.com/tensorflow/tpu/tree/master/models/official/efficientnet/condconv.\n\n**Conference:** condconv-conditionally-parameterized\n\n**Published:** 2019-04-10\n\n\n\n#### 4. Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation\n\n*From Search Query: \"efficient normalization neural networks\"*\n\n*Macduff Hughes, Hideto Kazawa, Stephan Gouws, \u0141ukasz Kaiser, Apurva Shah, Zhifeng Chen, Yoshikiyo Kato, Xiaobing Liu, Taku Kudo, Mohammad Norouzi, Jeff Klingner, Greg Corrado, Alex Rudnick, Quoc V. Le, Qin Gao, Nishant Patil, Mike Schuster, George Kurian, Cliff Young, Yonghui Wu, Melvin Johnson, Jason Smith, Jason Riesa, Yuan Cao, Wolfgang Macherey, Wei Wang, Oriol Vinyals, Maxim Krikun, Klaus Macherey, Keith Stevens, Jeffrey Dean*\n\n**Abstract:** Neural Machine Translation (NMT) is an end-to-end learning approach for\nautomated translation, with the potential to overcome many of the weaknesses of\nconventional phrase-based translation systems. Unfortunately, NMT systems are\nknown to be computationally expensive both in training and in translation\ninference. Also, most NMT systems have difficulty with rare words. These issues\nhave hindered NMT's use in practical deployments and services, where both\naccuracy and speed are essential. In this work, we present GNMT, Google's\nNeural Machine Translation system, which attempts to address many of these\nissues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder\nlayers using attention and residual connections. To improve parallelism and\ntherefore decrease training time, our attention mechanism connects the bottom\nlayer of the decoder to the top layer of the encoder. To accelerate the final\ntranslation speed, we employ low-precision arithmetic during inference\ncomputations. To improve handling of rare words, we divide words into a limited\nset of common sub-word units (\"wordpieces\") for both input and output. This\nmethod provides a good balance between the flexibility of \"character\"-delimited\nmodels and the efficiency of \"word\"-delimited models, naturally handles\ntranslation of rare words, and ultimately improves the overall accuracy of the\nsystem. Our beam search technique employs a length-normalization procedure and\nuses a coverage penalty, which encourages generation of an output sentence that\nis most likely to cover all the words in the source sentence. On the WMT'14\nEnglish-to-French and English-to-German benchmarks, GNMT achieves competitive\nresults to state-of-the-art. Using a human side-by-side evaluation on a set of\nisolated simple sentences, it reduces translation errors by an average of 60%\ncompared to Google's phrase-based production system.\n\n**Published:** 2016-09-26\n\n\n\n#### 5. Shallow Fusion of Weighted Finite-State Transducer and Language Model for Text Normalization\n\n*From Search Query: \"context-dependent normalization\"*\n\n*Boris Ginsburg, Yang Zhang, Evelina Bakhturina*\n\n**Abstract:** Text normalization (TN) systems in production are largely rule-based using weighted finite-state transducers (WFST). However, WFST-based systems struggle with ambiguous input when the normalized form is context-dependent. On the other hand, neural text normalization systems can take context into account but they suffer from unrecoverable errors and require labeled normalization datasets, which are hard to collect. We propose a new hybrid approach that combines the benefits of rule-based and neural systems. First, a non-deterministic WFST outputs all normalization candidates, and then a neural language model picks the best one -- similar to shallow fusion for automatic speech recognition. While the WFST prevents unrecoverable errors, the language model resolves contextual ambiguity. The approach is easy to extend and we show it is effective. It achieves comparable or better results than existing state-of-the-art TN models.\n\n**Published:** 2022-03-29\n\n\n\n#### 6. Enhancing Cross-Sectional Currency Strategies by Context-Aware Learning to Rank with Self-Attention\n\n*From Search Query: \"context-dependent normalization\"*\n\n*Stephen Roberts, Stefan Zohren, Bryan Lim, Daniel Poh*\n\n**Abstract:** The performance of a cross-sectional currency strategy depends crucially on accurately ranking instruments prior to portfolio construction. While this ranking step is traditionally performed using heuristics, or by sorting the outputs produced by pointwise regression or classification techniques, strategies using Learning to Rank algorithms have recently presented themselves as competitive and viable alternatives. Although the rankers at the core of these strategies are learned globally and improve ranking accuracy on average, they ignore the differences between the distributions of asset features over the times when the portfolio is rebalanced. This flaw renders them susceptible to producing sub-optimal rankings, possibly at important periods when accuracy is actually needed the most. For example, this might happen during critical risk-off episodes, which consequently exposes the portfolio to substantial, unwanted drawdowns. We tackle this shortcoming with an analogous idea from information retrieval: that a query's top retrieved documents or the local ranking context provide vital information about the query's own characteristics, which can then be used to refine the initial ranked list. In this work, we use a context-aware Learning-to-rank model that is based on the Transformer architecture to encode top/bottom ranked assets, learn the context and exploit this information to re-rank the initial results. Backtesting on a slate of 31 currencies, our proposed methodology increases the Sharpe ratio by around 30% and significantly enhances various performance metrics. Additionally, this approach also improves the Sharpe ratio when separately conditioning on normal and risk-off market states.\n\n**Published:** 2021-05-20\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on adaptive normalization, memory efficiency, and hardware optimization, here are some key findings and suggestions based on the provided keywords and details:\n\n## Adaptive Normalization Techniques\n\n### Context-Dependent Parameter Adaptation\n- The paper on \"Enhancing Neural Network Representations with Prior Knowledge\" introduces Context Normalization (CN), Context Normalization - Extended (CN-X), and Adaptive Context Normalization (ACN). These methods organize data into predefined contexts or dynamically build contexts in the latent space, which can be seen as context-dependent parameter adaptation. This approach can help in improving the model's ability to handle diverse data distributions and enhance training stability.\n\n### Position-Aware Normalization\n- Adaptive Layer Normalization (AdaLN) is mentioned in the context of molecular graph features, but the concept can be generalized. AdaLN adjusts normalization parameters based on numerical and categorical property constraints, which can be adapted to include position-aware scaling factors. This could be integrated with existing normalization techniques like RMSNorm to enhance the model's performance and stability.\n\n### Dynamic Parameter Adjustment\n- The concept of Adaptive Context Normalization (ACN) involves dynamically building contexts during training, which is a form of dynamic parameter adjustment. This method can be particularly useful in autoregressive models where the context changes with each input sequence.\n\n## Memory-Efficient Normalization\n\n### Parameter Re-parameterization Methods\n- The paper on \"SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization\" discusses progressive re-parameterization of batch normalization, which can be applied to other normalization techniques to reduce memory usage. This method involves gradually updating the normalization parameters, which can help in reducing the computational overhead and memory requirements.\n\n### Gradient Computation Optimization\n- Optimizing gradient computation is crucial for memory efficiency. Techniques like those described in \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\" can be adapted for normalization layers. This involves optimizing the computation of gradients to reduce memory usage and improve training efficiency.\n\n### State Compression Techniques\n- State compression methods can be applied to normalization layers to reduce memory usage. For example, compressing the state of normalization parameters or using sparse representations can help in reducing the memory footprint of the model.\n\n## Hardware-Optimized Normalization\n\n### IO-Aware Implementation\n- \"FlashAttention\" provides insights into IO-aware computation strategies, which can be applied to normalization layers. This involves optimizing the data access patterns to minimize IO overhead, which is crucial for large-scale models.\n\n### Cache-Friendly Normalization\n- Cache-friendly implementation details are important for optimizing the performance of normalization layers. Techniques such as blocking and tiling can be used to ensure that the normalization computations fit within the cache, reducing the number of cache misses and improving performance.\n\n### Parallel Processing Optimization\n- Parallel processing capabilities can be leveraged to optimize normalization layers. This can involve distributing the normalization computations across multiple GPUs or using parallel processing frameworks to speed up the computation.\n\n## Empirical Comparisons and Mathematical Formulations\n\n- For empirical comparisons, it would be beneficial to compare the proposed adaptive normalization techniques (like CN, CN-X, and ACN) with existing methods such as RMSNorm and Batch Normalization. This comparison should include metrics on training stability, convergence, and overall model performance.\n- Mathematically, the formulations of these normalization techniques should be derived and analyzed. For instance, the Hessian analysis provided in \"What Does It Mean to Be a Transformer?\" can offer insights into the optimization landscape of Transformers and how different normalization techniques affect this landscape.\n\nBy integrating these findings into the design of the autoregressive language model, the researcher can potentially achieve better performance, robustness, efficiency, and scalability.\n\n### Next Steps\n\n1. **Implement and Test Adaptive Normalization Techniques**: Implement CN, CN-X, and ACN within the autoregressive model and evaluate their impact on training stability and convergence.\n2. **Optimize Memory Usage**: Apply progressive re-parameterization and state compression techniques to reduce memory usage and optimize gradient computation.\n3. **Hardware Optimization**: Implement IO-aware and cache-friendly normalization layers and leverage parallel processing to optimize performance.\n4. **Empirical Comparisons**: Conduct thorough empirical comparisons with existing normalization methods to validate the improvements.\n\nThese steps should help in achieving the goal of improving the autoregressive language model design.\n",
            "analysis": "Initial Analysis of AdaptiveNormGPT Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Adaptive Normalization Mechanism:\n- Novel context-dependent parameter adaptation approach\n- Position-dependent scaling factors\n- Integration with existing RMSNorm\n- Impact on training stability and convergence\n\nb) Memory Efficiency Claims:\n- Progressive parameter re-parameterization technique\n- State compression methods\n- Gradient computation optimization\n- Comparison with existing efficient normalization methods\n\nc) Hardware Optimization:\n- IO-aware computation strategies\n- Cache-friendly implementation details\n- Parallel processing capabilities\n\n2. Investigation Plan:\n\nPhase 1: Normalization Techniques\n- Research existing adaptive normalization methods\n- Analyze context-dependent parameter adaptation approaches\n- Study position-aware normalization techniques\n\nPhase 2: Memory Efficiency\n- Investigate parameter re-parameterization methods\n- Research state compression techniques\n- Study gradient computation optimization strategies\n\nPhase 3: Hardware Efficiency\n- Research IO-aware normalization implementations\n- Analyze cache-friendly normalization techniques\n- Study parallel processing in normalization layers\n\nInitial Findings:\n\n1. Novelty Assessment:\n- The combination of context-aware parameter adaptation with progressive re-parameterization appears novel\n- The integration of position-dependent scaling with RMSNorm is unique\n- Hardware optimization approach seems aligned with recent trends\n\n2. Potential Impact:\n- Accuracy: Could improve model performance through better normalization\n- Robustness: Context-awareness may enhance stability\n- Efficiency: Memory optimizations look promising\n- Scalability: Hardware-aware design could aid scaling\n\n3. Comparison with Previous Designs:\n- Different from ODEAdaptGPT's continuous normalization approach\n- Distinct from StateHierGPT's hierarchical normalization\n- Novel compared to dynamicgatedconv's convolution-based approach\n\n4. Research Alignment:\n- Aligns with trend toward efficient, adaptive architectures\n- Consistent with focus on hardware-aware design\n- Matches interest in memory-efficient implementations\n\nReferences found so far:\n\n1. \"Root Mean Square Layer Normalization\" (Zhang & Sennrich, 2019)\n2. \"SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization\" (Guo et al., 2024)\n3. \"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\" (Dao et al., 2022)\n\nNext steps require deeper investigation into:\n1. Recent adaptive normalization techniques\n2. Memory-efficient normalization implementations\n3. Hardware-optimized normalization approaches"
        },
        {
            "ready": false,
            "query": "\"transformer normalization techniques\", \"efficient language model normalization\", \"adaptive normalization transformers\"",
            "detail": "Search for papers discussing:\n\n1. Recent advances in transformer normalization:\n- Specific normalization techniques for language models\n- Comparisons of different normalization approaches\n- Impact on model performance and stability\n\n2. Memory-efficient normalization in transformers:\n- Implementation details and optimizations\n- Memory-performance trade-offs\n- Practical deployment considerations\n\n3. Hardware-specific optimizations:\n- GPU/TPU-optimized implementations\n- Memory access patterns\n- Parallel processing strategies\n\nFocus on:\n- Concrete implementation details\n- Performance comparisons\n- Memory efficiency metrics\n- Hardware utilization statistics",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n\n1. Recent advances in transformer normalization:\n- Specific normalization techniques for language models\n- Comparisons of different normalization approaches\n- Impact on model performance and stability\n\n2. Memory-efficient normalization in transformers:\n- Implementation details and optimizations\n- Memory-performance trade-offs\n- Practical deployment considerations\n\n3. Hardware-specific optimizations:\n- GPU/TPU-optimized implementations\n- Memory access patterns\n- Parallel processing strategies\n\nFocus on:\n- Concrete implementation details\n- Performance comparisons\n- Memory efficiency metrics\n- Hardware utilization statistics\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.63)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.63)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 2. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.26)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 50/56 (Score: 0.26)*\n\n```\narXiv preprint arXiv:1609.08144, 2016. R. Xiong, Y. Yang, D. He, K. Zheng, S. Zheng, C. Xing, H. Zhang, Y. Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. In International Conference on Machine Learning, pages 10524-10533. PMLR, 2020. S. Zhai, W. Talbott, N. Srivastava, C. Huang, H. Goh, R. Zhang, and J. Susskind. An attention free transformer. arXiv preprint arXiv:2105.14103, 2021. B. Zhang and R. Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019. L. Zhu, B. Liao, Q. Zhang, X. Wang, W. Liu, and X. Wang. Vision mamba: Efficient visual representation learning with bidirectional state space model.\n```\n\n#### 3. Normalized Attention Without Probability Cage (Avg. Score: 0.20)\n\n*Oliver Richter, Roger Wattenhofer*\n\n**Published in:** arXiv.org (2020)\t**Cited by** 18  (*Influential: 2*)\n\n**TL;DR:** This work highlights the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors and proposes to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture.\n\n**Abstract:** Attention architectures are widely used; they recently gained renewed popularity with Transformers yielding a streak of state of the art results. Yet, the geometrical implications of softmax-attention remain largely unexplored. In this work we highlight the limitations of constraining attention weights to the probability simplex and the resulting convex hull of value vectors. We show that Transformers are sequence length dependent biased towards token isolation at initialization and contrast Transformers to simple max- and sum-pooling - two strong baselines rarely reported. We propose to replace the softmax in self-attention with normalization, yielding a hyperparameter and data-bias robust, generally applicable architecture. We support our insights with empirical results from more than 25,000 trained models. All results and implementations are made available.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.20)*\n\n```\nIn Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence d'Alch\u00e9-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, 8-14 December 2019, Vancouver, BC, Canada, pages $68-80,2019$. [24] Damian Pascual, Gino Brunner, and Roger Wattenhofer. Telling bert's full story: from local attention to global aggregation. arXiv preprint arXiv:2004.05916, 2020. [25] Ofir Press, Noah A Smith, and Omer Levy. Improving transformer models by reordering their sublayers. arXiv preprint arXiv:1911.03864, 2019. [26] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training. 2018. [27] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. [28] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683, 2019. [29] Tim Salimans and Diederik P. Kingma. Weight normalization: A simple reparameterization to accelerate training of deep neural networks. In Daniel D. Lee, Masashi Sugiyama, Ulrike von Luxburg, Isabelle Guyon, and Roman Garnett, editors, Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, page 901, 2016. [30] Nimrod Segol and Yaron Lipman. On universal equivariant set networks. In International Conference on Learning Representations, 2020. [31] Dmitry Ulyanov, Andrea Vedaldi, and Victor Lempitsky. Instance normalization: The missing ingredient for fast stylization. arXiv preprint arXiv:1607.08022, 2016. [32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von Luxburg, Samy Bengio, Hanna M.\n```\n\n#### 4. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.19)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 23/30 (Score: 0.19)*\n\n```\narXiv preprint arXiv:1803.07416, 2018. Sinong Wang, Belinda Z. Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity, 2020. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, et al. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 conference on empirical methods in natural language processing: system demonstrations, pages 38-45, 2020. Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, and Song Han. Smoothquant: Accurate and efficient post-training quantization for large language models. arXiv preprint arXiv:2211.10438, 2022. Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, and Tieyan Liu. On layer normalization in the transformer architecture.\n```\n\n#### 5. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.15)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 63/86 (Score: 0.15)*\n\n```\nIn: The International Conference on Machine Learning (ICML). PMLR. 2021, pp. 9355-9366. [90] Noam Shazeer. \"Fast Transformer Decoding: One Write-head is All You Need\". In: arXiv preprint arXiv:1911.02150 (2019). [91] Sam Shleifer, Jason Weston, and Myle Ott. \"NormFormer: Improved Transformer Pretraining with Extra Normalization\". In: arXiv preprint arXiv:2110.09456 (2021). [92] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. \"MegatronLM: Training Multi-Billion Parameter Language Models Using Model Parallelism\". In: arXiv preprint arXiv:1909.08053 (2019). [93] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. \"Simplified State Space Layers for Sequence Modeling\". In: The International Conference on Learning Representations (ICLR). 2023. [94] Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. \"Roformer: Enhanced Transformer with Rotary Position Embedding\". In: arXiv preprint arXiv:2104.09864 (2021). [95] Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, and Furu Wei. \"Retentive network: A successor to transformer for large language models\". In: arXiv preprint arXiv:2307.08621 (2023). [96] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. \"Efficient Transformers: A Survey\". In: ACM Computing Surveys 55.6 (2022), pp. 1-28. [97] Chameleon Team. \"Chameleon: Mixed-Modal Early-Fusion Foundation Models\".\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"transformer normalization techniques\", \"efficient language model normalization\", \"adaptive normalization transformers\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Zixuan Jiang, Jiaqi Gu, Hanqing Zhu, D. Pan*\n\n**TL;DR:** The equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference is formally established, implying that Pre- LN Transformers can be substituted with Pre-(C)RMSorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement.\n\n**Abstract:** Transformers have achieved great success in machine learning applications. Normalization techniques, such as Layer Normalization (LayerNorm, LN) and Root Mean Square Normalization (RMSNorm), play a critical role in accelerating and stabilizing the training of Transformers. While LayerNorm recenters and rescales input vectors, RMSNorm only rescales the vectors by their RMS value. Despite being more computationally efficient, RMSNorm may compromise the representation ability of Transformers. There is currently no consensus regarding the preferred normalization technique, as some models employ LayerNorm while others utilize RMSNorm, especially in recent large language models. It is challenging to convert Transformers with one normalization to the other type. While there is an ongoing disagreement between the two normalization types, we propose a solution to unify two mainstream Transformer architectures, Pre-LN and Pre-RMSNorm Transformers. By removing the inherent redundant mean information in the main branch of Pre-LN Transformers, we can reduce LayerNorm to RMSNorm, achieving higher efficiency. We further propose the Compressed RMSNorm (CRMSNorm) and Pre-CRMSNorm Transformer based on a lossless compression of the zero-mean vectors. We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference. It implies that Pre-LN Transformers can be substituted with Pre-(C)RMSNorm counterparts at almost no cost, offering the same arithmetic functionality along with free efficiency improvement. Experiments demonstrate that we can reduce the training and inference time of Pre-LN Transformers by 1% - 10%.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 2. MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Neo Wei Ming, Zhehui Wang, Cheng Liu, R. Goh, Tao Luo*\n\n**TL;DR:** MA-BERT is proposed, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss and enables hardware-friendly processing on various computing engines, including CPUs and GPUs.\n\n**Abstract:** Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 3. On Layer Normalization in the Transformer Architecture\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shuxin Zheng, Chen Xing, Huishuai Zhang, Yanyan Lan, Liwei Wang, Tie-Yan Liu*\n\n**TL;DR:** It is proved with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large and using a large learning rate makes the training unstable.\n\n**Abstract:** The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyperparameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 786  (*Influential: 47*)\n\n#### 4. Test-Time Distribution Normalization for Contrastively Learned Visual-language Models\n\n*From Search Query: \"efficient language model normalization\"*\n\n*Yi Zhou, Juntao Ren, Fengyu Li, Ramin Zabih, S. Lim*\n\n**TL;DR:** Distribution Normalization (DN) is proposed, where the mean representation of a batch of test samples is approximate and used to represent what would be analogous to negative samples in the InfoNCE loss, which requires no retraining or fine-tuning and can be effortlessly applied during inference.\n\n**Abstract:** Advances in the field of vision-language contrastive learning have made it possible for many downstream applications to be carried out efficiently and accurately by simply taking the dot product between image and text representations. One of the most representative approaches proposed recently known as CLIP has garnered widespread adoption due to its effectiveness. CLIP is trained with an InfoNCE loss that takes into account both positive and negative samples to help learn a much more robust representation space. This paper reveals that the common downstream practice of taking a dot product is only a zeroth-order approximation of the optimization goal, resulting in a loss of information during test-time. Intuitively, since the model has been optimized based on the InfoNCE loss, test-time procedures should also be in alignment. The question lies in how one can retrieve any semblance of negative samples information during inference in a computationally efficient way. To this end, we propose Distribution Normalization (DN), where we approximate the mean representation of a batch of test samples and use such a mean to represent what would be analogous to negative samples in the InfoNCE loss. DN requires no retraining or fine-tuning and can be effortlessly applied during inference. Extensive experiments on a wide variety of downstream tasks exhibit a clear advantage of DN over the dot product on top of other existing test-time augmentation methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 2*)\n\n#### 5. Long-Short Transformer: Efficient Transformers for Language and Vision\n\n*From Search Query: \"efficient language model normalization\"*\n\n*Chen Zhu, Wei Ping, Chaowei Xiao, Mohammad Shoeybi, T. Goldstein, Anima Anandkumar, Bryan Catanzaro*\n\n**TL;DR:** This paper proposes Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks, and proposes a dual normalization strategy to account for the scale mismatch between the two attention mechanisms.\n\n**Abstract:** Transformers have achieved success in both language and vision domains. However, it is prohibitively expensive to scale them to long sequences such as long documents or high-resolution images, because self-attention mechanism has quadratic time and memory complexities with respect to the input sequence length. In this paper, we propose Long-Short Transformer (Transformer-LS), an efficient self-attention mechanism for modeling long sequences with linear complexity for both language and vision tasks. It aggregates a novel long-range attention with dynamic projection to model distant correlations and a short-term attention to capture fine-grained local correlations. We propose a dual normalization strategy to account for the scale mismatch between the two attention mechanisms. Transformer-LS can be applied to both autoregressive and bidirectional models without additional complexity. Our method outperforms the state-of-the-art models on multiple tasks in language and vision domains, including the Long Range Arena benchmark, autoregressive language modeling, and ImageNet classification. For instance, Transformer-LS achieves 0.97 test BPC on enwik8 using half the number of parameters than previous method, while being faster and is able to handle 3x as long sequences compared to its full-attention version on the same hardware. On ImageNet, it can obtain the state-of-the-art results (e.g., a moderate size of 55.8M model solely trained on 224x224 ImageNet-1K can obtain Top-1 accuracy 84.1%), while being more scalable on high-resolution images. The source code and models are released at https://github.com/NVIDIA/transformer-ls .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 118  (*Influential: 14*)\n\n#### 6. HybridBERT - Making BERT Pretraining More Efficient Through Hybrid Mixture of Attention Mechanisms\n\n*From Search Query: \"efficient language model normalization\"*\n\n*Gokul Srinivasagan, Simon Ostermann*\n\n**TL;DR:** This work proposes two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization, and shows that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline.\n\n**Abstract:** Pretrained transformer-based language models have produced state-of-the-art performance in most natural language understanding tasks. These models undergo two stages of training: pretraining on a huge corpus of data and fine-tuning on a specific downstream task. The pretraining phase is extremely compute-intensive and requires several high-performance computing devices like GPUs and several days or even months of training, but it is crucial for the model to capture global knowledge and also has a significant impact on the fine-tuning task. This is a major roadblock for researchers without access to sophisticated computing resources. To overcome this challenge, we propose two novel hybrid architectures called HybridBERT (HBERT), which combine self-attention and additive attention mechanisms together with sub-layer normalization. We introduce a computing budget to the pretraining phase, limiting the training time and usage to a single GPU. We show that HBERT attains twice the pretraining accuracy of a vanilla-BERT baseline. We also evaluate our proposed models on two downstream tasks, where we outperform BERT-base while accelerating inference. Moreover, we study the effect of weight initialization with a limited pretraining budget. The code and models are publicly available at: www.github.com/gokulsg/HBERT/.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. MVSFormer++: Revealing the Devil in Transformer's Details for Multi-View Stereo\n\n*From Search Query: \"adaptive normalization transformers\"*\n\n*Chenjie Cao, Xinlin Ren, Yanwei Fu*\n\n**TL;DR:** This paper introduces MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline, and achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Abstract:** Recent advancements in learning-based Multi-View Stereo (MVS) methods have prominently featured transformer-based models with attention mechanisms. However, existing approaches have not thoroughly investigated the profound influence of transformers on different MVS modules, resulting in limited depth estimation capabilities. In this paper, we introduce MVSFormer++, a method that prudently maximizes the inherent characteristics of attention to enhance various components of the MVS pipeline. Formally, our approach involves infusing cross-view information into the pre-trained DINOv2 model to facilitate MVS learning. Furthermore, we employ different attention mechanisms for the feature encoder and cost volume regularization, focusing on feature and spatial aggregations respectively. Additionally, we uncover that some design details would substantially impact the performance of transformer modules in MVS, including normalized 3D positional encoding, adaptive attention scaling, and the position of layer normalization. Comprehensive experiments on DTU, Tanks-and-Temples, BlendedMVS, and ETH3D validate the effectiveness of the proposed method. Notably, MVSFormer++ achieves state-of-the-art performance on the challenging DTU and Tanks-and-Temples benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 8. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization transformers\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 9. On Separate Normalization in Self-supervised Transformers\n\n*From Search Query: \"adaptive normalization transformers\"*\n\n*Xiaohui Chen, Yinkai Wang, Yuanqi Du, S. Hassoun, Liping Liu*\n\n**TL;DR:** It is empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space.\n\n**Abstract:** Self-supervised training methods for transformers have demonstrated remarkable performance across various domains. Previous transformer-based models, such as masked autoencoders (MAE), typically utilize a single normalization layer for both the [CLS] symbol and the tokens. We propose in this paper a simple modification that employs separate normalization layers for the tokens and the [CLS] symbol to better capture their distinct characteristics and enhance downstream task performance. Our method aims to alleviate the potential negative effects of using the same normalization statistics for both token types, which may not be optimally aligned with their individual roles. We empirically show that by utilizing a separate normalization layer, the [CLS] embeddings can better encode the global contextual information and are distributed more uniformly in its anisotropic space. When replacing the conventional normalization layer with the two separate layers, we observe an average 2.7% performance improvement over the image, natural language, and graph domains.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. Robust Benchmarking for Machine Learning of Clinical Entity Extraction\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Monica Agrawal, Chloe O'Connell, Yasmin Fatemi, Ariel Levy, David Sontag*\n\n**Abstract:** Clinical studies often require understanding elements of a patient's\nnarrative that exist only in free text clinical notes. To transform notes into\nstructured data for downstream use, these elements are commonly extracted and\nnormalized to medical vocabularies. In this work, we audit the performance of\nand indicate areas of improvement for state-of-the-art systems. We find that\nhigh task accuracies for clinical entity normalization systems on the 2019 n2c2\nShared Task are misleading, and underlying performance is still brittle.\nNormalization accuracy is high for common concepts (95.3%), but much lower for\nconcepts unseen in training data (69.3%). We demonstrate that current\napproaches are hindered in part by inconsistencies in medical vocabularies,\nlimitations of existing labeling schemas, and narrow evaluation techniques. We\nreformulate the annotation framework for clinical entity extraction to factor\nin these issues to allow for robust end-to-end system benchmarking. We evaluate\nconcordance of annotations from our new framework between two annotators and\nachieve a Jaccard similarity of 0.73 for entity recognition and an agreement of\n0.83 for entity normalization. We propose a path forward to address the\ndemonstrated need for the creation of a reference standard to spur method\ndevelopment in entity recognition and normalization.\n\n**Published:** 2020-07-31T15:14:05Z  (*Updated: 2020-07-31T15:14:05Z*)\n\n\n\n#### 2. Understanding the Failure of Batch Normalization for Transformers in NLP\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Jiaxi Wang, Ji Wu, Lei Huang*\n\n**Abstract:** Batch Normalization (BN) is a core and prevalent technique in accelerating\nthe training of deep neural networks and improving the generalization on\nComputer Vision (CV) tasks. However, it fails to defend its position in Natural\nLanguage Processing (NLP), which is dominated by Layer Normalization (LN). In\nthis paper, we are trying to answer why BN usually performs worse than LN in\nNLP tasks with Transformer models. We find that the inconsistency between\ntraining and inference of BN is the leading cause that results in the failure\nof BN in NLP. We define Training Inference Discrepancy (TID) to quantitatively\nmeasure this inconsistency and reveal that TID can indicate BN's performance,\nsupported by extensive experiments, including image classification, neural\nmachine translation, language modeling, sequence labeling, and text\nclassification tasks. We find that BN can obtain much better test performance\nthan LN when TID keeps small through training. To suppress the explosion of\nTID, we propose Regularized BN (RBN) that adds a simple regularization term to\nnarrow the gap between batch statistics and population statistics of BN. RBN\nimproves the performance of BN consistently and outperforms or is on par with\nLN on 17 out of 20 settings, involving ten datasets and two common variants of\nTransformer\n  Our code is available at https://github.com/wjxts/RegularizedBN.\n\n**Published:** 2022-10-11T05:18:47Z  (*Updated: 2022-10-11T05:18:47Z*)\n\n\n\n#### 3. Capitalization Normalization for Language Modeling with an Accurate and\n  Efficient Hierarchical RNN Model\n\n*From Search Query: \"efficient language model normalization\"*\n\n*Hao Zhang, You-Chi Cheng, Shankar Kumar, W. Ronny Huang, Mingqing Chen, Rajiv Mathews*\n\n**Abstract:** Capitalization normalization (truecasing) is the task of restoring the\ncorrect case (uppercase or lowercase) of noisy text. We propose a fast,\naccurate and compact two-level hierarchical word-and-character-based recurrent\nneural network model. We use the truecaser to normalize user-generated text in\na Federated Learning framework for language modeling. A case-aware language\nmodel trained on this normalized text achieves the same perplexity as a model\ntrained on text with gold capitalization. In a real user A/B experiment, we\ndemonstrate that the improvement translates to reduced prediction error rates\nin a virtual keyboard application. Similarly, in an ASR language model fusion\nexperiment, we show reduction in uppercase character error rate and word error\nrate.\n\n**Published:** 2022-02-16T16:21:53Z  (*Updated: 2022-02-16T16:21:53Z*)\n\n\n\n#### 4. Full Stack Optimization of Transformer Inference: a Survey\n\n*From Search Query: \"adaptive normalization transformers\"*\n\n*Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, Amir Gholami*\n\n**Abstract:** Recent advances in state-of-the-art DNN architecture design have been moving\ntoward Transformer models. These models achieve superior accuracy across a wide\nrange of applications. This trend has been consistent over the past several\nyears since Transformer models were originally introduced. However, the amount\nof compute and bandwidth required for inference of recent Transformer models is\ngrowing at a significant rate, and this has made their deployment in\nlatency-sensitive applications challenging. As such, there has been an\nincreased focus on making Transformer models more efficient, with methods that\nrange from changing the architecture design, all the way to developing\ndedicated domain-specific accelerators. In this work, we survey different\napproaches for efficient Transformer inference, including: (i) analysis and\nprofiling of the bottlenecks in existing Transformer architectures and their\nsimilarities and differences with previous convolutional models; (ii)\nimplications of Transformer architecture on hardware, including the impact of\nnon-linear operations such as Layer Normalization, Softmax, and GELU, as well\nas linear operations, on hardware design; (iii) approaches for optimizing a\nfixed Transformer architecture; (iv) challenges in finding the right mapping\nand scheduling of operations for Transformer models; and (v) approaches for\noptimizing Transformer models by adapting the architecture using neural\narchitecture search. Finally, we perform a case study by applying the surveyed\noptimizations on Gemmini, the open-source, full-stack DNN accelerator\ngenerator, and we show how each of these approaches can yield improvements,\ncompared to previous benchmark results on Gemmini. Among other things, we find\nthat a full-stack co-design approach with the aforementioned methods can result\nin up to 88.7x speedup with a minimal performance degradation for Transformer\ninference.\n\n**Published:** 2023-02-27T18:18:13Z  (*Updated: 2023-02-27T18:18:13Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Patrick Legresley, Mostofa Patwary, Jared Casper, Mohammad Shoeybi, Raul Puri, Bryan Catanzaro*\n\n**Abstract:** Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).\n\n**Published:** 2019-09-17\n\n\n\n#### 2. Mastering Diverse Domains through World Models\n\n*From Search Query: \"transformer normalization techniques\"*\n\n*Timothy Lillicrap, Jimmy Ba, Jurgis Pasukonis, Danijar Hafner*\n\n**Abstract:** Developing a general algorithm that learns to solve tasks across a wide range of applications has been a fundamental challenge in artificial intelligence. Although current reinforcement learning algorithms can be readily applied to tasks similar to what they have been developed for, configuring them for new application domains requires significant human expertise and experimentation. We present DreamerV3, a general algorithm that outperforms specialized methods across over 150 diverse tasks, with a single configuration. Dreamer learns a model of the environment and improves its behavior by imagining future scenarios. Robustness techniques based on normalization, balancing, and transformations enable stable learning across domains. Applied out of the box, Dreamer is the first algorithm to collect diamonds in Minecraft from scratch without human data or curricula. This achievement has been posed as a significant challenge in artificial intelligence that requires exploring farsighted strategies from pixels and sparse rewards in an open world. Our work allows solving challenging control problems without extensive experimentation, making reinforcement learning broadly applicable.\n\n**Published:** 2023-01-10\n\n\n\n#### 3. Sigmoid Loss for Language Image Pre-Training\n\n*From Search Query: \"efficient language model normalization\"*\n\n*Lucas Beyer, Alexander Kolesnikov, Basil Mustafa, Xiaohua Zhai*\n\n**Abstract:** We propose a simple pairwise Sigmoid loss for Language-Image Pre-training (SigLIP). Unlike standard contrastive learning with softmax normalization, the sigmoid loss operates solely on image-text pairs and does not require a global view of the pairwise similarities for normalization. The sigmoid loss simultaneously allows further scaling up the batch size, while also performing better at smaller batch sizes. Combined with Locked-image Tuning, with only four TPUv4 chips, we train a SigLiT model that achieves 84.5% ImageNet zero-shot accuracy in two days. The disentanglement of the batch size from the loss further allows us to study the impact of examples vs pairs and negative to positive ratio. Finally, we push the batch size to the extreme, up to one million, and find that the benefits of growing batch size quickly diminish, with a more reasonable batch size of 32k being sufficient. We release our models at https://github.com/google-research/big_vision and hope our research motivates further explorations in improving the quality and efficiency of language-image pre-training.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-03-27\n\n\n\n#### 4. I-BERT: Integer-only BERT Quantization\n\n*From Search Query: \"efficient language model normalization\"*\n\n*Kurt Keutzer, Michael W. Mahoney, Zhewei Yao, Amir Gholami, Sehoon Kim*\n\n**Abstract:** Transformer based models, like BERT and RoBERTa, have achieved state-of-the-art results in many Natural Language Processing tasks. However, their memory footprint, inference latency, and power consumption are prohibitive efficient inference at the edge, and even at the data center. While quantization can be a viable solution for this, previous work on quantizing Transformer based models use floating-point arithmetic during inference, which cannot efficiently utilize integer-only logical units such as the recent Turing Tensor Cores, or traditional integer-only ARM processors. In this work, we propose I-BERT, a novel quantization scheme for Transformer based models that quantizes the entire inference with integer-only arithmetic. Based on lightweight integer-only approximation methods for nonlinear operations, e.g., GELU, Softmax, and Layer Normalization, I-BERT performs an end-to-end integer-only BERT inference without any floating point calculation. We evaluate our approach on GLUE downstream tasks using RoBERTa-Base/Large. We show that for both cases, I-BERT achieves similar (and slightly higher) accuracy as compared to the full-precision baseline. Furthermore, our preliminary implementation of I-BERT shows a speedup of 2.4-4.0x for INT8 inference on a T4 GPU system as compared to FP32 inference. The framework has been developed in PyTorch and has been open-sourced.\n\n**Published:** 2021-01-05\n\n\n\n#### 5. LaPE: Layer-adaptive Position Embedding for Vision Transformers with Independent Layer Normalization\n\n*From Search Query: \"adaptive normalization transformers\"*\n\n*Jie Chen, Xiangyang Ji, Haoyi Duan, Chang Liu, Kehan Li, Yinhuai Wang, Zhennan Wang, Runyi Yu*\n\n**Abstract:**     Position information is critical for Vision Transformers (VTs) due to the permutation-invariance of self-attention operations. A typical way to introduce position information is adding the absolute Position Embedding (PE) to patch embedding before entering VTs. However, this approach operates the same Layer Normalization (LN) to token embedding and PE, and delivers the same PE to each layer. This results in restricted and monotonic PE across layers, as the shared LN affine parameters are not dedicated to PE, and the PE cannot be adjusted on a per-layer basis. To overcome these limitations, we propose using two independent LNs for token embeddings and PE in each layer, and progressively delivering PE across layers. By implementing this approach, VTs will receive layer-adaptive and hierarchical PE. We name our method as Layer-adaptive Position Embedding, abbreviated as LaPE, which is simple, effective, and robust. Extensive experiments on image classification, object detection, and semantic segmentation demonstrate that LaPE significantly outperforms the default PE method. For example, LaPE improves +1.06% for CCT on CIFAR100, +1.57% for DeiT-Ti on ImageNet-1K, +0.7 box AP and +0.5 mask AP for ViT-Adapter-Ti on COCO, and +1.37 mIoU for tiny Segmenter on ADE20K. This is remarkable considering LaPE only increases negligible parameters, memory, and computational cost.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 6. Semantic Image Synthesis with Spatially-Adaptive Normalization\n\n*From Search Query: \"adaptive normalization transformers\"*\n\n*Jun-Yan Zhu, Ting-Chun Wang, Ming-Yu Liu, Taesung Park*\n\n**Abstract:** We propose spatially-adaptive normalization, a simple but effective layer for synthesizing photorealistic images given an input semantic layout. Previous methods directly feed the semantic layout as input to the deep network, which is then processed through stacks of convolution, normalization, and nonlinearity layers. We show that this is suboptimal as the normalization layers tend to ``wash away'' semantic information. To address the issue, we propose using the input layout for modulating the activations in normalization layers through a spatially-adaptive, learned transformation. Experiments on several challenging datasets demonstrate the advantage of the proposed method over existing approaches, regarding both visual fidelity and alignment with input layouts. Finally, our model allows user control over both semantic and style. Code is available at https://github.com/NVlabs/SPADE .\n\n**Conference:** semantic-image-synthesis-with-spatially-1\n\n**Published:** 2019-03-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient and adaptive normalization techniques, here are some key points and references that align with your requirements:\n\n## Recent Advances in Transformer Normalization\n\n### Specific Normalization Techniques\n- **Layer Normalization and RMSNorm**: Traditional LayerNorm normalizes inputs across features, while RMSNorm is a newer approach that has been shown to offer improved performance in certain architectures. RMSNorm normalizes the input by the root mean square of the input, which can be more effective in some cases.\n- **Normalized Transformer (nGPT)**: This architecture normalizes all vectors involved in embeddings, MLPs, attention matrices, and hidden states to have a unit norm. This approach accelerates learning and reduces the number of training steps needed to reach comparable accuracy.\n\n### Comparisons of Different Normalization Approaches\n- **AdaNorm, SALN, and DLN**: AdaNorm replaces bias and gain with a new transformation function, showing better results than LayerNorm on several datasets. SALN aligns gain and bias according to extracted style information, and DLN provides insights into speaker/context adaptation. These adaptive normalization approaches have been compared and shown to offer improvements over traditional normalization methods[Key References: Xu et al., 2019; Min et al., 2021; Kim et al., 2017].\n\n### Impact on Model Performance and Stability\n- **Stability and Convergence**: Normalization techniques like those proposed by Gilmer et al. (2023) have effectively addressed training instabilities in large-scale models, enabling smoother and more stable training processes. The nGPT architecture also demonstrates accelerated convergence and improved stability by normalizing vectors to unit norm.\n\n## Memory-Efficient Normalization in Transformers\n\n### Implementation Details and Optimizations\n- **Progressive Re-parameterization**: This method involves progressively updating parameters to reduce memory overhead. Structured approaches to parameter sharing, such as embedding layer sharing in models like Gemma and Qwen, can maintain performance while reducing the number of parameters.\n- **Efficient Gradient Computation**: Optimizing gradient computation strategies is crucial for memory efficiency. Techniques like asynchronous parallel linear operations can minimize the time waiting on incoming communication, thus increasing device efficiency.\n\n### Memory-Performance Trade-offs\n- **Parameter Sharing and Layer-wise Parameter Scaling**: These techniques reduce the number of parameters by reusing weights across different layers, enhancing efficiency without sacrificing performance. However, they require careful balancing to avoid compromising model performance.\n\n### Practical Deployment Considerations\n- **Sharding and Parallelism**: Large models require distributing model parameters across multiple compute devices. Techniques like sharding and asynchronous parallel linear operations are essential for managing distributed inputs and matrices efficiently.\n\n## Hardware-Specific Optimizations\n\n### GPU/TPU-Optimized Implementations\n- **IO-Aware Implementations**: Optimizing data access patterns to minimize IO overhead is crucial for performance. Cache-friendly computation patterns and parallel processing strategies are also important for efficient hardware utilization[Analysis Summary].\n\n### Memory Access Patterns\n- **Cache-Friendly Computation**: Ensuring that computation patterns are cache-friendly can significantly improve performance. This involves optimizing the order of operations to reduce memory access latency[Analysis Summary].\n\n### Parallel Processing Strategies\n- **Asynchronous Parallel Linear Operations**: This approach involves simultaneous communication of activations and weights between devices while performing computations, which minimizes waiting time and increases device efficiency.\n\n## Concrete Implementation Details and Performance Comparisons\n\n- **nGPT Architecture**: The nGPT architecture normalizes all vectors to unit norm and uses learnable eigen learning rates for attention and MLP blocks. This approach has been shown to accelerate learning and reduce the number of training steps needed by factors ranging from 4 to 20.\n- **AdaNorm and SALN**: These adaptive normalization techniques have been compared in various studies, showing improvements in model performance and stability. For example, AdaNorm demonstrated better results than LayerNorm on seven out of eight datasets[Key References: Xu et al., 2019; Min et al., 2021].\n\n## Memory Efficiency Metrics and Hardware Utilization Statistics\n\n- **Memory Overhead Reduction**: Techniques like progressive re-parameterization and parameter sharing can reduce memory overhead. For instance, omitting biases in QKV projections and LayerNorms in models like ViT-22B can enhance resource utilization by 3%.\n- **Hardware Utilization**: Optimizations such as asynchronous parallel linear operations and cache-friendly computation patterns can improve hardware utilization statistics, leading to more efficient use of GPUs and TPUs.\n\nBy focusing on these adaptive normalization techniques, memory-efficient implementations, and hardware-specific optimizations, researchers can significantly improve the performance, stability, and efficiency of autoregressive language models.\n",
            "analysis": "Summary of Analysis:\nThe initial search has revealed several promising directions for enhancing RMSNorm with adaptive and efficient mechanisms. Key findings center around three main areas:\n\n1. Adaptive Normalization Approaches:\n- AdaNorm shows strong potential for context-dependent normalization\n- Style-Adaptive Layer Normalization (SALN) demonstrates effective parameter adaptation\n- Dynamic Layer Normalization (DLN) provides insights into speaker/context adaptation\n\n2. Memory Efficiency Techniques:\n- Progressive re-parameterization methods show promise for reducing memory overhead\n- Structured approaches to parameter sharing can maintain performance while reducing memory\n- Efficient gradient computation strategies have been demonstrated in various contexts\n\n3. Hardware Optimization:\n- IO-aware implementations show significant performance gains\n- Cache-friendly computation patterns are crucial for efficiency\n- Parallel processing optimizations remain an important consideration\n\nKey References with Useful Excerpts:\n\n1. From \"Understanding and Improving Layer Normalization\" (Xu et al., 2019):\n\"AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets by replacing bias and gain with a new transformation function\"\n\n2. From \"Meta-StyleSpeech\" (Min et al., 2021):\n\"Style-Adaptive Layer Normalization (SALN) effectively aligns gain and bias according to extracted style information\"\n\n3. From \"Norm matters: efficient and accurate normalization schemes in deep networks\" (Hoffer et al., 2018):\n\"Novel view on normalization methods as tools to decouple weights' norm from the underlying optimized objective\"\n\nKey Insights for Design:\n\n1. Adaptive Mechanisms:\n- Context-dependent parameter generation shows strong promise\n- Style-based adaptation could be generalized to sequence contexts\n- Dynamic parameter adjustment needs careful stability consideration\n\n2. Efficiency Considerations:\n- Progressive parameter updates can reduce memory overhead\n- Structured sharing approaches maintain performance while reducing parameters\n- Gradient computation optimization is crucial for efficiency\n\n3. Implementation Strategy:\n- Need to carefully balance adaptivity with computational overhead\n- Hardware-aware design decisions are crucial\n- Stability during training requires careful consideration\n\nFuture Search Needed:\n1. More detailed investigation of recent adaptive normalization techniques in language models\n2. Specific implementations of memory-efficient normalization in transformers\n3. Hardware optimization strategies for normalization layers\n\nReferences:\n\n1. Xu, J., Sun, X., Zhang, Z., Zhao, G., & Lin, J. (2019). Understanding and Improving Layer Normalization. NeurIPS 2019.\n\n2. Min, D., Lee, D. B., Yang, E., & Hwang, S. (2021). Meta-StyleSpeech: Multi-Speaker Adaptive Text-to-Speech Generation. ICML 2021.\n\n3. Hoffer, E., Banner, R., Golan, I., & Soudry, D. (2018). Norm matters: efficient and accurate normalization schemes in deep networks. NeurIPS 2018.\n\n4. Kim, T., Song, I., & Bengio, Y. (2017). Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in Speech Recognition. arXiv:1707.06065.\n\n5. Sanyal, A., Torr, P.H.S., & Dokania, P. (2019). Stable Rank Normalization for Improved Generalization in Neural Networks and GANs. ICLR 2019."
        },
        {
            "ready": false,
            "query": "\"adaptive normalization implementation\", \"hardware efficient normalization\", \"transformer normalization optimization\"",
            "detail": "Search for papers discussing:\n\n1. Implementation details of adaptive normalization:\n- Specific architectures and implementations\n- Performance metrics and comparisons\n- Training stability considerations\n\n2. Hardware efficiency in normalization:\n- Memory access patterns\n- Cache utilization strategies\n- Parallel processing techniques\n\n3. Optimization strategies:\n- Parameter sharing approaches\n- Gradient computation efficiency\n- Memory-computation trade-offs\n\nFocus on:\n- Concrete implementation details\n- Performance benchmarks\n- Hardware utilization metrics\n- Stability analysis",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n\n1. Implementation details of adaptive normalization:\n- Specific architectures and implementations\n- Performance metrics and comparisons\n- Training stability considerations\n\n2. Hardware efficiency in normalization:\n- Memory access patterns\n- Cache utilization strategies\n- Parallel processing techniques\n\n3. Optimization strategies:\n- Parameter sharing approaches\n- Gradient computation efficiency\n- Memory-computation trade-offs\n\nFocus on:\n- Concrete implementation details\n- Performance benchmarks\n- Hardware utilization metrics\n- Stability analysis\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.02)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 6/40 (Score: 0.02)*\n\n```\non pp. 1, 2, 9, 16). [3] Colin White et al. \"Neural architecture search: Insights from 1000 papers\". In: arXiv preprint arXiv:2301.08727 (2023) (cit.\n```\n\n#### 2. An Empirical Study of Mamba-based Language Models (Avg. Score: 0.01)\n\n*R. Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, V. Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, Garvit Kulshreshtha, Vartika Singh, Jared Casper, Jan Kautz, M. Shoeybi, Bryan Catanzaro*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** While pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities or long-context reasoning, and it is found that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks evaluated.\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.01)*\n\n```\nIn: arXiv preprint arXiv:2312.04927 (2023). [4] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. \"Layer Normalization\". In: arXiv preprint arXiv:1607.06450 (2016). [5] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate\". In: arXiv preprint arXiv:1409.0473 (2014). [6] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. \"LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding\".\n```\n\n#### 3. xLSTM: Extended Long Short-Term Memory (Avg. Score: 0.01)\n\n*Maximilian Beck, Korbinian Poppel, M. Spanring, Andreas Auer, Oleksandra Prudnikova, Michael K Kopp, G. Klambauer, Johannes Brandstetter, Sepp Hochreiter*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n##### *Relevant Chunk: No. 88/97 (Score: 0.01)*\n\n```\nArXiv, 2112.12731, 2021. Y. Wu and K. He. Group normalization. In Proceedings of the European conference on computer vision (ECCV), pp. 3-19, 2018. L. Xue, N. Constant, A. Roberts, M. Kale, R. Al-Rfou, A. Siddhant, A. Barua, and C. Raffel. mT5: A massively multilingual pre-trained text-to-text transformer. In Conference of the North American Chapter of the Association for Computational Linguistics, pp. 483-498, 2021. doi: 10.18653/v1/2021.naacl-main. 41. S. Yang and Y. Zhang. FLA: A Triton-based library for hardware-efficient implementations of linear attention mechanism, 2024. URL https://github.com/sustcsonglin/ flash-linear-attention.\n```\n\n#### 4. SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization (Avg. Score: 0.01)\n\n*Jialong Guo, Xinghao Chen, Yehui Tang, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A novel method named PRepBN is proposed to progressively replace LayerNorm with re-parameterized BatchNorm in training and a simplified linear attention (SLA) module is proposed that is simple yet effective to achieve strong performance.\n\n**Abstract:** Transformers have become foundational architectures for both natural language and computer vision tasks. However, the high computational cost makes it quite challenging to deploy on resource-constraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6\\%$ top-1 accuracy on ImageNet-1K with $16.2$ms latency, which is $2.4$ms less than that of Flatten-Swin with $0.1\\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency.Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/tree/master/research/huawei-noah/SLAB.\n\n##### *Relevant Chunk: No. 2/25 (Score: 0.01)*\n\n```\nHowever, the high computational cost makes it quite challenging to deploy on resourceconstraint devices. This paper investigates the computational bottleneck modules of efficient transformer, i.e., normalization layers and attention modules. LayerNorm is commonly used in transformer architectures but is not computational friendly due to statistic calculation during inference. However, replacing LayerNorm with more efficient BatchNorm in transformer often leads to inferior performance and collapse in training. To address this problem, we propose a novel method named PRepBN to progressively replace LayerNorm with re-parameterized BatchNorm in training. Moreover, we propose a simplified linear attention (SLA) module that is simple yet effective to achieve strong performance. Extensive experiments on image classification as well as object detection demonstrate the effectiveness of our proposed method. For example, our SLAB-Swin obtains $83.6 \\%$ top-1 accuracy on ImageNet-1K with 16.2 ms latency, which is 2.4 ms less than that of Flatten-Swin with $0.1 \\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower latency. Codes are publicly available at https://github.com/xinghaochen/SLAB and https://github.com/mindspore-lab/models/. ## 1. Introduction\n\nIntroduced initially for tasks in natural language processing (Vaswani et al., 2017), transformer architecture has\n\n[^0]rapidly emerged as a preeminent model in the landscape of language models. Its influence has significantly expanded with the introduction of Vision Transformer (ViT) (Dosovitskiy et al., 2020), illustrating the efficacy and versatility of transformer-based architectures. These architectures have demonstrated their capability to achieve competitive performance benchmarks in comparison to convolutional neural networks (CNNs) across diverse vision tasks (Han et al., 2022; Wang et al., 2022; Zheng et al., 2023; Tang et al., 2023a; Carion et al., 2020; Xu et al., 2023). Due to its powerful performance, transformer has become the mainstream architecture in deep learning. However, the computational demands of transformer architecture pose a significant challenge, which is predominantly due to the quadratic computational complexity of its attention mechanism and the necessity for online statistic computation of LayerNorm component. Numerous efforts have been directed towards enhancing the efficiency of transformer architecture (Tang et al., 2024; Wu et al., 2023; Tang et al., 2023b). Several approaches have sought to mitigate computational complexity by limiting the scope of token interactions within self-attention mechanisms, such as downsampling the key and value matrices (Wang et al., 2021), implementing sparse global attention patterns (Child et al., 2019), and computing selfattention within smaller windows (Tu et al., 2022; Liu et al., 2021; Dong et al., 2022). Meanwhile, linear attention emerges as an alternative strategy to enhance computational efficiency by breaking down the attention mechanism into linear computational cost (Katharopoulos et al., 2020; Cai et al., 2022; Han et al., 2023; You et al., 2023), yet it is still a challenging task to obtain a good balance between efficiency and accuracy. Moreover, there are some explorations into substituting LayerNorm (LN) with BatchNorm (BN) within transformers, motivated by the additional computational overhead LayerNorm incurs during inference. Yang et al. (2022) propose to add a BatchNorm layer in-between the two linear layers in the feed forward network to stabilize the training. However, there still exists a performance gap between the LayerNorm-based and BatchNorm-based transformers. In this paper, we focus on obtaining efficient transformer\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_d98ef7f3d09ffeb83a3fg-02.jpg?height=508&width=770&top_left_y=234&top_left_x=209)\n\nFigure 1. Comparisons of different methods on ImageNet. architectures by digging deep into the computational inefficient modules, i.e., normalization layers and attention modules. We first explore to replace LayerNorm with BatchNorm to accelerate inference for transformer. BatchNorm leads to lower inference latency but may cause training collapse and inferior performance, while LayerNorm could stabilize the training yet has extra computational cost during inference. To this end, we first propose a progressive strategy to gradually replace LayerNorm with BatchNorm by using a hyper-parameter to control the proportion of both normalization layers. Initially the transformer architecture is dominated by the LayerNorm and gradually transits to pure BatchNorm at the end of training. This strategy effectively mitigates the risk of training collapse and also eliminating the need for calculating statistics during inference. In addition to the progressive strategy, we also propose a novel re-parameterization formula for BatchNorm (RepBN), to enhance training stability and overall performance. Furthermore, the computational cost of attention is critical for efficient transformer and prior methods struggle to obtain good balance of efficiency and accuracy. To this end, we propose a simplified linear attention (SLA) module which utilizes ReLU as the kernel function and incorporate a depthwise convolution to perform local feature enhancement. The proposed attention mechanism is more efficient than prior linear attention but still attains comparable performance. We extensively evaluate our proposed method for various architectures on various benchmarks. Our progressive reparameterized BatchNorm shows strong performance for image classification and object detection tasks, obtaining similar accuracy with lower inference latency. Moreover, coupled with the progressive RepBN and simplified linear attention module, our SLAB transformer achieves competitive accuracy compared to Flatten transformer with improved computational efficiency. For example, SLAB-SwinS achieves $83.6 \\%$ Top-1 accuracy on ImageNet-1K with 16.2 ms latency, which is 2.4 ms less than that of FlattenSwin-S with $0.1 \\%$ higher accuracy. We also evaluated our method for language modeling task and obtain comparable performance and lower inference latency.\n```\n\n#### 5. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.00)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.00)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive normalization implementation\", \"hardware efficient normalization\", \"transformer normalization optimization\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Adaptive Normalization for Non-stationary Time Series Forecasting: A Temporal Slice Perspective\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Zhiding Liu, Mingyue Cheng, Zhi Li, Zhenya Huang, Qi Liu, Yanhu Xie, Enhong Chen*\n\n**TL;DR:** A novel slice-level adaptive normalization scheme, referred to SAN, is proposed, which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization and could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data.\n\n**Abstract:** Deep learning models have progressively advanced time series forecasting due to their powerful capacity in capturing sequence dependence. Nevertheless, it is still challenging to make accurate predictions due to the existence of non-stationarity in real-world data, denoting the data distribution rapidly changes over time. To mitigate such a dilemma, several efforts have been conducted by reducing the non-stationarity with normalization operation. However, these methods typically overlook the distribution discrepancy between the input series and the horizon series, and assume that all time points within the same instance share the same statistical properties, which is too ideal and may lead to suboptimal relative improvements. To this end, we propose a novel slice-level adaptive normalization, referred to SAN , which is a novel scheme for empowering time series forecasting with more flexible normalization and denormalization. SAN includes two crucial designs. First, SAN tries to eliminate the non-stationarity of time series in units of a local temporal slice (i.e., sub-series) rather than a global instance. Second, SAN employs a slight network module to independently model the evolving trends of statistical properties of raw time series. Consequently, SAN could serve as a general model-agnostic plugin and better alleviate the impact of the non-stationary nature of time series data. We instantiate the proposed SAN on four widely used forecasting models and test their prediction results on benchmark datasets to evaluate its effectiveness. Also, we report some insightful findings to deeply analyze and understand our proposed SAN. We make our codes publicly available 2 .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 7*)\n\n#### 2. GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, Andrew Rabinovich*\n\n**TL;DR:** A gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes is presented, showing that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks.\n\n**Abstract:** Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly. We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes. We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques. GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter $\\alpha$. Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks. Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 1088  (*Influential: 144*)\n\n#### 3. MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Neo Wei Ming, Zhehui Wang, Cheng Liu, R. Goh, Tao Luo*\n\n**TL;DR:** MA-BERT is proposed, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss and enables hardware-friendly processing on various computing engines, including CPUs and GPUs.\n\n**Abstract:** Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 5. Simple Hardware-Efficient Long Convolutions for Sequence Modeling\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Daniel Y. Fu, Elliot L. Epstein, Eric N. D. Nguyen, A. Thomas, Michael Zhang, Tri Dao, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** It is found that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling.\n\n**Abstract:** State space models (SSMs) have high performance on long sequence modeling but require sophisticated initialization techniques and specialized implementations for high quality and runtime performance. We study whether a simple alternative can match SSMs in performance and efficiency: directly learning long convolutions over the sequence. We find that a key requirement to achieving high performance is keeping the convolution kernels smooth. We find that simple interventions--such as squashing the kernel weights--result in smooth kernels and recover SSM performance on a range of tasks including the long range arena, image classification, language modeling, and brain data modeling. Next, we develop FlashButterfly, an IO-aware algorithm to improve the runtime performance of long convolutions. FlashButterfly appeals to classic Butterfly decompositions of the convolution to reduce GPU memory IO and increase FLOP utilization. FlashButterfly speeds up convolutions by 2.2$\\times$, and allows us to train on Path256, a challenging task with sequence length 64K, where we set state-of-the-art by 29.1 points while training 7.2$\\times$ faster than prior work. Lastly, we introduce an extension to FlashButterfly that learns the coefficients of the Butterfly decomposition, increasing expressivity without increasing runtime. Using this extension, we outperform a Transformer on WikiText103 by 0.2 PPL with 30% fewer parameters.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 6. Improving Transformer Optimization Through Better Initialization\n\n*From Search Query: \"transformer normalization optimization\"*\n\n*X. Huang, Felipe P\u00e9rez, Jimmy Ba, M. Volkovs*\n\n**TL;DR:** This work investigates and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture and proposes a new weight initialization scheme with theoretical justi-\ufb01cation, that enables training without warmup or layer normalization.\n\n**Abstract:** The Transformer architecture has achieved considerable success recently; the key component of the Transformer is the attention layer that enables the model to focus on important regions within an input sequence. Gradient optimization with attention layers can be notoriously dif\ufb01cult requiring tricks such as learning rate warmup to prevent divergence. As Transformer models are becoming larger and more expensive to train, recent research has focused on understanding and improving optimization in these architectures. In this work our contributions are two-fold: we \ufb01rst investigate and empirically validate the source of optimization problems in the encoder-decoder Transformer architecture; we then propose a new weight initialization scheme with theoretical justi-\ufb01cation, that enables training without warmup or layer normalization. Empirical results on public machine translation benchmarks show that our approach achieves leading accuracy, allowing to train deep Transformer models with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without dif\ufb01culty. Code for this work is available here: https://github. com/layer6ai-labs/T-Fixup .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 125  (*Influential: 14*)\n\n#### 7. Lipschitz Constrained Parameter Initialization for Deep Transformers\n\n*From Search Query: \"transformer normalization optimization\"*\n\n*Hongfei Xu, Qiuhui Liu, Josef van Genabith, Deyi Xiong, Jingyi Zhang*\n\n**TL;DR:** This paper empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers and presents a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence.\n\n**Abstract:** The Transformer translation model employs residual connection and layer normalization to ease the optimization difficulties caused by its multi-layer encoder/decoder structure. Previous research shows that even with residual connection and layer normalization, deep Transformers still have difficulty in training, and particularly Transformer models with more than 12 encoder/decoder layers fail to converge. In this paper, we first empirically demonstrate that a simple modification made in the official implementation, which changes the computation order of residual connection and layer normalization, can significantly ease the optimization of deep Transformers. We then compare the subtle differences in computation order in considerable detail, and present a parameter initialization method that leverages the Lipschitz constraint on the initialization of Transformer parameters that effectively ensures training convergence. In contrast to findings in previous research we further demonstrate that with Lipschitz parameter initialization, deep Transformers with the original computation order can converge, and obtain significant BLEU improvements with up to 24 layers. In contrast to previous research which focuses on deep encoders, our approach additionally enables Transformers to also benefit from deep decoders.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 25  (*Influential: 0*)\n\n### 5 related papers from ArXiv\n\n#### 1. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 2. Differentiable Pooling for Unsupervised Acoustic Model Adaptation\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Pawel Swietojanski, Steve Renals*\n\n**Abstract:** We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.\n\n**Published:** 2016-03-31T15:10:40Z  (*Updated: 2016-07-13T18:12:49Z*)\n\n\n\n#### 3. Full Stack Optimization of Transformer Inference: a Survey\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Sehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Yakun Sophia Shao, Amir Gholami*\n\n**Abstract:** Recent advances in state-of-the-art DNN architecture design have been moving\ntoward Transformer models. These models achieve superior accuracy across a wide\nrange of applications. This trend has been consistent over the past several\nyears since Transformer models were originally introduced. However, the amount\nof compute and bandwidth required for inference of recent Transformer models is\ngrowing at a significant rate, and this has made their deployment in\nlatency-sensitive applications challenging. As such, there has been an\nincreased focus on making Transformer models more efficient, with methods that\nrange from changing the architecture design, all the way to developing\ndedicated domain-specific accelerators. In this work, we survey different\napproaches for efficient Transformer inference, including: (i) analysis and\nprofiling of the bottlenecks in existing Transformer architectures and their\nsimilarities and differences with previous convolutional models; (ii)\nimplications of Transformer architecture on hardware, including the impact of\nnon-linear operations such as Layer Normalization, Softmax, and GELU, as well\nas linear operations, on hardware design; (iii) approaches for optimizing a\nfixed Transformer architecture; (iv) challenges in finding the right mapping\nand scheduling of operations for Transformer models; and (v) approaches for\noptimizing Transformer models by adapting the architecture using neural\narchitecture search. Finally, we perform a case study by applying the surveyed\noptimizations on Gemmini, the open-source, full-stack DNN accelerator\ngenerator, and we show how each of these approaches can yield improvements,\ncompared to previous benchmark results on Gemmini. Among other things, we find\nthat a full-stack co-design approach with the aforementioned methods can result\nin up to 88.7x speedup with a minimal performance degradation for Transformer\ninference.\n\n**Published:** 2023-02-27T18:18:13Z  (*Updated: 2023-02-27T18:18:13Z*)\n\n\n\n#### 4. AdaLomo: Low-memory Optimization with Adaptive Learning Rate\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Kai Lv, Hang Yan, Qipeng Guo, Haijun Lv, Xipeng Qiu*\n\n**Abstract:** Large language models have achieved remarkable success, but their extensive\nparameter size necessitates substantial memory for training, thereby setting a\nhigh threshold. While the recently proposed low-memory optimization (LOMO)\nreduces memory footprint, its optimization technique, akin to stochastic\ngradient descent, is sensitive to hyper-parameters and exhibits suboptimal\nconvergence, failing to match the performance of the prevailing optimizer for\nlarge language models, AdamW. Through empirical analysis of the Adam optimizer,\nwe found that, compared to momentum, the adaptive learning rate is more\ncritical for bridging the gap. Building on this insight, we introduce the\nlow-memory optimization with adaptive learning rate (AdaLomo), which offers an\nadaptive learning rate for each parameter. To maintain memory efficiency, we\nemploy non-negative matrix factorization for the second-order moment estimation\nin the optimizer state. Additionally, we suggest the use of a grouped update\nnormalization to stabilize convergence. Our experiments with instruction-tuning\nand further pre-training demonstrate that AdaLomo achieves results on par with\nAdamW, while significantly reducing memory requirements, thereby lowering the\nhardware barrier to training large language models. The code is accessible at\nhttps://github.com/OpenLMLab/LOMO.\n\n**Published:** 2023-10-16T09:04:28Z  (*Updated: 2024-06-06T13:22:25Z*)\n\n\n\n#### 5. Riemannian Normalizing Flow on Variational Wasserstein Autoencoder for\n  Text Modeling\n\n*From Search Query: \"transformer normalization optimization\"*\n\n*Prince Zizhuang Wang, William Yang Wang*\n\n**Abstract:** Recurrent Variational Autoencoder has been widely used for language modeling\nand text generation tasks. These models often face a difficult optimization\nproblem, also known as the Kullback-Leibler (KL) term vanishing issue, where\nthe posterior easily collapses to the prior, and the model will ignore latent\ncodes in generative tasks. To address this problem, we introduce an improved\nWasserstein Variational Autoencoder (WAE) with Riemannian Normalizing Flow\n(RNF) for text modeling. The RNF transforms a latent variable into a space that\nrespects the geometric characteristics of input space, which makes posterior\nimpossible to collapse to the non-informative prior. The Wasserstein objective\nminimizes the distance between the marginal distribution and the prior directly\nand therefore does not force the posterior to match the prior. Empirical\nexperiments show that our model avoids KL vanishing over a range of datasets\nand has better performances in tasks such as language modeling, likelihood\napproximation, and text generation. Through a series of experiments and\nanalysis over latent space, we show that our model learns latent distributions\nthat respect latent space geometry and is able to generate sentences that are\nmore diverse.\n\n**Published:** 2019-04-04T08:13:42Z  (*Updated: 2019-04-22T21:19:28Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Junho Kim, Kwanghee Lee, Hyeonwoo Kang, Minjae Kim*\n\n**Abstract:** We propose a novel method for unsupervised image-to-image translation, which incorporates a new attention module and a new learnable normalization function in an end-to-end manner. The attention module guides our model to focus on more important regions distinguishing between source and target domains based on the attention map obtained by the auxiliary classifier. Unlike previous attention-based method which cannot handle the geometric changes between domains, our model can translate both images requiring holistic changes and images requiring large shape changes. Moreover, our new AdaLIN (Adaptive Layer-Instance Normalization) function helps our attention-guided model to flexibly control the amount of change in shape and texture by learned parameters depending on datasets. Experimental results show the superiority of the proposed method compared to the existing state-of-the-art models with a fixed network architecture and hyper-parameters. Our code and datasets are available at https://github.com/taki0112/UGATIT or https://github.com/znxlwm/UGATIT-pytorch.\n\n**Proceeding:** iclr-2020-1\n\n**Published:** 2019-07-25\n\n\n\n#### 2. Arbitrary Style Transfer in Real-time with Adaptive Instance Normalization\n\n*From Search Query: \"adaptive normalization implementation\"*\n\n*Serge Belongie, Xun Huang*\n\n**Abstract:** Gatys et al. recently introduced a neural algorithm that renders a content\nimage in the style of another image, achieving so-called style transfer.\nHowever, their framework requires a slow iterative optimization process, which\nlimits its practical application. Fast approximations with feed-forward neural\nnetworks have been proposed to speed up neural style transfer. Unfortunately,\nthe speed improvement comes at a cost: the network is usually tied to a fixed\nset of styles and cannot adapt to arbitrary new styles. In this paper, we\npresent a simple yet effective approach that for the first time enables\narbitrary style transfer in real-time. At the heart of our method is a novel\nadaptive instance normalization (AdaIN) layer that aligns the mean and variance\nof the content features with those of the style features. Our method achieves\nspeed comparable to the fastest existing approach, without the restriction to a\npre-defined set of styles. In addition, our approach allows flexible user\ncontrols such as content-style trade-off, style interpolation, color & spatial\ncontrols, all using a single feed-forward neural network.\n\n**Conference:** arbitrary-style-transfer-in-real-time-with-1\n\n**Published:** 2017-03-20\n\n\n\n#### 3. Training wide residual networks for deployment using a single bit for each weight\n\n*From Search Query: \"hardware efficient normalization\"*\n\n*Mark D. McDonnell*\n\n**Abstract:** For fast and energy-efficient deployment of trained deep neural networks on\nresource-constrained embedded hardware, each learned weight parameter should\nideally be represented and stored using a single bit. Error-rates usually\nincrease when this requirement is imposed. Here, we report large improvements\nin error rates on multiple datasets, for deep convolutional neural networks\ndeployed with 1-bit-per-weight. Using wide residual networks as our main\nbaseline, our approach simplifies existing methods that binarize weights by\napplying the sign function in training; we apply scaling factors for each layer\nwith constant unlearned values equal to the layer-specific standard deviations\nused for initialization. For CIFAR-10, CIFAR-100 and ImageNet, and models with\n1-bit-per-weight requiring less than 10 MB of parameter memory, we achieve\nerror rates of 3.9%, 18.5% and 26.0% / 8.5% (Top-1 / Top-5) respectively. We\nalso considered MNIST, SVHN and ImageNet32, achieving 1-bit-per-weight test\nresults of 0.27%, 1.9%, and 41.3% / 19.1% respectively. For CIFAR, our error\nrates halve previously reported values, and are within about 1% of our\nerror-rates for the same network with full-precision weights. For networks that\noverfit, we also show significant improvements in error rate by not learning\nbatch normalization scale and offset parameters. This applies to both full\nprecision and 1-bit-per-weight networks. Using a warm-restart learning-rate\nschedule, we found that training for 1-bit-per-weight is just as fast as\nfull-precision networks, with better accuracy than standard schedules, and\nachieved about 98%-99% of peak performance in just 62 training epochs for\nCIFAR-10/100. For full training code and trained models in MATLAB, Keras and\nPyTorch see https://github.com/McDonnell-Lab/1-bit-per-weight/ .\n\n**Conference:** training-wide-residual-networks-for-1\n\n**Published:** 2018-02-23\n\n\n\n#### 4. Metric3Dv2: A Versatile Monocular Geometric Foundation Model for Zero-shot Metric Depth and Surface Normal Estimation\n\n*From Search Query: \"transformer normalization optimization\"*\n\n*Shaojie Shen, Chunhua Shen, Gang Yu, Kaixuan Wang, Hao Chen, Xiaoxiao Long, Zhipeng Cai, Chi Zhang, Wei Yin, Mu Hu*\n\n**Abstract:** We introduce Metric3D v2, a geometric foundation model for zero-shot metric depth and surface normal estimation from a single image, which is crucial for metric 3D recovery. While depth and normal are geometrically related and highly complimentary, they present distinct challenges. SoTA monocular depth methods achieve zero-shot generalization by learning affine-invariant depths, which cannot recover real-world metrics. Meanwhile, SoTA normal estimation methods have limited zero-shot performance due to the lack of large-scale labeled data. To tackle these issues, we propose solutions for both metric depth estimation and surface normal estimation. For metric depth estimation, we show that the key to a zero-shot single-view model lies in resolving the metric ambiguity from various camera models and large-scale data training. We propose a canonical camera space transformation module, which explicitly addresses the ambiguity problem and can be effortlessly plugged into existing monocular models. For surface normal estimation, we propose a joint depth-normal optimization module to distill diverse data knowledge from metric depth, enabling normal estimators to learn beyond normal labels. Equipped with these modules, our depth-normal models can be stably trained with over 16 million of images from thousands of camera models with different-type annotations, resulting in zero-shot generalization to in-the-wild images with unseen camera settings. Our method enables the accurate recovery of metric 3D structures on randomly collected internet images, paving the way for plausible single-image metrology. Our project page is at https://JUGGHM.github.io/Metric3Dv2.\n\n**Conference:** metric3d-v2-a-versatile-monocular-geometric\n\n**Published:** 2024-03-22\n\n\n\n#### 5. On Layer Normalization in the Transformer Architecture\n\n*From Search Query: \"transformer normalization optimization\"*\n\n*Li-Wei Wang, Tie-Yan Liu, Chen Xing, Kai Zheng, Ruibin Xiong, Yanyan Lan, Di He, Yunchang Yang, Huishuai Zhang, Shuxin Zheng*\n\n**Abstract:** The Transformer is widely used in natural language processing tasks. To train a Transformer however, one usually needs a carefully designed learning rate warm-up stage, which is shown to be crucial to the final performance but will slow down the optimization and bring more hyper-parameter tunings. In this paper, we first study theoretically why the learning rate warm-up stage is essential and show that the location of layer normalization matters. Specifically, we prove with mean field theory that at initialization, for the original-designed Post-LN Transformer, which places the layer normalization between the residual blocks, the expected gradients of the parameters near the output layer are large. Therefore, using a large learning rate on those gradients makes the training unstable. The warm-up stage is practically helpful for avoiding this problem. On the other hand, our theory also shows that if the layer normalization is put inside the residual blocks (recently proposed as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates us to remove the warm-up stage for the training of Pre-LN Transformers. We show in our experiments that Pre-LN Transformers without the warm-up stage can reach comparable results with baselines while requiring significantly less training time and hyper-parameter tuning on a wide range of applications.\n\n**Proceeding:** icml-2020-1\n\n**Published:** 2020-02-12\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on adaptive normalization, hardware efficiency, and optimization strategies, here are some key findings and references that align with your requirements:\n\n## Implementation Details of Adaptive Normalization\n\n### Specific Architectures and Implementations\n- The paper on \"Enhancing Neural Network Representations with Prior Knowledge\" introduces a novel approach to multi-mode normalization, which includes Context Normalization (CN), Context Normalization - Extended (CN-X), and Adaptive Context Normalization (ACN). These methods organize data into predefined contexts or dynamically build contexts in the latent space, which can be particularly useful for adaptive normalization in autoregressive models.\n\n### Performance Metrics and Comparisons\n- The same paper provides performance metrics and comparisons across tasks such as image classification, domain adaptation, and image generation. While the context is different, the principles of context-based normalization could be adapted to language models, offering insights into how adaptive normalization can improve convergence and performance.\n\n### Training Stability Considerations\n- The analysis note highlights the importance of normalization layer position and stability. Pre-normalization architectures, as discussed in \"Pre-RMSNorm and Pre-CRMSNorm Transformers,\" show better training stability. Adaptive mechanisms, such as those proposed in the context normalization paper, need careful stability consideration to avoid issues like internal covariate shift and training inference discrepancy.\n\n## Hardware Efficiency in Normalization\n\n### Memory Access Patterns\n- Efficient memory access patterns are crucial for hardware efficiency. Papers like \"Full Stack Optimization of Transformer Inference\" discuss strategies for optimizing memory access, including structured parameter updates and cache-friendly computation patterns. These strategies can be applied to normalization layers to reduce memory overhead and improve performance.\n\n### Cache Utilization Strategies\n- Cache utilization is a key aspect of hardware efficiency. Optimizing normalization layers to have cache-friendly computation patterns, as mentioned in the analysis note, can significantly improve performance. Techniques such as progressive parameter updates and structured sharing can help maintain performance while reducing memory usage.\n\n### Parallel Processing Techniques\n- Parallel processing techniques can be leveraged to optimize normalization layers. For instance, the use of parallel processing in \"Switchable Normalization\" allows the model to adaptively select the most appropriate normalization method, which can be optimized for hardware efficiency by distributing the computation across multiple processing units.\n\n## Optimization Strategies\n\n### Parameter Sharing Approaches\n- Parameter sharing approaches can reduce the computational overhead and memory requirements of normalization layers. The concept of structured sharing, as mentioned in the analysis note, maintains performance while reducing the number of parameters. This approach can be integrated into adaptive normalization methods to enhance efficiency.\n\n### Gradient Computation Efficiency\n- Efficient gradient computation is essential for training stability and performance. Papers like \"On Layer Normalization in the Transformer Architecture\" provide insights into how the position of normalization layers affects gradient flow. Adaptive normalization methods should be designed with careful consideration of gradient computation efficiency to avoid large expected gradients near the output layer.\n\n### Memory-Computation Trade-offs\n- The trade-offs between memory and computation are critical in optimizing normalization layers. Techniques such as re-parameterization and IO-aware implementations, as discussed in the analysis note, can help balance these trade-offs. For example, using Gaussian mixture models for adaptive normalization, as proposed in another context, can mitigate issues while considering computational overhead.\n\n## Additional Insights\n\n- **Adaptive Normalization in Transformers**: The concept of adaptive normalization can be integrated into transformer architectures. For instance, the \"Crossformer\" and other time series forecasting transformers mentioned in the GitHub repository, while not directly focused on language models, demonstrate innovative uses of adaptive mechanisms that could be adapted for autoregressive language models.\n\n- **Hardware-Aware Design**: The analysis note emphasizes the importance of hardware-aware design. Papers like \"Full Stack Optimization of Transformer Inference\" provide comprehensive surveys on optimizing transformer inference, which includes normalization layers. These optimizations can be tailored to improve the performance of autoregressive language models on specific hardware.\n\nBy incorporating these insights and strategies, researchers can develop more efficient, stable, and performant autoregressive language models that leverage adaptive normalization techniques effectively.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights into adaptive normalization techniques and their implementation challenges:\n\na) Key Themes:\n- Adaptive normalization mechanisms showing superior performance\n- Memory efficiency through parameter sharing and re-parameterization\n- Hardware-specific optimizations for efficient implementation\n- Trade-offs between adaptivity and computational overhead\n\nb) Novel Elements in AdaptiveNormGPT:\n- Context-dependent parameter adaptation appears novel\n- Integration of position-aware scaling with RMSNorm is unique\n- Hardware-aware design aligns with current trends\n\n2. Useful References with Key Excerpts:\n\nFrom \"On Layer Normalization in the Transformer Architecture\" (Xiong et al., 2020):\n\"We prove with mean field theory that at initialization, for the original-designed Post-LN Transformer [...] the expected gradients of the parameters near the output layer are large.\"\n\nFrom \"Pre-RMSNorm and Pre-CRMSNorm Transformers\" (Jiang et al., 2023):\n\"We formally establish the equivalence of Pre-LN, Pre-RMSNorm, and Pre-CRMSNorm Transformer variants in both training and inference.\"\n\nFrom \"Understanding the Failure of Batch Normalization for Transformers\" (Wang et al., 2022):\n\"Training Inference Discrepancy (TID) can indicate BN's performance [...] BN can obtain much better test performance than LN when TID keeps small through training.\"\n\n3. Key Insights:\n\na) Normalization Stability:\n- Position of normalization layers critically affects gradient flow\n- Pre-normalization architectures show better training stability\n- Adaptive mechanisms need careful stability consideration\n\nb) Memory Efficiency:\n- Progressive parameter updates reduce memory overhead\n- Structured sharing maintains performance while reducing parameters\n- Cache-friendly computation patterns crucial for efficiency\n\nc) Hardware Considerations:\n- IO-aware implementations show significant gains\n- Memory access patterns affect performance substantially\n- Parallel processing optimization opportunities exist\n\n4. Future Search Needed:\n\nNeed to investigate:\n1. Specific implementations of adaptive normalization in production systems\n2. Empirical comparisons of different normalization strategies\n3. Hardware-specific optimization techniques for normalization layers\n\n5. References:\n\n[1] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., ... & Liu, T. Y. (2020). On layer normalization in the transformer architecture. In International Conference on Machine Learning (pp. 10524-10533).\n\n[2] Jiang, Z., Gu, J., Zhu, H., & Pan, D. (2023). Pre-RMSNorm and Pre-CRMSNorm Transformers: Equivalent and Efficient Pre-LN Transformers. Neural Information Processing Systems.\n\n[3] Wang, J., Wu, J., & Huang, L. (2022). Understanding the Failure of Batch Normalization for Transformers in NLP. arXiv preprint.\n\n[4] Kim, S., Hooper, C., Wattanawong, T., Kang, M., Yan, R., Genc, H., ... & Gholami, A. (2023). Full Stack Optimization of Transformer Inference: a Survey. arXiv preprint.\n\n[5] Chen, X., Wang, Y., Du, Y., Hassoun, S., & Liu, L. (2023). On Separate Normalization in Self-supervised Transformers. Neural Information Processing Systems."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 1.10871,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.2432990000000002,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "RMSNorm",
    "proposal": "# AdaptiveNormGPT: Efficient Context-Aware Normalization for Language Models\n\n## Motivation\n\nCurrent language models face significant challenges in maintaining stability and efficiency across varying sequence lengths and input distributions. While RMSNorm has proven effective for basic normalization, there remain opportunities for improvement in:\n\n1. **Adaptive Behavior**: Current normalization techniques use static parameters that don't adapt to input context\n2. **Memory Efficiency**: Traditional implementations require storing large amounts of intermediate states\n3. **Computational Overhead**: Standard normalization adds significant computation during inference\n4. **Gradient Flow**: Existing methods can suffer from unstable gradient propagation\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Static Normalization**:\n   - Fixed parameters regardless of input context\n   - Suboptimal handling of varying sequence lengths\n   - Limited ability to adapt to different input distributions\n\n2. **Memory Usage**:\n   - High memory requirements for storing statistics\n   - Inefficient gradient computation\n   - Large intermediate state storage\n\n3. **Computational Efficiency**:\n   - Redundant calculations during inference\n   - Suboptimal hardware utilization\n   - Unnecessary recomputation of statistics\n\n### Opportunities for Improvement\n\n1. **Adaptive Parameters**:\n   - Context-dependent normalization\n   - Position-aware parameter adjustment\n   - Dynamic adaptation to sequence characteristics\n\n2. **Memory Optimization**:\n   - Efficient gradient computation\n   - State compression techniques\n   - Smart caching strategies\n\n3. **Hardware Efficiency**:\n   - IO-aware implementation\n   - Optimized memory access patterns\n   - Efficient parallel processing\n\n## Core Idea and Philosophy\n\nAdaptiveRMSNorm introduces three key innovations:\n\n1. **Context-Aware Parameter Adaptation**:\n   - Dynamic adjustment of normalization parameters based on input context\n   - Position-dependent scaling factors\n   - Adaptive computation paths\n\n2. **Memory-Efficient Implementation**:\n   - Progressive parameter re-parameterization\n   - Efficient gradient computation through careful state management\n   - Optimized memory access patterns\n\n3. **Hardware-Optimized Design**:\n   - IO-aware computation strategies\n   - Efficient parallel processing\n   - Cache-friendly implementation\n\n## Design Plan\n\n### Mathematical Formulation\n\n1. **Adaptive Normalization**:\n   Given input x \u2208 \u211d^(B\u00d7L\u00d7D), the AdaptiveRMSNorm computes:\n\n   ```\n   y = (x / sqrt(var(x) + \u03b5)) * \u03b3(c, p) + \u03b2(c, p)\n   ```\n\n   where:\n   - var(x) is the variance computed along the feature dimension\n   - \u03b3(c, p) and \u03b2(c, p) are context and position dependent parameters\n   - c is the local context features\n   - p is the position information\n\n2. **Context-Dependent Parameters**:\n   ```\n   \u03b3(c, p) = \u03b3_base * (1 + f_\u03b3(c, p))\n   \u03b2(c, p) = \u03b2_base + f_\u03b2(c, p)\n   ```\n   \n   where f_\u03b3 and f_\u03b2 are small neural networks that compute context-dependent adjustments\n\n3. **Progressive Re-parameterization**:\n   During training:\n   ```\n   \u03b3_effective = \u03b1 * \u03b3_static + (1 - \u03b1) * \u03b3_dynamic\n   \u03b2_effective = \u03b1 * \u03b2_static + (1 - \u03b1) * \u03b2_dynamic\n   ```\n   \n   where \u03b1 decreases from 1 to 0 during training\n\n### Implementation Guidelines\n\n```python\nclass AdaptiveRMSNorm(nn.Module):\n    def __init__(self, dim, eps=1e-5):\n        super().__init__()\n        self.eps = eps\n        self.gamma_base = nn.Parameter(torch.ones(dim))\n        self.beta_base = nn.Parameter(torch.zeros(dim))\n        \n        # Context networks\n        self.gamma_net = nn.Sequential(\n            nn.Linear(dim, dim // 4),\n            nn.SiLU(),\n            nn.Linear(dim // 4, dim)\n        )\n        self.beta_net = nn.Sequential(\n            nn.Linear(dim, dim // 4),\n            nn.SiLU(),\n            nn.Linear(dim // 4, dim)\n        )\n        \n        # Progressive training parameter\n        self.register_buffer('alpha', torch.ones(1))\n        \n    def forward(self, x, context=None):\n        # Compute basic statistics\n        variance = x.pow(2).mean(-1, keepdim=True)\n        x_norm = x * torch.rsqrt(variance + self.eps)\n        \n        # Compute context-dependent parameters\n        if context is not None:\n            gamma_adj = self.gamma_net(context)\n            beta_adj = self.beta_net(context)\n            \n            # Progressive mixing\n            gamma = self.gamma_base * (1 + self.alpha * gamma_adj)\n            beta = self.beta_base + self.alpha * beta_adj\n        else:\n            gamma = self.gamma_base\n            beta = self.beta_base\n            \n        return x_norm * gamma + beta\n```\n\n### Memory Optimization\n\n1. **Gradient Computation**:\n   - Use checkpointing for context networks\n   - Implement efficient backward passes\n   - Optimize intermediate state storage\n\n2. **State Management**:\n   - Compress intermediate states\n   - Implement smart caching\n   - Optimize memory access patterns\n\n3. **Hardware Utilization**:\n   - Implement IO-aware computation\n   - Optimize for parallel processing\n   - Use cache-friendly data layouts\n\n## Theoretical Analysis\n\n1. **Stability Analysis**:\n   - Progressive re-parameterization ensures stable training\n   - Context-dependent parameters maintain adaptivity\n   - Bounded parameter adjustments preserve convergence\n\n2. **Memory Complexity**:\n   - O(D) parameter storage for base parameters\n   - O(D/4) storage for context networks\n   - O(1) additional memory during forward pass\n\n3. **Computational Complexity**:\n   - O(BLD) for basic normalization\n   - O(BD) for context network computation\n   - O(1) overhead for parameter adjustment\n\n## Expected Benefits\n\n1. **Improved Stability**:\n   - Better handling of varying sequence lengths\n   - More robust to input distribution shifts\n   - Stable gradient flow\n\n2. **Enhanced Efficiency**:\n   - Reduced memory requirements\n   - Optimized computation\n   - Better hardware utilization\n\n3. **Superior Adaptivity**:\n   - Context-aware normalization\n   - Position-dependent processing\n   - Dynamic parameter adjustment\n\n## References\n\n1. Zhang, B., & Sennrich, R. (2019). Root Mean Square Layer Normalization. NeurIPS.\n\n2. Guo, J., Chen, X., Tang, Y., & Wang, Y. (2024). SLAB: Efficient Transformers with Simplified Linear Attention and Progressive Re-parameterized Batch Normalization. arXiv.org.\n\n3. Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. NeurIPS.\n\n4. Narayanan, D., Phanishayee, A., Shi, K., Chen, X., & Zaharia, M. (2020). Memory-Efficient Pipeline-Parallel DNN Training. ICML.\n\n5. Bu, Z., Gopi, S., Kulkarni, J., Lee, Y., Shen, J. H., & Tantipongpipat, U. (2021). Fast and Memory Efficient Differentially Private-SGD via JL Projections. NeurIPS.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "AdaptiveNormGPT introduces a novel adaptive RMSNorm variant that dynamically adjusts normalization parameters based on input context and sequence position, while maintaining memory efficiency through careful gradient management and state compression. The design integrates progressive re-parameterization and IO-aware computation to achieve superior stability and performance with reduced memory overhead.",
    "ideation": null,
    "modelname": "adaptivenormgpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory access patterns\n- Include concrete examples of context-dependent parameter computation\n- Add guidelines for parallel processing implementation\n- Specify initialization strategies for adaptive parameters\n\n2. Stability Analysis:\n- Add theoretical analysis of convergence properties\n- Include stability analysis during progressive updates\n- Provide recommendations for hyperparameter tuning\n- Consider fallback mechanisms for stability\n\n3. Memory Management:\n- Elaborate on caching strategies\n- Include analysis of memory access patterns\n- Add guidelines for parameter sharing\n- Address potential cache thrashing issues\n\n4. Hardware Optimization:\n- Add specific guidance for different GPU architectures\n- Include strategies for memory bandwidth optimization\n- Provide more detailed cache optimization guidelines\n- Specify requirements for different scales of deployment\n\n5. Training Considerations:\n- Add specific guidance for handling gradient flow\n- Include stability analysis during training\n- Provide recommendations for learning rate schedules\n- Consider adaptive optimization strategies\n\n6. Evaluation Metrics:\n- Define specific metrics for measuring efficiency gains\n- Include guidelines for comparing with baseline models\n- Add ablation study recommendations\n- Specify benchmarking procedures",
    "user_input": ""
}