{
    "variantname": "HierarchicalAdaptiveAttentionV2",
    "review": "The HybridSSM-GPT proposal presents an innovative approach to enhancing transformer architectures by integrating selective state space models (SSMs) with hierarchical adaptive attention through a dual-stream architecture. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Technical Merit:\n- Novel integration of selective SSMs with hierarchical attention\n- Well-designed dual-stream architecture for efficient processing\n- Thoughtful consideration of hardware efficiency\n- Clear mathematical formulation and implementation guidelines\n\n2. Theoretical Foundation:\n- Strong grounding in state space theory\n- Clear connection to existing research in SSMs and attention\n- Well-reasoned approach to combining different architectures\n- Careful consideration of computational complexity\n\n3. Efficiency Considerations:\n- Linear complexity through careful design\n- Memory-efficient through selective state updates\n- Potential for improved parallel processing\n- Hardware-aware implementation strategies\n\n4. Scalability:\n- Design scales linearly with sequence length\n- Efficient handling of long-range dependencies\n- Memory management through selective updates\n- Clear strategy for parallel processing\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex coordination between SSM and attention streams\n- Potential challenges in maintaining numerical stability\n- Need for careful tuning of integration mechanisms\n- Complex memory access patterns requiring optimization\n\n2. Training Stability:\n- Dual-stream architecture may face convergence challenges\n- Complex gradient flow through multiple components\n- Need for careful initialization strategies\n- Possible instability in early training stages\n\n3. Memory Management:\n- While designed for efficiency, dual streams could lead to memory overhead\n- Complex state management across streams\n- Need for efficient caching strategies\n- Trade-offs between computation and memory access\n\n4. Hardware Dependencies:\n- Efficient implementation heavily depends on hardware-specific optimizations\n- Memory access patterns need careful design\n- Communication overhead between streams could be significant\n- Implementation efficiency depends on hardware capabilities\n\nThe proposal demonstrates strong potential for improving efficiency and scalability in language models. The combination of SSMs and hierarchical attention addresses key challenges in current architectures. However, several aspects require additional clarification and careful consideration during implementation.",
    "search_stack": [
        {
            "ready": false,
            "query": "selective state space models implementation attention",
            "detail": "Find specific implementation details and techniques for combining selective state space models with attention mechanisms, focusing on memory efficiency and hardware optimization.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementation details and techniques for combining selective state space models with attention mechanisms, focusing on memory efficiency and hardware optimization.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.98)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.98)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 3. Softmax Attention with Constant Cost per Token (Avg. Score: 0.98)\n\n*Franz A. Heinsen*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work proposes a simple modification to the conventional attention mechanism applied by Transformers, which quantifies pairwise query-key similarity with scaled dot-products with the logarithms of scaled dot-products of exponentials, and linearizes attention with exponential kernel feature maps.\n\n**Abstract:** We propose a simple modification to the conventional attention mechanism applied by Transformers: Instead of quantifying pairwise query-key similarity with scaled dot-products, we quantify it with the logarithms of scaled dot-products of exponentials. Our modification linearizes attention with exponential kernel feature maps, whose corresponding feature function is infinite dimensional. We show that our modification is expressible as a composition of log-sums of exponentials, with a latent space of constant size, enabling application with constant time and space complexity per token. We implement our modification, verify that it works in practice, and conclude that it is a promising alternative to conventional attention.\n\n##### *Relevant Chunk: No. 3/8 (Score: 0.98)*\n\n```\n(2021), and Poli et al. (2023). More recently, generalized state space models that build on previous research (Martin and Cundy, 2017) (Gu et al., 2021) have shown promise by incorporating data-driven mechanisms to control the evolution of a fixed-size latent state (Peng et al., 2023) (Gu and Dao, 2023) (Katsch, 2023), but their performance is inferior on certain tasks (e.g., recalling arbitrary parts of the input context), motivating the hypothesis that methods with a fixed-size latent space cannot outperform conventional attention (Jelassi et al., 2024). ### 1.1 Modifying Attention\n\nWe find that a simple modification to conventional attention linearizes it (Katharopoulos et al., 2020) with exponential kernel feature maps, and we show that this modification renders attention expressible as a composition of log-sums of exponentials, with a fixed-size latent space, for sequential application with constant cost per token. We implement our modification, verify that it works, and conclude that it is a promising alternative. The modification we propose is:\n\n$$\n\\begin{gathered}\n\\text { modified }(Q, K, V):= \\\\\n\\text { Attention }(Q) \\\\\n\\text { Softmax }\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V\n\\end{gathered}\n$$\n\nwhere queries $Q$, keys $K$ and values $V$ have $n_{Q} \\times d_{K}, n_{K} \\times d_{K}$, and $n_{K} \\times d_{V}$ elements, respectively, and $c$ is a scalar constant, all in $\\mathbb{R}$. We compute all exponentials elementwise. ### 1.2 As Log-Sums of Exponentials\n\nIn Section 2, we prove that\n\n$$\n\\begin{aligned}\n& \\text { modified } \\\\\n& \\operatorname{Attention}(Q, K, V)=\\exp (\\log S-\\log Z) \\text {, }\n\\end{aligned}\n$$\n\nwhere\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nThe elementwise sums are over compatible dimensions, broadcasting over all other dimensions, from left to right-e.g., before reduction, the broadcasted elementwise sum $K^{T}+\\log V$ has $d_{K} \\times n_{K} \\times d_{V}$ elements. The functions $\\operatorname{LSE}_{\\left[d_{K}\\right]}(\\cdot)$ and $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$ compute log-sums of exponentials over the dimension indexed by $\\left(1,2, \\ldots, d_{K}\\right)$ and $\\left(1,2, \\ldots, n_{K}\\right)$, respectively. If any of $V$ 's elements are negative, $\\log V$ is complex, and therefore so is $\\log S$, but all Softmax mixtures of $V$ remain over $\\mathbb{R}$ because they are a composition of operations under which $\\mathbb{R}$ is closed (1). ### 1.3 Autoregressive Case\n\nFor autoregressive attention, in which $n_{Q}=n_{K}$ and for each query at step $t$ we compute attention only over $t$ trailing tokens, we note that in (3), all sequential dependencies are modeled by the logsums computed with $\\operatorname{LSE}_{\\left[n_{K}\\right]}(\\cdot)$, so we can compute autoregressive $\\log S$ and $\\log Z$ with:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)}_{d_{K} \\times n_{K} \\times d_{V} \\text { elements }}) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q+\\underbrace{\\operatorname{LCSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)}_{d_{K} \\times n_{K} \\text { elements }})\n\\end{aligned}\n$$\n\nwhere the function $\\operatorname{LCSE}_{\\left[n_{K}\\right]}(\\cdot)$ computes a logcumulative-sum of exponentials over the dimension indexed by and $\\left(1,2, \\ldots, n_{K}\\right)$. For sequential application, given a new query $Q_{t}$ at step $t$, we need only the end-states of the two log-cumulative-sums of exponentials:\n\n$$\n\\begin{aligned}\n& \\log S_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(S)}}_{d_{K} \\times d_{V}}) \\\\\n& \\log Z_{t}=\\operatorname{LSE}_{\\left[d_{K}\\right]}(Q_{t}+\\underbrace{H_{t}^{(Z)}}_{d_{K}})\n\\end{aligned}\n$$\n\nwhere hidden states $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the states of the two log-cumulative-sums at step $t$ :\n\n$$\n\\begin{aligned}\n& H_{t}^{(S)}=\\log \\left(\\exp \\left(H_{t-1}^{(S)}\\right)+\\exp \\left(K_{t}+\\log V_{t}\\right)\\right) \\\\\n& H_{t}^{(Z)}=\\log \\left(\\exp \\left(H_{t-1}^{(Z)}\\right)+\\exp \\left(K_{t}\\right)\\right)\n\\end{aligned}\n$$\n\nwith zeros as their initial condition:\n\n$$\n\\begin{aligned}\n& H_{0}^{(S)}=\\{0\\}^{d_{K} \\times d_{V}} \\\\\n& H_{0}^{(Z)}=\\{0\\}^{d_{K}}\n\\end{aligned}\n$$\n\nTogether, $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ hold the latent, or hidden, state of autoregressive attention's computation at step $t$. They enable us to compute autoregressive attention sequentially with constant time and space complexity per token, $\\mathcal{O}(1)$. ### 1.4 Non-Autoregressive Case\n\nFor non-autoregressive attention, in which $n_{Q}$ may differ from $n_{K}$ and for each query we compute attention over all tokens in the sequence, we compute $\\log S$ and $\\log Z$ with (3). For sequential application, in which we add a new token to the input context at step $t$, with key $K_{t}$ and value $V_{t}$, we compute $\\log S$ and $\\log Z$ for all queries from the updated hidden states:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(S)}\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+H_{t}^{(Z)}\\right)\n\\end{aligned}\n$$\n\nwhere $H_{t}^{(S)}$ and $H_{t}^{(Z)}$ are the hidden states at step $t$ (6), with zeros as their initial condition (7). ## 2 Proof\n\nGiven a query $q$ and a key $k$ in $\\mathbb{R}^{d_{K}}$, the logarithm of the dot-product of their exponentials is $\\log \\left(\\sum(\\exp (q) \\odot \\exp (k))\\right)=\\operatorname{LSE}(q+k)$, where $\\odot$ denotes an elementwise product. Log-sums of exponentials are associative and commutative, making the proof fairly straightforward. For clarity's sake, we walk step-by-step through a sequence of algebraic manipulations. We start by expanding the Softmax function in (1) and simplifying the resulting expression. We obtain a form of linear attention (Katharopoulos et al., 2020) with exponential kernel feature maps:\n\n$$\n\\begin{gathered}\n\\operatorname{Softmax}\\left(\\log \\frac{\\exp (Q) \\exp (K)^{T}}{\\exp (c)}\\right) V= \\\\\n{\\left[\\frac{\\exp (Q) \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\exp (Q) \\exp (K)^{T}}\\right] V}\n\\end{gathered}\n$$\n\nwhere $\\sum_{\\left[n_{K}\\right]}$ normalizes each row to a probability distribution. The scaling constant $\\exp (c)$ disappears because it becomes a common divisor of numerator and denominator expressions. Note that the feature function corresponding to the exponential kernel is infinite dimensional. Substitute the dot-products of exponentiated queries and exponentiated keys with equivalent explicit summations over elementwise products:\n\n$$\n\\left[\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\\right] V\n$$\n\nwhere the elementwise product $\\odot$ is over compatible dimensions, broadcasting over any other dimensions, from left to right, such that the broadcasted elementwise product $\\exp (Q) \\odot \\exp (K)^{T}$ has $n_{Q} \\times d_{K} \\times n_{K}$ elements. ${ }^{2}$\nExpress matrix multiplication with $V$ as a summation over broadcasted elementwise products:\n\n$$\n\\frac{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T} \\odot V}{\\sum_{\\left[n_{K}\\right]} \\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\exp (K)^{T}}\n$$\n\nBoth $\\exp (K)^{T}$ and $V$ have a dimension indexed by $\\left(1,2, \\ldots, n_{K}\\right)$, but $\\exp (Q)$ does not, so we can sum over that dimension before broadcastmultiplying elementwise with $\\exp (Q)$ :\n\n$$\n\\frac{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V}{\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}}\n$$\n\nDefine $S$ and $Z$ as the expressions that compute numerators and denominators, respectively,\n\n$$\n\\begin{aligned}\n& S:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T} \\odot V \\\\\n& Z:=\\sum_{\\left[d_{K}\\right]} \\exp (Q) \\odot \\sum_{\\left[n_{K}\\right]} \\exp (K)^{T}\n\\end{aligned}\n$$\n\nand take their logarithms. We obtain:\n\n$$\n\\begin{aligned}\n& \\log S=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}+\\log V\\right)\\right) \\\\\n& \\log Z=\\operatorname{LSE}_{\\left[d_{K}\\right]}\\left(Q+\\operatorname{LSE}_{\\left[n_{K}\\right]}\\left(K^{T}\\right)\\right),\n\\end{aligned}\n$$\n\nwhich is the same as (3). [^1]\n## 3 Implementation\n\nAs proof of concept, we implement our attention mechanism for both autoregressive applications (e.g., generative language modeling) and nonautoregressive applications (e.g., masked language modeling). For simplicity and expediency, we limit our implementation in two significant ways: First, we restrict $V$ to elements $\\geq 0$ to avoid dealing with complex floating-point numbers, which incur greater overhead and are more cumbersome to manipulate than real floating-point numbers with existing software infrastructure. Second, when computing autoregressive attention over $n_{K}$ tokens, we first compute all $n_{K}$ hidden states with a parallel scan, and then reduce them, which is space-inefficient but easier to implement with existing software infrastructure. ${ }^{3}$\n\nWe apply our implementation in a small generative language model ( 125 M parameters, 50257 token ids, 768 embedding features). For numerical stability, in each layer we compute $\\log V$ over $\\mathbb{R}$ directly, with a dense feed-forward transformation of token states, implicitly defining $V$ as $\\log V$ 's exponential but never actually computing it. To remain in $\\mathbb{R}$, we use the logarithm of attention as input to subsequent transformations in the layer, i.e., the input to subsequent transformations is $\\log S-\\log Z$ instead of $\\exp (\\log S-\\log Z)$. Please see our published code for all model details. We train the model on 300B tokens from The Pile (Gao et al., 2020) with a conventional sequence length of 1024 tokens, and obtain a crossentropy loss of 2.47 , competitive with state-of-theart generative language models of similar size. ## 4 Conclusions\n\nBy all indications, our attention mechanism is a promising alternative to the conventional one, but the evidence we have so far is too scant to be conclusive. An adequate comparison requires addressing our implementation's temporary limitations and evaluating models with one to several orders of magnitude more parameters on a diverse set of benchmarks and downstream tasks. [^2]\n## References\n\nRewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. 2019. Generating long sequences with sparse transformers. CoRR abs/1904.10509. Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. Flashattention: Fast and memory-efficient exact attention with io-awareness.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.97)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.97)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 5. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.95)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.95)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space models implementation attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: selective state space models implementation attention*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: selective state space models implementation attention*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 273  (*Influential: 21*)\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: selective state space models implementation attention*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 4. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: selective state space models implementation attention*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n\n*From Search Query: selective state space models implementation attention*\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: selective state space models implementation attention*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. Computation-Efficient Era: A Comprehensive Survey of State Space Models in Medical Image Analysis\n\n*From Search Query: selective state space models implementation attention*\n\n*Ilker Hacihaliloglu, Dorit Merhof, Ulas Bagci, Ali Diba, Reza Azad, Soheila Hatami, Afshin Bozorgpour, Bobby Azad, Sanaz Karimijafarbigloo, Sina Ghorbani Kolahi, Moein Heidari*\n\n**Abstract:** Sequence modeling plays a vital role across various domains, with recurrent neural networks being historically the predominant method of performing these tasks. However, the emergence of transformers has altered this paradigm due to their superior performance. Built upon these advances, transformers have conjoined CNNs as two leading foundational models for learning visual representations. However, transformers are hindered by the $\\mathcal{O}(N^2)$ complexity of their attention mechanisms, while CNNs lack global receptive fields and dynamic weight allocation. State Space Models (SSMs), specifically the \\textit{\\textbf{Mamba}} model with selection mechanisms and hardware-aware architecture, have garnered immense interest lately in sequential modeling and visual representation learning, challenging the dominance of transformers by providing infinite context lengths and offering substantial efficiency maintaining linear complexity in the input sequence. Capitalizing on the advances in computer vision, medical imaging has heralded a new epoch with Mamba models. Intending to help researchers navigate the surge, this survey seeks to offer an encyclopedic review of Mamba models in medical imaging. Specifically, we start with a comprehensive theoretical review forming the basis of SSMs, including Mamba architecture and its alternatives for sequence modeling paradigms in this context. Next, we offer a structured classification of Mamba models in the medical field and introduce a diverse categorization scheme based on their application, imaging modalities, and targeted organs. Finally, we summarize key challenges, discuss different future research directions of the SSMs in the medical domain, and propose several directions to fulfill the demands of this field. In addition, we have compiled the studies discussed in this paper along with their open-source implementations on our GitHub repository.\n\n**Published:** 2024-06-05\n\n\n\n#### 3. Lightweight Single-Image Super-Resolution Network with Attentive Auxiliary Feature Learning\n\n*From Search Query: selective state space models implementation attention*\n\n*Long Chen, Lei Fan, Junchi Yan, Yuzhi Zhao, Qing Wang, Xuehui Wang*\n\n**Abstract:** Despite convolutional network-based methods have boosted the performance of single image super-resolution (SISR), the huge computation costs restrict their practical applicability. In this paper, we develop a computation efficient yet accurate network based on the proposed attentive auxiliary features (A$^2$F) for SISR. Firstly, to explore the features from the bottom layers, the auxiliary feature from all the previous layers are projected into a common space. Then, to better utilize these projected auxiliary features and filter the redundant information, the channel attention is employed to select the most important common feature based on current layer feature. We incorporate these two modules into a block and implement it with a lightweight network. Experimental results on large-scale dataset demonstrate the effectiveness of the proposed model against the state-of-the-art (SOTA) SR methods. Notably, when parameters are less than 320k, A$^2$F outperforms SOTA methods for all scales, which proves its ability to better utilize the auxiliary features. Codes are available at https://github.com/wxxxxxxh/A2F-SR.\n\n**Published:** 2020-11-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating selective state space models (SSMs) with attention mechanisms, focusing on memory efficiency and hardware optimization, here are some key implementation details and techniques:\n\n## Integration of SSMs with Attention Mechanisms\n\n### Combining Global and Local Context\nThe \"SPADE: State Space Augmented Transformer\" approach suggests augmenting a Transformer model with an SSM at the bottom layer. This combination allows the SSM to capture global information, which complements the local attention methods used in the other layers. This hybrid architecture can efficiently handle long-range dependencies and provide a more comprehensive context.\n\n### Dynamic State Transitions\nThe Mamba state space model (SSM) introduces dynamic state transitions based on input content. This is achieved through input-dependent parameters, where the state transition matrices depend on the input, allowing the model to adaptively filter and retain relevant information over time. This mechanism can be integrated with attention to enhance the model's ability to capture dependencies at different timescales.\n\n## Efficient Handling of Long-Range Dependencies\n\n### Fixed-Size Memory States\nSSMs like Mamba offer fixed-size memory states, which are particularly beneficial for handling long sequences efficiently. This approach avoids the quadratic complexity of traditional Transformer models, making it more scalable and memory-efficient.\n\n### Selective Updating of Internal States\nThe S7 model, a simplified yet powerful SSM, dynamically adjusts state transitions based on input content. This selective updating of internal states allows the model to balance long-term and short-term dependencies effectively, which is crucial for capturing complex dependencies in sequential data.\n\n## Memory and Computation Efficiency\n\n### Linear Complexity\nSSMs maintain linear complexity, which is a significant advantage over traditional models that have quadratic complexity. This linear complexity makes SSMs more scalable for long sequences and reduces the computational cost, especially when integrated with attention mechanisms.\n\n### Hardware-Aware Design\nFor hardware optimization, the integration of SSMs with attention mechanisms should consider efficient decoding and encoding processes. The MambaIR model, for example, uses residual state space blocks with convolution and channel attention, which are optimized for hardware efficiency and can produce restoration-specific feature representations efficiently.\n\n## Advanced Gating and Adaptation Mechanisms\n\n### Input-Controlled Transitions\nThe theoretical foundations of deep selective SSMs highlight the importance of input-controlled transitions. These transitions allow the hidden state to capture non-linear interactions between tokens at distinct timescales, enhancing the model's expressiveness and adaptability.\n\n### Hybrid Architectures\nCombining different computational primitives, such as SSMs and attention mechanisms, can improve the model's capabilities. The \"Mechanistic Design and Scaling of Hybrid Architectures\" suggests designing hybrid architectures that carefully balance the interaction between components to achieve both flexibility and computational efficiency.\n\n## Implementation Details\n\n### Residual State Space Blocks\nThe MambaIR model uses residual state space blocks, which employ convolution and channel attention to enhance the capabilities of the vanilla Mamba model. This approach can be adapted for language models to improve their handling of sequential data while maintaining efficiency.\n\n### Input-Dependent Filtering\nThe S7 model introduces input-dependent filtering, which allows the model to adaptively filter and retain relevant information over extended time horizons. This mechanism can be integrated into the attention structure to improve the model's ability to focus on relevant parts of the input data.\n\nBy combining these techniques, researchers can develop an autoregressive language model that leverages the strengths of selective state space models and attention mechanisms, ensuring efficient memory management, hardware optimization, and improved performance in capturing complex dependencies.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design HieraNorm-AttnGPT introduces hierarchical adaptive multi-head attention with dynamic layer normalization. While innovative, there are several areas that could be further improved:\n\n1. The hierarchical attention structure may not optimally capture dependencies at different scales\n2. The dynamic normalization approach could be enhanced with more sophisticated adaptation mechanisms\n3. The design could benefit from more efficient memory management and computational patterns\n\nKey areas identified for potential innovation:\n\n1. Integration of state space models (SSMs) with attention mechanisms\n2. More sophisticated gating and adaptation mechanisms\n3. Efficient handling of long-range dependencies\n4. Hardware-aware optimizations for memory and computation\n\nUseful References with Excerpts:\n\n1. From \"SPADE: State Space Augmented Transformer\":\n\"We propose SPADE, which augments a SSM into the bottom layer of SPADE, and we employ efficient local attention methods for the other layers. The SSM augments global information, which complements the lack of long-range dependency issue in local attention methods.\"\n\nThis suggests potential benefits of combining SSMs with attention mechanisms.\n\n2. From \"MambaByte\":\n\"Operating on bytes results in significantly longer sequences... The recent development of the Mamba state space model (SSM) offers an appealing alternative approach with a fixed-sized memory state and efficient decoding.\"\n\nShows how SSMs can efficiently handle long sequences.\n\n3. From \"Theoretical Foundations of Deep Selective State-Space Models\":\n\"When random linear recurrences are equipped with simple input-controlled transitions (selectivity mechanism), then the hidden state is provably a low-dimensional projection of a powerful mathematical object called the signature of the input -- capturing non-linear interactions between tokens at distinct timescales.\"\n\nProvides theoretical backing for selective SSMs.\n\n4. From \"Mechanistic Design and Scaling of Hybrid Architectures\":\n\"Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives.\"\n\nSuggests benefits of hybrid architectures combining different computational primitives.\n\nKey Insights and Analysis:\n\n1. SSM Integration:\n- SSMs offer efficient handling of long-range dependencies\n- Can complement attention mechanisms by providing global context\n- Theoretical foundations support their effectiveness\n\n2. Hybrid Architecture Benefits:\n- Combining different computational primitives can improve model capabilities\n- Need careful design of interaction between components\n- Should consider hardware efficiency\n\n3. Memory and Computation:\n- Fixed-size memory states from SSMs are advantageous\n- Need efficient mechanisms for managing different timescales\n- Hardware-aware design is crucial for performance\n\n4. Adaptation Mechanisms:\n- Input-controlled transitions improve expressiveness\n- Need balance between flexibility and computational efficiency\n- Should consider both local and global context\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Specific implementations of selective SSMs\n- Efficient hybrid attention-SSM architectures\n- Hardware-efficient implementations\n- Advanced gating mechanisms\n\n2. Focus areas for next search:\n- Technical details of SSM implementations\n- Memory-efficient attention variants\n- Hardware optimization techniques\n\nReferences:\n\n1. Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). Efficient Long Sequence Modeling via State Space Augmented Transformer. arXiv preprint.\n\n2. Wang, J., Gangavarapu, T., Yan, J. N., & Rush, A. M. (2024). MambaByte: Token-free Selective State Space Model. arXiv preprint.\n\n3. Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., & Lyons, T. (2024). Theoretical Foundations of Deep Selective State-Space Models. arXiv preprint.\n\n4. Poli, M., Thomas, A. W., Nguyen, E., Ponnusamy, P., Deiseroth, B., Kersting, K., ... & Massaroli, S. (2024). Mechanistic Design and Scaling of Hybrid Architectures. arXiv preprint."
        },
        {
            "ready": false,
            "query": "hybrid ssm attention implementation memory optimization",
            "detail": "Find specific implementation techniques and optimization strategies for combining SSM and attention mechanisms, focusing on memory management and hardware efficiency.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific implementation techniques and optimization strategies for combining SSM and attention mechanisms, focusing on memory management and hardware efficiency.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.99)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n#### 2. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.99)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n##### *Relevant Chunk: No. 4/86 (Score: 0.99)*\n\n```\nin the style of Megatron (Shoeybi et al. 2019)). The main ideas include introducing grouped-value attention (GVA) head structure, and moving all data-dependent projections to occur in parallel at the beginning of the block. The combination of the modified parallel Mamba block, together with using SSD as the inner SSM layer, results in the Mamba-2 architecture. We investigate Chinchilla scaling laws for Mamba-2 in the same setting as Mamba, finding that it Pareto dominates Mamba and Transformer++ in both perplexity and wall-clock time. We additionally train a family of\n\nMamba-2 models at varying sizes on the Pile, showing that it matches or outperforms Mamba and open source Transformers on standard downstream evaluations. For example, Mamba-2 with 2.7B parameters trained on 300B tokens on the Pile outperforms Mamba-2.8B, Pythia-2.8B and even Pythia-6.9B trained on the same dataset. Systems Optimizations. The SSD framework connects SSMs and Transformers, allowing us to leverage a rich body of work on systems optimizations developed for Transformers (Section 8). - For example, Tensor Parallelism (TP) is an important model parallelism technique to train large Transformer models by splitting each layer across GPUs on the same node. We design Mamba-2 to be TP-friendly, reducing the number of synchronization point per block by half. - For very long sequences whose activations do not fit on one device, sequence parallelism has been developed for the attention blocks. We describe how to train SSMs in general and Mamba-2 in particular with sequence parallelism, by passing the recurrent states between devices. - For finetuning with examples of different lengths, for best efficiency, Transformer requires sophisticated techniques to remove padding tokens and perform attention on variable length sequences. We show how Mamba-2 can be trained with variable sequence lengths efficiently, requiring no padding tokens. Section 9 empirically validates Mamba-2 on language modeling, training efficiency, and a difficult multi-query associative recall task (Arora, Eyuboglu, Zhang, et al. 2024). Finally, in Section 10, we provide an extended related work and discuss potential research directions opened up by our framework. Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 Background and Overview\n\n### 2.1 Structured State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a 1-dimensional sequence $x \\in \\mathbb{R}^{\\top} \\mapsto y \\in \\mathbb{R}^{\\top}$ through an implicit latent state $h \\in \\mathbb{R}^{(\\top, N)}$. A general discrete form of structured SSMs takes the form of equation (1). $$\n\\begin{aligned}\n& h_{t}=A h_{t-1}+B x_{t} \\\\\n& y_{t}=C^{\\top} h_{t}\n\\end{aligned}\n$$\n\n$$\n\\begin{aligned}\n& h_{t}=A_{t} h_{t-1}+B_{t} x_{t} \\\\\n& y_{t}=C_{t}^{\\top} h_{t}\n\\end{aligned}\n$$\n\nwhere $A \\in \\mathbb{R}^{(N, N)}, B \\in \\mathbb{R}^{(N, 1)}, C \\in \\mathbb{R}^{(N, 1)}$. Structured SSMs are so named because the $A$ matrix controlling the temporal dynamics must be structured in order to compute this sequence-to-sequence transformation efficiently enough to be used in deep neural networks. The original structures introduced were diagonal plus low-rank (DPLR) (Gu, Goel, and R\u00e9 2022) and diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; J. T. Smith, Warrington, and Linderman 2023), which remains the most popular structure. In this work, we use the term state space model (SSM) to refer to structured SSMs. There are many flavors of such SSMs, with deep ties to several major paradigms of neural sequence models such as continuous-time, recurrent, and convolutional models (Gu, Johnson, Goel, et al. 2021). We provide a brief overview below, and refer to prior work for more context and details (Gu 2023; Gu and Dao 2023). Continuous-time Models. The original structured SSMs originated as continuous-time maps on functions $x(t) \\in \\mathbb{R} \\mapsto$ $y(t) \\in \\mathbb{R}$, rather than operating directly on sequences. In the continuous-time perspective, in equation (1a) the matrices $(A, B)$ are not directly learned but generated from underlying parameters $(\\AA, B)$, along with a parameterized step size $\\Delta$. The \"continuous parameters\" $(\\Delta, \\AA, B)$ are converted to \"discrete parameters\" $(A, B)$ through fixed formulas $A=f_{A}(\\Delta, \\AA)$ and $B=f_{B}(\\Delta, B)$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Remark 1. While our main models adopt the same parameterization and discretization step as prior work (see Gu and Dao (2023) for details), for simplifying exposition and notation we omit it in the rest of this paper. We note that prior work on\nstructured SSMs referred to the continuous parameters $(\\AA$, have changed notation to simplify the presentation and focus directly on the discrete parameters, which govern the main SSM recurrence. Recurrent Models. Equations (1) and (2) take the form of a recurrence which is linear in its input $x$. Structured SSMs can therefore be viewed as types of recurrent neural networks (RNNs), where the linearity endows them with additional properties and allows them to avoid the sequential computation of traditional RNNs. Conversely, despite this simplification, SSMs are still fully expressive as sequence transformations (in the sense of universal approximation) (Kaul 2020; Orvieto et al. 2023; Shida Wang and Xue 2023). Convolutional Models. When the SSM's dynamics are constant through time as in equation (1), the model is called linear time-invariant (LTI). In this case, they are equivalent to convolutions. Thus, SSMs can also be viewed as types of CNNs, but where (i) the convolution kernels are implicitly parameterized through the SSM parameters $(A, B, C)$ and (ii) the convolution kernels are generally global instead of local. Conversely, through classical signal processing theory all sufficiently well-behaved convolutions can be represented as SSMs. Commonly, previous LTI SSMs would use the convolutional mode for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (1) for efficient autoregressive inference (where the inputs are seen one step at a time). Selective State Space Models. The form (2) where the parameters $(A, B, C)$ can also vary in time was introduced in Mamba as the selective SSM. Compared to the standard LTI formulation (1), this model can selectively choose to focus on or ignore inputs at every timestep. It was shown to perform much better than LTI SSMs on information-dense data such as language, especially as its state size N increases allowing for more information capacity. However, it can only be computed in recurrent instead of convolutional mode, and requires a careful hardware-aware implementation to be efficient. Even so, it is still less efficient than hardware-friendly models such as CNNs and Transformers because it does not leverage matrix multiplication units, which modern accelerators such as GPUs and TPUs are specialized for. While time-invariant SSMs are closely related to continuous, recurrent, and convolutional sequence models, they are not directly related to attention. In this paper, we show a deeper relationship between selective SSMs and attention, and use it to significantly improve the training speed of SSMs while simultaneously allowing for much larger state sizes N . ## Structured SSMs as Sequence Transformations. Definition 2.1. We use the term sequence transformation to refer to a parameterized map on sequences $Y=f_{\\theta}(X)$ where $X, Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ and $\\theta$ is an arbitrary collection of parameters. T represents the sequence or time axis; subscripts index into the first dimension, e.g. $X_{t}, Y_{t} \\in \\mathbb{R}^{P}$. Sequence transformations (e.g. SSMs, or self-attention) are the cornerstone of deep sequence models, where they are incorporated into neural network architectures (e.g. Transformers). The SSM in (1) or (2) is a sequence transformation with $P=1$; it can be generalized to $P>1$ by simply broadcasting across this dimension (in other words, viewing the input as $P$ independent sequences and applying the SSM to each). One can think of $P$ as a head dimension, which we will elaborate on in Section 7. Definition 2.2. We define the SSM operator $\\operatorname{SSM}(A, B, C)=\\operatorname{SSM}\\left(A_{0: T}, B_{0: T}, C_{0: T}\\right)$ as the sequence transformation $X \\in$ $\\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})} \\mapsto Y \\in \\mathbb{R}^{(\\mathrm{T}, \\mathrm{P})}$ defined by equation (2). In SSMs, the $N$ dimension is a free parameter called the state size or state dimension. We also call it the state expansion factor, because it expands the size of the input/output by a factor of $N$, with implications for the computational efficiency of these models. Finally, we remark that many types of sequence transformations, such as attention, can be represented as a single matrix multiplication across the sequence dimension. Definition 2.3. We call a sequence transformation $Y=f_{\\theta}(X)$ a matrix transformation if it can be written in the form $Y=M_{\\theta} X$ where $M$ is a matrix depending on the parameters $\\theta$. We identify the sequence transformation with the matrix $M$, and often drop the dependence on $\\theta$ when clear from context. ### 2.2 Attention\n\nAttention broadly refers to a type of computation that assigns scores to every pair of positions in a sequence, allowing each element to \"attend\" to the rest. By far the most common and important variant of attention is softmax self-attention, which can be defined as\n\n$$\nY=\\operatorname{softmax}\\left(Q K^{\\top}\\right) \\cdot V\n$$\n\nfor $Q, K, V \\in \\mathbb{R}^{(\\top, \\mathrm{P})}$. The mechanism of pairwise comparisons (induced by materializing $Q K^{\\top}$ ) leads to the characteristic quadratic training cost of attention. Many variants of attention have been proposed, but all share the underlying core of these attention scores, with various approximations (Tay et al. 2022). The most important variant for this work is linear attention (Katharopoulos et al. 2020). Roughly speaking, this family of methods drops the softmax by folding it into a kernel feature map, and uses associativity of matrix multiplication to rewrite $\\left(Q K^{\\top}\\right) \\cdot V=Q \\cdot\\left(K^{\\top} V\\right)$. Moreover, in the important case of causal (autoregressive) attention, they show that when the causal mask is incorporated into the left-hand side as $\\left(L \\circ Q K^{\\top}\\right) \\cdot V$, where $L$ is the lower-triangular 1's matrix, then the right-hand side can be expanded as a recurrence.\n```\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models (Avg. Score: 0.97)\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 200  (*Influential: 18*)\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n##### *Relevant Chunk: No. 14/49 (Score: 0.97)*\n\n```\nFlashConv maintains nearly linear scaling, even to very long sequence lengths. Fig. 2 shows overall $2-3 \\times$ speedup over FFTConv with cuFFT using our techniques (block FFT, state-passing). Simple kernel fusion (even without block FFT) can yield speedup over cuFFT for short sequences, since memory reads/writes are the bottleneck for short sequences. For long sequences, SSMs using state passing can be dozens of times faster than even the fastest attention implementation. Table 8: Speedup on the LRA benchmark. | Models | Speedup |\n| :---: | :---: |\n| Transformer | $1 \\times$ |\n| FlashAttention [15] | $2.4 \\times$ |\n| Block-sparse FlashAttention [15] | $2.8 \\times$ |\n| S4 [28] | $2.9 \\times$ |\n| S4 with FLASHConv | $5.8 \\times$ |\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_7a1798f4fbf723c76fd6g-10.jpg?height=540&width=1661&top_left_y=561&top_left_x=228)\n\nFigure 2: We compare the speed of different algorithms to perform FFT-based convolution, along with FlashAttention [15] (the fastest attention implementation we know of). We use batch size 8, hidden dimension 1024, and varying sequence length from 256 to 32 k , and measure on an A100-SMX4-40GB GPU. We see that kernel fusion gives up to $3.4 \\times$ speedup over naive FFTConv for short sequences (up to 512 ), block FFT gives up to $2 \\times$ speedup for medium length sequences $(1 \\mathrm{k}-8 \\mathrm{k})$, and state-passing allows $2.3 \\times$ faster FFTConv for long sequences ( 16 k and above). ## 7 Conclusion\n\nOur main goal is to understand and narrow the gap between attention and SSMs in language modeling in terms of modeling capabilities and hardware efficiency. Our exploration based on synthetic language tasks motivated us to design the H3 layer, which is surprisingly competitive with attention. Our BlockFFTCONV algorithm exploits matrix multiplication units and the dual recurrent-convolution view of SSMs to substantially speed up SSMs, reducing the hardware barrier between attention and SSMs. We are excited about several future directions. Our H3 layer is a simple combination of two SSMs, and more sophisticated designs could be more expressive. Our encouraging results on language models up to 1.3B parameters suggests that scaling SSMs to larger sizes is a promising avenue. Since simply adding two attention layers to H3 models already outperforms both the pure H3 model and Transformers, we are optimistic about combining the complementary strengths of SSMs and attention in the future. ## Acknowledgments\n\nWe thank Albert Gu for helpful discussion regarding the model architecture, and more importantly for sending us daily hippo videos. We thank Together Computer for providing portions of the compute used to train models in this paper. We gratefully acknowledge the support of NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ARL under No. W911NF-21-2-0251 (Interactive Human-AI Teaming); ONR under No. N000141712266 (Unifying Weak Supervision); ONR N00014-20-1-2480: Understanding and Applying Non-Euclidean Geometry in Machine Learning; N000142012275 (NEPTUNE); NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, Google Cloud, Salesforce, Total, the HAI-GCP Cloud Credits for Research program, the Stanford Data Science Initiative (SDSI), Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Wu Tsai Neuroscience Stanford Interdisciplinary Graduate Fellowship, and members of the Stanford DAWN project: Facebook, Google, and VMWare.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.92)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.92)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid ssm attention implementation memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Betty: An Automatic Differentiation Library for Multilevel Optimization\n\n*From Search Query: hybrid ssm attention implementation memory optimization*\n\n*Sang Keun Choe, W. Neiswanger, P. Xie, Eric P. Xing*\n\n**TL;DR:** Betty, a software library for large-scale MLO, is introduced with a novel dataflow graph that allows to develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O( d^2), and enables scaling MLO to models with hundreds of millions of parameters.\n\n**Abstract:** Gradient-based multilevel optimization (MLO) has gained attention as a framework for studying numerous problems, ranging from hyperparameter optimization and meta-learning to neural architecture search and reinforcement learning. However, gradients in MLO, which are obtained by composing best-response Jacobians via the chain rule, are notoriously difficult to implement and memory/compute intensive. We take an initial step towards closing this gap by introducing Betty, a software library for large-scale MLO. At its core, we devise a novel dataflow graph for MLO, which allows us to (1) develop efficient automatic differentiation for MLO that reduces the computational complexity from O(d^3) to O(d^2), (2) incorporate systems support such as mixed-precision and data-parallel training for scalability, and (3) facilitate implementation of MLO programs of arbitrary complexity while allowing a modular interface for diverse algorithmic and systems design choices. We empirically demonstrate that Betty can be used to implement an array of MLO programs, while also observing up to 11% increase in test accuracy, 14% decrease in GPU memory usage, and 20% decrease in training wall time over existing implementations on multiple benchmarks. We also showcase that Betty enables scaling MLO to models with hundreds of millions of parameters. We open-source the code at https://github.com/leopard-ai/betty.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 23  (*Influential: 6*)\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hybrid ssm attention implementation memory optimization*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hybrid ssm attention implementation memory optimization*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 71  (*Influential: 13*)\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid ssm attention implementation memory optimization*\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 5. ChunkAttention: Efficient Self-Attention with Prefix-Aware KV Cache and Two-Phase Partition\n\n*From Search Query: hybrid ssm attention implementation memory optimization*\n\n*Lu Ye, Ze Tao, Yong Huang, Yang Li*\n\n**TL;DR:** ChunkAttention is introduced, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache.\n\n**Abstract:** Self-attention is an essential component of large language models (LLM) but a significant source of inference latency for long sequences. In multi-tenant LLM serving scenarios, the compute and memory operation cost of self-attention can be optimized by using the probability that multiple LLM requests have shared system prompts in prefixes. In this paper, we introduce ChunkAttention, a prefix-aware self-attention module that can detect matching prompt prefixes across multiple requests and share their key/value tensors in memory at runtime to improve the memory utilization of KV cache. This is achieved by breaking monolithic key/value tensors into smaller chunks and structuring them into the auxiliary prefix tree. Consequently, on top of the prefix-tree based KV cache, we design an efficient self-attention kernel, where a two-phase partition algorithm is implemented to improve the data locality during self-attention computation in the presence of shared system prompts. Experiments show that ChunkAttention can speed up the self-attention kernel by 3.2-4.8$\\times$ compared to the state-of-the-art implementation, with the length of the system prompt ranging from 1024 to 4096.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 12  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by combining Selective State Space Models (SSMs) with attention mechanisms, focusing on memory management and hardware efficiency, here are some specific implementation techniques and optimization strategies:\n\n## Hybrid Architecture Implementation\n\n### Interleaved SSM and Attention Layers\nHybrid models like Falcon-Mamba, Zamba, and Jamba integrate SSM layers with Transformer attention layers. This design leverages the strengths of both architectures: SSMs provide efficient global context processing, while attention layers offer selective focus on relevant information. For example, Falcon-Mamba-7B uses a hybrid Mamba-Transformer design, where SSM layers are interleaved with attention layers, demonstrating superior performance and efficiency compared to pure Transformer models.\n\n### Hierarchical Processing\nModels like Zamba2-2.7B employ a hierarchical architecture with Mamba layers alternating with shared attention blocks in an ABAB pattern. This approach allows for better maintenance of information across depth and improves performance by balancing global and local processing.\n\n## Memory Management Innovations\n\n### Fixed-Size Memory States\nSSMs use fixed-size memory states, which reduce memory overhead compared to Transformers. This is particularly beneficial for handling long sequences, as SSMs do not require key-value caching, which dominates the memory footprint of Transformers.\n\n### Selective State Updates\nTechniques like those used in Mamba models involve selective state updates, where the model selectively propagates or forgets information along the sequence length dimension. This approach improves efficiency and reduces memory usage.\n\n### Sequential Prefill Method\nThe Sequential Prefill method, used in Falcon-Mamba, processes the prompt token by token or in larger chunks, mitigating the memory scaling issue seen with parallel prefill. This method allows SSMs to process arbitrarily long prompts efficiently.\n\n## Hardware Efficiency and Optimization\n\n### Hardware-Aware Algorithm Design\nMamba models are designed with hardware-aware algorithms that avoid memory access overhead, achieving linear time complexity and operating faster than traditional methods. This recurrent computation method is optimized for hardware efficiency.\n\n### Tiling and Divide-and-Conquer Strategies\nApproaches like those in CHELA (not explicitly mentioned in the sources but relevant from the analysis) use tiling and hierarchy to implement linear attention in a divide-and-conquer manner. This strategy improves memory access patterns and enables better parallelization, which is crucial for hardware efficiency[Analysis].\n\n### Optimized Cache Utilization\nModels like Falcon-Mamba-7B and Zamba2-2.7B focus on optimized cache utilization. For instance, Falcon-Mamba-7B uses a Sequential Prefill method to mitigate memory scaling issues, and Zamba2-2.7B applies LoRA projectors to shared MLP blocks, allowing for specialized processing with minimal parameter overhead.\n\n## Performance Metrics and Hardware Platforms\n\n### Inference Acceleration\nStudies on inference acceleration for generative LLMs highlight the importance of optimizing for different hardware platforms (CPUs, GPUs, FPGAs, ASICs, and PIM/NDPs). Techniques such as linear attention mechanisms (e.g., Linear Transformer, Attention-Free Transformer) and novel frequency-domain kernelization methods (e.g., DiJiang) can significantly reduce training costs and improve inference speed.\n\nBy integrating these strategies, researchers can develop autoregressive language models that are not only highly performant but also efficient in terms of memory usage and hardware resources. The hybrid SSM-attention architecture, combined with advanced memory management and hardware optimization techniques, offers a promising direction for achieving state-of-the-art performance while maintaining scalability and efficiency.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several key directions for improving the seed design through integration of selective state space models (SSMs) with attention mechanisms:\n\n1. Hybrid Architecture Opportunities:\n- Combining SSMs with attention can leverage complementary strengths\n- Need for careful balance between global and local processing\n- Potential for hardware-efficient implementations\n\n2. Memory Management Innovations:\n- Fixed-size memory states from SSMs offer efficiency advantages\n- Selective state updating can improve long-range dependency modeling\n- Opportunities for optimized cache utilization\n\nUseful References with Excerpts:\n\n1. From Mamba paper (Gu & Dao, 2023):\n\"Simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\n2. From SPADE (Zuo et al., 2022):\n\"SSM augments global information, which complements the lack of long-range dependency issue in local attention methods... employing efficient local attention methods for the other layers.\"\n\n3. From CHELA (Liu et al., 2024):\n\"Leverages the thought of tiling and hierarchy... implements linear attention in a divide-and-conquer manner... enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\n4. From DenseMamba (He et al., 2024):\n\"Selectively integrating shallow-layer hidden states into deeper layers... retains fine-grained information crucial for the final output.\"\n\nKey Insights and Analysis:\n\n1. Architectural Integration:\n- SSMs can provide efficient global context processing\n- Attention mechanisms can offer selective focus on relevant information\n- Hierarchical processing can balance efficiency and expressiveness\n\n2. Memory and State Management:\n- Fixed-size states from SSMs reduce memory overhead\n- Selective state updates improve efficiency\n- Need for careful cache optimization\n\n3. Hardware Considerations:\n- Tiling strategies can improve memory access patterns\n- Divide-and-conquer approaches enable better parallelization\n- Importance of hardware-aware algorithm design\n\n4. Performance Optimization:\n- Balance between computational efficiency and model expressiveness\n- Trade-offs between memory usage and processing speed\n- Opportunities for specialized implementations\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific techniques for efficient SSM-attention integration\n2. Advanced memory management strategies\n3. Hardware optimization approaches for hybrid architectures\n\nNext search queries should focus on:\n1. Implementation details of hybrid SSM-attention architectures\n2. Memory optimization techniques\n3. Hardware-efficient algorithms for sequence processing\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n2. Zuo, S., Liu, X., Jiao, J., Charles, D. X., Manavoglu, E., Zhao, T., & Gao, J. (2022). SPADE: State Space Augmented Transformer for Long Sequence Modeling. arXiv preprint.\n\n3. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n4. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n5. Smith, J. T. H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\n6. Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "multi head ssm implementation gating mechanisms",
            "detail": "Find specific techniques for implementing multi-head SSM architectures and advanced gating mechanisms, focusing on efficient parallel processing and hardware optimization.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques for implementing multi-head SSM architectures and advanced gating mechanisms, focusing on efficient parallel processing and hardware optimization.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.99)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.99)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.99)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.97)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 57/74 (Score: 0.97)*\n\n```\n2019. [113] Shuangfei Zhai, Walter Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, and Josh Susskind. \"An Attention Free Transformer\". In: arXiv preprint arXiv:2105.14103 (2021). [114] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. \"Effectively Modeling Time Series with Simple Discrete State Spaces\". In: The International Conference on Learning Representations (ICLR). 2023. [115] Lin Zheng, Chong Wang, and Lingpeng Kong. \"Linear complexity randomized self-attention mechanism\". In: International Conference on Machine Learning. PMLR. 2022, pp. 27011-27041. [116] Simiao Zuo, Xiaodong Liu, Jian Jiao, Denis Charles, Eren Manavoglu, Tuo Zhao, and Jianfeng Gao. \"Efficient Long Sequence Modeling via State Space Augmented Transformer\". In: arXiv preprint arXiv:2212.08136 (2022). ## A Discussion: Selection Mechanism\n\nOur selection mechanism is inspired by and related to concepts such as gating, hypernetworks, and data-dependence. It can also be viewed as related to \"fast weights\" (J. Ba et al. 2016; Schmidhuber 1992), which connects classical RNNs with the mechanism of linear attention (Schlag, Irie, and Schmidhuber 2021). However, we believe that it is a distinct concept that is worth clarifying. Gating. Gating originally referred to the gating mechanisms of RNNs such as the LSTM (Hochreiter and Schmidhuber 1997) and GRU (J. Chung et al. 2014), or the gated equation (5) in Theorem 1. This was interpreted as a particular mechanism for controlling whether to let an input into the hidden state of an RNN. In particular, this affects the propagation of signal through time and causes inputs to interact along the sequence length dimension. However, the concept of gating has since been relaxed in popular usage to simply mean any multiplicative interaction (often with an activation function). For example, elementwise multiplicative components of neural network architectures (that do not interact along sequence length) are now commonly referred to as gated architectures (Hua et al. 2022; Mehta et al. 2023), despite a very different meaning than the original RNN sense. Thus we believe the original concept of $R N N$ gating versus the popular usage of multiplicative gating actually have a very different semantic meaning. Hypernetworks. Hypernetworks refer to neural networks whose parameters are themselves generated by smaller neural networks. The original idea (Ha, Dai, and Quoc V. Le 2017) used it in a narrow sense to define a large RNN whose recurrent parameters are generated by a smaller RNN, and other variants have been around for a long time (Schmidhuber 1992). Data-dependence. Similar to hypernetworks, data-dependence can refer to any notion where some parameters of the model depend on the data (Poli et al. 2023). Example: GLU Activation. To illustrate the issues with these concepts, consider a simple diagonal linear layer $y=D x$, where $D$ is a diagonal weight parameter. Now suppose that $D$ is itself generated from a linear transformation of $x$, with an optional nonlinearity: $D=\\sigma(\\boldsymbol{W} x)$. Since it is diagonal, the multiplication becomes an elementwise product: $y=\\sigma(W x) \\circ x$. This is a rather trivial transformation, yet it technically satisfies the common meanings of gating (since it has a multiplicative \"branch\"), hypernetworks (since the parameter $\\boldsymbol{D}$ is generated by another layer), and data-dependent (since $\\boldsymbol{D}$ depends on the data $x$ ). However, this in fact simply defines a GLU function, which is so simple that it is often considered just an activation function (Dauphin et al. 2017; Shazeer 2020) instead of a meaningful layer. Selection. Thus, while selection mechanisms could be considered a special case of ideas such as architectural gating, hypernetworks, or data-dependence, so can an enormous range of other constructions-essentially anything with a multiplication, including standard attention mechanisms (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) as well-and we find it uninformative to think of them as such. Instead, we view it as most closely related to the gating mechanism of traditional RNNs, which is a special case (Theorem 1) and also has a deeper history of connections to SSMs through variable (input-dependent) discretization of $\\Delta$ (Funahashi and Nakamura 1993; Gu, Dao, et al. 2020; Tallec and Ollivier 2018). We also eschew the term \"gating\" in favor of selection to clarify the overloaded use of former. More narrowly, we use selection to refer to the mechanistic action of a model to select or ignore inputs and facilitate data interaction along the sequence length (Section 3.1). Beyond selective SSMs and gated RNNs, other examples may include input-dependent convolutions (Kosma, Nikolentzos, and Vazirgiannis 2023; Lioutas and Guo 2020; Lutati, Zimerman, and Wolf 2023; Yang et al. 2019) and even attention. ## B Related Work\n\nWe overview several prior works related to our methods. We mention that some of the most closely related models include recurrent layers such as S4, S5, and quasi-RNNs; as well as end-to-end architectures such as H3, RetNet, and RWKV. ## B. 1 S4 Variants and Derivatives\n\nWe describe a brief overview of some structured SSMs from past work, particularly those that have a relation to our method. - S4 (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) introduced the first structured SSM, describing diagonal structure and diagonal plus low-rank (DPLR). It focused on efficient convolutional algorithms for DPLR SSMs due to a connection to continuous-time online memorization (HIPPO) (Gu, Dao, et al. 2020). - DSS (Gupta, Gu, and Berant 2022) first discovered the empirical effectiveness of diagonal structured SSMs by approximating the HIPPO initialization. This was expanded on theoretically in S4D (Gu, Gupta, et al. 2022). - S5 (Smith, Warrington, and Linderman 2023) independently discovered the diagonal SSM approximation, and is the first S4 model to be computed recurrently with the parallel scan. However, this required lowering the effective state dimension, which they accomplished by switching the SSM dimensions from a SISO (single-input single-output) to MIMO (multi-input multi-output) formulation. Our proposed S6 shares the scan, but differs by (i) keeping the SISO dimensions, which provides a larger effective recurrent state, (ii) using a hardware-aware algorithm to overcome the computation issue, (iii) adding the selection mechanism. Lu et al. (2023) applied S5 to meta-RL in order to handle resetting the SSM state between episode trajectories. Their mechanism can be viewed as a particular hard-coded instance of a selection mechanism, where $\\bar{A}$ is manually set to 0 , instead of our learnable mechanism that depends on the input. It would be interesting to apply selective SSMs generically to this setting and probe if the model has learned to automatically reset its state on episode boundaries. - Mega (Ma et al. 2023) introduced a simplification of S4 to be real- instead of complex- valued, giving it an interpretation of being an exponential moving average (EMA). They additionally make an interesting connection of the discretization step of SSMs to an EMA damping term. Contrary to findings in the original S4 papers, this was the first model to show that real-valued SSMs are empirically effective in certain settings or when combined with different architectural components. - Liquid S4 (Hasani et al. 2023) is also motivated by augmenting S4 with an input-dependent state transition. From this perspective it shares similarity to selection mechanisms, although in a limited form which is still computed convolutionally and close to LTI. - SGConv (Y. Li et al. 2023), Hyena (Poli et al. 2023), LongConv (Fu et al. 2023), MultiresConv (J. Shi, K. A. Wang, and Fox 2023), and Toeplitz Neural Network (Qin, Han, W. Sun, B. He, et al. 2023) all focus on the convolutional representation of S4 and create global or long convolution kernels with different parameterizations. However, these methods cannot do fast autoregressive inference directly. Notably, all of these methods, and all other structured SSMs that we are aware of, have been non-selective and usually strictly LTI (linear time invariant). ## B. 2 SSM Architectures\n\nWe use SSM architectures or state space neural networks (SSNN) to refer to deep neural network architectures incorporating one of the previous SSMs as a black box layer. - GSS (Mehta et al. 2023) was the first gated neural network architecture incorporating SSMs. It is motivated by the gated attention unit (GAU) of Hua et al. (2022) and looks quite similar to our block, except with additional projections. Most importantly, its projection contracts the model dimension to reduce the state size of the SSM, while ours expands the model dimension in order to increase the state size, based on the motivation in Section 3.1. - Mega (Ma et al. 2023) combined the EMA simplification of S4 described above into a hybrid architecture using an efficient attention approximation. - H3 (Dao, Fu, Saab, et al. 2023) is motivated by combining S4 with linear attention (Katharopoulos et al. 2020). It is the first to generalize this formulation of linear attention to more general recurrences, which is also the basis of later architectures. - Selective S4 (J. Wang et al. 2023) incorporates S4 as a black box to generate a binary mask which is multiplied on the input. While sharing the \"selection\" name, we consider this an architectural modification that is closer to architectural gating than a selection mechanism (Appendix A). For example, we hypothesize that it would not solve the Selective\n\nCopying task because simply masking out the irrelevant inputs does not affect the spacing between the relevant ones (indeed, the Selective Copying task can even be viewed as coming pre-masked if the noise tokens are embedded to 0 ).\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.87)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.87)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: multi head ssm implementation gating mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks\n\n*From Search Query: multi head ssm implementation gating mechanisms*\n\n*Xiaocui Yang, Shi Feng, Yifei Zhang, Daling Wang*\n\n**TL;DR:** This paper proposes Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection and introduces multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset.\n\n**Abstract:** With the popularity of smartphones, we have witnessed the rapid proliferation of multimodal posts on various social media platforms. We observe that the multimodal sentiment expression has specific global characteristics, such as the interdependencies of objects or scenes within the image. However, most previous studies only considered the representation of a single image-text post and failed to capture the global co-occurrence characteristics of the dataset. In this paper, we propose Multi-channel Graph Neural Networks with Sentiment-awareness (MGNNS) for image-text sentiment detection. Specifically, we first encode different modalities to capture hidden representations. Then, we introduce multi-channel graph neural networks to learn multimodal representations based on the global characteristics of the dataset. Finally, we implement multimodal in-depth fusion with the multi-head attention mechanism to predict the sentiment of image-text pairs. Extensive experiments conducted on three publicly available datasets demonstrate the effectiveness of our approach for multimodal sentiment detection.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2021\n\n**Citations:** 71  (*Influential: 7*)\n\n#### 2. Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\n\n*From Search Query: multi head ssm implementation gating mechanisms*\n\n*Yekun Chai, Jin Shuo, Xinwen Hou*\n\n**TL;DR:** A gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations is introduced.\n\n**Abstract:** Self-attention mechanisms have made striking state-of-the-art (SOTA) progress in various sequence learning tasks, standing on the multi-headed dot product attention by attending to all the global contexts at different locations. Through a pseudo information highway, we introduce a gated component self-dependency units (SDU) that incorporates LSTM-styled gating units to replenish internal semantic importance within the multi-dimensional latent space of individual representations. The subsidiary content-based SDU gates allow for the information flow of modulated latent embeddings through skipped connections, leading to a clear margin of convergence speed with gradient descent algorithms. We may unveil the role of gating mechanism to aid in the context-based Transformer modules, with hypothesizing that SDU gates, especially on shallow layers, could push it faster to step towards suboptimal points during the optimization process.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 3. Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks\n\n*From Search Query: multi head ssm implementation gating mechanisms*\n\n*Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, Dimitris Papailiopoulos*\n\n**TL;DR:** A hybrid model is introduced, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently, and suggests that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Abstract:** State-space models (SSMs), such as Mamba (Gu&Dao, 2023), have been proposed as alternatives to Transformer networks in language modeling, by incorporating gating, convolutions, and input-dependent token selection to mitigate the quadratic cost of multi-head attention. Although SSMs exhibit competitive performance, their in-context learning (ICL) capabilities, a remarkable emergent property of modern language models that enables task execution without parameter optimization, remain underexplored compared to Transformers. In this study, we evaluate the ICL performance of SSMs, focusing on Mamba, against Transformer models across various tasks. Our results show that SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality. To address these limitations, we introduce a hybrid model, MambaFormer, that combines Mamba with attention blocks, surpassing individual models in tasks where they struggle independently. Our findings suggest that hybrid architectures offer promising avenues for enhancing ICL in language models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 44  (*Influential: 7*)\n\n#### 4. Reinforced Target-driven Conversational Promotion\n\n*From Search Query: multi head ssm implementation gating mechanisms*\n\n*Huy Dao, Lizi Liao, Dung D. Le, Yuxiang Nie*\n\n**TL;DR:** A RTCP framework for conversational promotion that integrates short-term and long-term planning via a balanced gating mechanism and has a strong capability in quickly adapting to unseen scenarios just by updating prefix parameters without re-training the whole model.\n\n**Abstract:** The ability to proactively engage with users towards pitching products is highly desired for conversational assistants. However, existing conversational recommendation methods overemphasize on acquiring user preferences while ignore the strategic planning for nudg-ing users towards accepting a designated item. Hence, these methods fail to promote specified items with engaging responses. In this work, we propose a R einforced T arget-driven C onversational P romotion (RTCP) framework for conversational promotion. Specifically, RTCP integrates short-term and long-term planning via a balanced gating mechanism. In-side which, the dialogue strategies are predicted via knowledge-integrated multi-head attention and guided via reinforcement learning rewards. RTCP then employs an action-guided prefix tuning method to generate relevant responses. Experimental results demonstrate that our model outperforms state-of-the-art models on both automatic metrics and human evaluation. Moreover, RTCP has a strong capability in quickly adapting to unseen scenarios just by updating prefix parameters without re-training the whole model. Code and data are here 1 .\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Attention over Heads: A Multi-Hop Attention for Neural Machine Translation\n\n*From Search Query: multi head ssm implementation gating mechanisms*\n\n*Shohei Iida, Ryuichiro Kimura, Hongyi Cui, Po-Hsuan Hung, T. Utsuro, M. Nagata*\n\n**TL;DR:** The proposed multi-hop attention for the Transformer refines the attention for an output symbol by integrating that of each head, and consists of two hops that efficiently increases the complexity of the model by adding dependencies between heads.\n\n**Abstract:** In this paper, we propose a multi-hop attention for the Transformer. It refines the attention for an output symbol by integrating that of each head, and consists of two hops. The first hop attention is the scaled dot-product attention which is the same attention mechanism used in the original Transformer. The second hop attention is a combination of multi-layer perceptron (MLP) attention and head gate, which efficiently increases the complexity of the model by adding dependencies between heads. We demonstrate that the translation accuracy of the proposed multi-hop attention outperforms the baseline Transformer significantly, +0.85 BLEU point for the IWSLT-2017 German-to-English task and +2.58 BLEU point for the WMT-2017 German-to-English task. We also find that the number of parameters required for a multi-hop attention is smaller than that for stacking another self-attention layer and the proposed model converges significantly faster than the original Transformer.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2019\n\n**Citations:** 13  (*Influential: 2*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using multi-head State Space Models (SSMs) and advanced gating mechanisms, several key techniques and insights can be leveraged from the provided sources and analysis:\n\n## Multi-Head SSM Implementations\n\n### Structured State Space Duality (SSD) Framework\nThe SSD framework, as mentioned in the analysis, allows for the optimal combination of SSM and attention properties. This framework can be used to implement multi-head SSM architectures efficiently. For instance, the SSD algorithm based on block decompositions of semiseparable matrices can be adapted to support multi-head structures, similar to how multi-head attention is implemented in Transformers. This approach enables optimal tradeoffs on efficiency axes such as computation, memory, and parallel processing[3,.\n\n### Bidirectional Mamba Blocks\nThe use of bidirectional Mamba blocks, as described in the context of Multi-Agent Mamba (MAM), can be adapted for multi-head SSM implementations. These blocks can process sequences in both forward and backward directions, enhancing the model's ability to capture global contextual information. This is particularly useful in models like the DBMamba network, which combines CNN-based feature extraction with bidirectional Mamba structures to improve performance.\n\n## Advanced Gating Mechanisms\n\n### Selective State-Space Mechanisms\nAdvanced gating mechanisms can be implemented using selective state-space models. For example, the Mamba model introduces a selective state-space mechanism that allows for efficient modeling of complex input-varying interactions across the sequence dimension. This mechanism involves flipping the linearly projected hidden states and using untied input and output embeddings, which have been shown to improve sample and compute efficiency.\n\n### Cross-Attention Mamba Blocks\nThe introduction of cross-attention Mamba blocks, as seen in the Multi-Agent Mamba (MAM) architecture, can replace traditional attention blocks and offer superior efficiency. These blocks can be used to generalize the RNN gating mechanism, allowing for more flexible and efficient processing of sequence data.\n\n## Efficient Parallel Processing Techniques\n\n### Kernel Fusion and Parallel Scan Algorithms\nKernel fusion and parallel scan algorithms can significantly reduce memory bandwidth requirements and improve the efficiency of SSM implementations. Techniques like FlashConv, which uses a fused block FFT algorithm, can optimize the processing of sequences up to 8K and introduce novel state passing algorithms to scale to longer sequences[3,.\n\n### Block-Based Processing\nBlock-based processing, as part of the SSD framework, can improve cache utilization and enable efficient parallel processing. This approach involves dividing the computation into blocks that can be processed independently, leveraging the linear SSM recurrence and quadratic dual form to achieve optimal efficiency[3,.\n\n## Memory-Bandwidth Optimization Strategies\n\n### Fixed-Size Memory States\nUsing fixed-size memory states from SSMs can reduce overhead and improve memory management. Selective state updates and careful recomputation strategies can minimize memory requirements, making the model more efficient in terms of memory usage[3,.\n\n### Tensor Parallelism\nImplementing tensor parallelism can be done efficiently using the SSD framework, which allows for larger state sizes without significant slowdown. This approach can handle variable sequence lengths without padding, making it suitable for a wide range of sequence modeling tasks[3,.\n\nBy integrating these techniques, researchers can develop autoregressive language models that leverage multi-head SSM architectures, advanced gating mechanisms, and efficient parallel processing and memory optimization strategies, ultimately leading to improved performance, efficiency, and scalability.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several key insights for improving the seed design through advanced integration of SSMs with attention mechanisms and hardware-aware optimizations:\n\n1. State Space Duality (SSD) Framework:\n- Provides theoretical connections between SSMs and attention variants\n- Enables 2-8x faster implementations than standard selective scan\n- Allows larger recurrent state sizes (8x or more) with minimal slowdown\n\n2. Hardware-Efficient Implementation Techniques:\n- Kernel fusion and parallel scan algorithms reduce memory bandwidth requirements\n- Block FFT algorithms improve efficiency for sequences up to 8K\n- Novel state passing algorithms exploit recurrent properties for longer sequences\n\n3. Memory Management Innovations:\n- Fixed-size memory states from SSMs reduce overhead\n- Selective state updates improve efficiency\n- Careful recomputation strategies minimize memory requirements\n\nUseful References with Excerpts:\n\n1. From \"Transformers are SSMs\" (Dao & Gu, 2024):\n\"Our SSD algorithm, based on block decompositions of semiseparable matrices, takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes\"\n\n2. From \"Hungry Hungry Hippos\" (Dao et al., 2022):\n\"FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences\"\n\n3. From \"CHELA\" (Liu et al., 2024):\n\"Leverages the thought of tiling and hierarchy... implements linear attention in a divide-and-conquer manner... enjoys global abstraction and data-dependent selection\"\n\nKey Insights and Analysis:\n\n1. Architectural Integration:\n- SSD framework enables optimal combination of SSM and attention properties\n- Multi-head structures can be adapted from attention to SSMs\n- Parallel processing can be achieved through careful block decomposition\n\n2. Memory Optimization:\n- Kernel fusion reduces memory bandwidth requirements\n- Block-based processing improves cache utilization\n- Recomputation strategies balance memory usage and computation\n\n3. Hardware Efficiency:\n- Parallel scan algorithms enable efficient implementation\n- Block FFT approaches optimize medium-length sequences\n- State passing algorithms handle long sequences effectively\n\n4. Scaling Considerations:\n- SSD enables larger state sizes without significant slowdown\n- Tensor parallelism can be implemented efficiently\n- Variable sequence length handling without padding\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific multi-head SSM implementations\n2. Advanced gating mechanisms for hybrid architectures\n3. Efficient parallel processing techniques\n4. Memory-bandwidth optimization strategies\n\nNext search queries should focus on:\n1. Multi-head SSM architectures and implementations\n2. Advanced gating mechanisms for combining SSM and attention\n3. Hardware-efficient parallel processing techniques\n\nReferences:\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. International Conference on Learning Representations.\n\n3. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv preprint.\n\n4. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n5. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning."
        },
        {
            "ready": false,
            "query": "multi scale ssm architecture normalization",
            "detail": "Find specific techniques for implementing multi-scale SSM architectures and advanced normalization methods, focusing on efficient integration of different scales and stable training.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind specific techniques for implementing multi-scale SSM architectures and advanced normalization methods, focusing on efficient integration of different scales and stable training.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.88)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.88)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.62)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.74)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.49)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.41)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.41)*\n\n```\nIt is important to note that the choice and design of the scaffolding is not well-understood, and often the one that is most performant in practice is selected. ## III. REVIEW OF EXISTING METHODS\n\nIn this section, we present an overview of the most prominent SSM proposals in the literature. Since existing SSMs build on each other, the order of presentation in this section is chronological. We provide details as to how each of the architectures tackles the considerations described in Section $\\Pi$ We also provide a summary of their main characteristics in Table I. ## A. Structured State Space Sequence Model (S4)\n\nThe S4 model [12] was the first proposed model based on a state space representation. a) Parametrization: The S4 model starts from a continuous time model (3), where the structure imposed on matrix $A$ is\n\n$$\nA=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)+r s^{\\star}\n$$\n\nwith $\\lambda_{i} \\in \\mathbb{C} \\forall i$, and $r, s \\in \\mathbb{C}^{p}$. This is, a diagonal matrix plus a low-rank update. We note that this structure resembles a closed-loop dynamics matrix $A_{C L}=A+B K$. b) Discretization: The discrete-time version (4) is computed by applying the bilinear transform to dynamics (3) with discretization step $\\Delta \\in \\mathbb{R}$, i.e.,\n\n$$\n\\bar{A}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1}\\left(I+\\frac{\\Delta}{2} A\\right), \\quad \\bar{B}=\\left(I-\\frac{\\Delta}{2} A\\right)^{-1} \\Delta B\n$$\n\n$\\bar{C}=C$ and $\\bar{D}=D$. Note that this choice of discretization method couples the parameterizations of $\\bar{A}$ and $\\bar{B}$ via the discretization step $\\Delta$, which is a common feature of most SSMs. c) Structure and Initialization: The model is structured in a single input single output (SISO) manner, i.e., each component of the input (referred to as input channel) $u_{i}$ for $i=1, \\ldots, q$ is fed into a separate system (4), each producing a scalar output $y_{j}$ with $j=1, \\ldots, q$. Each dynamics matrix $A$ for each of the $q$ SISO subsystems is initialized using HiPPO theory [13], resulting in the eigenvalues shown in Figure 2. In essence, the HiPPO theory provides a mathematically grounded way to place the eigenvalues of a continuous-time dynamics matrix such that it can compress information over long input sequences into its state. Although the original S4 does not bias the initialization towards marginal stability to ensure long-range memory (as per Lemma 2.2), the follow up work SaShiMi [23] enforces $\\operatorname{Re}\\left(\\lambda_{i}\\right) \\in \\mathbb{R}^{-} \\forall i$ to ensure stability. d) Implementation: At training time, a convolutional representation (5) is used. For efficient computation, the structure of $\\bar{A}$ (6) is exploited since the Sherman-Morrison formula [24] can be used to compute its inverse in (7), resulting in only the inversion of scalars. At inference time, the recurrent representation of the model 4 is directly used. e) Scaffolding: Initially, the scaffolding proposed for the pre- and post-processing of the S4 block was identical to the one used for gated MLPs. Later on, a more sophisticated scaffolding, $H 3$ [25], was introduced to mimic the operations of a Transformer. The H3 scaffolding uses the sum of the original signal with a time-shifted version of the input signal for the linear map of the upper signal and a standard linear map for the lower signal in Figure 1.A.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.22)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.22)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: multi scale ssm architecture normalization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Scaleformer: Iterative Multi-scale Refining Transformers for Time Series Forecasting\n\n*From Search Query: multi scale ssm architecture normalization*\n\n*Amin Shabani, A. Abdi, Li Meng, Tristan Sylvain*\n\n**TL;DR:** A general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.), and demonstrates the effectiveness of each of its contributions across the architecture and methodology.\n\n**Abstract:** The performance of time series forecasting has recently been greatly improved by the introduction of transformers. In this paper, we propose a general multi-scale framework that can be applied to the state-of-the-art transformer-based time series forecasting models (FEDformer, Autoformer, etc.). By iteratively refining a forecasted time series at multiple scales with shared weights, introducing architecture adaptations, and a specially-designed normalization scheme, we are able to achieve significant performance improvements, from 5.5% to 38.5% across datasets and transformer architectures, with minimal additional computational overhead. Via detailed ablation studies, we demonstrate the effectiveness of each of our contributions across the architecture and methodology. Furthermore, our experiments on various public datasets demonstrate that the proposed improvements outperform their corresponding baseline counterparts. Our code is publicly available in https://github.com/BorealisAI/scaleformer.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 41  (*Influential: 2*)\n\n#### 2. TransGAN: Two Pure Transformers Can Make One Strong GAN, and That Can Scale Up\n\n*From Search Query: multi scale ssm architecture normalization*\n\n*Yifan Jiang, Shiyu Chang, Zhangyang Wang*\n\n**TL;DR:** The goal is to conduct the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures, and develops a unique training recipe including a series of techniques that can mitigate the training instability issues of TransGAN.\n\n**Abstract:** The recent explosive interest on transformers has suggested their potential to become powerful\"universal\"models for computer vision tasks, such as classification, detection, and segmentation. While those attempts mainly study the discriminative models, we explore transformers on some more notoriously difficult vision tasks, e.g., generative adversarial networks (GANs). Our goal is to conduct the first pilot study in building a GAN completely free of convolutions, using only pure transformer-based architectures. Our vanilla GAN architecture, dubbed TransGAN, consists of a memory-friendly transformer-based generator that progressively increases feature resolution, and correspondingly a multi-scale discriminator to capture simultaneously semantic contexts and low-level textures. On top of them, we introduce the new module of grid self-attention for alleviating the memory bottleneck further, in order to scale up TransGAN to high-resolution generation. We also develop a unique training recipe including a series of techniques that can mitigate the training instability issues of TransGAN, such as data augmentation, modified normalization, and relative position encoding. Our best architecture achieves highly competitive performance compared to current state-of-the-art GANs using convolutional backbones. Specifically, TransGAN sets new state-of-the-art inception score of 10.43 and FID of 18.28 on STL-10, outperforming StyleGAN-V2. When it comes to higher-resolution (e.g. 256 x 256) generation tasks, such as on CelebA-HQ and LSUN-Church, TransGAN continues to produce diverse visual examples with high fidelity and impressive texture details. In addition, we dive deep into the transformer-based generation models to understand how their behaviors differ from convolutional ones, by visualizing training dynamics. The code is available at https://github.com/VITA-Group/TransGAN.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 335  (*Influential: 15*)\n\n#### 3. GNOT: A General Neural Operator Transformer for Operator Learning\n\n*From Search Query: multi scale ssm architecture normalization*\n\n*Zhongkai Hao, Chengyang Ying, Zhengyi Wang, Hang Su, Yinpeng Dong, Songming Liu, Ze Cheng, Jun Zhu, Jian Song*\n\n**TL;DR:** This work proposes a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators that is highly flexible to handle multiple input functions and irregular meshes and introduces a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems.\n\n**Abstract:** Learning partial differential equations' (PDEs) solution operators is an essential problem in machine learning. However, there are several challenges for learning operators in practical applications like the irregular mesh, multiple input functions, and complexity of the PDEs' solution. To address these challenges, we propose a general neural operator transformer (GNOT), a scalable and effective transformer-based framework for learning operators. By designing a novel heterogeneous normalized attention layer, our model is highly flexible to handle multiple input functions and irregular meshes. Besides, we introduce a geometric gating mechanism which could be viewed as a soft domain decomposition to solve the multi-scale problems. The large model capacity of the transformer architecture grants our model the possibility to scale to large datasets and practical problems. We conduct extensive experiments on multiple challenging datasets from different domains and achieve a remarkable improvement compared with alternative methods. Our code and data are publicly available at \\url{https://github.com/thu-ml/GNOT}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 98  (*Influential: 15*)\n\n#### 4. Wavelet Flow: Fast Training of High Resolution Normalizing Flows\n\n*From Search Query: multi scale ssm architecture normalization*\n\n*Jason J. Yu, K. Derpanis, Marcus A. Brubaker*\n\n**TL;DR:** Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15x faster to train.\n\n**Abstract:** Normalizing flows are a class of probabilistic generative models which allow for both fast density computation and efficient sampling and are effective at modelling complex distributions like images. A drawback among current methods is their significant training cost, sometimes requiring months of GPU training time to achieve state-of-the-art results. This paper introduces Wavelet Flow, a multi-scale, normalizing flow architecture based on wavelets. A Wavelet Flow has an explicit representation of signal scale that inherently includes models of lower resolution signals and conditional generation of higher resolution signals, i.e., super resolution. A major advantage of Wavelet Flow is the ability to construct generative models for high resolution data (e.g., 1024 x 1024 images) that are impractical with previous models. Furthermore, Wavelet Flow is competitive with previous normalizing flows in terms of bits per dimension on standard (low resolution) benchmarks while being up to 15x faster to train.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 38  (*Influential: 6*)\n\n#### 5. Pushing the Boundaries of Boundary Detection using Deep Learning\n\n*From Search Query: multi scale ssm architecture normalization*\n\n*Iasonas Kokkinos*\n\n**TL;DR:** This work shows that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection, and examines the potential of the boundary detector in conjunction with thetask of semantic segmentation.\n\n**Abstract:** In this work we show that adapting Deep Convolutional Neural Network training to the task of boundary detection can result in substantial improvements over the current state-of-the-art in boundary detection. \nOur contributions consist firstly in combining a careful design of the loss for boundary detection training, a multi-resolution architecture and training with external data to improve the detection accuracy of the current state of the art. When measured on the standard Berkeley Segmentation Dataset, we improve theoptimal dataset scale F-measure from 0.780 to 0.808 - while human performance is at 0.803. We further improve performance to 0.813 by combining deep learning with grouping, integrating the Normalized Cuts technique within a deep network. \nWe also examine the potential of our boundary detector in conjunction with the task of semantic segmentation and demonstrate clear improvements over state-of-the-art systems. Our detector is fully integrated in the popular Caffe framework and processes a 320x420 image in less than a second.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2015\n\n**Citations:** 225  (*Influential: 21*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using multi-scale State Space Models (SSMs) and advanced normalization methods, here are some specific techniques and insights from the provided sources:\n\n## Multi-Scale SSM Architectures\n\n### Multi-Scale Vision Mamba (MSVMamba)\n- MSVMamba introduces a multi-scale 2D scanning technique, which can be adapted for sequential data by considering multiple scales of sequence lengths. This approach improves the efficiency of SSMs in handling long-range dependencies and complex patterns. It incorporates a Convolutional Feed-Forward Network (ConvFFN) to enhance channel mixing, which can be analogous to handling different scales of contextual information in language models.\n\n### Spatial-Mamba\n- Spatial-Mamba models extend the SSM architecture to handle multi-dimensional data by using a structure-aware SSM and a Feed-Forward Network (FFN) with skip connections. This architecture can be generalized to handle sequential data at multiple scales by using hierarchical features and local perception units. The use of depth-wise convolution and local neighborhood connectivity in state spaces maintains linear complexity while capturing multi-scale dependencies.\n\n### MambaMixer\n- MambaMixer introduces a dual selection mechanism across tokens and channels, known as the Selective Token and Channel Mixer. This architecture allows layers to access early features directly, which can be beneficial for multi-scale processing by selectively focusing on different parts of the sequence at various scales. This approach can be integrated into language models to improve their ability to capture both local and long-range dependencies.\n\n## Advanced Normalization Methods\n\n### Normalization in Hybrid Architectures\n- When integrating SSMs with other architectures like Transformers, normalization techniques play a crucial role in stabilizing the training process. Techniques such as layer normalization and instance normalization can be adapted to the SSM context. For example, the Heracles model combines a global state space model with a local convolutional network and attention-based token interaction, which includes normalization layers to stabilize the model and improve local detail capture.\n\n### Data-Dependent Normalization\n- The Mamba model's data-dependent selection mechanism can be extended to include data-dependent normalization. This involves normalizing the state space based on the input data, which can help in maintaining stability and efficiency during training. This approach ensures that the model adapts to the input data dynamically, improving its robustness and performance.\n\n## Efficient Integration and Training\n\n### Hardware-Aware Implementation\n- Efficient integration of multi-scale SSMs can be achieved through hardware-aware implementations, such as kernel fusion, parallel scan algorithms, and block-based processing. These techniques reduce memory bandwidth requirements and improve cache utilization, making the model more efficient and scalable.\n\n### Selective State Updates\n- Selective state updates in SSMs, as seen in the Mamba model, can significantly improve efficiency by selectively propagating or forgetting information along the sequence length dimension. This mechanism can be combined with advanced normalization techniques to ensure stable and efficient training.\n\nBy incorporating these techniques, researchers can develop multi-scale SSM architectures that efficiently capture dependencies at various scales, while advanced normalization methods ensure stable and efficient training. These approaches align with the goals of improving autoregressive language model design by enhancing performance, efficiency, and scalability.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for improving the seed design through advanced integration of SSMs, attention mechanisms, and hardware-aware optimizations:\n\n1. State Space Duality (SSD) Framework:\n- Enables optimal combination of SSM and attention properties\n- Provides 2-8x faster implementations than standard selective scan\n- Allows larger recurrent state sizes with minimal slowdown\n\n2. Selection Mechanisms:\n- Input-dependent parameter adaptation improves model flexibility\n- Selective state updates enhance efficiency\n- Dynamic control of information flow through sequence dimension\n\n3. Hardware-Aware Implementation:\n- Kernel fusion reduces memory bandwidth requirements\n- Parallel scan algorithms enable efficient computation\n- Block-based processing improves cache utilization\n\nUseful References with Excerpts:\n\n1. From \"Transformers are SSMs\":\n\"Our SSD algorithm, based on block decompositions of semiseparable matrices, takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes\"\n\nThis provides a framework for efficient implementation of SSM-based architectures.\n\n2. From \"Mamba\":\n\"Simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension\"\n\nShows how selective mechanisms can improve model capabilities.\n\n3. From \"Can Mamba Learn How to Learn?\":\n\"SSMs perform comparably to Transformers in standard regression ICL tasks, while outperforming them in tasks like sparse parity learning. However, SSMs fall short in tasks involving non-standard retrieval functionality.\"\n\nIndicates potential areas where hybrid approaches could be beneficial.\n\nKey Insights and Analysis:\n\n1. Architectural Integration:\n- SSD framework enables efficient combination of SSM and attention\n- Multi-head structures can be adapted from attention to SSMs\n- Selection mechanisms improve model flexibility\n\n2. Memory Management:\n- Fixed-size states reduce memory overhead\n- Selective updates improve efficiency\n- Block-based processing enhances cache utilization\n\n3. Hardware Efficiency:\n- Kernel fusion reduces bandwidth requirements\n- Parallel algorithms enable efficient computation\n- Block decomposition supports parallel processing\n\n4. Scaling Considerations:\n- SSD enables larger state sizes\n- Tensor parallelism can be implemented efficiently\n- Variable sequence length handling without padding\n\nFuture Search Plan:\n\nNeed to investigate:\n1. Specific techniques for multi-scale processing in SSMs\n2. Advanced normalization strategies for hybrid architectures\n3. Memory-efficient attention variants\n4. Hardware-specific optimization techniques\n\nNext search queries should focus on:\n1. Multi-scale SSM architectures\n2. Advanced normalization methods\n3. Memory-efficient attention mechanisms\n\nReferences:\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n3. Park, J., Park, J., Xiong, Z., Lee, N., Cho, J., Oymak, S., Lee, K., & Papailiopoulos, D. (2024). Can Mamba Learn How to Learn? A Comparative Study on In-Context Learning Tasks. arXiv preprint.\n\n4. Yang, X., Feng, S., Zhang, Y., & Wang, D. (2021). Multimodal Sentiment Detection Based on Multi-channel Graph Neural Networks. Annual Meeting of the Association for Computational Linguistics.\n\n5. Chai, Y., Shuo, J., & Hou, X. (2020). Highway Transformer: Self-Gating Enhanced Self-Attentive Networks. Annual Meeting of the Association for Computational Linguistics."
        },
        {
            "ready": false,
            "query": "training stability normalization techniques SSM",
            "detail": "specific techniques for ensuring training stability in hybrid SSM-attention architectures, focusing on normalization methods, gradient flow, and convergence properties",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nspecific techniques for ensuring training stability in hybrid SSM-attention architectures, focusing on normalization methods, gradient flow, and convergence properties\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.04)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 14/31 (Score: 0.04)*\n\n```\nWe provide further details about this study after this section. Spectral filtering. The technique of spectral filtering for learning linear dynamical systems was put forth in [HSZ17]. This work studies online prediction of the sequence of observations $y_{t}$, and the goal is to predict as well as the best symmetric LDS using past inputs and observations. Directly\nlearning the dynamics is a non-convex optimization problem, and spectral filtering is developed as an improper learning technique with an efficient, polynomial-time algorithm and near-optimal regret guarantees. Different from regression-based methods that aim to identify the system dynamics, spectral filtering's guarantee does not depend on the stability of the underlying system, and is the first method to obtain condition number-free regret guarantees for the MIMO setting. Extension to asymmetric dynamical systems was further studied in $\\mathrm{HLS}^{+} 18$. Convolutional Models for Sequence Modeling Exploiting the connnection between Linear dynamical systems and convolutions (as highlighted by [GGR21]) various convolutional models have been proposed for sequence modelling. [FEN ${ }^{+}$23] employ direct learning of convolutional kernels directly to sequence modelling but find that they underperform SSMs. They find the non-smoothness of kernels to be the culprit and propose applying explicit smoothing and squashing operations to the kernels to match performance on the Long Range Arena benchmark. The proposed model still contains significantly large number of parameters growing with the sequence length. [ $\\left.\\mathrm{LCZ}^{+} 22\\right]$ identifies two key characteristics of convolutions to be crucial for long range modelling, decay in filters and small number of parameters parameterizing the kernel. They achieve this via a specific form of the kernel derived by repeating and scaling the kernel in a dyadic fashion. [SWF23] propose a multiresolution kernel structure inspired from the wavelet transform and multiresolution analysis. All these methods parameterize the kernels with specific structures and/or add further regularizations to emulate the convolution kernels implied by SSMs. In contrast our proposed kernels are fixed and thereby parameter-free and the number of parameters scale in the number of kernels and not the size of the kernel. Furthermore our kernels are provably more expressive than linear dynamical systems capable of directly capturing and improving the performance of SSMs without the need for specific initializations. Naturally our kernels (see Fig 1] by default satisfy both the smoothness and the decay condition identified (and explicitly enforced) by $\\left\\lfloor\\mathrm{LCZ}^{+} 22\\right\\rfloor$ and $\\left[\\mathrm{FEN}^{+}\\right.$23]. ## A. 1 Ablations performed by OSG $^{+}$23]\n\nMotivated by the success of SSMs, $\\left[\\mathrm{OSG}^{+} 23\\right]$ revisit the RNN model (under the same deep stacked structure as SSMs) to investigate their efficiency. They begin from a simple linear RNN (a directly parameterized LDS) and add multiple components inspired from the SSM literature to ensure numerical stability and trainability of the model especially as the sequences grow larger. Overall they demonstrate that carefully designed parameterizations and initializations of LDS parameters as well as specifically designed normalizations are all necessary for model to learn consistently over the LRA dataset and in particularly over the 16 K context length task PathX. These interventions are driven by specific intuitions such as an inductive bias towards larger memory or controlling the loss blowup at initialization under long contexts but as such come with no theoretical guarantees towards alleviating the problem. We provide some quick details towards what these interventions are and refer the reader to $\\left[\\mathrm{OSG}^{+} 23\\right]$ to understand the motivations behind them and comparisons with similar ideas existing in previous SSM literature. The LRU model considered by $\\left[\\mathrm{OSG}^{+} 23\\right]$ is given by\n\n$$\ny_{k}=\\operatorname{diag}(\\lambda) y_{k-1}+\\gamma \\odot B u_{k}\n$$\n\nIn the above the learned parameters are $\\lambda$ and $B$ and note that $\\operatorname{diag}(\\lambda)$ corresponds to a diagonal $A$. $\\gamma$ is a specific normalization technique they develop to control the loss blowup under long-context detailed below. They perform the following interventions towards stable training\n\n- Stable Exponential Parameterization: They parameterize $\\lambda$ as\n\n$$\n\\lambda_{j}=\\underbrace{\\exp \\left(-\\exp \\left(\\nu_{j}^{\\log }\\right)\\right.}_{\\text {magnitude }}+i \\underbrace{\\exp \\left(\\theta_{j}^{\\log }\\right)}_{\\text {phase }})\n$$\n\nThe above is done to ensure a bound on the magnitude of eigenvalues of the effective A matrix as well as to ensure more resolution in the parameter space closer to the value of 1 . - Ring Initialization: They initialize the $\\lambda_{j}$ in the complex annulus [min_rad, max_rad]. This ensures that at initialization the magnitude of $\\lambda_{j}$ chosen randomly lies in $\\in$ [min_rad, max_rad] and the phase is chosen randomly. When not applying this intervention min_rad and max_rad are chosen to be 0,1 respectively. When applying this intervention these values are chosen to be closer to 1 , e.g. $0.9,0.999$ respectively. - $\\gamma$-Normalization: They set $\\gamma_{j}=\\sqrt{1-\\left|\\lambda_{j}\\right|^{2}}$\n- Restricting Phase at initialization: Instead of drawing a random phase at initialization the authors recommend selecting the initial phase from $[0, \\pi / 10]$. The authors claim that uniform phase inherently biases the network towards learning spurious features in the input sequence. $\\left[\\mathrm{OSG}^{+} 23\\right]$ provide the following ablation in the paper. In particular we see that all the above interventions are necessary to make the model get to non-trivial accuracy on PathX. On the contrary, as we show the STU model achieves comparable accuracy without requiring any specific initialization or normalization. | Model | Specification | sCIFAR | ListOps | Pathfinder | PathX |\n| :---: | :---: | :---: | :---: | :---: | :---: |\n| LRU | Dense A | 72.2 | 50.4 | $\\boldsymbol{X}$ | $\\boldsymbol{X}$ |\n|  | $\\Lambda$ Exp.\n```\n\n#### 2. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.03)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.03)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.02)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.02)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 4. From generalization analysis to optimization designs for state space models (Avg. Score: 0.02)\n\n*Fusheng Liu, Qianxiao Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n##### *Relevant Chunk: No. 4/32 (Score: 0.02)*\n\n```\n(2018); Chen et al. (2019); Tu et al. (2019) proved norm-based generalization bounds, improving the VC dimension-based bounds by the Rademacher complexity technique (Bartlett and Mendelson, 2002) under the uniform-convergence framework. In the overparameterization settings, it was\nshown in Allen-Zhu and Li (2019) that RNNs can learn some concept class in polynomial time given that the model size is large enough. These generalization bounds, however, do not take into account the temporal dependencies and their effects on generalization. In this work, we provide a new generalization bound by combining the memory structure of the model and the temporal structure of the data. Temporal structure analysis on RNNs. Sequence data has long-range temporal dependencies across the time domain, which notably set it apart from non-sequence data. Recent studies have studied the effects of such temporal dependencies on the approximation and optimization of RNNs. For example, in the two works of Li et al. (2021; 2022), a \"curse of memory\" phenomenon was discovered when using linear RNNs to model the temporal input-output relationships. Particularly, when the target relationship between the input and output has a long-term memory, then both approximation and optimization become extremely challenging. In Wang et al. (2023), the \"curse of memory\" phenomenon on approximation and optimization was extended to non-linear RNNs based on the temporal relationships. In this paper, we conduct a finegrained analysis on the effects of the temporal structure analysis on the generalization of RNNs. Optimization of SSMs. RNN optimization is known for two issues: training stability and computational cost (Bengio et al., 1994; Pascanu et al., 2013). To address these issues and capture the long dependencies efficiently in sequence modeling, the S4 model was proposed by new paraemterization, initialization and discretization (Gu et al., 2022a). Recent variants for the S4 model simplified the hidden state matrix by a diagonal matrix to enhance computational efficiency (Gu et al., 2022b; Gupta et al., 2022; Smith et al., 2023; Orvieto et al., 2023). Regularization methods are also applied for SSMs to prevent overfitting, such as dropout, weight decay and the data continuity regularizer ( Qu et al., 2023). However, the principled way to regularize and initialize the parameters still remains to be explored. In this study, we design a new regularization and initialization scheme to improve both optimization and generalization. ## 3 Preliminaries\n\nIn this section, we briefly introduce the SSM in Section 3.1 and the motivation for optimization designs based on the generalization analysis in Section 3.2. ### 3.1 Introduction to SSMs\n\nIn this paper, we consider the following single-input single-output SSM,\n\n$$\nh^{\\prime}(t)=A h(t)+B x(t), \\quad y(t)=C h(t), \\quad t \\geq 0\n$$\n\nwhere $x$ is the input from an input space ${ }^{1} \\mathcal{X}:=C_{0}\\left(\\mathbb{R}_{\\geq 0}, \\mathbb{R}\\right) ; y(t) \\in \\mathbb{R}$ is the output at time $t ; h(t) \\in \\mathbb{R}^{m}$ is the hidden state with $h(0)=0 ; A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times 1}, C \\in \\mathbb{R}^{1 \\times m}$ are trainable parameters. Then (1) has an explicit solution $y(t)=\\int_{0}^{t} \\rho_{\\theta}(s) x(t-s) d s$, where $\\rho_{\\theta}(s):=C e^{A s} B$ with $\\theta=(C, A, B)$. The function $\\rho_{\\theta}(s)$ captures the memory structure of the model and the temporal input-output relationship (Li et al., 2022). For the S4 model and its variants (Gu et al., 2022a;b; Gupta et al., 2022; Gu et al., 2023), (1) is usually discretized by the Zero-Order Hold method, i.e., given a timescale $\\Delta \\in \\mathbb{R}, h_{k+1}=$ $\\bar{A} h_{k}+\\bar{B} x_{k}, \\quad y_{k}=\\bar{C} h_{k}, \\quad k=0,1, \\ldots$, where $\\bar{A}=e^{\\Delta \\cdot A}, \\bar{B}=\\left(\\bar{A}-\\mathbb{I}_{m}\\right) A^{-1} B, \\bar{C}=C$. Then, $y_{k}=\\bar{C} \\bar{A}^{k} \\bar{B} x_{0}+\\bar{C} \\bar{A}^{k-1} \\bar{B} x_{1}+\\ldots+\\bar{C} \\bar{B} x_{k}=[\\bar{K} * x]_{k}$ where $\\bar{K}=\\left(\\bar{C} \\bar{B}, \\bar{C} \\bar{A} \\bar{B}, \\ldots, \\bar{C} \\bar{A}{ }^{k} \\bar{B}\\right)$ and $*$ represents to convolution. [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-04.jpg?height=421&width=1435&top_left_y=242&top_left_x=402)\n\nFigure 1: The logic diagram goes from generalization analysis to optimization designs. ### 3.2 Motivation: a linear regression model\n\nIn this subsection, we use a linear regression model on non-sequential data as an example to illustrate the connection between the generalization analysis and the optimization designs. This example then motivates us to extend the connection to SSMs on sequential data. Linear regression. We consider a simple linear model $y=\\theta^{\\top} x$ with input $x \\in \\mathbb{R}^{d}$, output $y \\in \\mathbb{R}$ and parameter $\\theta \\in \\mathbb{R}^{d}$. Let the training data $\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{n}$ be i.i.d. sampled from a distribution $\\mathcal{D}$ such that $\\left\\|x_{i}\\right\\|_{2}=r,\\left|y_{i}\\right| \\leq 1(\\forall i \\in[1: n])$. Define the empirical risk $\\mathcal{L}_{n}(\\theta):=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\theta^{\\top} x_{i}-y_{i}\\right)^{2}$ and the population risk $\\mathcal{L}_{\\mathcal{D}}(\\theta):=\\mathbb{E}_{x, y}\\left[\\left(\\theta^{\\top} x-y\\right)^{2}\\right]$. Then given a norm-constrained space $\\Theta:=\\left\\{\\theta \\in \\mathbb{R}^{d}:\\|\\theta\\|_{2} \\leq\\right.$ $R\\}$, with probability at least $1-\\delta$ over $\\mathcal{D}$,\n\n$$\n\\sup _{\\theta \\in \\Theta}\\left|\\mathcal{L}_{n}(\\theta)-\\mathcal{L}_{\\mathcal{D}}(\\theta)\\right| \\leq(r R+1)^{2} \\cdot \\mathcal{O}(\\sqrt{\\log (1 / \\delta) / n})\n$$\n\nThis is a well-known norm-based generalization bound based on the Rademacher theory (Mohri et al., 2012), and we provide a proof in Appendix B for completeness. Notice that the key term $r^{2} R^{2}$ in the generalization bound (2) is also an upper bound for the magnitude of the linear model output, i.e., $\\sup _{\\theta \\in \\Theta}\\left(\\theta^{\\top} x_{i}\\right)^{2} \\leq r^{2} R^{2}$. Thus, we connect the model stability with the generalization bound stability, and this connection induces an initialization scheme for the initialization $\\theta^{(0)}$ by setting $\\left\\|\\theta^{(0)}\\right\\|_{2} \\sim \\mathcal{O}(1 / r)$. In particular, if we normalize each input $x_{i}$ such that $r$ is also $\\mathcal{O}(1)$, then $\\left\\|\\theta^{(0)}\\right\\|_{2} \\sim \\mathcal{O}(1)$. Since $\\theta^{(0)} \\in \\mathbb{R}^{d}$, one possible initialization scheme is that $\\theta^{(0)}$ follows a Uniform distribution $U[-1 / \\sqrt{d}, 1 / \\sqrt{d}]$, which corresponds to the Kaiming initialization (up to some constant) (He et al., 2015). When treating the term $r^{2} R^{2}$ as a regularizer to improve the generalization, we get the weight decay method, i.e., the $\\ell_{2}$ regularization w.r.t.\n```\n\n#### 5. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.02)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.02)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: training stability normalization techniques SSM\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Training Energy-Based Normalizing Flow with Score-Matching Objectives\n\n*From Search Query: training stability normalization techniques SSM*\n\n*Chen-Hao Chao, Wei-Fang Sun, Yen-Chang Hsu, Z. Kira, Chun-Yi Lee*\n\n**TL;DR:** By optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed, which enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity.\n\n**Abstract:** In this paper, we establish a connection between the parameterization of flow-based and energy-based generative models, and present a new flow-based modeling approach called energy-based normalizing flow (EBFlow). We demonstrate that by optimizing EBFlow with score-matching objectives, the computation of Jacobian determinants for linear transformations can be entirely bypassed. This feature enables the use of arbitrary linear layers in the construction of flow-based models without increasing the computational time complexity of each training iteration from $O(D^2L)$ to $O(D^3L)$ for an $L$-layered model that accepts $D$-dimensional inputs. This makes the training of EBFlow more efficient than the commonly-adopted maximum likelihood training method. In addition to the reduction in runtime, we enhance the training stability and empirical performance of EBFlow through a number of techniques developed based on our analysis of the score-matching methods. The experimental results demonstrate that our approach achieves a significant speedup compared to maximum likelihood estimation while outperforming prior methods with a noticeable margin in terms of negative log-likelihood (NLL).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. How Does Batch Normalization Help Optimization? (No, It Is Not About Internal Covariate Shift)\n\n*From Search Query: training stability normalization techniques SSM*\n\n*Shibani Santurkar, Dimitris Tsipras, Andrew Ilyas, A. Madry*\n\n**TL;DR:** It is demonstrated that such distributional stability of layer inputs has little to do with the success of BatchNorm, and this smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.\n\n**Abstract:** Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called \"internal covariate shift\". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training. These findings bring us closer to a true understanding of our DNN training toolkit.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1435  (*Influential: 70*)\n\n#### 3. Why Spectral Normalization Stabilizes GANs: Analysis and Improvements\n\n*From Search Query: training stability normalization techniques SSM*\n\n*Zinan Lin, Vyas Sekar, G. Fanti*\n\n**TL;DR:** This work shows that SN controls two important failure modes of GAN training: exploding and vanishing gradients, and proposes Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization.\n\n**Abstract:** Spectral normalization (SN) is a widely-used technique for improving the stability of Generative Adversarial Networks (GANs) by forcing each layer of the discriminator to have unit spectral norm. This approach controls the Lipschitz constant of the discriminator, and is empirically known to improve sample quality in many GAN architectures. However, there is currently little understanding of why SN is so effective. In this work, we show that SN controls two important failure modes of GAN training: exploding and vanishing gradients. Our proofs illustrate a (perhaps unintentional) connection with the successful LeCun initialization technique, proposed over two decades ago to control gradients in the training of deep neural networks. This connection helps to explain why the most popular implementation of SN for GANs requires no hyperparameter tuning, whereas stricter implementations of SN have poor empirical performance out-of-the-box. Unlike LeCun initialization which only controls gradient vanishing at the beginning of training, we show that SN tends to preserve this property throughout training. Finally, building on this theoretical understanding, we propose Bidirectional Spectral Normalization (BSN), a modification of SN inspired by Xavier initialization, a later improvement to LeCun initialization. Theoretically, we show that BSN gives better gradient control than SN. Empirically, we demonstrate that BSN outperforms SN in sample quality on several benchmark datasets, while also exhibiting better training stability.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 35  (*Influential: 3*)\n\n#### 4. Beyond BatchNorm: Towards a Unified Understanding of Normalization in Deep Learning\n\n*From Search Query: training stability normalization techniques SSM*\n\n*Ekdeep Singh Lubana, R. Dick, Hidenori Tanaka*\n\n**TL;DR:** A unified set of mechanisms that underpin the success of normalization methods in deep learning are revealed, providing a compass to systematically explore the vast design space of DNN normalization layers.\n\n**Abstract:** Inspired by BatchNorm, there has been an explosion of normalization layers in deep learning. Recent works have identified a multitude of beneficial properties in BatchNorm to explain its success. However, given the pursuit of alternative normalization layers, these properties need to be generalized so that any given layer's success/failure can be accurately predicted. In this work, we take a first step towards this goal by extending known properties of BatchNorm in randomly initialized deep neural networks (DNNs) to several recently proposed normalization layers. Our primary findings follow: (i) similar to BatchNorm, activations-based normalization layers can prevent exponential growth of activations in ResNets, but parametric techniques require explicit remedies; (ii) use of GroupNorm can ensure an informative forward propagation, with different samples being assigned dissimilar activations, but increasing group size results in increasingly indistinguishable activations for different samples, explaining slow convergence speed in models with LayerNorm; and (iii) small group sizes result in large gradient norm in earlier layers, hence explaining training instability issues in Instance Normalization and illustrating a speed-stability tradeoff in GroupNorm. Overall, our analysis reveals a unified set of mechanisms that underpin the success of normalization methods in deep learning, providing us with a compass to systematically explore the vast design space of DNN normalization layers.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 29  (*Influential: 1*)\n\n#### 5. Adversarial Lipschitz Regularization\n\n*From Search Query: training stability normalization techniques SSM*\n\n*D\u00e1vid Terj\u00e9k*\n\n**TL;DR:** It is shown that using an explicit Lipschitz penalty is indeed viable and leads to competitive performance when applied to Wasserstein GANs, highlighting an important connection between LipsChitz regularization and adversarial training.\n\n**Abstract:** Generative adversarial networks (GANs) are one of the most popular approaches when it comes to training generative models, among which variants of Wasserstein GANs are considered superior to the standard GAN formulation in terms of learning stability and sample quality. However, Wasserstein GANs require the critic to be 1-Lipschitz, which is often enforced implicitly by penalizing the norm of its gradient, or by globally restricting its Lipschitz constant via weight normalization techniques. Training with a regularization term penalizing the violation of the Lipschitz constraint explicitly, instead of through the norm of the gradient, was found to be practically infeasible in most situations. With a novel generalization of Virtual Adversarial Training, called Adversarial Lipschitz Regularization, we show that using an explicit Lipschitz penalty is indeed viable and leads to competitive performance when applied to Wasserstein GANs, highlighting an important connection between Lipschitz regularization and adversarial training.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 51  (*Influential: 4*)\n\n\n\n---\n## Web search results\n\n To improve the training stability and overall performance of hybrid State Space Models (SSMs) combined with attention mechanisms in autoregressive language models, here are some specific techniques and insights from the provided sources and additional context:\n\n## Normalization Techniques\n- **Stable Reparameterization**: The S7 model introduces a stable reparameterization technique to ensure that the state transitions remain well-behaved over time. This involves modifying the transition matrix using a stability function to prevent unstable behavior, which is crucial for maintaining training stability.\n\n## Gradient Flow and Convergence Properties\n- **Gradient Norm Control**: The S7 model also ensures that the gradient norm is controlled, preventing issues like exploding or vanishing gradients. This is achieved through stable reparameterization, which promotes smooth transitions and stable learning.\n- **Lipschitz Continuity**: Ensuring that the mappings from model parameters to state transition matrices are Lipschitz continuous helps in promoting stable learning and smooth transitions over time. This is a key assumption in the S7 model, which ensures that small parameter changes lead to small changes in the state transition matrices.\n\n## Hybrid SSM-Attention Architectures\n- **Input-Dependent State Transitions**: The S7 model incorporates input-dependent state transitions, which can be beneficial in hybrid architectures. This allows the model to dynamically adjust state transitions based on input content, enhancing efficiency and performance.\n- **Hierarchical Processing**: Combining SSMs with hierarchical attention mechanisms can reduce computational complexity and improve the flow of information. For example, DenseMamba enhances the flow of hidden information between layers in SSMs, which can be integrated with hierarchical attention to improve overall efficiency.\n\n## Training Stability in Long-Sequence Modeling\n- **SPikE-SSM**: This model introduces a soft reset mechanism and additional trainable dynamic variables, which can help in long-sequence learning. The explicit training of neuron hyperparameters and temporal dynamics can improve the stability and performance of the model, especially when combined with attention mechanisms.\n\n## Hardware Optimization and Memory Management\n- While the primary focus is on training stability, efficient hardware optimization is also crucial. Techniques such as state compression, efficient cache utilization, and careful gradient flow management are essential. For instance, LOCOST and DenseMamba provide strategies for reducing memory footprint and optimizing computational complexity, which can be integrated with the stability techniques mentioned above.\n\n### Summary of Key Techniques\n\n- **Stable Reparameterization**: Use stability functions to modify transition matrices and ensure well-behaved state transitions.\n- **Gradient Norm Control**: Control gradient norms to prevent exploding or vanishing gradients.\n- **Lipschitz Continuity**: Ensure mappings from parameters to state transition matrices are Lipschitz continuous for stable learning.\n- **Input-Dependent State Transitions**: Dynamically adjust state transitions based on input content to enhance efficiency and performance.\n- **Hierarchical Processing**: Combine SSMs with hierarchical attention to reduce complexity and improve information flow.\n- **Soft Reset Mechanisms**: Employ soft reset mechanisms and trainable dynamic variables for improved long-sequence learning stability.\n\nThese techniques can help in designing a more stable and efficient hybrid SSM-attention architecture for autoregressive language models.\n",
            "analysis": "Analysis Summary:\n\nThe search results provide valuable insights into implementing hybrid architectures combining SSMs with hierarchical attention. Key findings:\n\n1. SSM Implementation Techniques:\nFrom Mamba paper:\n\"The selection mechanism is designed to overcome the limitations of LTI models... addressing this with three classical techniques: kernel fusion, parallel scan, and recomputation.\"\n\n2. Memory Efficiency:\nFrom DenseMamba:\n\"DenseSSM enhances the flow of hidden information between layers in SSMs by selectively integrating shallow-layer hidden states into deeper layers, retaining fine-grained information crucial for the final output.\"\n\n3. Hardware Optimization:\nFrom LOCOST:\n\"With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models while saving up to 50% memory during training and up to 87% during inference.\"\n\nKey Insights:\n\n1. Selective State Updates:\n- Input-dependent state transitions improve efficiency\n- Dynamic parameter generation based on context\n- Careful memory management through state compression\n\n2. Integration Strategies:\n- Hierarchical processing reduces complexity\n- Efficient combination of local and global information\n- Hardware-aware implementation crucial for performance\n\n3. Memory Management:\n- State compression techniques reduce memory footprint\n- Efficient cache utilization important\n- Careful gradient flow management needed\n\nFuture Search Needed:\n\n1. Training Stability:\n- Need more information on normalization techniques\n- Gradient flow in hybrid architectures\n- Convergence properties\n\n2. Hardware Implementation:\n- Specific optimization techniques\n- Memory access patterns\n- Cache utilization strategies\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\" arXiv preprint.\n\n2. He, W., et al. (2024). \"DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models.\" arXiv preprint.\n\n3. Le Bronnec, F., et al. (2024). \"LOCOST: State-Space Models for Long Document Abstractive Summarization.\" EACL.\n\n4. Liu, Z., et al. (2024). \"Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\" arXiv preprint.\n\nNeed additional search focusing on training stability and hardware optimization."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "selective state space models hierarchical attention hybrid",
            "detail": "implementation techniques for combining selective state space models with hierarchical attention mechanisms, focusing on memory efficiency, computational complexity, and training stability",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nimplementation techniques for combining selective state space models with hierarchical attention mechanisms, focusing on memory efficiency, computational complexity, and training stability\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 1.00)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.98)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.99)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.99)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 3. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.95)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.95)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 4. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.95)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.95)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space models hierarchical attention hybrid\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: selective state space models hierarchical attention hybrid*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 273  (*Influential: 21*)\n\n#### 2. Efficient Classification of Long Documents via State-Space Models\n\n*From Search Query: selective state space models hierarchical attention hybrid*\n\n*Peng Lu, Suyuchen Wang, Mehdi Rezagholizadeh, Bang Liu, I. Kobyzev*\n\n**TL;DR:** This paper investigates the use of State-Space Models (SSMs) for long document classification tasks and introduces the SSM-pooler model, which achieves comparable performance while being on average 36% more efficient than self-attention-based models.\n\n**Abstract:** Transformer-based models have achieved state-of-the-art performance on numerous NLP applications. However, long documents which are prevalent in real-world scenarios cannot be efficiently processed by transformers with the vanilla self-attention module due to their quadratic computation complexity and limited length extrapolation ability. Instead of tack-ling the computation difficulty for self-attention with sparse or hierarchical structures, in this paper, we investigate the use of State-Space Models (SSMs) for long document classification tasks. We conducted extensive experiments on six long document classification datasets, including binary, multi-class, and multi-label classification, comparing SSMs (with and without pre-training) to self-attention-based models. We also introduce the SSM-pooler model and demonstrate that it achieves comparable performance while being on average 36% more efficient. Additionally our method exhibits higher robustness to the input noise even in the extreme scenario of 40%.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: selective state space models hierarchical attention hybrid*\n\n*Tri Dao, Albert Gu*\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 163  (*Influential: 37*)\n\n#### 4. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space models hierarchical attention hybrid*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Hierarchical Attention Prototypical Networks for Few-Shot Text Classification\n\n*From Search Query: selective state space models hierarchical attention hybrid*\n\n*Shengli Sun, Qingfeng Sun, Kevin Zhou, Tengchao Lv*\n\n**TL;DR:** This work proposes a hierarchical attention prototypical networks (HAPN) for few-shot text classification and designs the feature level, word level, and instance level multi cross attention for this model to enhance the expressive ability of semantic space.\n\n**Abstract:** Most of the current effective methods for text classification tasks are based on large-scale labeled data and a great number of parameters, but when the supervised training data are few and difficult to be collected, these models are not available. In this work, we propose a hierarchical attention prototypical networks (HAPN) for few-shot text classification. We design the feature level, word level, and instance level multi cross attention for our model to enhance the expressive ability of semantic space, so it can highlight or weaken the importance of the features, words, and instances separately. We verify the effectiveness of our model on two standard benchmark few-shot text classification datasets\u2014FewRel and CSID, and achieve the state-of-the-art performance. The visualization of hierarchical attention layers illustrates that our model can capture more important features, words, and instances. In addition, our attention mechanism increases support set augmentability and accelerates convergence speed in the training stage.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 110  (*Influential: 7*)\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model by integrating selective state space models (SSMs) with hierarchical attention mechanisms, several key aspects and techniques can be considered based on the provided sources and the detailed analysis.\n\n## Integration of SSMs with Hierarchical Attention\n\n### Hybrid Architectures\n- The concept of combining global state space models with local attention-based mechanisms is explored in the \"Heracles\" model, which integrates a global state space model, a local convolutional network, and attention-based token interaction. This approach can be adapted for language models to stabilize scaling and improve local detail capture.\n- The \"MambaMixer\" architecture uses a dual selection mechanism across tokens and channels, which could be a starting point for integrating SSMs with hierarchical attention. This model allows layers to access early features directly, potentially enhancing the synergy between SSMs and attention mechanisms.\n\n### Selective State Space Models\n- The \"Mamba\" framework introduces the selective scan mechanism (S6) with a hardware-aware algorithm, which enhances both training and inference efficiency. This can be crucial for scaling the model linearly with respect to sequence length while maintaining performance.\n- \"CU-Mamba\" integrates a dual SSM framework for efficient long-range dependency modeling, using Spatial SSM for global context and Channel SSM to preserve channel correlations. This dual approach could be adapted for hierarchical attention in language models.\n\n## Dual-Stream Architecture\n\n### Parallel Processing Streams\n- The \"Heracles\" model demonstrates the effectiveness of combining a global state space model with a local convolutional network, which can be translated into a dual-stream architecture for language models. This approach can help in parallel processing and reduce computational overhead.\n- The \"Hybrid Attention Separable Network\" (HASN) uses depthwise separable convolutions and fully connected layers to reduce computational load and the number of parameters. Similar techniques can be applied to language models to optimize memory and computational efficiency.\n\n## Advanced Normalization Techniques\n\n### Impact on Training Stability\n- Advanced normalization techniques such as layer normalization and activation functions (e.g., ReLU6) are used in the HASN model to stabilize feature learning and prevent the vanishing gradient problem. These techniques can be applied to the hybrid SSM-attention architecture to improve training stability.\n\n## Hardware Efficiency\n\n### Implementation Feasibility and Memory Access Patterns\n- The \"Mamba\" framework's hardware-aware algorithm ensures that the model scales linearly with respect to sequence length, which is crucial for efficient implementation and memory access patterns. This approach can be adapted to ensure hardware efficiency in the proposed hybrid language model.\n- The use of depthwise separable convolutions and selective state space models can significantly reduce the computational load and the number of parameters, making the model more hardware-efficient.\n\n## Specific Implementation Techniques\n\n### Hierarchical Attention Mechanisms\n- Hierarchical attention can be implemented by combining channel attention and spatial attention, as seen in the HASN model. This hybrid attention approach can be integrated with SSMs to leverage their complementary advantages.\n\n### Integration Mechanisms\n- The \"Mamba-R\" architecture introduces register tokens to improve feature map quality, which can be a useful mechanism for integrating SSMs with hierarchical attention. This ensures cleaner and more focused feature maps, potentially enhancing the model's performance.\n\n### Training Stability and Efficiency\n- The \"Vision Mamba\" model replaces self-attention with bidirectional SSMs, improving visual representation learning while enhancing computational and memory efficiency. Similar strategies can be applied to language models to improve training stability and efficiency.\n\nBy combining these insights, researchers can design an autoregressive language model that integrates selective state space models with hierarchical attention mechanisms, ensuring memory efficiency, computational complexity reduction, and improved training stability.\n\n- **Integration**: Use the selective scan mechanism from Mamba and combine it with hierarchical attention, similar to the Heracles model's approach of integrating global and local mechanisms.\n- **Dual-Stream Architecture**: Implement parallel processing streams using techniques from HASN and Heracles to optimize memory and computational efficiency.\n- **Normalization**: Apply advanced normalization techniques like layer normalization and specific activation functions to stabilize feature learning and training dynamics.\n- **Hardware Efficiency**: Ensure the model scales linearly with sequence length using hardware-aware algorithms from the Mamba framework, and optimize memory access patterns and parallel processing capabilities.\n\nThis approach can lead to a novel autoregressive language model that outperforms existing state-of-the-art models in terms of perplexity, accuracy, robustness, and scalability.\n",
            "analysis": "Initial Analysis of HybridSSM-GPT Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Integration of SSMs with Hierarchical Attention:\n- Novel combination of selective state space models with hierarchical attention\n- Potential synergies and trade-offs\n- Implementation complexity and efficiency\n\n2. Dual-Stream Architecture:\n- Effectiveness of parallel processing streams\n- Memory and computational overhead\n- Integration mechanisms between streams\n\n3. Advanced Normalization Techniques:\n- Impact on training stability\n- Computational overhead\n- Memory efficiency\n\n4. Hardware Efficiency:\n- Implementation feasibility\n- Memory access patterns\n- Parallel processing capabilities\n\nInvestigation Plan:\n\n1. SSM and Attention Integration:\n- Search for existing hybrid architectures combining SSMs and attention\n- Investigate performance characteristics of similar approaches\n- Analyze implementation challenges\n\n2. Dual-Stream Processing:\n- Research parallel processing architectures in language models\n- Study memory management techniques\n- Evaluate integration strategies\n\n3. Normalization and Training Stability:\n- Examine advanced normalization techniques\n- Study training dynamics in hybrid architectures\n- Analyze stability issues\n\n4. Hardware Considerations:\n- Research efficient implementations\n- Study memory access patterns\n- Investigate parallel processing strategies\n\nInitial search results suggest this is a novel approach, as no direct precedent combining selective SSMs with hierarchical attention in this specific way was found. However, there are related works in hybrid architectures and efficient implementations that can inform the design.\n\nThe proposal shows potential for significant impact:\n- Accuracy: Dual-stream processing could improve model performance\n- Robustness: Multiple processing paths provide redundancy\n- Efficiency: Linear complexity through careful design\n- Scalability: Hardware-aware implementation supports scaling\n\nFuture search needed to:\n1. Find similar hybrid architectures\n2. Study SSM implementations\n3. Investigate attention mechanisms\n4. Research normalization techniques\n5. Examine hardware optimization strategies"
        },
        {
            "ready": false,
            "query": "hierarchical attention implementation training stability",
            "detail": "specific techniques for implementing hierarchical attention in transformer models, focusing on training stability, gradient flow, and convergence properties",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nspecific techniques for implementing hierarchical attention in transformer models, focusing on training stability, gradient flow, and convergence properties\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning (Avg. Score: 0.83)\n\n*Aniket Didolkar, Kshitij Gupta, Anirudh Goyal, Alex Lamb, Nan Rosemary Ke, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 14  (*Influential: 3*)\n\n**TL;DR:** The proposed approach hopes to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream and shows the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines.\n\n**Abstract:** Recurrent neural networks have a strong inductive bias towards learning temporally compressed representations, as the entire history of a sequence is represented by a single vector. By contrast, Transformers have little inductive bias towards learning temporally compressed representations, as they allow for attention over all previously computed elements in a sequence. Having a more compressed representation of a sequence may be beneficial for generalization, as a high-level representation may be more easily re-used and re-purposed and will contain fewer irrelevant details. At the same time, excessive compression of representations comes at the cost of expressiveness. We propose a solution which divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation, by forcing chunks of $K$ time steps into a single representation which is divided into multiple vectors. At the same time, a fast stream is parameterized as a Transformer to process chunks consisting of $K$ time-steps conditioned on the information in the slow-stream. In the proposed approach we hope to gain the expressiveness of the Transformer, while encouraging better compression and structuring of representations in the slow stream. We show the benefits of the proposed method in terms of improved sample efficiency and generalization performance as compared to various competitive baselines for visual perception and sequential decision making tasks.\n\n##### *Relevant Chunk: No. 42/46 (Score: 0.83)*\n\n```\n[N/A]\n(b) Did you describe any potential participant risks, with links to Institutional Review Board (IRB) approvals, if applicable? [N/A]\n(c) Did you include the estimated hourly wage paid to participants and the total amount spent on participant compensation? [N/A]\n\n## Appendix\n\n## 6 Related Work\n\nHierarchical or Multiscale Recurrent neural networks. This work takes inspiration from a wide array of work on introducing multiple scales of processing into recurrent neural networks (Chung et al. 2016; Hihi \\& Bengio, 1995; Mozer, 1991, Schmidhuber, 1991, Koutn\u00edk et al., 2014). These works divide the processing into multiple streams each operating at a different temporal granularity. While these works mainly focus on recurrent neural networks and their application is mainly on natural language tasks, we focus on introducing multiple streams of processing and a hierarchical structure into Transformers while also focusing on a broader range of domains beyond natural language. Transformers. Some of the components we describe in the proposed model have been used previously in various Transformer models. Transformer XL (Dai et al., 2019) also divides the input into segments. Each segment considers the tokens from the current segment and the previous segment for attention without passing gradients into the previous segments. A number of previous works (Zhang et al., 2021; Liu et al., 2021b, Wu et al., 2021, Yuan et al., 2021, Wang et al., 2021; Yang et al., 2021) have worked on introducing a hierarchical structure in Transformers mainly in the domain of vision. The main goal of these works has been to introduce convolution-like hierarchies into Vision Transformers (Dosovitskiy et al. 2020). While these works progressively reduce the spatial resolution of the inputs in order to introduce hierarchies, we introduce hierarchies by adding another slow stream of information processing and without reducing the spatial resolution of the inputs. We also provision for the higher level of the hierarchy (i.e. the slow stream) to provide information to the lower levels as top-down conditioning which is not possible in any of the previous works. Top-Down Conditioning. Top-down information is information propagated from higher to lower levels of the network. It represents the models beliefs of the world and provides context for interpreting perceptual information.\n```\n\n#### 2. H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences (Avg. Score: 0.83)\n\n*Zhenhai Zhu, Radu Soricut*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2021)\t**Cited by** 32  (*Influential: 7*)\n\n**TL;DR:** This work describes an efficient hierarchical method to compute attention in the Transformer architecture that exploits a matrix structure similar to the Hierarchical Matrix developed by the numerical analysis community, and has linear run time and memory complexity.\n\n**Abstract:** We describe an efficient hierarchical method to compute attention in the Transformer architecture. The proposed attention mechanism exploits a matrix structure similar to the Hierarchical Matrix (H-Matrix) developed by the numerical analysis community, and has linear run time and memory complexity. We perform extensive experiments to show that the inductive bias embodied by our hierarchical attention is effective in capturing the hierarchical structure in the sequences typical for natural language and vision tasks. Our method is superior to alternative sub-quadratic proposals by over +6 points on average on the Long Range Arena benchmark. It also sets a new SOTA test perplexity on One-Billion Word dataset with 5x fewer model parameters than that of the previous-best Transformer-based models.\n\n##### *Relevant Chunk: No. 1/34 (Score: 0.83)*\n\n```\n# H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences \n\nZhenhai Zhu<br>Google Research<br>zhenhai@google.com\n\nRadu Soricut<br>Google Research<br>rsoricut@google.com\n\n\n#### Abstract\n\nWe describe an efficient hierarchical method to compute attention in the Transformer architecture.\n```\n\n#### 3. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.73)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 30/49 (Score: 0.73)*\n\n```\nURL: http://mattmahoney. net/ dc/text.html. Chengzhi Mao, Lu Jiang, Mostafa Dehghani, Carl Vondrick, Rahul Sukthankar, and Irfan Essa. Discrete representations strengthen vision transformer robustness. In International Conference on Learning Representations, 2022. URL https://openreview.net/forum?id= 8 hWs60AZcWk . Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces, 2022. URLhttp://arxiv.org/abs/2206.13947. Piotr Nawrot, Szymon Tworkowski, Michal Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, and Henryk Michalewski. Hierarchical transformers are more efficient language models. CoRR, abs/2110.13711, 2021. URLhttps://arxiv.org/abs/2110.13711. Piotr Nawrot, Jan Chorowski, Adrian \u0141a\u0144cucki, and Edoardo M. Ponti. Efficient transformers with dynamic token pooling, 2023. URLhttp://arxiv.org/abs/2211.09761. Emilio Parisotto, H. Francis Song, Jack W. Rae, Razvan Pascanu, \u00c7aglar G\u00fcl\u00e7ehre, Siddhant M. Jayakumar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, Matthew M. Botvinick, Nicolas Heess, and Raia Hadsell. Stabilizing transformers for reinforcement learning. CoRR, abs/1910.06764, 2019. URL/http://arxiv.org/abs/1910.06764. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, Krishna Sri Ipsit Mantri, Ferdinand Mom, Atsushi Saito, Xiangru Tang, Bolun Wang, Johan S. Wind, Stansilaw Wozniak, Ruichong Zhang, Zhenyuan Zhang, Qihang Zhao, Peng Zhou, Jian Zhu, and Rui-Jie Zhu. RWKV: Reinventing RNNs for the transformer era, 2023. URL http://arxiv.org/abs/2305 13048\n\nHao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. Random feature attention. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=QtTKTdVrFBB. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models, 2023.\n```\n\n#### 4. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 0.72)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 2/25 (Score: 0.72)*\n\n```\nRemarkably, Transformers can handle long sequences, which allows them to produce long coherent outputs: entire paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark. ## 1 Introduction\n\nTransformer models (Vaswani et al., 2017) are capable of solving many sequence modeling tasks, including classical NLP tasks (Devlin et al., 2019), summarization (Zhang et al., 2020), language modeling (Radford et al., 2019; Brown et al., 2020), code generation (Chen et al., 2021), or even music generation (Huang et al., 2018; Dhariwal et al., 2020) and image generation (Parmar et al., 2018; Chen et al., 2020; Ramesh et al., 2021). One compelling feature of Transformers is their ability to handle long contexts given as part of the input. This is particularly visible in tasks where the output depends on parts of the context that may not be\n\n[^0]close-by in the generated sequence, like in summarization, where the summary may need to refer to information scattered across the context, or in largescale image generation, where pixels belonging to the same object may be far apart in the generation order. Transformers excel at such tasks thanks to self-attention, and they are used with longer and longer contexts. ![](https://cdn.mathpix.com/cropped/2024_09_12_71b80301f7da51238d79g-01.jpg?height=498&width=695&top_left_y=1162&top_left_x=1063)\n\nFigure 1: Bits-per-character vs. training cost for baseline (orange) and hierarchical Transformers (green). We observe significant perplexity improvements on enwik8 over the vanilla Transformer-XL baseline, see text for details. The ability of Transformers to handle long contexts comes at a price: each self-attention layer, at least in its original form, has complexity quadratic in the length of the context. When a stack of $n$ Transformer layers is used, both memory and time complexity is equal to $O\\left(L^{2} n\\right)$ where $L$ is a sequence length and $n$ number of decoder blocks. Due to this limitation, vanilla transformers are infeasible to train on tasks with very long input sequences, for instance, on high-resolution images. This issue has been studied extensively, and a number of techniques were introduced that modify attention mechanism without changing overall transformer architecture (Child et al., 2019; Roy et al., 2020; Ren et al., 2021). These sparse attention mechanisms reduce the complexity of self-attention\nbut still force the model to operate on the sequence of the same length as the input. For generative Transformer models, operating at the original scale of the input sequence is necessary, at least in the early and final layers, as the input must be processed at first and generated at the end (Section 4.3). But forcing the models to operate at this granularity throughout the layer stack has both fundamental and practical shortcomings:\n\n- Fundamentally, we aim for the models to create high-level representations of words, entities, or even whole events - which occur at a very different granularity than single letters that the model receives on input. - On the practical side, even layers with linear complexity can be slow and memory-intensive when processing very long sequences. To alleviate these issues, we propose to change the Transformer architecture to first shorten the internal sequence of activations when going deeper in the layer stack and then expand it back before generation. We merge tokens into groups using a shortening operation (Section 2.1) and so reduce the overall sequence length, and then up-sample them again combining with the sequence from earlier layers (Section 2.3), The first part is analogous to the Funnel-Transformer architecture (Dai et al., 2020), and the whole architecture takes inspiration from U-Nets (Ronneberger et al., 2015). In contrast to both these architectures, the model we present is autoregressive, which is harder to ensure in hierarchical models than in vanilla Transformers. The resulting model - which we call Hourglass is an autoregressive Transformer language model that operates on shortened sequences. It yields significant performance improvements for different attention types (Fig. 6,7). We tested Hourglass with Transformer-XL (Dai et al., 2019) and Reformer (Kitaev et al., 2020) blocks on enwik8 dataset. In both cases, it is not only better in terms of perplexity, but it is faster and uses less memory during training. We also propose a regularization technique for hierarchical Transformers called shorten factor dropout which improves perplexity upon baselines trained with fixed shorten factor (see Section 4.1). Finally, Hourglass achieves the new stateof-the-art among Transformer models for image generation of ImageNet32 (see Tab. 3). ## 2 Model\n\nStandard self-attention mechanism uses full tokenlevel sequence representations. In the Hourglass, we bring efficiency to the model by utilizing shortening, which allows us to use the Transformer layers on inputs with significantly smaller lengths. A high-level overview of our proposed model architecture is shown in figures 2 and 3. Attention type in the vanilla layers and shortened layers is a configurable parameter. By default we use relative attention defined in Transformer-XL (Dai et al., 2019). Any attention module can be used - we show significant efficiency gains when applying Hourglass also for LSH (Kitaev et al., 2020) attention (see Section 3.2 and Fig. 7). ### 2.1 Methods of shortening the input sequence\n\nShortening can be defined as any function $S$ that accepts a tensor $x$ of shape $(l, d)$ and returns a tensor $x^{\\prime}$ of shape $\\left(\\frac{l}{k}, d\\right)$, where $k$ is a hyperparameter called shorten factor. A simple shortening method is 1D average pooling with stride $k$ and pool size $k$, applied along the sequence dimension $l$. Another way of shortening is what we will further call linear pooling ( $l$ and $d$ denote sequence length and $\\left.d_{\\text {model }}\\right)$ :\n\n```\nAlgorithm 2 LinearPooling\n    \\(x^{\\prime} \\leftarrow \\operatorname{Reshape}\\left(x,\\left(\\frac{l}{k}, k \\cdot d\\right)\\right)\\)\n    \\(x^{\\prime} \\leftarrow\\) Linear Projection \\(\\left(x^{\\prime}\\right)\\)\n```\n\nShortening can be also performed by attention, as was introduced in (Dai et al., 2020): $x^{\\prime}=S(x)+$ $\\operatorname{Attention}(Q=S(x), K=V=x$ ) where $S$ is shortening function, originally $S=A v g P o o l$. Directly after this attention operation, a positionwise feed-forward with a residual is performed, so that these two layers form a Transformer block (Vaswani et al., 2017).\n```\n\n#### 5. Self-attention Networks Localize When QK-eigenspectrum Concentrates (Avg. Score: 0.71)\n\n*Han Bao, Ryuichiro Hataya, Ryo Karakida*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The notion of attention localization by the eigenspectrum of query-key parameter matrices is characterized and it is revealed that a small eigenspectrum variance leads attention to be localized, leading to better model expressivity and trainability.\n\n**Abstract:** The self-attention mechanism prevails in modern machine learning. It has an interesting functionality of adaptively selecting tokens from an input sequence by modulating the degree of attention localization, which many researchers speculate is the basis of the powerful model performance but complicates the underlying mechanism of the learning dynamics. In recent years, mainly two arguments have connected attention localization to the model performances. One is the rank collapse, where the embedded tokens by a self-attention block become very similar across different tokens, leading to a less expressive network. The other is the entropy collapse, where the attention probability approaches non-uniform and entails low entropy, making the learning dynamics more likely to be trapped in plateaus. These two failure modes may apparently contradict each other because the rank and entropy collapses are relevant to uniform and non-uniform attention, respectively. To this end, we characterize the notion of attention localization by the eigenspectrum of query-key parameter matrices and reveal that a small eigenspectrum variance leads attention to be localized. Interestingly, the small eigenspectrum variance prevents both rank and entropy collapse, leading to better model expressivity and trainability.\n\n##### *Relevant Chunk: No. 16/27 (Score: 0.71)*\n\n```\n[19] Noci, L., Anagnostidis, S., Biggio, L., Orvieto, A., Singh, S. P., and Lucchi, A. Signal propagation in transformers: Theoretical perspectives and the role of rank collapse. Advances in Neural Information Processing Systems, 35:27198-27211, 2022. [20] Ott, M., Edunov, S., Baevski, A., Fan, A., Gross, S., Ng, N., Grangier, D., and Auli, M. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pp. 48-53, 2019 . [21] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. Spike no more: Stabilizing the pre-training of large language models. arXiv preprint arXiv:2312.16903, 2023. [22] Takase, S., Kiyono, S., Kobayashi, S., and Suzuki, J. B2T connection: Serving stability and performance in deep transformers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 3078-3095, 2023. [23] Tarzanagh, D. A., Li, Y., Thrampoulidis, C., and Oymak, S. Transformers as support vector machines. arXiv preprint arXiv:2308.16898, 2023. [24] Tarzanagh, D. A., Li, Y., Zhang, X., and Oymak, S. Max-margin token selection in attention mechanism. Advances in Neural Information Processing Systems, 36, 2023. [25] Tian, Y., Wang, Y., Chen, B., and Du, S. Scan and snap: Understanding training dynamics and token composition in 1-layer transformer. Advances in Neural Information Processing Systems, 36, 2023 . [26] Tian, Y., Wang, Y., Zhang, Z., Chen, B., and Du, S. JoMA: Demystifying multilayer transformers via JOint Dynamics of MLP and Attention. arXiv preprint arXiv:2310.00535, 2023. [27] Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., and J\u00e9gou, H. Training data-efficient image transformers \\& distillation through attention. In Proceedings of the 38th International Conference on Machine Learning, pp. 10347-10357. PMLR, 2021. [28] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in Neural Information Processing Systems, 30: 6000-6010, 2017. [29] Xie, S. M., Raghunathan, A., Liang, P., and Ma, T. An explanation of in-context learning as implicit Bayesian inference. In Proceedings of the 10th International Conference on Learning Representations, 2022. [30] Xiong, R., Yang, Y., He, D., Zheng, K., Zheng, S., Xing, C., Zhang, H., Lan, Y., Wang, L., and Liu, T. On layer normalization in the transformer architecture. In Proceedings of the 37th International Conference on Machine Learning, pp. 10524-10533. PMLR, 2020. [31] Zhai, S., Likhomanenko, T., Littwin, E., Busbridge, D., Ramapuram, J., Zhang, Y., Gu, J., and Susskind, J. M. Stabilizing transformer training by preventing attention entropy collapse. In Proceedings of the 40th International Conference on Machine Learning, pp.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical attention implementation training stability\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. The Numerical Stability of Hyperbolic Representation Learning\n\n*From Search Query: hierarchical attention implementation training stability*\n\n*Gal Mishne, Zhengchao Wan, Yusu Wang, Sheng Yang*\n\n**TL;DR:** This work carefully analyzes the limitation of two popular models for the hyperbolic space, namely, the Poincar\\'e ball and the Lorentz model and identifies one Euclidean parametrization of the hyperBolic space which can alleviate these limitations.\n\n**Abstract:** Given the exponential growth of the volume of the ball w.r.t. its radius, the hyperbolic space is capable of embedding trees with arbitrarily small distortion and hence has received wide attention for representing hierarchical datasets. However, this exponential growth property comes at a price of numerical instability such that training hyperbolic learning models will sometimes lead to catastrophic NaN problems, encountering unrepresentable values in floating point arithmetic. In this work, we carefully analyze the limitation of two popular models for the hyperbolic space, namely, the Poincar\\'e ball and the Lorentz model. We first show that, under the 64 bit arithmetic system, the Poincar\\'e ball has a relatively larger capacity than the Lorentz model for correctly representing points. Then, we theoretically validate the superiority of the Lorentz model over the Poincar\\'e ball from the perspective of optimization. Given the numerical limitations of both models, we identify one Euclidean parametrization of the hyperbolic space which can alleviate these limitations. We further extend this Euclidean parametrization to hyperbolic hyperplanes and exhibits its ability in improving the performance of hyperbolic SVM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 3*)\n\n#### 2. Sparse Attention with Linear Units\n\n*From Search Query: hierarchical attention implementation training stability*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**TL;DR:** This work introduces a novel, simple method for achieving sparsity in attention: it replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges from such a formulation.\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. \u2018switch off\u2019) for some queries, which is not possible with sparsified softmax alternatives.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 4*)\n\n#### 3. Tree Transformer: Integrating Tree Structures into Self-Attention\n\n*From Search Query: hierarchical attention implementation training stability*\n\n*Yau-Shian Wang, Hung-yi Lee, Yun-Nung (Vivian) Chen*\n\n**TL;DR:** Tree Transformer is proposed, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures.\n\n**Abstract:** Pre-training Transformer from large-scale raw texts and fine-tuning on the desired task have achieved state-of-the-art results on diverse NLP tasks. However, it is unclear what the learned attention captures. The attention computed by attention heads seems not to match human intuitions about hierarchical structures. This paper proposes Tree Transformer, which adds an extra constraint to attention heads of the bidirectional Transformer encoder in order to encourage the attention heads to follow tree structures. The tree structures can be automatically induced from raw texts by our proposed \u201cConstituent Attention\u201d module, which is simply implemented by self-attention between two adjacent words. With the same training procedure identical to BERT, the experiments demonstrate the effectiveness of Tree Transformer in terms of inducing tree structures, better language modeling, and further learning more explainable attention scores.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2019\n\n**Citations:** 139  (*Influential: 16*)\n\n#### 4. GraphCodeBERT: Pre-training Code Representations with Data Flow\n\n*From Search Query: hierarchical attention implementation training stability*\n\n*Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie Liu, Long Zhou, Nan Duan, Jian Yin, Daxin Jiang, M. Zhou*\n\n**TL;DR:** Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks and it is shown that the model prefers structure-level attentions over token- level attentions in the task of code search.\n\n**Abstract:** Pre-trained models for programming language have achieved dramatic empirical improvements on a variety of code-related tasks such as code search, code completion, code summarization, etc. However, existing pre-trained models regard a code snippet as a sequence of tokens, while ignoring the inherent structure of code, which provides crucial code semantics and would enhance the code understanding process. We present GraphCodeBERT, a pre-trained model for programming language that considers the inherent structure of code. Instead of taking syntactic-level structure of code like abstract syntax tree (AST), we use data flow in the pre-training stage, which is a semantic-level structure of code that encodes the relation of \"where-the-value-comes-from\" between variables. Such a semantic-level structure is neat and does not bring an unnecessarily deep hierarchy of AST, the property of which makes the model more efficient. We develop GraphCodeBERT based on Transformer. In addition to using the task of masked language modeling, we introduce two structure-aware pre-training tasks. One is to predict code structure edges, and the other is to align representations between source code and code structure. We implement the model in an efficient way with a graph-guided masked attention function to incorporate the code structure. We evaluate our model on four tasks, including code search, clone detection, code translation, and code refinement. Results show that code structure and newly introduced pre-training tasks can improve GraphCodeBERT and achieves state-of-the-art performance on the four downstream tasks. We further show that the model prefers structure-level attentions over token-level attentions in the task of code search.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 916  (*Influential: 188*)\n\n#### 5. Human MotionFormer: Transferring Human Motions with Vision Transformers\n\n*From Search Query: hierarchical attention implementation training stability*\n\n*Hongyu Liu, Xintong Han, Chengbin Jin, Lihui Qian, Huawei Wei, Zhe Lin, Faqiang Wang, Haoye Dong, Yibing Song, Jia Xu, Qifeng Chen*\n\n**TL;DR:** A hierarchical ViT framework that leverages global and local perceptions to capture large and subtle motion matching, respectively, and set the new state-of-the-art performance both qualitatively and quantitatively is proposed.\n\n**Abstract:** Human motion transfer aims to transfer motions from a target dynamic person to a source static one for motion synthesis. An accurate matching between the source person and the target motion in both large and subtle motion changes is vital for improving the transferred motion quality. In this paper, we propose Human MotionFormer, a hierarchical ViT framework that leverages global and local perceptions to capture large and subtle motion matching, respectively. It consists of two ViT encoders to extract input features (i.e., a target motion image and a source human image) and a ViT decoder with several cascaded blocks for feature matching and motion transfer. In each block, we set the target motion feature as Query and the source person as Key and Value, calculating the cross-attention maps to conduct a global feature matching. Further, we introduce a convolutional layer to improve the local perception after the global cross-attention computations. This matching process is implemented in both warping and generation branches to guide the motion transfer. During training, we propose a mutual learning loss to enable the co-supervision between warping and generation branches for better motion representations. Experiments show that our Human MotionFormer sets the new state-of-the-art performance both qualitatively and quantitatively. Project page: \\url{https://github.com/KumapowerLIU/Human-MotionFormer}\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hierarchical attention implementations, training stability, gradient flow, and convergence properties, here are some key insights and techniques from the provided sources and additional context:\n\n## Hierarchical Attention Implementations\n\n### Multi-Head Attention and Layer Interactions\nIn Transformer models, hierarchical attention is inherently implemented through the multi-head attention mechanism. Each layer processes input data through self-attention and feedforward networks, allowing the model to capture different representations of the input data. The use of multiple attention heads enhances the model's ability to understand complex patterns and capture long-range dependencies.\n\n### Differential Attention Mechanism\nThe Differential Transformer introduces a hierarchical attention mechanism by calculating attention scores as the difference between two separate attention maps. This approach amplifies attention to relevant context while canceling noise, leading to sparse attention patterns. This mechanism can be particularly useful in reducing the impact of irrelevant context and improving the model's focus on key information.\n\n## Training Stability\n\n### Residual Connections and Layer Normalization\nResidual connections in Transformer models help mitigate the vanishing gradient problem, allowing gradients to flow back through the network during training. Layer normalization applied after each sub-layer stabilizes the learning process and improves convergence rates.\n\n### Gradient Flow Dynamics\nUnderstanding the gradient flow dynamics is crucial. The PDE model analysis suggests that Transformers exhibit superior robustness to input perturbations, although they may have lower gradient magnitudes compared to other models. This indicates that careful tuning of learning rates and stability analysis can refine the learning process.\n\n### Token-Parameter Attention\nThe token-parameter attention layer in the proposed method of treating model parameters as tokens can enhance training stability. This approach allows for dynamic interactions between input tokens and parameter tokens, which can improve the flexibility and scalability of the model without compromising performance.\n\n## Convergence Properties\n\n### Progressive Model Scaling\nThe method of progressive model scaling using token-parameter attention shows promising results in reducing the computational cost while maintaining performance. This approach allows for incremental scaling of the model, achieving comparable performance to a Transformer model trained from scratch but with significantly lower training costs.\n\n### Normalization Techniques\nThe use of normalization techniques such as RMSNorm in the Differential Transformer is crucial for maintaining training stability. Applying normalization to each head independently improves gradient statistics and helps in achieving sparser attention patterns.\n\n## Hardware Optimization and Memory Efficiency\n\n### Memory Access Patterns and Cache Utilization\nFor efficient implementation, careful consideration of memory access patterns and cache utilization is essential. Techniques like kernel fusion, parallel scan, and recomputation can help achieve linear complexity and optimize hardware-aware implementations.\n\n### Hierarchical Processing\nImplementing hierarchical processing, such as tiling and divide-and-conquer approaches, can improve efficiency. This is demonstrated in models like CHELA, which leverages these techniques to optimize linear attention mechanisms.\n\nBy integrating these techniques, researchers can design autoregressive language models that are more efficient, stable, and scalable:\n\n- **Hierarchical Attention**: Implement multi-head attention and differential attention mechanisms to enhance the model's ability to capture complex patterns and reduce noise.\n- **Training Stability**: Utilize residual connections, layer normalization, and careful learning rate tuning to ensure stable training.\n- **Convergence Properties**: Employ progressive model scaling and normalization techniques to improve convergence rates and reduce computational costs.\n- **Hardware Optimization**: Optimize memory access patterns, cache utilization, and use hierarchical processing to achieve efficient hardware-aware implementations.\n\nThese strategies can help in developing novel autoregressive language models that outperform existing state-of-the-art models in terms of perplexity, accuracy, robustness, and efficiency.\n",
            "analysis": "Summary of Analysis:\n\nThe initial search results provide valuable insights into integrating selective state space models (SSMs) with hierarchical attention mechanisms. Key findings include:\n\n1. SSM Architectures and Implementations:\n- Mamba introduces selective SSMs with hardware-efficient implementation\n- SPADE demonstrates successful integration of SSMs with attention mechanisms\n- CHELA shows how to combine SSMs with hierarchical processing\n\n2. Memory and Computational Efficiency:\n- Linear complexity O(L log L) achievable through careful design\n- Hardware-aware implementations crucial for performance\n- Memory access patterns and cache utilization important considerations\n\nKey References with Excerpts:\n\n1. From Mamba paper:\n\"The selection mechanism is designed to overcome the limitations of LTI models... we address this with three classical techniques: kernel fusion, parallel scan, and recomputation.\"\n\n2. From SPADE:\n\"SSM augments global information, which complements the lack of long-range dependency issue in local attention methods.\"\n\n3. From CHELA:\n\"leverages the thought of tiling and hierarchy... implements linear attention in a divide-and-conquer manner.\"\n\nKey Insights:\n\n1. Integration Strategies:\n- Need careful balance between SSM and attention mechanisms\n- Hierarchical processing can improve efficiency\n- Hardware-aware implementation crucial\n\n2. Performance Considerations:\n- Memory access patterns critical for efficiency\n- Cache utilization needs optimization\n- Training stability requires careful design\n\n3. Implementation Challenges:\n- Complex coordination between components\n- Memory management crucial\n- Training dynamics need careful handling\n\nFuture Search Needed:\n\n1. Specific Hierarchical Attention Implementations:\n- Detailed architectures\n- Performance characteristics\n- Integration techniques\n\n2. Training Stability:\n- Normalization techniques\n- Gradient flow\n- Convergence properties\n\n3. Hardware Optimization:\n- Memory access patterns\n- Cache utilization\n- Parallel processing strategies\n\nReferences:\n\n1. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\n\n2. Zuo, S., et al. (2022). SPADE: State Space Augmented Transformer for Long Sequence Modeling.\n\n3. Liu, Z., et al. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences.\n\n4. He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models.\n\n5. Le Bronnec, F., et al. (2024). LOCOST: State-Space Models for Long Document Abstractive Summarization.\n\nNext Search Plan:\n1. Focus on hierarchical attention implementations and techniques\n2. Investigate training stability in hybrid architectures\n3. Research hardware optimization strategies for hybrid models"
        },
        {
            "ready": false,
            "query": "selective state space models implementation memory efficiency",
            "detail": "techniques for implementing selective state space models with attention mechanisms, focusing on memory efficiency, state compression, and hardware optimization",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\ntechniques for implementing selective state space models with attention mechanisms, focusing on memory efficiency, state compression, and hardware optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.99)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.93)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.93)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 4. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 0.86)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.86)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space models implementation memory efficiency\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Robustifying State-space Models for Long Sequences via Approximate Diagonalization\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 2. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, A. Rudra, Christopher R'e*\n\n**TL;DR:** A simple sequence model inspired by control systems that generalizes RNN heuristics, temporal convolutions, and neural differential equations while addressing their shortcomings, and introduces a trainable subset of structured matrices that endow LSSLs with long-range memory.\n\n**Abstract:** Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 358  (*Influential: 22*)\n\n#### 3. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 363  (*Influential: 58*)\n\n#### 4. Simplified State Space Layers for Sequence Modeling\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Jimmy Smith, Andrew Warrington, Scott W. Linderman*\n\n**TL;DR:** A state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks.\n\n**Abstract:** Models using structured state space sequence (S4) layers have achieved state-of-the-art performance on long-range sequence modeling tasks. An S4 layer combines linear state space models (SSMs), the HiPPO framework, and deep learning to achieve high performance. We build on the design of the S4 layer and introduce a new state space layer, the S5 layer. Whereas an S4 layer uses many independent single-input, single-output SSMs, the S5 layer uses one multi-input, multi-output SSM. We establish a connection between S5 and S4, and use this to develop the initialization and parameterization used by the S5 model. The result is a state space layer that can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4, while also achieving state-of-the-art performance on several long-range sequence modeling tasks. S5 averages 87.4% on the long range arena benchmark, and 98.5% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 335  (*Influential: 32*)\n\n#### 5. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n#### 2. AutoDNNchip: An Automated DNN Chip Predictor and Builder for Both FPGAs and ASICs\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Yue Wang, Zetong Guan, Yongan Zhang, Yingyan Lin, Deming Chen, Cong Hao, Xiaofan Zhang, Pengfei Xu, Chaojian Li, Yang Zhao*\n\n**Abstract:** Recent breakthroughs in Deep Neural Networks (DNNs) have fueled a growing demand for DNN chips. However, designing DNN chips is non-trivial because: (1) mainstream DNNs have millions of parameters and operations; (2) the large design space due to the numerous design choices of dataflows, processing elements, memory hierarchy, etc.; and (3) an algorithm/hardware co-design is needed to allow the same DNN functionality to have a different decomposition, which would require different hardware IPs to meet the application specifications. Therefore, DNN chips take a long time to design and require cross-disciplinary experts. To enable fast and effective DNN chip design, we propose AutoDNNchip - a DNN chip generator that can automatically generate both FPGA- and ASIC-based DNN chip implementation given DNNs from machine learning frameworks (e.g., PyTorch) for a designated application and dataset. Specifically, AutoDNNchip consists of two integrated enablers: (1) a Chip Predictor, built on top of a graph-based accelerator representation, which can accurately and efficiently predict a DNN accelerator's energy, throughput, and area based on the DNN model parameters, hardware configuration, technology-based IPs, and platform constraints; and (2) a Chip Builder, which can automatically explore the design space of DNN chips (including IP selection, block configuration, resource balancing, etc.), optimize chip design via the Chip Predictor, and then generate optimized synthesizable RTL to achieve the target design metrics. Experimental results show that our Chip Predictor's predicted performance differs from real-measured ones by < 10% when validated using 15 DNN models and 4 platforms (edge-FPGA/TPU/GPU and ASIC). Furthermore, accelerators generated by our AutoDNNchip can achieve better (up to 3.86X improvement) performance than that of expert-crafted state-of-the-art accelerators.\n\n**Published:** 2020-01-06\n\n\n\n#### 3. New Insights on Reducing Abrupt Representation Change in Online Continual Learning\n\n*From Search Query: selective state space models implementation memory efficiency*\n\n*Eugene Belilovsky, Joelle Pineau, Tinne Tuytelaars, Nader Asadi, Rahaf Aljundi, Lucas Caccia*\n\n**Abstract:** In the online continual learning paradigm, agents must learn from a changing distribution while respecting memory and compute constraints. Experience Replay (ER), where a small subset of past data is stored and replayed alongside new data, has emerged as a simple and effective learning strategy. In this work, we focus on the change in representations of observed data that arises when previously unobserved classes appear in the incoming data stream, and new classes must be distinguished from previous ones. We shed new light on this question by showing that applying ER causes the newly added classes' representations to overlap significantly with the previous classes, leading to highly disruptive parameter updates. Based on this empirical analysis, we propose a new method which mitigates this issue by shielding the learned representations from drastic adaptation to accommodate new classes. We show that using an asymmetric update rule pushes new classes to adapt to the older ones (rather than the reverse), which is more effective especially at task boundaries, where much of the forgetting typically occurs. Empirical results show significant gains over strong baselines on standard continual learning benchmarks.\n\n**Published:** 2021-04-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using selective state space models (SSMs) with a focus on memory efficiency, state compression, and hardware optimization, here are some key insights and techniques from the provided sources and additional relevant information:\n\n## Selective State Updates and Memory Efficiency\n\n- **Mamba and S7 Models**: These models introduce input-dependent state space parameters, which allow for selective updating of internal states. The Mamba model, for instance, boasts fast inference and linear scaling in sequence length, making it efficient for long-sequence tasks.\n  - The S7 model extends this by incorporating input-dependent state transitions, which helps in balancing long-term and short-term dependencies and improves memory management. This selective updating allows S7 to retain relevant information over extended time horizons efficiently.\n\n## Integration with Attention Mechanisms\n\n- **Combining SSMs with Attention**: The work on Multi-Agent Mamba (MAM) and SPADE provides insights into how SSMs can be integrated with attention mechanisms. For example, MAM replaces traditional attention blocks in the Multi-Agent Transformer with Mamba blocks, demonstrating comparable performance with significant efficiency gains.\n  - SPADE augments global information using SSMs, which complements local attention methods by addressing the lack of long-range dependency capture.\n\n## State Compression Techniques\n\n- **Dynamic Gating and Event Pooling**: The S7 model uses dynamic gating mechanisms to regulate the flow of information based on the input signal and current state. This helps in selectively retaining relevant information, thus compressing the state effectively.\n  - Event Pooling in the S7 model reduces computational load by pooling hidden states over a window, further optimizing state representation and memory usage.\n\n## Hardware Optimization\n\n- **Hardware-Aware Parallel Algorithm**: The Mamba model employs a hardware-aware parallel algorithm in recurrent mode, which is crucial for efficient training and inference. This approach ensures that the model can scale linearly in sequence length without the quadratic computational complexity of traditional Transformers.\n  - **Asynchronous Discretization**: The S7 model uses asynchronous discretization to update the hidden state based on varying time intervals between events, enabling efficient processing of real-time data and optimizing memory access patterns.\n\n## Training Dynamics and Stability\n\n- **Reparameterization Techniques**: The S7 model uses reparameterization techniques to improve training stability. This involves making the state transition matrices depend on the input, which helps in adjusting state transitions dynamically and maintaining the efficiency of recurrent models.\n  - **Layer Normalization and Gradient Flow Management**: Ensuring layer normalization and careful architecture design for gradient flow management are crucial for the convergence and stability of these models, as highlighted in the analysis of hierarchical attention mechanisms[Analysis Summary].\n\nBy integrating these techniques, researchers can design autoregressive language models that are not only efficient in terms of memory usage and computational cost but also robust and scalable for handling long sequences and complex data. The selective state space models, such as Mamba and S7, offer promising avenues for achieving these goals.\n",
            "analysis": "Summary of Analysis:\n\nThe search results provide valuable insights into hierarchical attention mechanisms, state space models (SSMs), and training stability. Key findings reveal:\n\n1. Hierarchical Processing Approaches:\n- Temporal Latent Bottleneck architecture shows benefits of dual-stream processing\n- H-Transformer-1D demonstrates efficient hierarchical attention computation\n- Tree Transformer reveals importance of structural constraints in attention\n\n2. Integration Techniques:\n- SPADE successfully combines SSMs with attention mechanisms\n- Sparse attention with linear units offers efficient implementation\n- Hierarchical transformers show improved efficiency in language modeling\n\nKey References with Excerpts:\n\n1. From Temporal Latent Bottleneck:\n\"divides computation into two streams. A slow stream that is recurrent in nature aims to learn a specialized and compressed representation [...] a fast stream is parameterized as a Transformer\"\n\n2. From H-Transformer-1D:\n\"describes an efficient hierarchical method to compute attention [...] has linear run time and memory complexity\"\n\n3. From SPADE:\n\"SSM augments global information, which complements the lack of long-range dependency issue in local attention methods\"\n\nKey Insights:\n\n1. Dual-Stream Architecture:\n- Separation of fast and slow processing paths\n- Complementary strengths of different mechanisms\n- Efficient information integration\n\n2. Training Stability:\n- Layer normalization crucial for convergence\n- Gradient flow management through careful architecture design\n- Progressive parameter updates improve stability\n\n3. Implementation Efficiency:\n- Linear complexity achievable through hierarchical processing\n- Memory efficiency through selective state updates\n- Hardware-aware optimizations important\n\nFuture Search Needed:\n\n1. SSM Implementation Details:\n- Specific architectures for selective state updates\n- Memory management techniques\n- Training dynamics\n\n2. Integration Mechanisms:\n- Methods for combining SSMs with attention\n- Efficient information flow between streams\n- State compression techniques\n\n3. Hardware Optimization:\n- Parallel processing strategies\n- Memory access patterns\n- Cache utilization techniques\n\nReferences:\n\n1. Didolkar, A., et al. (2022). \"Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning.\" Neural Information Processing Systems.\n\n2. Zhu, Z., & Soricut, R. (2021). \"H-Transformer-1D: Fast One-Dimensional Hierarchical Attention for Sequences.\" Annual Meeting of the Association for Computational Linguistics.\n\n3. Zuo, S., et al. (2022). \"SPADE: State Space Augmented Transformer for Long Sequence Modeling.\" arXiv preprint.\n\n4. Zhang, B., Titov, I., & Sennrich, R. (2021). \"Sparse Attention with Linear Units.\" Conference on Empirical Methods in Natural Language Processing.\n\n5. Wang, Y.S., Lee, H.Y., & Chen, Y.N. (2019). \"Tree Transformer: Integrating Tree Structures into Self-Attention.\" Conference on Empirical Methods in Natural Language Processing."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 1.124511,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.147755,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalAdaptiveAttention",
    "proposal": "\n# HybridSSM-GPT: Integrating Selective State Space Models with Hierarchical Adaptive Attention\n\n## Motivation\n\nModern language models face significant challenges in efficiently capturing both local patterns and long-range dependencies while maintaining computational efficiency. The seed design HieraNorm-AttnGPT introduced hierarchical adaptive attention but could be enhanced through integration with selective state space models (SSMs) and more sophisticated normalization techniques. Recent advances in SSMs, particularly through the State Space Duality (SSD) framework, provide opportunities for efficient implementation of hybrid architectures that combine the strengths of both approaches.\n\n## Related Work\n\n### Selective State Space Models\nRecent work has shown that SSMs can effectively model long-range dependencies with linear complexity:\n- Mamba introduced selective state propagation for improved expressiveness\n- SSD framework enabled efficient implementation with 2-8x speedup\n- Theoretical foundations support their effectiveness in capturing multi-scale dependencies\n\n### Hierarchical Attention Mechanisms\nHierarchical attention structures have demonstrated benefits in capturing dependencies at different scales:\n- Multi-scale processing improves model capabilities\n- Adaptive gating mechanisms enhance selective focus\n- Hardware-efficient implementations reduce computational overhead\n\n### Advanced Normalization Techniques\nNormalization plays a crucial role in stabilizing training and improving model performance:\n- Dynamic normalization adapts to input context\n- Multi-scale normalization handles different levels of abstraction\n- Hardware-aware implementations optimize memory usage\n\n## Problem Analysis\n\n### Key Challenges\n\n1. **Efficient Integration**: Combining SSMs with attention while maintaining computational efficiency\n2. **Memory Management**: Optimizing memory usage across different architectural components\n3. **Training Stability**: Ensuring stable training with complex hybrid architecture\n4. **Hardware Efficiency**: Implementing efficient computation patterns for modern hardware\n\n### Core Philosophy\n\nThe proposed design aims to leverage the complementary strengths of SSMs and hierarchical attention through:\n1. Dual-stream processing for local and global dependencies\n2. Selective state updates for efficient information propagation\n3. Advanced normalization for stable training\n4. Hardware-aware implementation for optimal performance\n\n## Design Plan\n\n### Architecture Overview\n\nThe proposed HybridSSM-GPT block consists of:\n1. SSM Stream: Captures long-range dependencies efficiently\n2. Hierarchical Attention Stream: Processes local patterns adaptively\n3. Integration Layer: Combines information from both streams\n4. Advanced Normalization: Stabilizes training and improves performance\n\n### Mathematical Formulation\n\n1. **SSM Stream**:\n```python\nh_t = A(x_t)h_{t-1} + B(x_t)x_t\ny_ssm = C(x_t)h_t + D(x_t)x_t\n```\nwhere A(x_t), B(x_t), C(x_t), D(x_t) are input-dependent parameters.\n\n2. **Hierarchical Attention Stream**:\n```python\nQ_s = W_Q^s(x)  # Scale-specific queries\nK_s = W_K^s(x)  # Scale-specific keys\nV_s = W_V^s(x)  # Scale-specific values\ny_attn_s = softmax(Q_sK_s^T/sqrt(d_k))V_s\n```\nfor each scale s.\n\n3. **Integration Layer**:\n```python\ng = sigmoid(W_g[y_ssm; y_attn])\ny = g * y_ssm + (1-g) * y_attn\n```\n\n4. **Advanced Normalization**:\n```python\ny_norm = gamma(x) * (y - mu(y))/sqrt(sigma^2(y) + eps) + beta(x)\n```\nwhere gamma(x) and beta(x) are input-dependent parameters.\n\n### Implementation Details\n\n```python\nclass HybridSSMBlock(nn.Module):\n    def __init__(self, config):\n        super().__init__()\n        self.ssm = SelectiveSSM(config)\n        self.attn = HierarchicalAdaptiveAttentionV2(config)\n        self.integration = IntegrationLayer(config)\n        self.norm = AdaptiveNorm(config)\n        \n    def forward(self, x, state=None):\n        # SSM Stream\n        y_ssm, new_state = self.ssm(x, state)\n        \n        # Attention Stream\n        y_attn = self.attn(x)\n        \n        # Integration\n        y = self.integration(y_ssm, y_attn)\n        \n        # Normalization\n        y = self.norm(y)\n        \n        return y, new_state\n```\n\n### Optimization Techniques\n\n1. **Memory Management**:\n- Use kernel fusion for efficient computation\n- Implement selective state updates\n- Optimize cache utilization through block-based processing\n\n2. **Parallel Processing**:\n- Leverage parallel scan algorithms for SSM computation\n- Implement efficient attention patterns\n- Use hardware-aware tensor operations\n\n3. **Training Stability**:\n- Apply gradient clipping\n- Implement adaptive learning rates\n- Use careful initialization strategies\n\n## Theoretical Analysis\n\n### Computational Complexity\n- SSM Stream: O(L) complexity\n- Attention Stream: O(L) through hierarchical processing\n- Overall: Linear complexity in sequence length\n\n### Memory Usage\n- Fixed-size state in SSM stream\n- Efficient attention patterns reduce memory requirements\n- Careful recomputation strategies minimize overhead\n\n### Stability Analysis\n- Input-dependent normalization ensures stable gradients\n- Dual-stream architecture provides redundancy\n- Selective updates prevent gradient explosion\n\n## Conclusion\n\nHybridSSM-GPT represents a significant advancement in language model design by:\n1. Efficiently combining SSMs with hierarchical attention\n2. Maintaining linear computational complexity\n3. Ensuring stable training through advanced normalization\n4. Optimizing for modern hardware architectures\n\nThe design addresses key limitations of existing approaches while introducing novel mechanisms for improved performance and efficiency.\n\n## References\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv preprint.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n3. Cirone, N. M., Orvieto, A., Walker, B., Salvi, C., & Lyons, T. (2024). Theoretical Foundations of Deep Selective State-Space Models. arXiv preprint.\n\n4. Poli, M., et al. (2024). Mechanistic Design and Scaling of Hybrid Architectures. arXiv preprint.\n\n5. Zuo, S., et al. (2022). SPADE: State Space Augmented Transformer for Long Sequence Modeling. arXiv preprint.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "HybridSSM-GPT integrates selective state space models with hierarchical adaptive attention through a novel dual-stream architecture, enabling efficient modeling of both local and global dependencies while maintaining linear computational complexity and stable training dynamics through advanced normalization techniques.",
    "ideation": null,
    "modelname": "hybridssm_gpt",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory requirements across streams\n- Include concrete examples of state update mechanisms\n- Add guidelines for hardware-specific optimizations\n- Specify initialization strategies for both streams\n\n2. Training Considerations:\n- Add specific guidance for training with dual streams\n- Include stability analysis for gradient flow\n- Provide recommendations for hyperparameter tuning\n- Address potential convergence issues\n\n3. Memory Analysis:\n- Include quantitative analysis of memory savings\n- Provide detailed cache utilization strategies\n- Address potential memory bottlenecks\n- Specify memory management strategies for long sequences\n\n4. Hardware Optimization:\n- Add specific hardware-aware implementation guidelines\n- Include analysis of memory access patterns\n- Provide recommendations for efficient parallel processing\n- Address communication overhead between streams\n\n5. Documentation:\n- Provide more detailed API specifications\n- Include example configurations\n- Add debugging guidelines\n- Specify testing and validation procedures",
    "user_input": ""
}