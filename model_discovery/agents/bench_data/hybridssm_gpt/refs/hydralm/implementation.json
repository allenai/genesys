{
    "implementation": {
        "review": "",
        "root": "HydraLMBlock",
        "proposal": "",
        "proposal_traces": [],
        "rating": 0,
        "declares": {
            "SelectiveSSM": "{\"unitname\":\"SelectiveSSM\",\"requirements\":\"Implements a selective State Space Model that processes input sequences to capture global dependencies. Should maintain states across time steps and apply selective updates based on input patterns.\",\"inputs\":[\"X\"],\"outputs\":[\"state\"]}",
            "DualProjection": "{\"unitname\":\"DualProjection\",\"requirements\":\"Projects SSM states to output embeddings using state space duality principles. Should transform state representations while preserving temporal dependencies.\",\"inputs\":[\"state\"],\"outputs\":[\"Y\"]}",
            "FastStreamUnit": "{\"unitname\":\"FastStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
            "SlowStreamUnit": "{\"unitname\":\"SlowStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
            "HydraLMBlock": "{\"unitname\":\"HydraLMBlock\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
        },
        "units": {
            "FastStreamUnit": {
                "review": "# FastStreamUnit Implementation Review\n\n```rating 4.5```\n\n## Strengths\n\n1. **Excellent Causality Implementation**\n   - Successfully implemented causal convolution with left-only padding\n   - Proper use of cumulative sums for causal attention\n   - Clear mathematical formulation in documentation\n\n2. **Efficient Architecture**\n   - Linear computational complexity maintained\n   - Effective combination of convolution and attention mechanisms\n   - Smart use of gating mechanisms for feature selection\n\n3. **Robust Implementation**\n   - Proper gradient clipping for numerical stability\n   - Well-structured parameter initialization\n   - Comprehensive error checking and assertions\n\n4. **Documentation Quality**\n   - Detailed mathematical formulations\n   - Clear component descriptions\n   - Well-documented input/output specifications\n\n## Areas for Improvement\n\n1. **Missing Unit Tests**\n   Add comprehensive unit tests:\n   ```python\n   @gau_test\n   def test_fast_stream_unit(device=None, dtype=None):\n       # Test initialization\n       model = FastStreamUnit(embed_dim=64, block_loc=(0,0), kwarg_all={}, \n                            device=device, dtype=dtype)\n       \n       # Test basic forward pass\n       x = torch.randn(2, 32, 64, device=device, dtype=dtype)\n       y, z = model(x)\n       assert y.shape == x.shape\n       \n       # Test causality\n       x_modified = x.clone()\n       x_modified[:, 16:] = torch.randn_like(x_modified[:, 16:])\n       y_new, _ = model(x_modified)\n       assert torch.allclose(y[:, :16], y_new[:, :16], atol=1e-5)\n       \n       # Test gradient flow\n       y.sum().backward()\n       assert all(p.grad is not None for p in model.parameters())\n       \n       print(\"All FastStreamUnit tests passed!\")\n   ```\n\n2. **Memory Optimization**\n   Consider adding optional memory-efficient variants:\n   ```python\n   def __init__(self, ..., use_checkpoint=False):\n       self.use_checkpoint = use_checkpoint\n       ...\n\n   def _forward(self, X, **Z):\n       if self.use_checkpoint:\n           return torch.utils.checkpoint.checkpoint(self._forward_impl, X)\n       return self._forward_impl(X)\n   ```\n\n3. **Performance Optimization**\n   Add fast attention variants:\n   ```python\n   def _forward(self, X, **Z):\n       # ... existing code ...\n       \n       if hasattr(F, 'scaled_dot_product_attention'):  # Use PyTorch 2.0 attention if available\n           attn_output = F.scaled_dot_product_attention(\n               Q_prime, K_prime, V, \n               is_causal=True,\n               scale=1.0/math.sqrt(self.head_dim)\n           )\n       else:\n           # Fallback to current implementation\n           ...\n   ```\n\n4. **Configuration Flexibility**\n   Add more configurable parameters:\n   ```python\n   def __init__(self, ..., conv_kernel_size=3, dropout_rate=0.1):\n       self.local_conv = nn.Conv1d(\n           embed_dim, embed_dim,\n           kernel_size=conv_kernel_size,\n           padding=0,\n           bias=True,\n           **self.factory_kwargs\n       )\n       self.dropout = nn.Dropout(dropout_rate)\n   ```\n\n## Innovation and Impact\n\n1. **Novel Contributions**\n   - Efficient integration of causal convolution with gated linear attention\n   - Innovative use of cumulative sums for maintaining causality\n   - Smart parameter initialization strategy\n\n2. **Potential Impact**\n   - Could serve as an efficient alternative to traditional attention mechanisms\n   - Linear complexity makes it suitable for long sequence processing\n   - Modular design allows for easy integration in various architectures\n\n3. **Scalability Considerations**\n   - Linear memory and computation complexity\n   - Parallelizable operations\n   - Efficient gradient flow through residual connections\n\n## Integration Considerations\n\n1. **Interface with SlowStreamUnit**\n   - Clean interface through state dictionary\n   - Compatible tensor shapes\n   - Proper gradient flow\n\n2. **Memory Management**\n   - Consider adding memory profiling:\n   ```python\n   def _profile_memory(self, X):\n       torch.cuda.reset_peak_memory_stats()\n       self._forward(X)\n       return torch.cuda.max_memory_allocated()\n   ```\n\n## Recommendations\n\n1. **Immediate Actions**\n   - Implement comprehensive unit tests\n   - Add memory optimization options\n   - Document memory and computation complexity\n\n2. **Future Improvements**\n   - Consider implementing Toeplitz convolution variant\n   - Add support for sparse attention patterns\n   - Implement adaptive kernel size selection\n\n3. **Documentation Updates**\n   - Add performance benchmarks\n   - Document memory requirements\n   - Add integration guidelines\n\n4. **Code Organization**\n   Consider splitting into sub-modules:\n   ```python\n   class CausalConvolution(nn.Module):\n       \"\"\"Implements causal convolution with proper padding\"\"\"\n       \n   class GatedLinearAttention(nn.Module):\n       \"\"\"Implements gated linear attention mechanism\"\"\"\n   ```\n\n## Final Notes\n\nThe implementation shows excellent attention to detail, particularly in maintaining causality and numerical stability. The combination of causal convolution and gated linear attention is innovative and well-executed. The code is production-ready but could benefit from additional testing and optimization options.\n\nThe high rating (4.5/5) reflects the strong technical implementation, clear documentation, and successful passing of functionality checks. The missing 0.5 points are due to the lack of unit tests and some potential optimization opportunities.\n\nThe implementation successfully balances efficiency, functionality, and maintainability, making it a valuable contribution to the HydraLM architecture. The causal guarantees and linear complexity make it particularly suitable for large-scale language modeling applications.",
                "requirements": "N/A",
                "reuse_from": "efficientselectivefastttt.EfficientSelectiveFastTTT",
                "desc": null,
                "gautests": {
                    "test_fast_stream_unit": "@gau_test\ndef test_FastStreamUnit_test_fast_stream_unit(device=None, dtype=None):\n    \"\"\"Test FastStreamUnit functionality and causality\"\"\"\n    embed_dim = 64\n    model = FastStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all\n        ={}, device=device, dtype=dtype)\n    batch_sizes = [1, 2]\n    seq_lengths = [16, 32]\n    for batch_size in batch_sizes:\n        for seq_len in seq_lengths:\n            X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n                dtype=dtype)\n            Y, Z = model(X)\n            assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n            assert isinstance(Z, dict), 'Z should be a dictionary'\n            assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n            assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n            assert not torch.isnan(Y).any(), 'Output contains NaN values'\n            assert not torch.isinf(Y).any(), 'Output contains infinite values'\n            X_modified = X.clone()\n            X_modified[:, seq_len // 2:] = torch.randn_like(X_modified[:, \n                seq_len // 2:])\n            Y_new, _ = model(X_modified)\n            assert torch.allclose(Y[:, :seq_len // 2], Y_new[:, :seq_len //\n                2], rtol=0.0001, atol=0.0001), 'Causality test failed'\n            if Y.requires_grad:\n                loss = Y.sum()\n                loss.backward()\n                for param in model.parameters():\n                    assert param.grad is not None, 'Gradients were not computed'\n                    assert not torch.isnan(param.grad).any(\n                        ), 'Gradients contain NaN values'\n                    assert not torch.isinf(param.grad).any(\n                        ), 'Gradients contain infinite values'\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastStreamUnit(GAUBase):\n    \"\"\"\n    FastStreamUnit for HydraLM that captures local patterns using efficient convolution and gated mechanisms.\n    \n    This unit combines causal convolution with gated linear attention to efficiently process local patterns\n    in the input sequence. It maintains linear computational complexity while effectively capturing\n    short-range dependencies in a strictly causal manner.\n\n    **Core Components:**\n    \n    1. Causal Local Convolution: Uses a 1D convolution with left-only padding to ensure causality\n    2. Causal Gated Linear Attention: Processes local dependencies with linear complexity\n    3. Residual Connections: Maintains gradient flow and information preservation\n    \n    **Mathematical Formulation:**\n\n    1. Causal Local Convolution:\n       .. math::\n          X_{pad} = [pad(X_{1:t-1}), X_t]\n          X_{conv} = Conv1D(X_{pad})\n          Y_t = X_t + X_{conv,t}\n\n    2. Causal Gated Linear Attention:\n       .. math::\n          Q_t = G_Q * norm(W_Q X_t)\n          K_t = G_K * norm(W_K X_t)\n          V_t = W_V X_t\n          \n          Y_t = \\\\sum_{i=1}^t \frac{Q_t K_i^T}{\\\\sum_{j=1}^t K_j^T + \\\\epsilon} V_i\n\n    where G_Q and G_K are learned gates, and norm represents layer normalization.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n        num_attention_heads (int, optional): Number of attention heads. Default: 4\n        eps (float, optional): Epsilon for numerical stability. Default: 1e-6\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.eps = eps\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=0, bias=True, **self.factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=eps, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=eps, **self.factory_kwargs)\n        self.final_norm = nn.LayerNorm(embed_dim, eps=eps, **self.\n            factory_kwargs)\n        self._init_parameters()\n\n    def _init_parameters(self):\n        \"\"\"Initialize model parameters using Xavier uniform initialization\"\"\"\n        for module in [self.W_Q, self.W_K, self.W_V, self.output_proj, self\n            .gate_Q, self.gate_K]:\n            if hasattr(module, 'weight'):\n                nn.init.xavier_uniform_(module.weight)\n            if hasattr(module, 'bias') and module.bias is not None:\n                nn.init.zeros_(module.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='replicate')\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)\n        X = X + X_conv\n        Q = self.q_norm(self.W_Q(X))\n        K = self.k_norm(self.W_K(X))\n        V = self.W_V(X)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = torch.clamp(Q * G_Q, -10, 10)\n        K = torch.clamp(K * G_K, -10, 10)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        V_weighted = K_prime * V\n        QV_cumsum = torch.cumsum(V_weighted, dim=2)\n        attn_weights = torch.einsum('bhld,bhld->bhl', Q_prime, K_cumsum)\n        attn_output = torch.einsum('bhld,bhl->bhld', QV_cumsum, 1.0 / (\n            attn_weights + self.eps))\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output = self.final_norm(output)\n        return output, Z\n",
                "rating": 4.5,
                "spec": "{\"unitname\":\"FastStreamUnit\",\"document\":\"FastStreamUnit for HydraLM that captures local patterns using efficient convolution and gated mechanisms.\\n\\nThis unit combines causal convolution with gated linear attention to efficiently process local patterns\\nin the input sequence. It maintains linear computational complexity while effectively capturing\\nshort-range dependencies in a strictly causal manner.\\n\\n**Core Components:**\\n\\n1. Causal Local Convolution: Uses a 1D convolution with left-only padding to ensure causality\\n2. Causal Gated Linear Attention: Processes local dependencies with linear complexity\\n3. Residual Connections: Maintains gradient flow and information preservation\\n\\n**Mathematical Formulation:**\\n\\n1. Causal Local Convolution:\\n   .. math::\\n      X_{pad} = [pad(X_{1:t-1}), X_t]\\n      X_{conv} = Conv1D(X_{pad})\\n      Y_t = X_t + X_{conv,t}\\n\\n2. Causal Gated Linear Attention:\\n   .. math::\\n      Q_t = G_Q * norm(W_Q X_t)\\n      K_t = G_K * norm(W_K X_t)\\n      V_t = W_V X_t\\n      \\n      Y_t = \\\\sum_{i=1}^t \\frac{Q_t K_i^T}{\\\\sum_{j=1}^t K_j^T + \\\\epsilon} V_i\\n\\nwhere G_Q and G_K are learned gates, and norm represents layer normalization.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4\\n    eps (float, optional): Epsilon for numerical stability. Default: 1e-6\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "num_attention_heads": 4,
                    "eps": 1e-06
                },
                "design_traces": null
            },
            "SlowStreamUnit": {
                "review": "# Comprehensive Review of SlowStreamUnit Implementation\n\n## Overall Rating\n```rating 4.3```\n\n## Strengths\n\n1. **Well-Structured Architecture**\n   - Clear separation of concerns between SSM processing and dual projection\n   - Effective use of residual connections and normalization\n   - Clean implementation of the forward pass with logical flow\n\n2. **Comprehensive Documentation**\n   - Excellent docstring with detailed mathematical formulations\n   - Clear explanation of components and their purposes\n   - Well-referenced with relevant academic papers\n   - Good examples and usage instructions\n\n3. **Robust Implementation**\n   - Proper handling of device and dtype configurations\n   - Appropriate use of LayerNorm and dropout for stability\n   - Clean integration with the parent HydraLM architecture\n\n4. **Theoretical Soundness**\n   - Implementation aligns well with state space model theory\n   - Incorporates state space duality concepts effectively\n   - Maintains linear computational complexity\n\n## Areas for Improvement\n\n1. **State Management**\n```python\ndef _forward(self, X, **Z):\n    # Consider adding state caching\n    X_norm = self.norm(X)\n    state, Z = self.ssm(X_norm, **Z)\n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\nSuggestion: Add state caching mechanism for improved efficiency:\n```python\ndef _forward(self, X, **Z):\n    cache_key = f\"ssm_state_{self.block_loc}\"\n    X_norm = self.norm(X)\n    \n    if cache_key in Z:\n        state = Z[cache_key]\n    else:\n        state, Z = self.ssm(X_norm, **Z)\n        Z[cache_key] = state\n        \n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\n\n2. **Configuration Flexibility**\nAdd configuration options for SSM and projection parameters:\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Add configuration parameters\n    ssm_config = kwargs.get('ssm_config', {})\n    proj_config = kwargs.get('proj_config', {})\n    \n    self.ssm = SelectiveSSM(embed_dim=self.embed_dim, \n                           **ssm_config,\n                           **self.factory_kwargs)\n    self.dual_proj = DualProjection(embed_dim=self.embed_dim,\n                                   **proj_config,\n                                   **self.factory_kwargs)\n```\n\n3. **Performance Optimization**\n   - Consider adding gradient checkpointing for memory efficiency\n   - Implement parallel computation of SSM states where possible\n   - Add optional fast path for inference\n\n## Innovation and Impact\n\n### Innovative Aspects\n1. **Selective SSM Integration**\n   - Novel combination of selective state space modeling with dual projection\n   - Potential for improved efficiency in capturing long-range dependencies\n\n2. **Dual Projection Mechanism**\n   - Innovative use of state space duality for embedding projection\n   - Could lead to better representation learning\n\n### Potential Impact\n1. **Scalability**\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage through selective state processing\n\n2. **Model Performance**\n   - Improved capture of global dependencies\n   - Better handling of long-range relationships in text\n\n## Integration Considerations\n\n1. **Child Unit Dependencies**\n   - Need to implement SelectiveSSM and DualProjection units\n   - Consider providing default implementations for testing\n\n2. **Memory Management**\n   - Monitor state size growth in long sequences\n   - Implement state pruning mechanism if needed\n\n## Recommendations for the Coder\n\n1. **Implementation Priority**\n```python\n# Priority 1: Implement child units\nclass SelectiveSSM(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n\nclass DualProjection(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n```\n\n2. **Add Unit Tests**\n```python\n@gau_test\ndef test_slow_stream_unit(device=None, dtype=None):\n    # Test initialization\n    unit = SlowStreamUnit(embed_dim=512, block_loc=(0,0), \n                         kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test forward pass\n    batch_size, seq_len = 2, 128\n    X = torch.randn(batch_size, seq_len, 512, \n                   device=device, dtype=dtype)\n    Y, Z = unit(X)\n    \n    # Assert output shape\n    assert Y.shape == X.shape\n    \n    # Test state persistence\n    Y2, Z = unit(X, **Z)\n    assert Y2.shape == X.shape\n```\n\n3. **Documentation Updates**\n   - Add performance characteristics and memory usage patterns\n   - Document expected behavior with different sequence lengths\n   - Provide examples of configuration options\n\n4. **Optimization Opportunities**\n   - Implement parallel state computation\n   - Add gradient checkpointing option\n   - Consider sparse state updates\n\n## Additional Suggestions\n\n1. **Error Handling**\n```python\ndef _forward(self, X, **Z):\n    if X.dim() != 3:\n        raise ValueError(f\"Expected 3D input, got {X.dim()}D\")\n    if X.size(-1) != self.embed_dim:\n        raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {X.size(-1)}\")\n    # ... rest of implementation\n```\n\n2. **Configuration Validation**\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Validate configurations\n    if embed_dim <= 0:\n        raise ValueError(f\"embed_dim must be positive, got {embed_dim}\")\n    if kwargs.get('dropout', 0.1) < 0 or kwargs.get('dropout', 0.1) > 1:\n        raise ValueError(\"dropout must be between 0 and 1\")\n```\n\n3. **Performance Monitoring**\n   - Add hooks for tracking state size and computation time\n   - Implement optional logging of performance metrics\n\nThe implementation shows promise in advancing language model capabilities through efficient handling of global dependencies. Focus on implementing the child units and adding comprehensive tests to ensure robust performance across different scenarios.",
                "requirements": "N/A",
                "reuse_from": "dualstategpt.DualStateProcessor",
                "desc": null,
                "gautests": {
                    "test_slow_stream_unit": "@gau_test\ndef test_SlowStreamUnit_test_slow_stream_unit(device=None, dtype=None):\n    \"\"\"Unit test for SlowStreamUnit\"\"\"\n    batch_size = 2\n    seq_len = 128\n    embed_dim = 256\n    unit = SlowStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all=\n        {'dropout': 0.1}, device=device, dtype=dtype)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = unit(X, **Z)\n    assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n    assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n    assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    for seq_len in [64, 256]:\n        X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n            dtype=dtype)\n        Y, Z = unit(X, **Z)\n        assert Y.shape == X.shape, f'Failed for sequence length {seq_len}'\n    print('All tests passed!')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nimport math\n\n\nclass SlowStreamUnit(GAUBase):\n    \"\"\"\n    SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\n\n    This unit processes input sequences through a selective State Space Model (SSM) and applies\n    state space duality via a dual projection to capture long-range dependencies efficiently.\n    It serves as the \"Slow Stream\" in HydraLM's dual-stream architecture.\n\n    **Core Components:**\n\n    - `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\n    - `DualProjection`: Projects SSM states to output embeddings using state space duality\n\n    **Mathematical Formulation:**\n\n    1. State Space Model Processing:\n       .. math::\n          h_t = A h_{t-1} + B x_t\n          y_t = C h_t + D x_t\n\n       where h_t is the hidden state, x_t is input, and y_t is output at time t.\n       A, B, C, D are learnable parameters.\n\n    2. Dual Projection:\n       .. math::\n          Y = \text{DualProj}(h) + X\n\n       where h represents the SSM states and X is the residual connection.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n\n    Example:\n        slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        y, z = slow_stream(x)\n\n    Note:\n        This unit is designed to work efficiently with long sequences by maintaining\n        linear computational complexity through the use of SSMs.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\n        [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \n            Algorithms Through Structured State Space Duality.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ssm = SelectiveSSM(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dual_proj = DualProjection(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n\n    def _forward(self, X, **Z):\n        X_norm = self.norm(X)\n        state, Z = self.ssm(X_norm, **Z)\n        Y, Z = self.dual_proj(state, **Z)\n        Y = self.dropout(Y) + X\n        return Y, Z\n",
                "rating": 4.3,
                "spec": "{\"unitname\":\"SlowStreamUnit\",\"document\":\"SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\\n\\nThis unit processes input sequences through a selective State Space Model (SSM) and applies\\nstate space duality via a dual projection to capture long-range dependencies efficiently.\\nIt serves as the \\\"Slow Stream\\\" in HydraLM's dual-stream architecture.\\n\\n**Core Components:**\\n\\n- `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\\n- `DualProjection`: Projects SSM states to output embeddings using state space duality\\n\\n**Mathematical Formulation:**\\n\\n1. State Space Model Processing:\\n   .. math::\\n      h_t = A h_{t-1} + B x_t\\n      y_t = C h_t + D x_t\\n\\n   where h_t is the hidden state, x_t is input, and y_t is output at time t.\\n   A, B, C, D are learnable parameters.\\n\\n2. Dual Projection:\\n   .. math::\\n      Y =   ext{DualProj}(h) + X\\n\\n   where h represents the SSM states and X is the residual connection.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\\n\\nExample:\\n    slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    y, z = slow_stream(x)\\n\\nNote:\\n    This unit is designed to work efficiently with long sequences by maintaining\\n    linear computational complexity through the use of SSMs.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\\n    [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \\n        Algorithms Through Structured State Space Duality.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                "children": [
                    "SelectiveSSM",
                    "DualProjection"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "HydraLMBlock": {
                "review": "# Comprehensive Review of HydraLMBlock Implementation\n\n## Overall Rating\n```rating 4.5```\n\n## Strengths\n\n1. **Clear Architecture Implementation**\n- Successfully implements the proposed dual-stream architecture with Fast and Slow processing paths\n- Clean separation of concerns between components\n- Well-structured initialization and forward pass logic\n\n2. **Code Quality**\n- Excellent docstring documentation following best practices\n- Clear variable naming and logical code organization\n- Proper use of PyTorch modules and tensor operations\n\n3. **Technical Implementation**\n- Efficient use of nn.Sequential for the gating mechanism\n- Proper handling of device and dtype through factory_kwargs\n- Good use of LayerNorm for stabilizing outputs\n\n4. **Modularity**\n- Clean separation between Fast and Slow streams\n- Modular design allows for easy modification or replacement of components\n- Well-structured inheritance from GAUBase\n\n## Areas for Improvement\n\n1. **State Management**\n- Consider adding state persistence mechanisms for the Slow Stream\n- Could benefit from explicit state initialization in __init__\n\n```python\ndef __init__(self, ...):\n    # ... existing code ...\n    self.register_buffer('hidden_state', torch.zeros(embed_dim))\n```\n\n2. **Error Handling**\n- Add input validation for dimensions and types\n- Include checks for numerical stability\n\n```python\ndef _forward(self, X, **Z):\n    if torch.isnan(X).any():\n        raise ValueError(\"Input contains NaN values\")\n    # ... rest of implementation\n```\n\n3. **Memory Efficiency**\n- Consider implementing gradient checkpointing for memory efficiency\n- Add optional memory-efficient attention variants\n\n4. **Performance Optimization**\n- Consider adding JIT compilation support\n- Implement optional fused operations for better performance\n\n## Innovation Assessment\n\n### Novel Aspects\n1. **Adaptive Gating Mechanism**\n- The implementation successfully realizes the proposed dynamic integration of Fast and Slow streams\n- The gating mechanism is efficiently implemented using nn.Sequential\n\n2. **Dual Stream Processing**\n- Clean implementation of parallel processing streams\n- Effective combination of local and global feature extraction\n\n### Potential Impact\n1. **Scalability**\n- The implementation should scale well with sequence length due to linear complexity\n- Modular design allows for easy scaling of model capacity\n\n2. **Performance**\n- The dual-stream architecture could provide better performance on both short and long-range dependencies\n- Adaptive gating mechanism should help in efficient resource utilization\n\n## Integration and Scalability Recommendations\n\n1. **Stream Synchronization**\n```python\ndef _forward(self, X, **Z):\n    # Add synchronization point\n    torch.cuda.synchronize() if X.is_cuda else None\n    H_fast, Z = self.fast_stream(X, **Z)\n    H_slow, Z = self.slow_stream(X, **Z)\n    # ... rest of implementation\n```\n\n2. **Memory Management**\n```python\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # Enable automatic mixed precision\n    # ... existing implementation\n```\n\n3. **Batch Processing Optimization**\n```python\ndef _forward(self, X, **Z):\n    # Add batch dimension handling\n    B, L, D = X.shape\n    if B > 1:\n        # Process in chunks if needed\n        chunk_size = min(B, 32)\n        results = []\n        for i in range(0, B, chunk_size):\n            chunk = X[i:i+chunk_size]\n            # Process chunk\n            # ... existing implementation\n```\n\n## Recommendations for the Coder\n\n1. **Implementation Priorities**\n- Add comprehensive unit tests covering edge cases\n- Implement memory-efficient variants for large-scale deployment\n- Add support for gradient checkpointing\n\n2. **Code Structure**\n```python\nclass HydraLMBlock(GAUBase):\n    def __init__(self, ...):\n        # Add configuration validation\n        assert embed_dim > 0, \"embed_dim must be positive\"\n        # ... rest of implementation\n\n    @torch.jit.script_method\n    def _forward(self, X, **Z):\n        # Add JIT support\n        # ... existing implementation\n```\n\n3. **Documentation**\n- Add more detailed documentation about the expected shapes and types of intermediate tensors\n- Include performance characteristics and memory requirements\n- Document any assumptions about input distributions or constraints\n\n4. **Testing**\n```python\n@gau_test\ndef test_hydralm_block(device=None, dtype=None):\n    # Add comprehensive tests\n    block = HydraLMBlock(embed_dim=256, block_loc=(0,0), \n                        kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test various sequence lengths\n    for seq_len in [128, 256, 512]:\n        X = torch.randn(2, seq_len, 256, device=device, dtype=dtype)\n        Y, Z = block(X)\n        assert not torch.isnan(Y).any(), f\"Output contains NaN values for seq_len={seq_len}\"\n        assert Y.shape == X.shape, f\"Output shape mismatch for seq_len={seq_len}\"\n```\n\n5. **Performance Optimization**\n- Profile the implementation to identify bottlenecks\n- Consider implementing custom CUDA kernels for critical operations\n- Add support for quantization and pruning\n\n## Final Notes\n\nThe implementation is strong and closely follows the proposed design. The dual-stream architecture with adaptive gating is well-implemented, and the code is clean and well-documented. The main areas for improvement are around optimization, testing, and advanced features like gradient checkpointing and quantization support.\n\nThe high rating (4.5) reflects the solid implementation of the core architecture while leaving room for optimization and advanced features. The code provides a strong foundation for further development and optimization.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_hydralm_block": "@gau_test\ndef test_HydraLMBlock_test_hydralm_block(device=None, dtype=None):\n    batch_size = 2\n    seq_len = 16\n    embed_dim = 32\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    block = HydraLMBlock(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        }, device=device, dtype=dtype)\n    Y, Z = block(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    X_long = torch.randn(batch_size, seq_len * 2, embed_dim, device=device,\n        dtype=dtype)\n    Y_long, Z = block(X_long, **Z)\n    assert Y_long.shape == X_long.shape, 'Failed to handle longer sequence'\n    print('HydraLMBlock tests passed!')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HydraLMBlock(GAUBase):\n    \"\"\"\n    HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\n    \n    This block processes input sequences through two parallel streams:\n    1. Fast Stream: Captures local patterns using Toeplitz Convolution\n    2. Slow Stream: Models global dependencies using State Space Models\n    \n    The outputs of both streams are combined using an adaptive gating mechanism.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        \n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.fast_stream = FastStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.slow_stream = SlowStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.gate = nn.Sequential(nn.Linear(2 * embed_dim, embed_dim, **\n            self.factory_kwargs), nn.Sigmoid())\n        self.output_proj = nn.Linear(embed_dim, embed_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        H_fast, Z = self.fast_stream(X, **Z)\n        H_slow, Z = self.slow_stream(X, **Z)\n        H_concat = torch.cat([H_fast, H_slow], dim=-1)\n        G = self.gate(H_concat)\n        Y = G * H_fast + (1 - G) * H_slow\n        Y = self.output_proj(Y)\n        Y = self.norm(Y)\n        return Y, Z\n",
                "rating": 4.5,
                "spec": "{\"unitname\":\"HydraLMBlock\",\"document\":\"HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\\n\\nThis block processes input sequences through two parallel streams:\\n1. Fast Stream: Captures local patterns using Toeplitz Convolution\\n2. Slow Stream: Models global dependencies using State Space Models\\n\\nThe outputs of both streams are combined using an adaptive gating mechanism.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    \\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                "children": [
                    "FastStreamUnit",
                    "SlowStreamUnit"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            }
        },
        "suggestions": "",
        "name": "hydralm"
    },
    "status": "initial_pass",
    "history": [
        {
            "tree": {
                "review": "",
                "root": "HydraLMBlock",
                "proposal": "",
                "proposal_traces": [],
                "rating": 0,
                "declares": {
                    "DualProjection": "{\"unitname\":\"DualProjection\",\"requirements\":\"Projects SSM states to output embeddings using state space duality principles. Should transform state representations while preserving temporal dependencies.\",\"inputs\":[\"state\"],\"outputs\":[\"Y\"]}",
                    "SelectiveSSM": "{\"unitname\":\"SelectiveSSM\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "FastStreamUnit": "{\"unitname\":\"FastStreamUnit\",\"requirements\":\"Captures local patterns using Toeplitz Convolution. Should process input sequence X and return processed sequence of same shape.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "SlowStreamUnit": "{\"unitname\":\"SlowStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "HydraLMBlock": "{\"unitname\":\"HydraLMBlock\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "units": {
                    "SelectiveSSM": {
                        "review": "# Feedback Report for SelectiveSSM Implementation\n\n## Overall Rating\n```rating 3.5```\n\n## Analysis of Failed Checks\n\nThe implementation has two main issues that need to be addressed:\n\n1. **Unit Test Failure**:\n```python\nAssertionError: Expected state shape (1, 1, 32), got torch.Size([1, 1, 64])\n```\nThis indicates that the state dimension is not being properly set - the implementation is using embed_dim (64) instead of state_dim (32) as the output dimension.\n\n2. **Differentiability Test Failure**:\n```\nError: Used parameter backbone.blocks.0.gab.root.slow_stream.ssm.D requires gradients but has none.\nError: Used parameter backbone.blocks.0.gab.root.slow_stream.ssm.C.weight requires gradients but has none.\n```\nThis suggests that some parameters are not being properly registered for gradient computation.\n\n### Fixes Required:\n\n1. **State Dimension Issue**:\nThe output state needs to be properly projected to state_dim. Modify the forward pass:\n\n```python\ndef _forward(self, X: torch.Tensor, **Z) -> tuple:\n    batch_size, seq_len, _ = X.shape\n    device = X.device\n    h = Z.get('state', torch.zeros(batch_size, self.state_dim, device=device, dtype=X.dtype))\n    states = []\n    for t in range(seq_len):\n        x_t = X[:, t, :]\n        g_t = self._compute_gate(x_t, h)\n        # Project input to state dimension\n        h_new = self.norm(g_t * (self.A * h) + (1 - g_t) * self.B(x_t))\n        h = self.dropout(h_new)\n        states.append(h)\n    state = torch.stack(states, dim=1)\n    # Ensure state is in state_dim\n    assert state.shape == (batch_size, seq_len, self.state_dim)\n    Z['state'] = h\n    return state, Z\n```\n\n2. **Gradient Issues**:\nParameters need to be properly registered. Modify the initialization:\n\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, state_dim: int=None, **kwargs):\n    self.factory_kwargs = {'device': device, 'dtype': dtype}\n    super().__init__(embed_dim, block_loc, kwarg_all)\n    \n    # Ensure state_dim is set\n    self.state_dim = state_dim if state_dim is not None else embed_dim\n    \n    # Register parameters properly\n    self.A = nn.Parameter(torch.randn(self.state_dim, **self.factory_kwargs) / math.sqrt(self.state_dim))\n    self.B = nn.Linear(self.embed_dim, self.state_dim, **self.factory_kwargs)\n    self.C = nn.Linear(self.state_dim, self.embed_dim, **self.factory_kwargs)\n    self.D = nn.Parameter(torch.randn(self.embed_dim, **self.factory_kwargs) / math.sqrt(self.embed_dim))\n    \n    # Ensure parameters require gradients\n    self.A.requires_grad_(True)\n    self.D.requires_grad_(True)\n    \n    # Initialize other layers\n    self.W_g = nn.Linear(self.embed_dim, self.state_dim, **self.factory_kwargs)\n    self.U_g = nn.Linear(self.state_dim, self.state_dim, **self.factory_kwargs)\n    self.norm = nn.LayerNorm(self.state_dim, **self.factory_kwargs)\n    self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n    \n    self._init_parameters()\n```\n\n3. **Performance Optimization**:\nThe checker indicates that the model is not efficient (7.98x slower than benchmark). To improve performance:\n\n```python\ndef _forward(self, X: torch.Tensor, **Z) -> tuple:\n    batch_size, seq_len, _ = X.shape\n    device = X.device\n    \n    # Vectorize computations instead of using for loop\n    # Reshape X to (batch_size * seq_len, embed_dim)\n    X_flat = X.reshape(-1, self.embed_dim)\n    \n    # Initialize states\n    h = Z.get('state', torch.zeros(batch_size, self.state_dim, device=device, dtype=X.dtype))\n    h_expanded = h.unsqueeze(1).expand(-1, seq_len, -1)\n    h_flat = h_expanded.reshape(-1, self.state_dim)\n    \n    # Compute gates for all timesteps at once\n    g_t = self._compute_gate(X_flat, h_flat)\n    \n    # Update states\n    h_new = self.norm(g_t * (self.A * h_flat) + (1 - g_t) * self.B(X_flat))\n    h_new = self.dropout(h_new)\n    \n    # Reshape back to sequence\n    state = h_new.reshape(batch_size, seq_len, self.state_dim)\n    \n    # Store final state\n    Z['state'] = state[:, -1]\n    \n    return state, Z\n```\n\n## Strengths\n\n1. **Well-structured Implementation**: The code is well-organized with clear separation of concerns between initialization, parameter management, and forward pass.\n\n2. **Comprehensive Documentation**: Excellent docstrings with clear mathematical formulations and usage examples.\n\n3. **Stable Initialization**: Good use of initialization schemes for different parameters.\n\n4. **Selective Mechanism**: The gating mechanism is well-designed for controlling state updates.\n\n## Areas for Improvement\n\n1. **Efficiency**: \n   - Replace sequential processing with vectorized operations\n   - Consider using torch.jit.script for performance optimization\n   - Implement parallel state updates where possible\n\n2. **Numerical Stability**:\n   - Add gradient clipping\n   - Consider using layer normalization before gating\n   - Add safeguards against exploding gradients\n\n3. **Memory Efficiency**:\n   - Implement checkpoint gradients for long sequences\n   - Consider using reversible layers\n   - Optimize memory usage in state updates\n\n## Innovation and Impact\n\nThe SelectiveSSM implementation shows promise in:\n1. Adaptive state updates through gating\n2. Flexible state dimensionality\n3. Integration of normalization and dropout for stability\n\nHowever, concerns include:\n1. Computational efficiency needs improvement\n2. Memory scaling with sequence length\n3. Integration complexity with other components\n\n## Recommendations\n\n1. **Immediate Fixes**:\n   - Implement the suggested fixes for state dimension and gradient issues\n   - Add vectorized computations for better performance\n   - Add proper parameter registration\n\n2. **Optimization**:\n   - Add gradient checkpointing for long sequences\n   - Implement parallel state updates\n   - Consider using torch.jit.script\n\n3. **Testing**:\n   - Add more comprehensive unit tests\n   - Include gradient flow tests\n   - Test with varying sequence lengths and batch sizes\n\n4. **Documentation**:\n   - Add performance characteristics\n   - Document memory requirements\n   - Include integration guidelines\n\nThe implementation shows promise but needs refinement in efficiency and stability. Focus on fixing the gradient and dimension issues first, then optimize for performance.",
                        "requirements": "N/A",
                        "reuse_from": "dualstategpt.SelectiveCompressor",
                        "desc": null,
                        "gautests": {
                            "test_selective_ssm": "@gau_test\ndef test_SelectiveSSM_test_selective_ssm(device=None, dtype=None):\n    \"\"\"Test the SelectiveSSM unit\"\"\"\n    embed_dim = 64\n    state_dim = 32\n    ssm = SelectiveSSM(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        'state_dim': state_dim}, device=device, dtype=dtype)\n    batch_sizes = [1, 2, 4]\n    seq_lengths = [1, 16, 128]\n    for batch_size in batch_sizes:\n        for seq_len in seq_lengths:\n            X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n                dtype=dtype)\n            state, Z = ssm(X)\n            assert state.shape == (batch_size, seq_len, state_dim\n                ), f'Expected state shape {batch_size, seq_len, state_dim}, got {state.shape}'\n            assert 'state' in Z, 'Final state should be stored in Z'\n            assert Z['state'].shape == (batch_size, state_dim\n                ), f\"Expected final state shape {batch_size, state_dim}, got {Z['state'].shape}\"\n            assert state.dtype == dtype, f'Expected dtype {dtype}, got {state.dtype}'\n            assert state.device == device, f'Expected device {device}, got {state.device}'\n            state1, Z1 = ssm(X)\n            state2, Z2 = ssm(X, **Z1)\n            assert torch.allclose(Z1['state'], Z2['state'], rtol=0.0001\n                ), 'State should be continuous across forward passes'\n    print('All tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveSSM(GAUBase):\n    \"\"\"\n    Selective State Space Model (SSM) for HydraLM's SlowStreamUnit.\n    \n    This unit implements a selective SSM that processes input sequences to capture global dependencies\n    while maintaining states across time steps. It uses selective update gates to control state \n    transitions based on input patterns.\n\n    **Core Components:**\n\n    1. State Space Model:\n       - Maintains hidden states across time steps\n       - Updates states selectively based on input relevance\n       - Projects states to output space\n\n    2. Selective Mechanism:\n       - Computes update gates to control state transitions\n       - Allows the model to focus on relevant information\n       - Maintains stability through normalized updates\n\n    **Mathematical Formulation:**\n\n    1. State Update:\n       .. math::\n          g_t = \u03c3(W_g x_t + U_g h_{t-1})\n          h_t = g_t \u2299 (A h_{t-1}) + (1-g_t) \u2299 (B x_t)\n          \n    2. Output Projection:\n       .. math::\n          s_t = C h_t + D x_t\n\n    where:\n        - g_t: update gate\n        - h_t: hidden state\n        - x_t: input at time t\n        - A, B, C, D: learnable parameters\n        - \u2299: element-wise multiplication\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        state_dim (int, optional): Dimension of hidden state. Default: embed_dim\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary containing:\n            - state (optional): Previous hidden state of shape (batch_size, state_dim)\n\n    Outputs:\n        state (Tensor): Updated state of shape (batch_size, seq_len, state_dim)\n        Z (dict): Updated intermediate variables including:\n            - state: Final hidden state for potential use in next block\n\n    Example:\n        >>> ssm = SelectiveSSM(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        >>> state, z = ssm(x)\n        >>> print(state.shape)  # torch.Size([2, 128, 512])\n\n    Note:\n        The selective mechanism allows the model to maintain stable state updates\n        while focusing on relevant information in the input sequence. The state\n        is maintained across time steps through Z, enabling the capture of\n        long-range dependencies.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with \n            Structured State Spaces.\n        [2] Gupta, A., & Gu, A. (2022). On the Parameterization and \n            Initialization of Diagonal State Space Models.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: int=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, **self.\n            factory_kwargs) / math.sqrt(self.state_dim))\n        self.B = nn.Linear(self.embed_dim, self.state_dim, **self.\n            factory_kwargs)\n        self.C = nn.Linear(self.state_dim, self.embed_dim, **self.\n            factory_kwargs)\n        self.D = nn.Parameter(torch.randn(self.embed_dim, **self.\n            factory_kwargs) / math.sqrt(self.embed_dim))\n        self.W_g = nn.Linear(self.embed_dim, self.state_dim, **self.\n            factory_kwargs)\n        self.U_g = nn.Linear(self.state_dim, self.state_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(self.state_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n        self._init_parameters()\n\n    def _init_parameters(self):\n        \"\"\"Initialize parameters using stable initialization schemes\"\"\"\n        with torch.no_grad():\n            self.A.data = -0.5 + torch.rand_like(self.A) * 0.1\n        nn.init.constant_(self.W_g.bias, 1.0)\n        nn.init.constant_(self.U_g.bias, 1.0)\n        for layer in [self.B, self.C]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def _compute_gate(self, x: torch.Tensor, h: torch.Tensor) ->torch.Tensor:\n        \"\"\"Compute selective update gate\"\"\"\n        return torch.sigmoid(self.W_g(x) + self.U_g(h))\n\n    def _forward(self, X: torch.Tensor, **Z) ->tuple:\n        \"\"\"\n        Forward pass of the selective SSM.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Dictionary containing optional previous state\n            \n        Returns:\n            tuple: (state tensor, updated Z dictionary)\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        device = X.device\n        h = Z.get('state', torch.zeros(batch_size, self.state_dim, device=\n            device, dtype=X.dtype))\n        states = []\n        for t in range(seq_len):\n            x_t = X[:, t, :]\n            g_t = self._compute_gate(x_t, h)\n            h_new = self.norm(g_t * (self.A * h) + (1 - g_t) * self.B(x_t))\n            h = self.dropout(h_new)\n            states.append(h)\n        state = torch.stack(states, dim=1)\n        Z['state'] = h\n        return state, Z\n",
                        "rating": 3.5,
                        "spec": "{\"unitname\":\"SelectiveSSM\",\"document\":\"Selective State Space Model (SSM) for HydraLM's SlowStreamUnit.\\n\\nThis unit implements a selective SSM that processes input sequences to capture global dependencies\\nwhile maintaining states across time steps. It uses selective update gates to control state \\ntransitions based on input patterns.\\n\\n**Core Components:**\\n\\n1. State Space Model:\\n   - Maintains hidden states across time steps\\n   - Updates states selectively based on input relevance\\n   - Projects states to output space\\n\\n2. Selective Mechanism:\\n   - Computes update gates to control state transitions\\n   - Allows the model to focus on relevant information\\n   - Maintains stability through normalized updates\\n\\n**Mathematical Formulation:**\\n\\n1. State Update:\\n   .. math::\\n      g_t = \u03c3(W_g x_t + U_g h_{t-1})\\n      h_t = g_t \u2299 (A h_{t-1}) + (1-g_t) \u2299 (B x_t)\\n      \\n2. Output Projection:\\n   .. math::\\n      s_t = C h_t + D x_t\\n\\nwhere:\\n    - g_t: update gate\\n    - h_t: hidden state\\n    - x_t: input at time t\\n    - A, B, C, D: learnable parameters\\n    - \u2299: element-wise multiplication\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    state_dim (int, optional): Dimension of hidden state. Default: embed_dim\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary containing:\\n        - state (optional): Previous hidden state of shape (batch_size, state_dim)\\n\\nOutputs:\\n    state (Tensor): Updated state of shape (batch_size, seq_len, state_dim)\\n    Z (dict): Updated intermediate variables including:\\n        - state: Final hidden state for potential use in next block\\n\\nExample:\\n    >>> ssm = SelectiveSSM(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    >>> state, z = ssm(x)\\n    >>> print(state.shape)  # torch.Size([2, 128, 512])\\n\\nNote:\\n    The selective mechanism allows the model to maintain stable state updates\\n    while focusing on relevant information in the input sequence. The state\\n    is maintained across time steps through Z, enabling the capture of\\n    long-range dependencies.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with \\n        Structured State Spaces.\\n    [2] Gupta, A., & Gu, A. (2022). On the Parameterization and \\n        Initialization of Diagonal State Space Models.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "state_dim": null
                        },
                        "design_traces": null
                    },
                    "SlowStreamUnit": {
                        "review": "# Comprehensive Review of SlowStreamUnit Implementation\n\n## Overall Rating\n```rating 4.3```\n\n## Strengths\n\n1. **Well-Structured Architecture**\n   - Clear separation of concerns between SSM processing and dual projection\n   - Effective use of residual connections and normalization\n   - Clean implementation of the forward pass with logical flow\n\n2. **Comprehensive Documentation**\n   - Excellent docstring with detailed mathematical formulations\n   - Clear explanation of components and their purposes\n   - Well-referenced with relevant academic papers\n   - Good examples and usage instructions\n\n3. **Robust Implementation**\n   - Proper handling of device and dtype configurations\n   - Appropriate use of LayerNorm and dropout for stability\n   - Clean integration with the parent HydraLM architecture\n\n4. **Theoretical Soundness**\n   - Implementation aligns well with state space model theory\n   - Incorporates state space duality concepts effectively\n   - Maintains linear computational complexity\n\n## Areas for Improvement\n\n1. **State Management**\n```python\ndef _forward(self, X, **Z):\n    # Consider adding state caching\n    X_norm = self.norm(X)\n    state, Z = self.ssm(X_norm, **Z)\n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\nSuggestion: Add state caching mechanism for improved efficiency:\n```python\ndef _forward(self, X, **Z):\n    cache_key = f\"ssm_state_{self.block_loc}\"\n    X_norm = self.norm(X)\n    \n    if cache_key in Z:\n        state = Z[cache_key]\n    else:\n        state, Z = self.ssm(X_norm, **Z)\n        Z[cache_key] = state\n        \n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\n\n2. **Configuration Flexibility**\nAdd configuration options for SSM and projection parameters:\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Add configuration parameters\n    ssm_config = kwargs.get('ssm_config', {})\n    proj_config = kwargs.get('proj_config', {})\n    \n    self.ssm = SelectiveSSM(embed_dim=self.embed_dim, \n                           **ssm_config,\n                           **self.factory_kwargs)\n    self.dual_proj = DualProjection(embed_dim=self.embed_dim,\n                                   **proj_config,\n                                   **self.factory_kwargs)\n```\n\n3. **Performance Optimization**\n   - Consider adding gradient checkpointing for memory efficiency\n   - Implement parallel computation of SSM states where possible\n   - Add optional fast path for inference\n\n## Innovation and Impact\n\n### Innovative Aspects\n1. **Selective SSM Integration**\n   - Novel combination of selective state space modeling with dual projection\n   - Potential for improved efficiency in capturing long-range dependencies\n\n2. **Dual Projection Mechanism**\n   - Innovative use of state space duality for embedding projection\n   - Could lead to better representation learning\n\n### Potential Impact\n1. **Scalability**\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage through selective state processing\n\n2. **Model Performance**\n   - Improved capture of global dependencies\n   - Better handling of long-range relationships in text\n\n## Integration Considerations\n\n1. **Child Unit Dependencies**\n   - Need to implement SelectiveSSM and DualProjection units\n   - Consider providing default implementations for testing\n\n2. **Memory Management**\n   - Monitor state size growth in long sequences\n   - Implement state pruning mechanism if needed\n\n## Recommendations for the Coder\n\n1. **Implementation Priority**\n```python\n# Priority 1: Implement child units\nclass SelectiveSSM(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n\nclass DualProjection(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n```\n\n2. **Add Unit Tests**\n```python\n@gau_test\ndef test_slow_stream_unit(device=None, dtype=None):\n    # Test initialization\n    unit = SlowStreamUnit(embed_dim=512, block_loc=(0,0), \n                         kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test forward pass\n    batch_size, seq_len = 2, 128\n    X = torch.randn(batch_size, seq_len, 512, \n                   device=device, dtype=dtype)\n    Y, Z = unit(X)\n    \n    # Assert output shape\n    assert Y.shape == X.shape\n    \n    # Test state persistence\n    Y2, Z = unit(X, **Z)\n    assert Y2.shape == X.shape\n```\n\n3. **Documentation Updates**\n   - Add performance characteristics and memory usage patterns\n   - Document expected behavior with different sequence lengths\n   - Provide examples of configuration options\n\n4. **Optimization Opportunities**\n   - Implement parallel state computation\n   - Add gradient checkpointing option\n   - Consider sparse state updates\n\n## Additional Suggestions\n\n1. **Error Handling**\n```python\ndef _forward(self, X, **Z):\n    if X.dim() != 3:\n        raise ValueError(f\"Expected 3D input, got {X.dim()}D\")\n    if X.size(-1) != self.embed_dim:\n        raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {X.size(-1)}\")\n    # ... rest of implementation\n```\n\n2. **Configuration Validation**\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Validate configurations\n    if embed_dim <= 0:\n        raise ValueError(f\"embed_dim must be positive, got {embed_dim}\")\n    if kwargs.get('dropout', 0.1) < 0 or kwargs.get('dropout', 0.1) > 1:\n        raise ValueError(\"dropout must be between 0 and 1\")\n```\n\n3. **Performance Monitoring**\n   - Add hooks for tracking state size and computation time\n   - Implement optional logging of performance metrics\n\nThe implementation shows promise in advancing language model capabilities through efficient handling of global dependencies. Focus on implementing the child units and adding comprehensive tests to ensure robust performance across different scenarios.",
                        "requirements": "N/A",
                        "reuse_from": "dualstategpt.DualStateProcessor",
                        "desc": null,
                        "gautests": {
                            "test_slow_stream_unit": "@gau_test\ndef test_SlowStreamUnit_test_slow_stream_unit(device=None, dtype=None):\n    \"\"\"Unit test for SlowStreamUnit\"\"\"\n    batch_size = 2\n    seq_len = 128\n    embed_dim = 256\n    unit = SlowStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all=\n        {'dropout': 0.1}, device=device, dtype=dtype)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = unit(X, **Z)\n    assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n    assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n    assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    for seq_len in [64, 256]:\n        X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n            dtype=dtype)\n        Y, Z = unit(X, **Z)\n        assert Y.shape == X.shape, f'Failed for sequence length {seq_len}'\n    print('All tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nimport math\n\n\nclass SlowStreamUnit(GAUBase):\n    \"\"\"\n    SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\n\n    This unit processes input sequences through a selective State Space Model (SSM) and applies\n    state space duality via a dual projection to capture long-range dependencies efficiently.\n    It serves as the \"Slow Stream\" in HydraLM's dual-stream architecture.\n\n    **Core Components:**\n\n    - `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\n    - `DualProjection`: Projects SSM states to output embeddings using state space duality\n\n    **Mathematical Formulation:**\n\n    1. State Space Model Processing:\n       .. math::\n          h_t = A h_{t-1} + B x_t\n          y_t = C h_t + D x_t\n\n       where h_t is the hidden state, x_t is input, and y_t is output at time t.\n       A, B, C, D are learnable parameters.\n\n    2. Dual Projection:\n       .. math::\n          Y = \text{DualProj}(h) + X\n\n       where h represents the SSM states and X is the residual connection.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n\n    Example:\n        slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        y, z = slow_stream(x)\n\n    Note:\n        This unit is designed to work efficiently with long sequences by maintaining\n        linear computational complexity through the use of SSMs.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\n        [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \n            Algorithms Through Structured State Space Duality.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ssm = SelectiveSSM(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dual_proj = DualProjection(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n\n    def _forward(self, X, **Z):\n        X_norm = self.norm(X)\n        state, Z = self.ssm(X_norm, **Z)\n        Y, Z = self.dual_proj(state, **Z)\n        Y = self.dropout(Y) + X\n        return Y, Z\n",
                        "rating": 4.3,
                        "spec": "{\"unitname\":\"SlowStreamUnit\",\"document\":\"SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\\n\\nThis unit processes input sequences through a selective State Space Model (SSM) and applies\\nstate space duality via a dual projection to capture long-range dependencies efficiently.\\nIt serves as the \\\"Slow Stream\\\" in HydraLM's dual-stream architecture.\\n\\n**Core Components:**\\n\\n- `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\\n- `DualProjection`: Projects SSM states to output embeddings using state space duality\\n\\n**Mathematical Formulation:**\\n\\n1. State Space Model Processing:\\n   .. math::\\n      h_t = A h_{t-1} + B x_t\\n      y_t = C h_t + D x_t\\n\\n   where h_t is the hidden state, x_t is input, and y_t is output at time t.\\n   A, B, C, D are learnable parameters.\\n\\n2. Dual Projection:\\n   .. math::\\n      Y =   ext{DualProj}(h) + X\\n\\n   where h represents the SSM states and X is the residual connection.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\\n\\nExample:\\n    slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    y, z = slow_stream(x)\\n\\nNote:\\n    This unit is designed to work efficiently with long sequences by maintaining\\n    linear computational complexity through the use of SSMs.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\\n    [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \\n        Algorithms Through Structured State Space Duality.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "SelectiveSSM",
                            "DualProjection"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "HydraLMBlock": {
                        "review": "# Comprehensive Review of HydraLMBlock Implementation\n\n## Overall Rating\n```rating 4.5```\n\n## Strengths\n\n1. **Clear Architecture Implementation**\n- Successfully implements the proposed dual-stream architecture with Fast and Slow processing paths\n- Clean separation of concerns between components\n- Well-structured initialization and forward pass logic\n\n2. **Code Quality**\n- Excellent docstring documentation following best practices\n- Clear variable naming and logical code organization\n- Proper use of PyTorch modules and tensor operations\n\n3. **Technical Implementation**\n- Efficient use of nn.Sequential for the gating mechanism\n- Proper handling of device and dtype through factory_kwargs\n- Good use of LayerNorm for stabilizing outputs\n\n4. **Modularity**\n- Clean separation between Fast and Slow streams\n- Modular design allows for easy modification or replacement of components\n- Well-structured inheritance from GAUBase\n\n## Areas for Improvement\n\n1. **State Management**\n- Consider adding state persistence mechanisms for the Slow Stream\n- Could benefit from explicit state initialization in __init__\n\n```python\ndef __init__(self, ...):\n    # ... existing code ...\n    self.register_buffer('hidden_state', torch.zeros(embed_dim))\n```\n\n2. **Error Handling**\n- Add input validation for dimensions and types\n- Include checks for numerical stability\n\n```python\ndef _forward(self, X, **Z):\n    if torch.isnan(X).any():\n        raise ValueError(\"Input contains NaN values\")\n    # ... rest of implementation\n```\n\n3. **Memory Efficiency**\n- Consider implementing gradient checkpointing for memory efficiency\n- Add optional memory-efficient attention variants\n\n4. **Performance Optimization**\n- Consider adding JIT compilation support\n- Implement optional fused operations for better performance\n\n## Innovation Assessment\n\n### Novel Aspects\n1. **Adaptive Gating Mechanism**\n- The implementation successfully realizes the proposed dynamic integration of Fast and Slow streams\n- The gating mechanism is efficiently implemented using nn.Sequential\n\n2. **Dual Stream Processing**\n- Clean implementation of parallel processing streams\n- Effective combination of local and global feature extraction\n\n### Potential Impact\n1. **Scalability**\n- The implementation should scale well with sequence length due to linear complexity\n- Modular design allows for easy scaling of model capacity\n\n2. **Performance**\n- The dual-stream architecture could provide better performance on both short and long-range dependencies\n- Adaptive gating mechanism should help in efficient resource utilization\n\n## Integration and Scalability Recommendations\n\n1. **Stream Synchronization**\n```python\ndef _forward(self, X, **Z):\n    # Add synchronization point\n    torch.cuda.synchronize() if X.is_cuda else None\n    H_fast, Z = self.fast_stream(X, **Z)\n    H_slow, Z = self.slow_stream(X, **Z)\n    # ... rest of implementation\n```\n\n2. **Memory Management**\n```python\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # Enable automatic mixed precision\n    # ... existing implementation\n```\n\n3. **Batch Processing Optimization**\n```python\ndef _forward(self, X, **Z):\n    # Add batch dimension handling\n    B, L, D = X.shape\n    if B > 1:\n        # Process in chunks if needed\n        chunk_size = min(B, 32)\n        results = []\n        for i in range(0, B, chunk_size):\n            chunk = X[i:i+chunk_size]\n            # Process chunk\n            # ... existing implementation\n```\n\n## Recommendations for the Coder\n\n1. **Implementation Priorities**\n- Add comprehensive unit tests covering edge cases\n- Implement memory-efficient variants for large-scale deployment\n- Add support for gradient checkpointing\n\n2. **Code Structure**\n```python\nclass HydraLMBlock(GAUBase):\n    def __init__(self, ...):\n        # Add configuration validation\n        assert embed_dim > 0, \"embed_dim must be positive\"\n        # ... rest of implementation\n\n    @torch.jit.script_method\n    def _forward(self, X, **Z):\n        # Add JIT support\n        # ... existing implementation\n```\n\n3. **Documentation**\n- Add more detailed documentation about the expected shapes and types of intermediate tensors\n- Include performance characteristics and memory requirements\n- Document any assumptions about input distributions or constraints\n\n4. **Testing**\n```python\n@gau_test\ndef test_hydralm_block(device=None, dtype=None):\n    # Add comprehensive tests\n    block = HydraLMBlock(embed_dim=256, block_loc=(0,0), \n                        kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test various sequence lengths\n    for seq_len in [128, 256, 512]:\n        X = torch.randn(2, seq_len, 256, device=device, dtype=dtype)\n        Y, Z = block(X)\n        assert not torch.isnan(Y).any(), f\"Output contains NaN values for seq_len={seq_len}\"\n        assert Y.shape == X.shape, f\"Output shape mismatch for seq_len={seq_len}\"\n```\n\n5. **Performance Optimization**\n- Profile the implementation to identify bottlenecks\n- Consider implementing custom CUDA kernels for critical operations\n- Add support for quantization and pruning\n\n## Final Notes\n\nThe implementation is strong and closely follows the proposed design. The dual-stream architecture with adaptive gating is well-implemented, and the code is clean and well-documented. The main areas for improvement are around optimization, testing, and advanced features like gradient checkpointing and quantization support.\n\nThe high rating (4.5) reflects the solid implementation of the core architecture while leaving room for optimization and advanced features. The code provides a strong foundation for further development and optimization.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_hydralm_block": "@gau_test\ndef test_HydraLMBlock_test_hydralm_block(device=None, dtype=None):\n    batch_size = 2\n    seq_len = 16\n    embed_dim = 32\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    block = HydraLMBlock(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        }, device=device, dtype=dtype)\n    Y, Z = block(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    X_long = torch.randn(batch_size, seq_len * 2, embed_dim, device=device,\n        dtype=dtype)\n    Y_long, Z = block(X_long, **Z)\n    assert Y_long.shape == X_long.shape, 'Failed to handle longer sequence'\n    print('HydraLMBlock tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HydraLMBlock(GAUBase):\n    \"\"\"\n    HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\n    \n    This block processes input sequences through two parallel streams:\n    1. Fast Stream: Captures local patterns using Toeplitz Convolution\n    2. Slow Stream: Models global dependencies using State Space Models\n    \n    The outputs of both streams are combined using an adaptive gating mechanism.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        \n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.fast_stream = FastStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.slow_stream = SlowStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.gate = nn.Sequential(nn.Linear(2 * embed_dim, embed_dim, **\n            self.factory_kwargs), nn.Sigmoid())\n        self.output_proj = nn.Linear(embed_dim, embed_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        H_fast, Z = self.fast_stream(X, **Z)\n        H_slow, Z = self.slow_stream(X, **Z)\n        H_concat = torch.cat([H_fast, H_slow], dim=-1)\n        G = self.gate(H_concat)\n        Y = G * H_fast + (1 - G) * H_slow\n        Y = self.output_proj(Y)\n        Y = self.norm(Y)\n        return Y, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"HydraLMBlock\",\"document\":\"HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\\n\\nThis block processes input sequences through two parallel streams:\\n1. Fast Stream: Captures local patterns using Toeplitz Convolution\\n2. Slow Stream: Models global dependencies using State Space Models\\n\\nThe outputs of both streams are combined using an adaptive gating mechanism.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    \\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "FastStreamUnit",
                            "SlowStreamUnit"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    }
                },
                "suggestions": "",
                "name": "hydralm"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "o1_preview",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "claude3.5_sonnet",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 0.40593,
                "IMPLEMENTATION_CODER": 0.062292,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 0.057126,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": "",
                "root": "HydraLMBlock",
                "proposal": "",
                "proposal_traces": [],
                "rating": 0,
                "declares": {
                    "DualProjection": "{\"unitname\":\"DualProjection\",\"requirements\":\"Projects SSM states to output embeddings using state space duality principles. Should transform state representations while preserving temporal dependencies.\",\"inputs\":[\"state\"],\"outputs\":[\"Y\"]}",
                    "SelectiveSSM": "{\"unitname\":\"SelectiveSSM\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "FastStreamUnit": "{\"unitname\":\"FastStreamUnit\",\"requirements\":\"Captures local patterns using Toeplitz Convolution. Should process input sequence X and return processed sequence of same shape.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                    "SlowStreamUnit": "{\"unitname\":\"SlowStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "HydraLMBlock": "{\"unitname\":\"HydraLMBlock\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "units": {
                    "SelectiveSSM": {
                        "review": "# Feedback Report for SelectiveSSM Implementation\n\n## Overall Rating\n```rating 3.5```\n\n## Analysis of Failed Checks\n\nThe implementation has two main issues that need to be addressed:\n\n1. **Unit Test Failure**:\n```python\nAssertionError: Expected state shape (1, 1, 32), got torch.Size([1, 1, 64])\n```\nThis indicates that the state dimension is not being properly set - the implementation is using embed_dim (64) instead of state_dim (32) as the output dimension.\n\n2. **Differentiability Test Failure**:\n```\nError: Used parameter backbone.blocks.0.gab.root.slow_stream.ssm.D requires gradients but has none.\nError: Used parameter backbone.blocks.0.gab.root.slow_stream.ssm.C.weight requires gradients but has none.\n```\nThis suggests that some parameters are not being properly registered for gradient computation.\n\n### Fixes Required:\n\n1. **State Dimension Issue**:\nThe output state needs to be properly projected to state_dim. Modify the forward pass:\n\n```python\ndef _forward(self, X: torch.Tensor, **Z) -> tuple:\n    batch_size, seq_len, _ = X.shape\n    device = X.device\n    h = Z.get('state', torch.zeros(batch_size, self.state_dim, device=device, dtype=X.dtype))\n    states = []\n    for t in range(seq_len):\n        x_t = X[:, t, :]\n        g_t = self._compute_gate(x_t, h)\n        # Project input to state dimension\n        h_new = self.norm(g_t * (self.A * h) + (1 - g_t) * self.B(x_t))\n        h = self.dropout(h_new)\n        states.append(h)\n    state = torch.stack(states, dim=1)\n    # Ensure state is in state_dim\n    assert state.shape == (batch_size, seq_len, self.state_dim)\n    Z['state'] = h\n    return state, Z\n```\n\n2. **Gradient Issues**:\nParameters need to be properly registered. Modify the initialization:\n\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, state_dim: int=None, **kwargs):\n    self.factory_kwargs = {'device': device, 'dtype': dtype}\n    super().__init__(embed_dim, block_loc, kwarg_all)\n    \n    # Ensure state_dim is set\n    self.state_dim = state_dim if state_dim is not None else embed_dim\n    \n    # Register parameters properly\n    self.A = nn.Parameter(torch.randn(self.state_dim, **self.factory_kwargs) / math.sqrt(self.state_dim))\n    self.B = nn.Linear(self.embed_dim, self.state_dim, **self.factory_kwargs)\n    self.C = nn.Linear(self.state_dim, self.embed_dim, **self.factory_kwargs)\n    self.D = nn.Parameter(torch.randn(self.embed_dim, **self.factory_kwargs) / math.sqrt(self.embed_dim))\n    \n    # Ensure parameters require gradients\n    self.A.requires_grad_(True)\n    self.D.requires_grad_(True)\n    \n    # Initialize other layers\n    self.W_g = nn.Linear(self.embed_dim, self.state_dim, **self.factory_kwargs)\n    self.U_g = nn.Linear(self.state_dim, self.state_dim, **self.factory_kwargs)\n    self.norm = nn.LayerNorm(self.state_dim, **self.factory_kwargs)\n    self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n    \n    self._init_parameters()\n```\n\n3. **Performance Optimization**:\nThe checker indicates that the model is not efficient (7.98x slower than benchmark). To improve performance:\n\n```python\ndef _forward(self, X: torch.Tensor, **Z) -> tuple:\n    batch_size, seq_len, _ = X.shape\n    device = X.device\n    \n    # Vectorize computations instead of using for loop\n    # Reshape X to (batch_size * seq_len, embed_dim)\n    X_flat = X.reshape(-1, self.embed_dim)\n    \n    # Initialize states\n    h = Z.get('state', torch.zeros(batch_size, self.state_dim, device=device, dtype=X.dtype))\n    h_expanded = h.unsqueeze(1).expand(-1, seq_len, -1)\n    h_flat = h_expanded.reshape(-1, self.state_dim)\n    \n    # Compute gates for all timesteps at once\n    g_t = self._compute_gate(X_flat, h_flat)\n    \n    # Update states\n    h_new = self.norm(g_t * (self.A * h_flat) + (1 - g_t) * self.B(X_flat))\n    h_new = self.dropout(h_new)\n    \n    # Reshape back to sequence\n    state = h_new.reshape(batch_size, seq_len, self.state_dim)\n    \n    # Store final state\n    Z['state'] = state[:, -1]\n    \n    return state, Z\n```\n\n## Strengths\n\n1. **Well-structured Implementation**: The code is well-organized with clear separation of concerns between initialization, parameter management, and forward pass.\n\n2. **Comprehensive Documentation**: Excellent docstrings with clear mathematical formulations and usage examples.\n\n3. **Stable Initialization**: Good use of initialization schemes for different parameters.\n\n4. **Selective Mechanism**: The gating mechanism is well-designed for controlling state updates.\n\n## Areas for Improvement\n\n1. **Efficiency**: \n   - Replace sequential processing with vectorized operations\n   - Consider using torch.jit.script for performance optimization\n   - Implement parallel state updates where possible\n\n2. **Numerical Stability**:\n   - Add gradient clipping\n   - Consider using layer normalization before gating\n   - Add safeguards against exploding gradients\n\n3. **Memory Efficiency**:\n   - Implement checkpoint gradients for long sequences\n   - Consider using reversible layers\n   - Optimize memory usage in state updates\n\n## Innovation and Impact\n\nThe SelectiveSSM implementation shows promise in:\n1. Adaptive state updates through gating\n2. Flexible state dimensionality\n3. Integration of normalization and dropout for stability\n\nHowever, concerns include:\n1. Computational efficiency needs improvement\n2. Memory scaling with sequence length\n3. Integration complexity with other components\n\n## Recommendations\n\n1. **Immediate Fixes**:\n   - Implement the suggested fixes for state dimension and gradient issues\n   - Add vectorized computations for better performance\n   - Add proper parameter registration\n\n2. **Optimization**:\n   - Add gradient checkpointing for long sequences\n   - Implement parallel state updates\n   - Consider using torch.jit.script\n\n3. **Testing**:\n   - Add more comprehensive unit tests\n   - Include gradient flow tests\n   - Test with varying sequence lengths and batch sizes\n\n4. **Documentation**:\n   - Add performance characteristics\n   - Document memory requirements\n   - Include integration guidelines\n\nThe implementation shows promise but needs refinement in efficiency and stability. Focus on fixing the gradient and dimension issues first, then optimize for performance.",
                        "requirements": "N/A",
                        "reuse_from": "dualstategpt.SelectiveCompressor",
                        "desc": null,
                        "gautests": {
                            "test_selective_ssm": "@gau_test\ndef test_SelectiveSSM_test_selective_ssm(device=None, dtype=None):\n    \"\"\"Test the SelectiveSSM unit\"\"\"\n    embed_dim = 64\n    state_dim = 32\n    ssm = SelectiveSSM(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        'state_dim': state_dim}, device=device, dtype=dtype)\n    batch_sizes = [1, 2, 4]\n    seq_lengths = [1, 16, 128]\n    for batch_size in batch_sizes:\n        for seq_len in seq_lengths:\n            X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n                dtype=dtype)\n            state, Z = ssm(X)\n            assert state.shape == (batch_size, seq_len, state_dim\n                ), f'Expected state shape {batch_size, seq_len, state_dim}, got {state.shape}'\n            assert 'state' in Z, 'Final state should be stored in Z'\n            assert Z['state'].shape == (batch_size, state_dim\n                ), f\"Expected final state shape {batch_size, state_dim}, got {Z['state'].shape}\"\n            assert state.dtype == dtype, f'Expected dtype {dtype}, got {state.dtype}'\n            assert state.device == device, f'Expected device {device}, got {state.device}'\n            state1, Z1 = ssm(X)\n            state2, Z2 = ssm(X, **Z1)\n            assert torch.allclose(Z1['state'], Z2['state'], rtol=0.0001\n                ), 'State should be continuous across forward passes'\n    print('All tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveSSM(GAUBase):\n    \"\"\"\n    Selective State Space Model (SSM) for HydraLM's SlowStreamUnit.\n    \n    This unit implements a selective SSM that processes input sequences to capture global dependencies\n    while maintaining states across time steps. It uses selective update gates to control state \n    transitions based on input patterns.\n\n    **Core Components:**\n\n    1. State Space Model:\n       - Maintains hidden states across time steps\n       - Updates states selectively based on input relevance\n       - Projects states to output space\n\n    2. Selective Mechanism:\n       - Computes update gates to control state transitions\n       - Allows the model to focus on relevant information\n       - Maintains stability through normalized updates\n\n    **Mathematical Formulation:**\n\n    1. State Update:\n       .. math::\n          g_t = \u03c3(W_g x_t + U_g h_{t-1})\n          h_t = g_t \u2299 (A h_{t-1}) + (1-g_t) \u2299 (B x_t)\n          \n    2. Output Projection:\n       .. math::\n          s_t = C h_t + D x_t\n\n    where:\n        - g_t: update gate\n        - h_t: hidden state\n        - x_t: input at time t\n        - A, B, C, D: learnable parameters\n        - \u2299: element-wise multiplication\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        state_dim (int, optional): Dimension of hidden state. Default: embed_dim\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary containing:\n            - state (optional): Previous hidden state of shape (batch_size, state_dim)\n\n    Outputs:\n        state (Tensor): Updated state of shape (batch_size, seq_len, state_dim)\n        Z (dict): Updated intermediate variables including:\n            - state: Final hidden state for potential use in next block\n\n    Example:\n        >>> ssm = SelectiveSSM(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        >>> x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        >>> state, z = ssm(x)\n        >>> print(state.shape)  # torch.Size([2, 128, 512])\n\n    Note:\n        The selective mechanism allows the model to maintain stable state updates\n        while focusing on relevant information in the input sequence. The state\n        is maintained across time steps through Z, enabling the capture of\n        long-range dependencies.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with \n            Structured State Spaces.\n        [2] Gupta, A., & Gu, A. (2022). On the Parameterization and \n            Initialization of Diagonal State Space Models.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim: int=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.A = nn.Parameter(torch.randn(self.state_dim, **self.\n            factory_kwargs) / math.sqrt(self.state_dim))\n        self.B = nn.Linear(self.embed_dim, self.state_dim, **self.\n            factory_kwargs)\n        self.C = nn.Linear(self.state_dim, self.embed_dim, **self.\n            factory_kwargs)\n        self.D = nn.Parameter(torch.randn(self.embed_dim, **self.\n            factory_kwargs) / math.sqrt(self.embed_dim))\n        self.W_g = nn.Linear(self.embed_dim, self.state_dim, **self.\n            factory_kwargs)\n        self.U_g = nn.Linear(self.state_dim, self.state_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(self.state_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n        self._init_parameters()\n\n    def _init_parameters(self):\n        \"\"\"Initialize parameters using stable initialization schemes\"\"\"\n        with torch.no_grad():\n            self.A.data = -0.5 + torch.rand_like(self.A) * 0.1\n        nn.init.constant_(self.W_g.bias, 1.0)\n        nn.init.constant_(self.U_g.bias, 1.0)\n        for layer in [self.B, self.C]:\n            nn.init.xavier_uniform_(layer.weight)\n            nn.init.zeros_(layer.bias)\n\n    def _compute_gate(self, x: torch.Tensor, h: torch.Tensor) ->torch.Tensor:\n        \"\"\"Compute selective update gate\"\"\"\n        return torch.sigmoid(self.W_g(x) + self.U_g(h))\n\n    def _forward(self, X: torch.Tensor, **Z) ->tuple:\n        \"\"\"\n        Forward pass of the selective SSM.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Dictionary containing optional previous state\n            \n        Returns:\n            tuple: (state tensor, updated Z dictionary)\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        device = X.device\n        h = Z.get('state', torch.zeros(batch_size, self.state_dim, device=\n            device, dtype=X.dtype))\n        states = []\n        for t in range(seq_len):\n            x_t = X[:, t, :]\n            g_t = self._compute_gate(x_t, h)\n            h_new = self.norm(g_t * (self.A * h) + (1 - g_t) * self.B(x_t))\n            h = self.dropout(h_new)\n            states.append(h)\n        state = torch.stack(states, dim=1)\n        Z['state'] = h\n        return state, Z\n",
                        "rating": 3.5,
                        "spec": "{\"unitname\":\"SelectiveSSM\",\"document\":\"Selective State Space Model (SSM) for HydraLM's SlowStreamUnit.\\n\\nThis unit implements a selective SSM that processes input sequences to capture global dependencies\\nwhile maintaining states across time steps. It uses selective update gates to control state \\ntransitions based on input patterns.\\n\\n**Core Components:**\\n\\n1. State Space Model:\\n   - Maintains hidden states across time steps\\n   - Updates states selectively based on input relevance\\n   - Projects states to output space\\n\\n2. Selective Mechanism:\\n   - Computes update gates to control state transitions\\n   - Allows the model to focus on relevant information\\n   - Maintains stability through normalized updates\\n\\n**Mathematical Formulation:**\\n\\n1. State Update:\\n   .. math::\\n      g_t = \u03c3(W_g x_t + U_g h_{t-1})\\n      h_t = g_t \u2299 (A h_{t-1}) + (1-g_t) \u2299 (B x_t)\\n      \\n2. Output Projection:\\n   .. math::\\n      s_t = C h_t + D x_t\\n\\nwhere:\\n    - g_t: update gate\\n    - h_t: hidden state\\n    - x_t: input at time t\\n    - A, B, C, D: learnable parameters\\n    - \u2299: element-wise multiplication\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    state_dim (int, optional): Dimension of hidden state. Default: embed_dim\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary containing:\\n        - state (optional): Previous hidden state of shape (batch_size, state_dim)\\n\\nOutputs:\\n    state (Tensor): Updated state of shape (batch_size, seq_len, state_dim)\\n    Z (dict): Updated intermediate variables including:\\n        - state: Final hidden state for potential use in next block\\n\\nExample:\\n    >>> ssm = SelectiveSSM(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    >>> x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    >>> state, z = ssm(x)\\n    >>> print(state.shape)  # torch.Size([2, 128, 512])\\n\\nNote:\\n    The selective mechanism allows the model to maintain stable state updates\\n    while focusing on relevant information in the input sequence. The state\\n    is maintained across time steps through Z, enabling the capture of\\n    long-range dependencies.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with \\n        Structured State Spaces.\\n    [2] Gupta, A., & Gu, A. (2022). On the Parameterization and \\n        Initialization of Diagonal State Space Models.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "state_dim": null
                        },
                        "design_traces": null
                    },
                    "SlowStreamUnit": {
                        "review": "# Comprehensive Review of SlowStreamUnit Implementation\n\n## Overall Rating\n```rating 4.3```\n\n## Strengths\n\n1. **Well-Structured Architecture**\n   - Clear separation of concerns between SSM processing and dual projection\n   - Effective use of residual connections and normalization\n   - Clean implementation of the forward pass with logical flow\n\n2. **Comprehensive Documentation**\n   - Excellent docstring with detailed mathematical formulations\n   - Clear explanation of components and their purposes\n   - Well-referenced with relevant academic papers\n   - Good examples and usage instructions\n\n3. **Robust Implementation**\n   - Proper handling of device and dtype configurations\n   - Appropriate use of LayerNorm and dropout for stability\n   - Clean integration with the parent HydraLM architecture\n\n4. **Theoretical Soundness**\n   - Implementation aligns well with state space model theory\n   - Incorporates state space duality concepts effectively\n   - Maintains linear computational complexity\n\n## Areas for Improvement\n\n1. **State Management**\n```python\ndef _forward(self, X, **Z):\n    # Consider adding state caching\n    X_norm = self.norm(X)\n    state, Z = self.ssm(X_norm, **Z)\n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\nSuggestion: Add state caching mechanism for improved efficiency:\n```python\ndef _forward(self, X, **Z):\n    cache_key = f\"ssm_state_{self.block_loc}\"\n    X_norm = self.norm(X)\n    \n    if cache_key in Z:\n        state = Z[cache_key]\n    else:\n        state, Z = self.ssm(X_norm, **Z)\n        Z[cache_key] = state\n        \n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\n\n2. **Configuration Flexibility**\nAdd configuration options for SSM and projection parameters:\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Add configuration parameters\n    ssm_config = kwargs.get('ssm_config', {})\n    proj_config = kwargs.get('proj_config', {})\n    \n    self.ssm = SelectiveSSM(embed_dim=self.embed_dim, \n                           **ssm_config,\n                           **self.factory_kwargs)\n    self.dual_proj = DualProjection(embed_dim=self.embed_dim,\n                                   **proj_config,\n                                   **self.factory_kwargs)\n```\n\n3. **Performance Optimization**\n   - Consider adding gradient checkpointing for memory efficiency\n   - Implement parallel computation of SSM states where possible\n   - Add optional fast path for inference\n\n## Innovation and Impact\n\n### Innovative Aspects\n1. **Selective SSM Integration**\n   - Novel combination of selective state space modeling with dual projection\n   - Potential for improved efficiency in capturing long-range dependencies\n\n2. **Dual Projection Mechanism**\n   - Innovative use of state space duality for embedding projection\n   - Could lead to better representation learning\n\n### Potential Impact\n1. **Scalability**\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage through selective state processing\n\n2. **Model Performance**\n   - Improved capture of global dependencies\n   - Better handling of long-range relationships in text\n\n## Integration Considerations\n\n1. **Child Unit Dependencies**\n   - Need to implement SelectiveSSM and DualProjection units\n   - Consider providing default implementations for testing\n\n2. **Memory Management**\n   - Monitor state size growth in long sequences\n   - Implement state pruning mechanism if needed\n\n## Recommendations for the Coder\n\n1. **Implementation Priority**\n```python\n# Priority 1: Implement child units\nclass SelectiveSSM(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n\nclass DualProjection(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n```\n\n2. **Add Unit Tests**\n```python\n@gau_test\ndef test_slow_stream_unit(device=None, dtype=None):\n    # Test initialization\n    unit = SlowStreamUnit(embed_dim=512, block_loc=(0,0), \n                         kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test forward pass\n    batch_size, seq_len = 2, 128\n    X = torch.randn(batch_size, seq_len, 512, \n                   device=device, dtype=dtype)\n    Y, Z = unit(X)\n    \n    # Assert output shape\n    assert Y.shape == X.shape\n    \n    # Test state persistence\n    Y2, Z = unit(X, **Z)\n    assert Y2.shape == X.shape\n```\n\n3. **Documentation Updates**\n   - Add performance characteristics and memory usage patterns\n   - Document expected behavior with different sequence lengths\n   - Provide examples of configuration options\n\n4. **Optimization Opportunities**\n   - Implement parallel state computation\n   - Add gradient checkpointing option\n   - Consider sparse state updates\n\n## Additional Suggestions\n\n1. **Error Handling**\n```python\ndef _forward(self, X, **Z):\n    if X.dim() != 3:\n        raise ValueError(f\"Expected 3D input, got {X.dim()}D\")\n    if X.size(-1) != self.embed_dim:\n        raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {X.size(-1)}\")\n    # ... rest of implementation\n```\n\n2. **Configuration Validation**\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Validate configurations\n    if embed_dim <= 0:\n        raise ValueError(f\"embed_dim must be positive, got {embed_dim}\")\n    if kwargs.get('dropout', 0.1) < 0 or kwargs.get('dropout', 0.1) > 1:\n        raise ValueError(\"dropout must be between 0 and 1\")\n```\n\n3. **Performance Monitoring**\n   - Add hooks for tracking state size and computation time\n   - Implement optional logging of performance metrics\n\nThe implementation shows promise in advancing language model capabilities through efficient handling of global dependencies. Focus on implementing the child units and adding comprehensive tests to ensure robust performance across different scenarios.",
                        "requirements": "N/A",
                        "reuse_from": "dualstategpt.DualStateProcessor",
                        "desc": null,
                        "gautests": {
                            "test_slow_stream_unit": "@gau_test\ndef test_SlowStreamUnit_test_slow_stream_unit(device=None, dtype=None):\n    \"\"\"Unit test for SlowStreamUnit\"\"\"\n    batch_size = 2\n    seq_len = 128\n    embed_dim = 256\n    unit = SlowStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all=\n        {'dropout': 0.1}, device=device, dtype=dtype)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = unit(X, **Z)\n    assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n    assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n    assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    for seq_len in [64, 256]:\n        X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n            dtype=dtype)\n        Y, Z = unit(X, **Z)\n        assert Y.shape == X.shape, f'Failed for sequence length {seq_len}'\n    print('All tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nimport math\n\n\nclass SlowStreamUnit(GAUBase):\n    \"\"\"\n    SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\n\n    This unit processes input sequences through a selective State Space Model (SSM) and applies\n    state space duality via a dual projection to capture long-range dependencies efficiently.\n    It serves as the \"Slow Stream\" in HydraLM's dual-stream architecture.\n\n    **Core Components:**\n\n    - `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\n    - `DualProjection`: Projects SSM states to output embeddings using state space duality\n\n    **Mathematical Formulation:**\n\n    1. State Space Model Processing:\n       .. math::\n          h_t = A h_{t-1} + B x_t\n          y_t = C h_t + D x_t\n\n       where h_t is the hidden state, x_t is input, and y_t is output at time t.\n       A, B, C, D are learnable parameters.\n\n    2. Dual Projection:\n       .. math::\n          Y = \text{DualProj}(h) + X\n\n       where h represents the SSM states and X is the residual connection.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n\n    Example:\n        slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        y, z = slow_stream(x)\n\n    Note:\n        This unit is designed to work efficiently with long sequences by maintaining\n        linear computational complexity through the use of SSMs.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\n        [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \n            Algorithms Through Structured State Space Duality.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ssm = SelectiveSSM(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dual_proj = DualProjection(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n\n    def _forward(self, X, **Z):\n        X_norm = self.norm(X)\n        state, Z = self.ssm(X_norm, **Z)\n        Y, Z = self.dual_proj(state, **Z)\n        Y = self.dropout(Y) + X\n        return Y, Z\n",
                        "rating": 4.3,
                        "spec": "{\"unitname\":\"SlowStreamUnit\",\"document\":\"SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\\n\\nThis unit processes input sequences through a selective State Space Model (SSM) and applies\\nstate space duality via a dual projection to capture long-range dependencies efficiently.\\nIt serves as the \\\"Slow Stream\\\" in HydraLM's dual-stream architecture.\\n\\n**Core Components:**\\n\\n- `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\\n- `DualProjection`: Projects SSM states to output embeddings using state space duality\\n\\n**Mathematical Formulation:**\\n\\n1. State Space Model Processing:\\n   .. math::\\n      h_t = A h_{t-1} + B x_t\\n      y_t = C h_t + D x_t\\n\\n   where h_t is the hidden state, x_t is input, and y_t is output at time t.\\n   A, B, C, D are learnable parameters.\\n\\n2. Dual Projection:\\n   .. math::\\n      Y =   ext{DualProj}(h) + X\\n\\n   where h represents the SSM states and X is the residual connection.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\\n\\nExample:\\n    slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    y, z = slow_stream(x)\\n\\nNote:\\n    This unit is designed to work efficiently with long sequences by maintaining\\n    linear computational complexity through the use of SSMs.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\\n    [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \\n        Algorithms Through Structured State Space Duality.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "SelectiveSSM",
                            "DualProjection"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "HydraLMBlock": {
                        "review": "# Comprehensive Review of HydraLMBlock Implementation\n\n## Overall Rating\n```rating 4.5```\n\n## Strengths\n\n1. **Clear Architecture Implementation**\n- Successfully implements the proposed dual-stream architecture with Fast and Slow processing paths\n- Clean separation of concerns between components\n- Well-structured initialization and forward pass logic\n\n2. **Code Quality**\n- Excellent docstring documentation following best practices\n- Clear variable naming and logical code organization\n- Proper use of PyTorch modules and tensor operations\n\n3. **Technical Implementation**\n- Efficient use of nn.Sequential for the gating mechanism\n- Proper handling of device and dtype through factory_kwargs\n- Good use of LayerNorm for stabilizing outputs\n\n4. **Modularity**\n- Clean separation between Fast and Slow streams\n- Modular design allows for easy modification or replacement of components\n- Well-structured inheritance from GAUBase\n\n## Areas for Improvement\n\n1. **State Management**\n- Consider adding state persistence mechanisms for the Slow Stream\n- Could benefit from explicit state initialization in __init__\n\n```python\ndef __init__(self, ...):\n    # ... existing code ...\n    self.register_buffer('hidden_state', torch.zeros(embed_dim))\n```\n\n2. **Error Handling**\n- Add input validation for dimensions and types\n- Include checks for numerical stability\n\n```python\ndef _forward(self, X, **Z):\n    if torch.isnan(X).any():\n        raise ValueError(\"Input contains NaN values\")\n    # ... rest of implementation\n```\n\n3. **Memory Efficiency**\n- Consider implementing gradient checkpointing for memory efficiency\n- Add optional memory-efficient attention variants\n\n4. **Performance Optimization**\n- Consider adding JIT compilation support\n- Implement optional fused operations for better performance\n\n## Innovation Assessment\n\n### Novel Aspects\n1. **Adaptive Gating Mechanism**\n- The implementation successfully realizes the proposed dynamic integration of Fast and Slow streams\n- The gating mechanism is efficiently implemented using nn.Sequential\n\n2. **Dual Stream Processing**\n- Clean implementation of parallel processing streams\n- Effective combination of local and global feature extraction\n\n### Potential Impact\n1. **Scalability**\n- The implementation should scale well with sequence length due to linear complexity\n- Modular design allows for easy scaling of model capacity\n\n2. **Performance**\n- The dual-stream architecture could provide better performance on both short and long-range dependencies\n- Adaptive gating mechanism should help in efficient resource utilization\n\n## Integration and Scalability Recommendations\n\n1. **Stream Synchronization**\n```python\ndef _forward(self, X, **Z):\n    # Add synchronization point\n    torch.cuda.synchronize() if X.is_cuda else None\n    H_fast, Z = self.fast_stream(X, **Z)\n    H_slow, Z = self.slow_stream(X, **Z)\n    # ... rest of implementation\n```\n\n2. **Memory Management**\n```python\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # Enable automatic mixed precision\n    # ... existing implementation\n```\n\n3. **Batch Processing Optimization**\n```python\ndef _forward(self, X, **Z):\n    # Add batch dimension handling\n    B, L, D = X.shape\n    if B > 1:\n        # Process in chunks if needed\n        chunk_size = min(B, 32)\n        results = []\n        for i in range(0, B, chunk_size):\n            chunk = X[i:i+chunk_size]\n            # Process chunk\n            # ... existing implementation\n```\n\n## Recommendations for the Coder\n\n1. **Implementation Priorities**\n- Add comprehensive unit tests covering edge cases\n- Implement memory-efficient variants for large-scale deployment\n- Add support for gradient checkpointing\n\n2. **Code Structure**\n```python\nclass HydraLMBlock(GAUBase):\n    def __init__(self, ...):\n        # Add configuration validation\n        assert embed_dim > 0, \"embed_dim must be positive\"\n        # ... rest of implementation\n\n    @torch.jit.script_method\n    def _forward(self, X, **Z):\n        # Add JIT support\n        # ... existing implementation\n```\n\n3. **Documentation**\n- Add more detailed documentation about the expected shapes and types of intermediate tensors\n- Include performance characteristics and memory requirements\n- Document any assumptions about input distributions or constraints\n\n4. **Testing**\n```python\n@gau_test\ndef test_hydralm_block(device=None, dtype=None):\n    # Add comprehensive tests\n    block = HydraLMBlock(embed_dim=256, block_loc=(0,0), \n                        kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test various sequence lengths\n    for seq_len in [128, 256, 512]:\n        X = torch.randn(2, seq_len, 256, device=device, dtype=dtype)\n        Y, Z = block(X)\n        assert not torch.isnan(Y).any(), f\"Output contains NaN values for seq_len={seq_len}\"\n        assert Y.shape == X.shape, f\"Output shape mismatch for seq_len={seq_len}\"\n```\n\n5. **Performance Optimization**\n- Profile the implementation to identify bottlenecks\n- Consider implementing custom CUDA kernels for critical operations\n- Add support for quantization and pruning\n\n## Final Notes\n\nThe implementation is strong and closely follows the proposed design. The dual-stream architecture with adaptive gating is well-implemented, and the code is clean and well-documented. The main areas for improvement are around optimization, testing, and advanced features like gradient checkpointing and quantization support.\n\nThe high rating (4.5) reflects the solid implementation of the core architecture while leaving room for optimization and advanced features. The code provides a strong foundation for further development and optimization.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_hydralm_block": "@gau_test\ndef test_HydraLMBlock_test_hydralm_block(device=None, dtype=None):\n    batch_size = 2\n    seq_len = 16\n    embed_dim = 32\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    block = HydraLMBlock(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        }, device=device, dtype=dtype)\n    Y, Z = block(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    X_long = torch.randn(batch_size, seq_len * 2, embed_dim, device=device,\n        dtype=dtype)\n    Y_long, Z = block(X_long, **Z)\n    assert Y_long.shape == X_long.shape, 'Failed to handle longer sequence'\n    print('HydraLMBlock tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HydraLMBlock(GAUBase):\n    \"\"\"\n    HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\n    \n    This block processes input sequences through two parallel streams:\n    1. Fast Stream: Captures local patterns using Toeplitz Convolution\n    2. Slow Stream: Models global dependencies using State Space Models\n    \n    The outputs of both streams are combined using an adaptive gating mechanism.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        \n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.fast_stream = FastStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.slow_stream = SlowStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.gate = nn.Sequential(nn.Linear(2 * embed_dim, embed_dim, **\n            self.factory_kwargs), nn.Sigmoid())\n        self.output_proj = nn.Linear(embed_dim, embed_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        H_fast, Z = self.fast_stream(X, **Z)\n        H_slow, Z = self.slow_stream(X, **Z)\n        H_concat = torch.cat([H_fast, H_slow], dim=-1)\n        G = self.gate(H_concat)\n        Y = G * H_fast + (1 - G) * H_slow\n        Y = self.output_proj(Y)\n        Y = self.norm(Y)\n        return Y, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"HydraLMBlock\",\"document\":\"HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\\n\\nThis block processes input sequences through two parallel streams:\\n1. Fast Stream: Captures local patterns using Toeplitz Convolution\\n2. Slow Stream: Models global dependencies using State Space Models\\n\\nThe outputs of both streams are combined using an adaptive gating mechanism.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    \\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "FastStreamUnit",
                            "SlowStreamUnit"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    }
                },
                "suggestions": "",
                "name": "hydralm"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "o1_preview",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "claude3.5_sonnet",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 0.640065,
                "IMPLEMENTATION_CODER": 0.07390500000000001,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 0.06484199999999998,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": "",
                "root": "HydraLMBlock",
                "proposal": "",
                "proposal_traces": [],
                "rating": 0,
                "declares": {
                    "DualProjection": "{\"unitname\":\"DualProjection\",\"requirements\":\"Projects SSM states to output embeddings using state space duality principles. Should transform state representations while preserving temporal dependencies.\",\"inputs\":[\"state\"],\"outputs\":[\"Y\"]}",
                    "SelectiveSSM": "{\"unitname\":\"SelectiveSSM\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "FastStreamUnit": "{\"unitname\":\"FastStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "SlowStreamUnit": "{\"unitname\":\"SlowStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "HydraLMBlock": "{\"unitname\":\"HydraLMBlock\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "units": {
                    "SelectiveSSM": {
                        "review": "# Feedback Report for SelectiveSSM Implementation\n\n## Critical Issues Analysis\n\nThe implementation has failed two major checks that need to be addressed immediately:\n\n1. **Unit Test Failure**: \n```python\nAssertionError: GAU output Y must be a sequence with the same shape as input of torch.Size([2, 10, 64]), got torch.Size([2, 10, 32]) instead\n```\nThis error occurs because the SelectiveSSM is outputting state vectors with dimension `state_dim` (32) instead of matching the input embedding dimension (64).\n\n2. **Efficiency Test Failure**:\n```\nThe model is not efficient. The training time is overly long. Its 8.06 times of the benchmark.\n```\nThe sequential processing in the forward pass is causing significant performance bottlenecks.\n\n## Detailed Solutions\n\n### 1. Shape Mismatch Fix\n\nThe SelectiveSSM needs to project its state back to the original embedding dimension. Add an output projection layer:\n\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, state_dim=None, eps=1e-06, **kwargs):\n    # ... existing initialization code ...\n    \n    # Add output projection\n    self.output_proj = nn.Linear(self.state_dim, embed_dim, **self.factory_kwargs)\n\ndef _forward(self, X, **Z):\n    # ... existing state processing code ...\n    \n    state = torch.stack(states, dim=1)\n    Z['ssm_state'] = h_prev\n    \n    # Project state back to embed_dim\n    output = self.output_proj(state)\n    return output, Z\n```\n\n### 2. Performance Optimization\n\nThe current implementation processes states sequentially, which is inefficient. Here's an optimized version using parallel computation:\n\n```python\ndef _forward(self, X, **Z):\n    B, L, _ = X.shape\n    h_prev = Z.get('ssm_state', torch.zeros(B, self.state_dim, device=X.device, dtype=X.dtype))\n    \n    # Compute all linear transformations in parallel\n    Bx = self.B(X)  # [B, L, state_dim]\n    Wx = self.W_g(X)  # [B, L, state_dim]\n    \n    # Initialize states tensor\n    states = torch.zeros(B, L, self.state_dim, device=X.device, dtype=X.dtype)\n    \n    # Parallel computation of A matrix products\n    A_powers = [self.A]\n    for i in range(L-1):\n        A_powers.append(torch.matmul(A_powers[-1], self.A))\n    A_powers = torch.stack(A_powers)  # [L, state_dim, state_dim]\n    \n    # Compute states in parallel\n    for t in range(L):\n        if t == 0:\n            h_candidate = torch.matmul(h_prev, self.A.t()) + Bx[:, 0]\n        else:\n            h_candidate = torch.matmul(states[:, t-1], self.A.t()) + Bx[:, t]\n            \n        g_t = torch.sigmoid(Wx[:, t] + self.U_g(states[:, t-1] if t > 0 else h_prev) + self.b_g)\n        states[:, t] = self.norm(g_t * h_candidate + (1 - g_t) * (states[:, t-1] if t > 0 else h_prev))\n    \n    Z['ssm_state'] = states[:, -1]\n    output = self.output_proj(states)\n    return output, Z\n```\n\n### 3. Additional Optimizations\n\n1. **Memory Efficiency**:\n```python\n# Add gradient checkpointing\nfrom torch.utils.checkpoint import checkpoint\n\ndef _forward(self, X, **Z):\n    def create_custom_forward(module):\n        def custom_forward(*inputs):\n            return module(inputs[0])\n        return custom_forward\n    \n    # Use checkpointing for heavy computations\n    Bx = checkpoint(create_custom_forward(self.B), X)\n    Wx = checkpoint(create_custom_forward(self.W_g), X)\n```\n\n2. **Numerical Stability**:\n```python\ndef _init_parameters(self):\n    # Improve initialization for better stability\n    nn.init.orthogonal_(self.A)  # Use orthogonal initialization for A\n    with torch.no_grad():\n        # Make A skew-symmetric for better stability\n        self.A.data = (self.A.data - self.A.data.t()) / 2\n```\n\n## Overall Assessment\n\n```rating 3.5```\n\n### Strengths\n1. Well-structured implementation with clear mathematical formulation\n2. Good use of selective gating mechanism for adaptive state updates\n3. Proper handling of state persistence across time steps\n4. Clean initialization and parameter management\n\n### Areas for Improvement\n1. Shape mismatch in output dimension needs fixing\n2. Sequential processing causing performance bottlenecks\n3. Memory efficiency could be improved\n4. Initialization strategy could be more robust\n\n### Innovation and Impact\n- The selective gating mechanism is innovative and allows for adaptive processing\n- Integration with the larger HydraLM architecture is well thought out\n- The state space approach provides good theoretical foundation for long-range dependencies\n\n### Recommendations\n1. Implement the suggested fixes for shape matching and performance optimization\n2. Add proper documentation for the output projection layer\n3. Consider adding skip connections for better gradient flow\n4. Implement proper unit tests that verify both functionality and performance\n5. Add assertions for shape checking throughout the forward pass\n6. Consider using torch.jit.script for performance optimization\n\n### Integration Notes\n1. Ensure state handling is consistent with the SlowStreamUnit's expectations\n2. Document the state dictionary keys and formats\n3. Consider adding a warm-up phase for the state initialization\n4. Add proper cleanup of states in the Z dictionary when needed\n\nThe implementation shows promise but needs the critical fixes outlined above to be production-ready. Focus first on fixing the shape mismatch and then on optimizing performance.",
                        "requirements": "N/A",
                        "reuse_from": "adaptiveselectivettt.StateLevel",
                        "desc": null,
                        "gautests": {
                            "test_selective_ssm": "@gau_test\ndef test_SelectiveSSM_test_selective_ssm(device=None, dtype=None):\n    \"\"\"Test SelectiveSSM implementation\"\"\"\n    embed_dim = 64\n    state_dim = 32\n    batch_size = 2\n    seq_len = 10\n    ssm = SelectiveSSM(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={},\n        device=device, dtype=dtype, state_dim=state_dim)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    state, Z = ssm(X, **Z)\n    assert state.shape == (batch_size, seq_len, state_dim\n        ), f'Expected state shape {batch_size, seq_len, state_dim}, got {state.shape}'\n    assert 'ssm_state' in Z, 'SSM state not found in intermediate variables'\n    assert Z['ssm_state'].shape == (batch_size, state_dim\n        ), f\"Expected final state shape {batch_size, state_dim}, got {Z['ssm_state'].shape}\"\n    prev_state = torch.randn(batch_size, state_dim, device=device, dtype=dtype)\n    Z = {'ssm_state': prev_state}\n    state, Z = ssm(X, **Z)\n    assert not torch.isnan(state).any(), 'Output contains NaN values'\n    assert not torch.isinf(state).any(), 'Output contains infinite values'\n    print('SelectiveSSM tests passed successfully!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveSSM(GAUBase):\n    \"\"\"\n    SelectiveSSM: Implements a selective State Space Model for capturing global dependencies.\n    \n    This unit processes input sequences using a State Space Model (SSM) with selective updates\n    based on input patterns. It maintains states across time steps and applies selective \n    updates to capture global dependencies efficiently.\n\n    **Mathematical Formulation:**\n\n    1. State Update:\n       .. math::\n          h_t = A h_{t-1} + B x_t\n          \n    2. Selective Gate:\n       .. math::\n          g_t = \\\\sigma(W_g x_t + U_g h_{t-1} + b_g)\n          \n    3. Selective Update:\n       .. math::\n          h_t = g_t \\\\odot (A h_{t-1} + B x_t) + (1 - g_t) \\\\odot h_{t-1}\n\n    where:\n        - h_t is the hidden state at time t\n        - x_t is the input at time t\n        - g_t is the selective gate\n        - A, B are learnable state transition matrices\n        - W_g, U_g are gate parameters\n        - \\\\odot denotes element-wise multiplication\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n        state_dim (int, optional): Dimension of hidden state. Default: embed_dim\n        eps (float, optional): Epsilon for numerical stability. Default: 1e-6\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary containing intermediate variables including previous state\n\n    Outputs:\n        state (Tensor): Updated state representations of shape (batch_size, seq_len, state_dim)\n        Z (dict): Updated intermediate variables\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\n        [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \n            Algorithms Through Structured State Space Duality.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, state_dim=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.state_dim = state_dim if state_dim is not None else embed_dim\n        self.eps = eps\n        self.A = nn.Parameter(torch.randn(self.state_dim, self.state_dim,\n            **self.factory_kwargs) / math.sqrt(self.state_dim))\n        self.B = nn.Linear(embed_dim, self.state_dim, bias=True, **self.\n            factory_kwargs)\n        self.W_g = nn.Linear(embed_dim, self.state_dim, bias=False, **self.\n            factory_kwargs)\n        self.U_g = nn.Linear(self.state_dim, self.state_dim, bias=False, **\n            self.factory_kwargs)\n        self.b_g = nn.Parameter(torch.zeros(self.state_dim, **self.\n            factory_kwargs))\n        self.norm = nn.LayerNorm(self.state_dim, eps=eps, **self.factory_kwargs\n            )\n        self._init_parameters()\n\n    def _init_parameters(self):\n        \"\"\"Initialize model parameters\"\"\"\n        nn.init.normal_(self.A, std=0.1)\n        with torch.no_grad():\n            self.A.data = self.A.data - self.A.data.t()\n        nn.init.xavier_uniform_(self.W_g.weight)\n        nn.init.xavier_uniform_(self.U_g.weight)\n        nn.init.zeros_(self.b_g)\n        nn.init.xavier_uniform_(self.B.weight)\n        if self.B.bias is not None:\n            nn.init.zeros_(self.B.bias)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Forward pass of SelectiveSSM.\n        \n        Args:\n            X: Input tensor of shape (batch_size, seq_len, embed_dim)\n            Z: Dictionary containing intermediate variables\n            \n        Returns:\n            state: State tensor of shape (batch_size, seq_len, state_dim)\n            Z: Updated intermediate variables\n        \"\"\"\n        batch_size, seq_len, _ = X.shape\n        h_prev = Z.get('ssm_state', torch.zeros(batch_size, self.state_dim,\n            device=X.device, dtype=X.dtype))\n        states = []\n        for t in range(seq_len):\n            x_t = X[:, t, :]\n            h_candidate = torch.matmul(h_prev, self.A.t()) + self.B(x_t)\n            g_t = torch.sigmoid(self.W_g(x_t) + self.U_g(h_prev) + self.b_g)\n            h_t = g_t * h_candidate + (1 - g_t) * h_prev\n            h_t = self.norm(h_t)\n            states.append(h_t)\n            h_prev = h_t\n        state = torch.stack(states, dim=1)\n        Z['ssm_state'] = h_prev\n        return state, Z\n",
                        "rating": 3.5,
                        "spec": "{\"unitname\":\"SelectiveSSM\",\"document\":\"SelectiveSSM: Implements a selective State Space Model for capturing global dependencies.\\n\\nThis unit processes input sequences using a State Space Model (SSM) with selective updates\\nbased on input patterns. It maintains states across time steps and applies selective \\nupdates to capture global dependencies efficiently.\\n\\n**Mathematical Formulation:**\\n\\n1. State Update:\\n   .. math::\\n      h_t = A h_{t-1} + B x_t\\n      \\n2. Selective Gate:\\n   .. math::\\n      g_t = \\\\sigma(W_g x_t + U_g h_{t-1} + b_g)\\n      \\n3. Selective Update:\\n   .. math::\\n      h_t = g_t \\\\odot (A h_{t-1} + B x_t) + (1 - g_t) \\\\odot h_{t-1}\\n\\nwhere:\\n    - h_t is the hidden state at time t\\n    - x_t is the input at time t\\n    - g_t is the selective gate\\n    - A, B are learnable state transition matrices\\n    - W_g, U_g are gate parameters\\n    - \\\\odot denotes element-wise multiplication\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n    state_dim (int, optional): Dimension of hidden state. Default: embed_dim\\n    eps (float, optional): Epsilon for numerical stability. Default: 1e-6\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary containing intermediate variables including previous state\\n\\nOutputs:\\n    state (Tensor): Updated state representations of shape (batch_size, seq_len, state_dim)\\n    Z (dict): Updated intermediate variables\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\\n    [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \\n        Algorithms Through Structured State Space Duality.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "state_dim": null,
                            "eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "FastStreamUnit": {
                        "review": "# FastStreamUnit Implementation Review\n\n```rating 4.5```\n\n## Strengths\n\n1. **Excellent Causality Implementation**\n   - Successfully implemented causal convolution with left-only padding\n   - Proper use of cumulative sums for causal attention\n   - Clear mathematical formulation in documentation\n\n2. **Efficient Architecture**\n   - Linear computational complexity maintained\n   - Effective combination of convolution and attention mechanisms\n   - Smart use of gating mechanisms for feature selection\n\n3. **Robust Implementation**\n   - Proper gradient clipping for numerical stability\n   - Well-structured parameter initialization\n   - Comprehensive error checking and assertions\n\n4. **Documentation Quality**\n   - Detailed mathematical formulations\n   - Clear component descriptions\n   - Well-documented input/output specifications\n\n## Areas for Improvement\n\n1. **Missing Unit Tests**\n   Add comprehensive unit tests:\n   ```python\n   @gau_test\n   def test_fast_stream_unit(device=None, dtype=None):\n       # Test initialization\n       model = FastStreamUnit(embed_dim=64, block_loc=(0,0), kwarg_all={}, \n                            device=device, dtype=dtype)\n       \n       # Test basic forward pass\n       x = torch.randn(2, 32, 64, device=device, dtype=dtype)\n       y, z = model(x)\n       assert y.shape == x.shape\n       \n       # Test causality\n       x_modified = x.clone()\n       x_modified[:, 16:] = torch.randn_like(x_modified[:, 16:])\n       y_new, _ = model(x_modified)\n       assert torch.allclose(y[:, :16], y_new[:, :16], atol=1e-5)\n       \n       # Test gradient flow\n       y.sum().backward()\n       assert all(p.grad is not None for p in model.parameters())\n       \n       print(\"All FastStreamUnit tests passed!\")\n   ```\n\n2. **Memory Optimization**\n   Consider adding optional memory-efficient variants:\n   ```python\n   def __init__(self, ..., use_checkpoint=False):\n       self.use_checkpoint = use_checkpoint\n       ...\n\n   def _forward(self, X, **Z):\n       if self.use_checkpoint:\n           return torch.utils.checkpoint.checkpoint(self._forward_impl, X)\n       return self._forward_impl(X)\n   ```\n\n3. **Performance Optimization**\n   Add fast attention variants:\n   ```python\n   def _forward(self, X, **Z):\n       # ... existing code ...\n       \n       if hasattr(F, 'scaled_dot_product_attention'):  # Use PyTorch 2.0 attention if available\n           attn_output = F.scaled_dot_product_attention(\n               Q_prime, K_prime, V, \n               is_causal=True,\n               scale=1.0/math.sqrt(self.head_dim)\n           )\n       else:\n           # Fallback to current implementation\n           ...\n   ```\n\n4. **Configuration Flexibility**\n   Add more configurable parameters:\n   ```python\n   def __init__(self, ..., conv_kernel_size=3, dropout_rate=0.1):\n       self.local_conv = nn.Conv1d(\n           embed_dim, embed_dim,\n           kernel_size=conv_kernel_size,\n           padding=0,\n           bias=True,\n           **self.factory_kwargs\n       )\n       self.dropout = nn.Dropout(dropout_rate)\n   ```\n\n## Innovation and Impact\n\n1. **Novel Contributions**\n   - Efficient integration of causal convolution with gated linear attention\n   - Innovative use of cumulative sums for maintaining causality\n   - Smart parameter initialization strategy\n\n2. **Potential Impact**\n   - Could serve as an efficient alternative to traditional attention mechanisms\n   - Linear complexity makes it suitable for long sequence processing\n   - Modular design allows for easy integration in various architectures\n\n3. **Scalability Considerations**\n   - Linear memory and computation complexity\n   - Parallelizable operations\n   - Efficient gradient flow through residual connections\n\n## Integration Considerations\n\n1. **Interface with SlowStreamUnit**\n   - Clean interface through state dictionary\n   - Compatible tensor shapes\n   - Proper gradient flow\n\n2. **Memory Management**\n   - Consider adding memory profiling:\n   ```python\n   def _profile_memory(self, X):\n       torch.cuda.reset_peak_memory_stats()\n       self._forward(X)\n       return torch.cuda.max_memory_allocated()\n   ```\n\n## Recommendations\n\n1. **Immediate Actions**\n   - Implement comprehensive unit tests\n   - Add memory optimization options\n   - Document memory and computation complexity\n\n2. **Future Improvements**\n   - Consider implementing Toeplitz convolution variant\n   - Add support for sparse attention patterns\n   - Implement adaptive kernel size selection\n\n3. **Documentation Updates**\n   - Add performance benchmarks\n   - Document memory requirements\n   - Add integration guidelines\n\n4. **Code Organization**\n   Consider splitting into sub-modules:\n   ```python\n   class CausalConvolution(nn.Module):\n       \"\"\"Implements causal convolution with proper padding\"\"\"\n       \n   class GatedLinearAttention(nn.Module):\n       \"\"\"Implements gated linear attention mechanism\"\"\"\n   ```\n\n## Final Notes\n\nThe implementation shows excellent attention to detail, particularly in maintaining causality and numerical stability. The combination of causal convolution and gated linear attention is innovative and well-executed. The code is production-ready but could benefit from additional testing and optimization options.\n\nThe high rating (4.5/5) reflects the strong technical implementation, clear documentation, and successful passing of functionality checks. The missing 0.5 points are due to the lack of unit tests and some potential optimization opportunities.\n\nThe implementation successfully balances efficiency, functionality, and maintainability, making it a valuable contribution to the HydraLM architecture. The causal guarantees and linear complexity make it particularly suitable for large-scale language modeling applications.",
                        "requirements": "N/A",
                        "reuse_from": "efficientselectivefastttt.EfficientSelectiveFastTTT",
                        "desc": null,
                        "gautests": {
                            "test_fast_stream_unit": "@gau_test\ndef test_FastStreamUnit_test_fast_stream_unit(device=None, dtype=None):\n    \"\"\"Test FastStreamUnit functionality and causality\"\"\"\n    embed_dim = 64\n    model = FastStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all\n        ={}, device=device, dtype=dtype)\n    batch_sizes = [1, 2]\n    seq_lengths = [16, 32]\n    for batch_size in batch_sizes:\n        for seq_len in seq_lengths:\n            X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n                dtype=dtype)\n            Y, Z = model(X)\n            assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n            assert isinstance(Z, dict), 'Z should be a dictionary'\n            assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n            assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n            assert not torch.isnan(Y).any(), 'Output contains NaN values'\n            assert not torch.isinf(Y).any(), 'Output contains infinite values'\n            X_modified = X.clone()\n            X_modified[:, seq_len // 2:] = torch.randn_like(X_modified[:, \n                seq_len // 2:])\n            Y_new, _ = model(X_modified)\n            assert torch.allclose(Y[:, :seq_len // 2], Y_new[:, :seq_len //\n                2], rtol=0.0001, atol=0.0001), 'Causality test failed'\n            if Y.requires_grad:\n                loss = Y.sum()\n                loss.backward()\n                for param in model.parameters():\n                    assert param.grad is not None, 'Gradients were not computed'\n                    assert not torch.isnan(param.grad).any(\n                        ), 'Gradients contain NaN values'\n                    assert not torch.isinf(param.grad).any(\n                        ), 'Gradients contain infinite values'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastStreamUnit(GAUBase):\n    \"\"\"\n    FastStreamUnit for HydraLM that captures local patterns using efficient convolution and gated mechanisms.\n    \n    This unit combines causal convolution with gated linear attention to efficiently process local patterns\n    in the input sequence. It maintains linear computational complexity while effectively capturing\n    short-range dependencies in a strictly causal manner.\n\n    **Core Components:**\n    \n    1. Causal Local Convolution: Uses a 1D convolution with left-only padding to ensure causality\n    2. Causal Gated Linear Attention: Processes local dependencies with linear complexity\n    3. Residual Connections: Maintains gradient flow and information preservation\n    \n    **Mathematical Formulation:**\n\n    1. Causal Local Convolution:\n       .. math::\n          X_{pad} = [pad(X_{1:t-1}), X_t]\n          X_{conv} = Conv1D(X_{pad})\n          Y_t = X_t + X_{conv,t}\n\n    2. Causal Gated Linear Attention:\n       .. math::\n          Q_t = G_Q * norm(W_Q X_t)\n          K_t = G_K * norm(W_K X_t)\n          V_t = W_V X_t\n          \n          Y_t = \\\\sum_{i=1}^t \frac{Q_t K_i^T}{\\\\sum_{j=1}^t K_j^T + \\\\epsilon} V_i\n\n    where G_Q and G_K are learned gates, and norm represents layer normalization.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n        num_attention_heads (int, optional): Number of attention heads. Default: 4\n        eps (float, optional): Epsilon for numerical stability. Default: 1e-6\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.eps = eps\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=0, bias=True, **self.factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=eps, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=eps, **self.factory_kwargs)\n        self.final_norm = nn.LayerNorm(embed_dim, eps=eps, **self.\n            factory_kwargs)\n        self._init_parameters()\n\n    def _init_parameters(self):\n        \"\"\"Initialize model parameters using Xavier uniform initialization\"\"\"\n        for module in [self.W_Q, self.W_K, self.W_V, self.output_proj, self\n            .gate_Q, self.gate_K]:\n            if hasattr(module, 'weight'):\n                nn.init.xavier_uniform_(module.weight)\n            if hasattr(module, 'bias') and module.bias is not None:\n                nn.init.zeros_(module.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='replicate')\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)\n        X = X + X_conv\n        Q = self.q_norm(self.W_Q(X))\n        K = self.k_norm(self.W_K(X))\n        V = self.W_V(X)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = torch.clamp(Q * G_Q, -10, 10)\n        K = torch.clamp(K * G_K, -10, 10)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        V_weighted = K_prime * V\n        QV_cumsum = torch.cumsum(V_weighted, dim=2)\n        attn_weights = torch.einsum('bhld,bhld->bhl', Q_prime, K_cumsum)\n        attn_output = torch.einsum('bhld,bhl->bhld', QV_cumsum, 1.0 / (\n            attn_weights + self.eps))\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output = self.final_norm(output)\n        return output, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"FastStreamUnit\",\"document\":\"FastStreamUnit for HydraLM that captures local patterns using efficient convolution and gated mechanisms.\\n\\nThis unit combines causal convolution with gated linear attention to efficiently process local patterns\\nin the input sequence. It maintains linear computational complexity while effectively capturing\\nshort-range dependencies in a strictly causal manner.\\n\\n**Core Components:**\\n\\n1. Causal Local Convolution: Uses a 1D convolution with left-only padding to ensure causality\\n2. Causal Gated Linear Attention: Processes local dependencies with linear complexity\\n3. Residual Connections: Maintains gradient flow and information preservation\\n\\n**Mathematical Formulation:**\\n\\n1. Causal Local Convolution:\\n   .. math::\\n      X_{pad} = [pad(X_{1:t-1}), X_t]\\n      X_{conv} = Conv1D(X_{pad})\\n      Y_t = X_t + X_{conv,t}\\n\\n2. Causal Gated Linear Attention:\\n   .. math::\\n      Q_t = G_Q * norm(W_Q X_t)\\n      K_t = G_K * norm(W_K X_t)\\n      V_t = W_V X_t\\n      \\n      Y_t = \\\\sum_{i=1}^t \\frac{Q_t K_i^T}{\\\\sum_{j=1}^t K_j^T + \\\\epsilon} V_i\\n\\nwhere G_Q and G_K are learned gates, and norm represents layer normalization.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4\\n    eps (float, optional): Epsilon for numerical stability. Default: 1e-6\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "num_attention_heads": 4,
                            "eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SlowStreamUnit": {
                        "review": "# Comprehensive Review of SlowStreamUnit Implementation\n\n## Overall Rating\n```rating 4.3```\n\n## Strengths\n\n1. **Well-Structured Architecture**\n   - Clear separation of concerns between SSM processing and dual projection\n   - Effective use of residual connections and normalization\n   - Clean implementation of the forward pass with logical flow\n\n2. **Comprehensive Documentation**\n   - Excellent docstring with detailed mathematical formulations\n   - Clear explanation of components and their purposes\n   - Well-referenced with relevant academic papers\n   - Good examples and usage instructions\n\n3. **Robust Implementation**\n   - Proper handling of device and dtype configurations\n   - Appropriate use of LayerNorm and dropout for stability\n   - Clean integration with the parent HydraLM architecture\n\n4. **Theoretical Soundness**\n   - Implementation aligns well with state space model theory\n   - Incorporates state space duality concepts effectively\n   - Maintains linear computational complexity\n\n## Areas for Improvement\n\n1. **State Management**\n```python\ndef _forward(self, X, **Z):\n    # Consider adding state caching\n    X_norm = self.norm(X)\n    state, Z = self.ssm(X_norm, **Z)\n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\nSuggestion: Add state caching mechanism for improved efficiency:\n```python\ndef _forward(self, X, **Z):\n    cache_key = f\"ssm_state_{self.block_loc}\"\n    X_norm = self.norm(X)\n    \n    if cache_key in Z:\n        state = Z[cache_key]\n    else:\n        state, Z = self.ssm(X_norm, **Z)\n        Z[cache_key] = state\n        \n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\n\n2. **Configuration Flexibility**\nAdd configuration options for SSM and projection parameters:\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Add configuration parameters\n    ssm_config = kwargs.get('ssm_config', {})\n    proj_config = kwargs.get('proj_config', {})\n    \n    self.ssm = SelectiveSSM(embed_dim=self.embed_dim, \n                           **ssm_config,\n                           **self.factory_kwargs)\n    self.dual_proj = DualProjection(embed_dim=self.embed_dim,\n                                   **proj_config,\n                                   **self.factory_kwargs)\n```\n\n3. **Performance Optimization**\n   - Consider adding gradient checkpointing for memory efficiency\n   - Implement parallel computation of SSM states where possible\n   - Add optional fast path for inference\n\n## Innovation and Impact\n\n### Innovative Aspects\n1. **Selective SSM Integration**\n   - Novel combination of selective state space modeling with dual projection\n   - Potential for improved efficiency in capturing long-range dependencies\n\n2. **Dual Projection Mechanism**\n   - Innovative use of state space duality for embedding projection\n   - Could lead to better representation learning\n\n### Potential Impact\n1. **Scalability**\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage through selective state processing\n\n2. **Model Performance**\n   - Improved capture of global dependencies\n   - Better handling of long-range relationships in text\n\n## Integration Considerations\n\n1. **Child Unit Dependencies**\n   - Need to implement SelectiveSSM and DualProjection units\n   - Consider providing default implementations for testing\n\n2. **Memory Management**\n   - Monitor state size growth in long sequences\n   - Implement state pruning mechanism if needed\n\n## Recommendations for the Coder\n\n1. **Implementation Priority**\n```python\n# Priority 1: Implement child units\nclass SelectiveSSM(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n\nclass DualProjection(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n```\n\n2. **Add Unit Tests**\n```python\n@gau_test\ndef test_slow_stream_unit(device=None, dtype=None):\n    # Test initialization\n    unit = SlowStreamUnit(embed_dim=512, block_loc=(0,0), \n                         kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test forward pass\n    batch_size, seq_len = 2, 128\n    X = torch.randn(batch_size, seq_len, 512, \n                   device=device, dtype=dtype)\n    Y, Z = unit(X)\n    \n    # Assert output shape\n    assert Y.shape == X.shape\n    \n    # Test state persistence\n    Y2, Z = unit(X, **Z)\n    assert Y2.shape == X.shape\n```\n\n3. **Documentation Updates**\n   - Add performance characteristics and memory usage patterns\n   - Document expected behavior with different sequence lengths\n   - Provide examples of configuration options\n\n4. **Optimization Opportunities**\n   - Implement parallel state computation\n   - Add gradient checkpointing option\n   - Consider sparse state updates\n\n## Additional Suggestions\n\n1. **Error Handling**\n```python\ndef _forward(self, X, **Z):\n    if X.dim() != 3:\n        raise ValueError(f\"Expected 3D input, got {X.dim()}D\")\n    if X.size(-1) != self.embed_dim:\n        raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {X.size(-1)}\")\n    # ... rest of implementation\n```\n\n2. **Configuration Validation**\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Validate configurations\n    if embed_dim <= 0:\n        raise ValueError(f\"embed_dim must be positive, got {embed_dim}\")\n    if kwargs.get('dropout', 0.1) < 0 or kwargs.get('dropout', 0.1) > 1:\n        raise ValueError(\"dropout must be between 0 and 1\")\n```\n\n3. **Performance Monitoring**\n   - Add hooks for tracking state size and computation time\n   - Implement optional logging of performance metrics\n\nThe implementation shows promise in advancing language model capabilities through efficient handling of global dependencies. Focus on implementing the child units and adding comprehensive tests to ensure robust performance across different scenarios.",
                        "requirements": "N/A",
                        "reuse_from": "dualstategpt.DualStateProcessor",
                        "desc": null,
                        "gautests": {
                            "test_slow_stream_unit": "@gau_test\ndef test_SlowStreamUnit_test_slow_stream_unit(device=None, dtype=None):\n    \"\"\"Unit test for SlowStreamUnit\"\"\"\n    batch_size = 2\n    seq_len = 128\n    embed_dim = 256\n    unit = SlowStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all=\n        {'dropout': 0.1}, device=device, dtype=dtype)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = unit(X, **Z)\n    assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n    assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n    assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    for seq_len in [64, 256]:\n        X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n            dtype=dtype)\n        Y, Z = unit(X, **Z)\n        assert Y.shape == X.shape, f'Failed for sequence length {seq_len}'\n    print('All tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nimport math\n\n\nclass SlowStreamUnit(GAUBase):\n    \"\"\"\n    SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\n\n    This unit processes input sequences through a selective State Space Model (SSM) and applies\n    state space duality via a dual projection to capture long-range dependencies efficiently.\n    It serves as the \"Slow Stream\" in HydraLM's dual-stream architecture.\n\n    **Core Components:**\n\n    - `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\n    - `DualProjection`: Projects SSM states to output embeddings using state space duality\n\n    **Mathematical Formulation:**\n\n    1. State Space Model Processing:\n       .. math::\n          h_t = A h_{t-1} + B x_t\n          y_t = C h_t + D x_t\n\n       where h_t is the hidden state, x_t is input, and y_t is output at time t.\n       A, B, C, D are learnable parameters.\n\n    2. Dual Projection:\n       .. math::\n          Y = \text{DualProj}(h) + X\n\n       where h represents the SSM states and X is the residual connection.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n\n    Example:\n        slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        y, z = slow_stream(x)\n\n    Note:\n        This unit is designed to work efficiently with long sequences by maintaining\n        linear computational complexity through the use of SSMs.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\n        [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \n            Algorithms Through Structured State Space Duality.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ssm = SelectiveSSM(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dual_proj = DualProjection(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n\n    def _forward(self, X, **Z):\n        X_norm = self.norm(X)\n        state, Z = self.ssm(X_norm, **Z)\n        Y, Z = self.dual_proj(state, **Z)\n        Y = self.dropout(Y) + X\n        return Y, Z\n",
                        "rating": 4.3,
                        "spec": "{\"unitname\":\"SlowStreamUnit\",\"document\":\"SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\\n\\nThis unit processes input sequences through a selective State Space Model (SSM) and applies\\nstate space duality via a dual projection to capture long-range dependencies efficiently.\\nIt serves as the \\\"Slow Stream\\\" in HydraLM's dual-stream architecture.\\n\\n**Core Components:**\\n\\n- `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\\n- `DualProjection`: Projects SSM states to output embeddings using state space duality\\n\\n**Mathematical Formulation:**\\n\\n1. State Space Model Processing:\\n   .. math::\\n      h_t = A h_{t-1} + B x_t\\n      y_t = C h_t + D x_t\\n\\n   where h_t is the hidden state, x_t is input, and y_t is output at time t.\\n   A, B, C, D are learnable parameters.\\n\\n2. Dual Projection:\\n   .. math::\\n      Y =   ext{DualProj}(h) + X\\n\\n   where h represents the SSM states and X is the residual connection.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\\n\\nExample:\\n    slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    y, z = slow_stream(x)\\n\\nNote:\\n    This unit is designed to work efficiently with long sequences by maintaining\\n    linear computational complexity through the use of SSMs.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\\n    [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \\n        Algorithms Through Structured State Space Duality.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "SelectiveSSM",
                            "DualProjection"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "HydraLMBlock": {
                        "review": "# Comprehensive Review of HydraLMBlock Implementation\n\n## Overall Rating\n```rating 4.5```\n\n## Strengths\n\n1. **Clear Architecture Implementation**\n- Successfully implements the proposed dual-stream architecture with Fast and Slow processing paths\n- Clean separation of concerns between components\n- Well-structured initialization and forward pass logic\n\n2. **Code Quality**\n- Excellent docstring documentation following best practices\n- Clear variable naming and logical code organization\n- Proper use of PyTorch modules and tensor operations\n\n3. **Technical Implementation**\n- Efficient use of nn.Sequential for the gating mechanism\n- Proper handling of device and dtype through factory_kwargs\n- Good use of LayerNorm for stabilizing outputs\n\n4. **Modularity**\n- Clean separation between Fast and Slow streams\n- Modular design allows for easy modification or replacement of components\n- Well-structured inheritance from GAUBase\n\n## Areas for Improvement\n\n1. **State Management**\n- Consider adding state persistence mechanisms for the Slow Stream\n- Could benefit from explicit state initialization in __init__\n\n```python\ndef __init__(self, ...):\n    # ... existing code ...\n    self.register_buffer('hidden_state', torch.zeros(embed_dim))\n```\n\n2. **Error Handling**\n- Add input validation for dimensions and types\n- Include checks for numerical stability\n\n```python\ndef _forward(self, X, **Z):\n    if torch.isnan(X).any():\n        raise ValueError(\"Input contains NaN values\")\n    # ... rest of implementation\n```\n\n3. **Memory Efficiency**\n- Consider implementing gradient checkpointing for memory efficiency\n- Add optional memory-efficient attention variants\n\n4. **Performance Optimization**\n- Consider adding JIT compilation support\n- Implement optional fused operations for better performance\n\n## Innovation Assessment\n\n### Novel Aspects\n1. **Adaptive Gating Mechanism**\n- The implementation successfully realizes the proposed dynamic integration of Fast and Slow streams\n- The gating mechanism is efficiently implemented using nn.Sequential\n\n2. **Dual Stream Processing**\n- Clean implementation of parallel processing streams\n- Effective combination of local and global feature extraction\n\n### Potential Impact\n1. **Scalability**\n- The implementation should scale well with sequence length due to linear complexity\n- Modular design allows for easy scaling of model capacity\n\n2. **Performance**\n- The dual-stream architecture could provide better performance on both short and long-range dependencies\n- Adaptive gating mechanism should help in efficient resource utilization\n\n## Integration and Scalability Recommendations\n\n1. **Stream Synchronization**\n```python\ndef _forward(self, X, **Z):\n    # Add synchronization point\n    torch.cuda.synchronize() if X.is_cuda else None\n    H_fast, Z = self.fast_stream(X, **Z)\n    H_slow, Z = self.slow_stream(X, **Z)\n    # ... rest of implementation\n```\n\n2. **Memory Management**\n```python\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # Enable automatic mixed precision\n    # ... existing implementation\n```\n\n3. **Batch Processing Optimization**\n```python\ndef _forward(self, X, **Z):\n    # Add batch dimension handling\n    B, L, D = X.shape\n    if B > 1:\n        # Process in chunks if needed\n        chunk_size = min(B, 32)\n        results = []\n        for i in range(0, B, chunk_size):\n            chunk = X[i:i+chunk_size]\n            # Process chunk\n            # ... existing implementation\n```\n\n## Recommendations for the Coder\n\n1. **Implementation Priorities**\n- Add comprehensive unit tests covering edge cases\n- Implement memory-efficient variants for large-scale deployment\n- Add support for gradient checkpointing\n\n2. **Code Structure**\n```python\nclass HydraLMBlock(GAUBase):\n    def __init__(self, ...):\n        # Add configuration validation\n        assert embed_dim > 0, \"embed_dim must be positive\"\n        # ... rest of implementation\n\n    @torch.jit.script_method\n    def _forward(self, X, **Z):\n        # Add JIT support\n        # ... existing implementation\n```\n\n3. **Documentation**\n- Add more detailed documentation about the expected shapes and types of intermediate tensors\n- Include performance characteristics and memory requirements\n- Document any assumptions about input distributions or constraints\n\n4. **Testing**\n```python\n@gau_test\ndef test_hydralm_block(device=None, dtype=None):\n    # Add comprehensive tests\n    block = HydraLMBlock(embed_dim=256, block_loc=(0,0), \n                        kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test various sequence lengths\n    for seq_len in [128, 256, 512]:\n        X = torch.randn(2, seq_len, 256, device=device, dtype=dtype)\n        Y, Z = block(X)\n        assert not torch.isnan(Y).any(), f\"Output contains NaN values for seq_len={seq_len}\"\n        assert Y.shape == X.shape, f\"Output shape mismatch for seq_len={seq_len}\"\n```\n\n5. **Performance Optimization**\n- Profile the implementation to identify bottlenecks\n- Consider implementing custom CUDA kernels for critical operations\n- Add support for quantization and pruning\n\n## Final Notes\n\nThe implementation is strong and closely follows the proposed design. The dual-stream architecture with adaptive gating is well-implemented, and the code is clean and well-documented. The main areas for improvement are around optimization, testing, and advanced features like gradient checkpointing and quantization support.\n\nThe high rating (4.5) reflects the solid implementation of the core architecture while leaving room for optimization and advanced features. The code provides a strong foundation for further development and optimization.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_hydralm_block": "@gau_test\ndef test_HydraLMBlock_test_hydralm_block(device=None, dtype=None):\n    batch_size = 2\n    seq_len = 16\n    embed_dim = 32\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    block = HydraLMBlock(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        }, device=device, dtype=dtype)\n    Y, Z = block(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    X_long = torch.randn(batch_size, seq_len * 2, embed_dim, device=device,\n        dtype=dtype)\n    Y_long, Z = block(X_long, **Z)\n    assert Y_long.shape == X_long.shape, 'Failed to handle longer sequence'\n    print('HydraLMBlock tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HydraLMBlock(GAUBase):\n    \"\"\"\n    HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\n    \n    This block processes input sequences through two parallel streams:\n    1. Fast Stream: Captures local patterns using Toeplitz Convolution\n    2. Slow Stream: Models global dependencies using State Space Models\n    \n    The outputs of both streams are combined using an adaptive gating mechanism.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        \n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.fast_stream = FastStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.slow_stream = SlowStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.gate = nn.Sequential(nn.Linear(2 * embed_dim, embed_dim, **\n            self.factory_kwargs), nn.Sigmoid())\n        self.output_proj = nn.Linear(embed_dim, embed_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        H_fast, Z = self.fast_stream(X, **Z)\n        H_slow, Z = self.slow_stream(X, **Z)\n        H_concat = torch.cat([H_fast, H_slow], dim=-1)\n        G = self.gate(H_concat)\n        Y = G * H_fast + (1 - G) * H_slow\n        Y = self.output_proj(Y)\n        Y = self.norm(Y)\n        return Y, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"HydraLMBlock\",\"document\":\"HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\\n\\nThis block processes input sequences through two parallel streams:\\n1. Fast Stream: Captures local patterns using Toeplitz Convolution\\n2. Slow Stream: Models global dependencies using State Space Models\\n\\nThe outputs of both streams are combined using an adaptive gating mechanism.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    \\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "FastStreamUnit",
                            "SlowStreamUnit"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    }
                },
                "suggestions": "",
                "name": "hydralm"
            },
            "user_input": "",
            "status": "unfinished",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "o1_preview",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "claude3.5_sonnet",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 5.182575,
                "IMPLEMENTATION_CODER": 3.985098,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 5.965682999999999,
                "SEARCH_ASSISTANT": 0
            }
        },
        {
            "tree": {
                "review": "",
                "root": "HydraLMBlock",
                "proposal": "",
                "units": {
                    "FastStreamUnit": {
                        "review": "# FastStreamUnit Implementation Review\n\n```rating 4.5```\n\n## Strengths\n\n1. **Excellent Causality Implementation**\n   - Successfully implemented causal convolution with left-only padding\n   - Proper use of cumulative sums for causal attention\n   - Clear mathematical formulation in documentation\n\n2. **Efficient Architecture**\n   - Linear computational complexity maintained\n   - Effective combination of convolution and attention mechanisms\n   - Smart use of gating mechanisms for feature selection\n\n3. **Robust Implementation**\n   - Proper gradient clipping for numerical stability\n   - Well-structured parameter initialization\n   - Comprehensive error checking and assertions\n\n4. **Documentation Quality**\n   - Detailed mathematical formulations\n   - Clear component descriptions\n   - Well-documented input/output specifications\n\n## Areas for Improvement\n\n1. **Missing Unit Tests**\n   Add comprehensive unit tests:\n   ```python\n   @gau_test\n   def test_fast_stream_unit(device=None, dtype=None):\n       # Test initialization\n       model = FastStreamUnit(embed_dim=64, block_loc=(0,0), kwarg_all={}, \n                            device=device, dtype=dtype)\n       \n       # Test basic forward pass\n       x = torch.randn(2, 32, 64, device=device, dtype=dtype)\n       y, z = model(x)\n       assert y.shape == x.shape\n       \n       # Test causality\n       x_modified = x.clone()\n       x_modified[:, 16:] = torch.randn_like(x_modified[:, 16:])\n       y_new, _ = model(x_modified)\n       assert torch.allclose(y[:, :16], y_new[:, :16], atol=1e-5)\n       \n       # Test gradient flow\n       y.sum().backward()\n       assert all(p.grad is not None for p in model.parameters())\n       \n       print(\"All FastStreamUnit tests passed!\")\n   ```\n\n2. **Memory Optimization**\n   Consider adding optional memory-efficient variants:\n   ```python\n   def __init__(self, ..., use_checkpoint=False):\n       self.use_checkpoint = use_checkpoint\n       ...\n\n   def _forward(self, X, **Z):\n       if self.use_checkpoint:\n           return torch.utils.checkpoint.checkpoint(self._forward_impl, X)\n       return self._forward_impl(X)\n   ```\n\n3. **Performance Optimization**\n   Add fast attention variants:\n   ```python\n   def _forward(self, X, **Z):\n       # ... existing code ...\n       \n       if hasattr(F, 'scaled_dot_product_attention'):  # Use PyTorch 2.0 attention if available\n           attn_output = F.scaled_dot_product_attention(\n               Q_prime, K_prime, V, \n               is_causal=True,\n               scale=1.0/math.sqrt(self.head_dim)\n           )\n       else:\n           # Fallback to current implementation\n           ...\n   ```\n\n4. **Configuration Flexibility**\n   Add more configurable parameters:\n   ```python\n   def __init__(self, ..., conv_kernel_size=3, dropout_rate=0.1):\n       self.local_conv = nn.Conv1d(\n           embed_dim, embed_dim,\n           kernel_size=conv_kernel_size,\n           padding=0,\n           bias=True,\n           **self.factory_kwargs\n       )\n       self.dropout = nn.Dropout(dropout_rate)\n   ```\n\n## Innovation and Impact\n\n1. **Novel Contributions**\n   - Efficient integration of causal convolution with gated linear attention\n   - Innovative use of cumulative sums for maintaining causality\n   - Smart parameter initialization strategy\n\n2. **Potential Impact**\n   - Could serve as an efficient alternative to traditional attention mechanisms\n   - Linear complexity makes it suitable for long sequence processing\n   - Modular design allows for easy integration in various architectures\n\n3. **Scalability Considerations**\n   - Linear memory and computation complexity\n   - Parallelizable operations\n   - Efficient gradient flow through residual connections\n\n## Integration Considerations\n\n1. **Interface with SlowStreamUnit**\n   - Clean interface through state dictionary\n   - Compatible tensor shapes\n   - Proper gradient flow\n\n2. **Memory Management**\n   - Consider adding memory profiling:\n   ```python\n   def _profile_memory(self, X):\n       torch.cuda.reset_peak_memory_stats()\n       self._forward(X)\n       return torch.cuda.max_memory_allocated()\n   ```\n\n## Recommendations\n\n1. **Immediate Actions**\n   - Implement comprehensive unit tests\n   - Add memory optimization options\n   - Document memory and computation complexity\n\n2. **Future Improvements**\n   - Consider implementing Toeplitz convolution variant\n   - Add support for sparse attention patterns\n   - Implement adaptive kernel size selection\n\n3. **Documentation Updates**\n   - Add performance benchmarks\n   - Document memory requirements\n   - Add integration guidelines\n\n4. **Code Organization**\n   Consider splitting into sub-modules:\n   ```python\n   class CausalConvolution(nn.Module):\n       \"\"\"Implements causal convolution with proper padding\"\"\"\n       \n   class GatedLinearAttention(nn.Module):\n       \"\"\"Implements gated linear attention mechanism\"\"\"\n   ```\n\n## Final Notes\n\nThe implementation shows excellent attention to detail, particularly in maintaining causality and numerical stability. The combination of causal convolution and gated linear attention is innovative and well-executed. The code is production-ready but could benefit from additional testing and optimization options.\n\nThe high rating (4.5/5) reflects the strong technical implementation, clear documentation, and successful passing of functionality checks. The missing 0.5 points are due to the lack of unit tests and some potential optimization opportunities.\n\nThe implementation successfully balances efficiency, functionality, and maintainability, making it a valuable contribution to the HydraLM architecture. The causal guarantees and linear complexity make it particularly suitable for large-scale language modeling applications.",
                        "requirements": "N/A",
                        "reuse_from": "efficientselectivefastttt.EfficientSelectiveFastTTT",
                        "desc": null,
                        "gautests": {
                            "test_fast_stream_unit": "@gau_test\ndef test_FastStreamUnit_test_fast_stream_unit(device=None, dtype=None):\n    \"\"\"Test FastStreamUnit functionality and causality\"\"\"\n    embed_dim = 64\n    model = FastStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all\n        ={}, device=device, dtype=dtype)\n    batch_sizes = [1, 2]\n    seq_lengths = [16, 32]\n    for batch_size in batch_sizes:\n        for seq_len in seq_lengths:\n            X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n                dtype=dtype)\n            Y, Z = model(X)\n            assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n            assert isinstance(Z, dict), 'Z should be a dictionary'\n            assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n            assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n            assert not torch.isnan(Y).any(), 'Output contains NaN values'\n            assert not torch.isinf(Y).any(), 'Output contains infinite values'\n            X_modified = X.clone()\n            X_modified[:, seq_len // 2:] = torch.randn_like(X_modified[:, \n                seq_len // 2:])\n            Y_new, _ = model(X_modified)\n            assert torch.allclose(Y[:, :seq_len // 2], Y_new[:, :seq_len //\n                2], rtol=0.0001, atol=0.0001), 'Causality test failed'\n            if Y.requires_grad:\n                loss = Y.sum()\n                loss.backward()\n                for param in model.parameters():\n                    assert param.grad is not None, 'Gradients were not computed'\n                    assert not torch.isnan(param.grad).any(\n                        ), 'Gradients contain NaN values'\n                    assert not torch.isinf(param.grad).any(\n                        ), 'Gradients contain infinite values'\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass FastStreamUnit(GAUBase):\n    \"\"\"\n    FastStreamUnit for HydraLM that captures local patterns using efficient convolution and gated mechanisms.\n    \n    This unit combines causal convolution with gated linear attention to efficiently process local patterns\n    in the input sequence. It maintains linear computational complexity while effectively capturing\n    short-range dependencies in a strictly causal manner.\n\n    **Core Components:**\n    \n    1. Causal Local Convolution: Uses a 1D convolution with left-only padding to ensure causality\n    2. Causal Gated Linear Attention: Processes local dependencies with linear complexity\n    3. Residual Connections: Maintains gradient flow and information preservation\n    \n    **Mathematical Formulation:**\n\n    1. Causal Local Convolution:\n       .. math::\n          X_{pad} = [pad(X_{1:t-1}), X_t]\n          X_{conv} = Conv1D(X_{pad})\n          Y_t = X_t + X_{conv,t}\n\n    2. Causal Gated Linear Attention:\n       .. math::\n          Q_t = G_Q * norm(W_Q X_t)\n          K_t = G_K * norm(W_K X_t)\n          V_t = W_V X_t\n          \n          Y_t = \\\\sum_{i=1}^t \frac{Q_t K_i^T}{\\\\sum_{j=1}^t K_j^T + \\\\epsilon} V_i\n\n    where G_Q and G_K are learned gates, and norm represents layer normalization.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n        num_attention_heads (int, optional): Number of attention heads. Default: 4\n        eps (float, optional): Epsilon for numerical stability. Default: 1e-6\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_attention_heads=4, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.num_heads = num_attention_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_attention_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.eps = eps\n        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False, **self.\n            factory_kwargs)\n        self.gate_Q = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.gate_K = nn.Linear(embed_dim, embed_dim, bias=True, **self.\n            factory_kwargs)\n        self.local_conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=3,\n            padding=0, bias=True, **self.factory_kwargs)\n        self.output_proj = nn.Linear(embed_dim, embed_dim, bias=False, **\n            self.factory_kwargs)\n        self.q_norm = nn.LayerNorm(embed_dim, eps=eps, **self.factory_kwargs)\n        self.k_norm = nn.LayerNorm(embed_dim, eps=eps, **self.factory_kwargs)\n        self.final_norm = nn.LayerNorm(embed_dim, eps=eps, **self.\n            factory_kwargs)\n        self._init_parameters()\n\n    def _init_parameters(self):\n        \"\"\"Initialize model parameters using Xavier uniform initialization\"\"\"\n        for module in [self.W_Q, self.W_K, self.W_V, self.output_proj, self\n            .gate_Q, self.gate_K]:\n            if hasattr(module, 'weight'):\n                nn.init.xavier_uniform_(module.weight)\n            if hasattr(module, 'bias') and module.bias is not None:\n                nn.init.zeros_(module.bias)\n        nn.init.xavier_uniform_(self.local_conv.weight)\n        nn.init.zeros_(self.local_conv.bias)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.size()\n        H = self.num_heads\n        D_H = self.head_dim\n        X_pad = F.pad(X.transpose(1, 2), (2, 0), mode='replicate')\n        X_conv = self.local_conv(X_pad)\n        X_conv = X_conv.transpose(1, 2)\n        X = X + X_conv\n        Q = self.q_norm(self.W_Q(X))\n        K = self.k_norm(self.W_K(X))\n        V = self.W_V(X)\n        G_Q = torch.sigmoid(self.gate_Q(X))\n        G_K = torch.sigmoid(self.gate_K(X))\n        Q = torch.clamp(Q * G_Q, -10, 10)\n        K = torch.clamp(K * G_K, -10, 10)\n        Q = Q.view(B, L, H, D_H).transpose(1, 2)\n        K = K.view(B, L, H, D_H).transpose(1, 2)\n        V = V.view(B, L, H, D_H).transpose(1, 2)\n        Q_prime = F.elu(Q) + 1\n        K_prime = F.elu(K) + 1\n        K_cumsum = torch.cumsum(K_prime, dim=2)\n        V_weighted = K_prime * V\n        QV_cumsum = torch.cumsum(V_weighted, dim=2)\n        attn_weights = torch.einsum('bhld,bhld->bhl', Q_prime, K_cumsum)\n        attn_output = torch.einsum('bhld,bhl->bhld', QV_cumsum, 1.0 / (\n            attn_weights + self.eps))\n        output = attn_output.transpose(1, 2).contiguous().view(B, L, D)\n        output = self.output_proj(output)\n        output = X + output\n        output = self.final_norm(output)\n        return output, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"FastStreamUnit\",\"document\":\"FastStreamUnit for HydraLM that captures local patterns using efficient convolution and gated mechanisms.\\n\\nThis unit combines causal convolution with gated linear attention to efficiently process local patterns\\nin the input sequence. It maintains linear computational complexity while effectively capturing\\nshort-range dependencies in a strictly causal manner.\\n\\n**Core Components:**\\n\\n1. Causal Local Convolution: Uses a 1D convolution with left-only padding to ensure causality\\n2. Causal Gated Linear Attention: Processes local dependencies with linear complexity\\n3. Residual Connections: Maintains gradient flow and information preservation\\n\\n**Mathematical Formulation:**\\n\\n1. Causal Local Convolution:\\n   .. math::\\n      X_{pad} = [pad(X_{1:t-1}), X_t]\\n      X_{conv} = Conv1D(X_{pad})\\n      Y_t = X_t + X_{conv,t}\\n\\n2. Causal Gated Linear Attention:\\n   .. math::\\n      Q_t = G_Q * norm(W_Q X_t)\\n      K_t = G_K * norm(W_K X_t)\\n      V_t = W_V X_t\\n      \\n      Y_t = \\\\sum_{i=1}^t \\frac{Q_t K_i^T}{\\\\sum_{j=1}^t K_j^T + \\\\epsilon} V_i\\n\\nwhere G_Q and G_K are learned gates, and norm represents layer normalization.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n    num_attention_heads (int, optional): Number of attention heads. Default: 4\\n    eps (float, optional): Epsilon for numerical stability. Default: 1e-6\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "num_attention_heads": 4,
                            "eps": 1e-06
                        },
                        "design_traces": null
                    },
                    "SlowStreamUnit": {
                        "review": "# Comprehensive Review of SlowStreamUnit Implementation\n\n## Overall Rating\n```rating 4.3```\n\n## Strengths\n\n1. **Well-Structured Architecture**\n   - Clear separation of concerns between SSM processing and dual projection\n   - Effective use of residual connections and normalization\n   - Clean implementation of the forward pass with logical flow\n\n2. **Comprehensive Documentation**\n   - Excellent docstring with detailed mathematical formulations\n   - Clear explanation of components and their purposes\n   - Well-referenced with relevant academic papers\n   - Good examples and usage instructions\n\n3. **Robust Implementation**\n   - Proper handling of device and dtype configurations\n   - Appropriate use of LayerNorm and dropout for stability\n   - Clean integration with the parent HydraLM architecture\n\n4. **Theoretical Soundness**\n   - Implementation aligns well with state space model theory\n   - Incorporates state space duality concepts effectively\n   - Maintains linear computational complexity\n\n## Areas for Improvement\n\n1. **State Management**\n```python\ndef _forward(self, X, **Z):\n    # Consider adding state caching\n    X_norm = self.norm(X)\n    state, Z = self.ssm(X_norm, **Z)\n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\nSuggestion: Add state caching mechanism for improved efficiency:\n```python\ndef _forward(self, X, **Z):\n    cache_key = f\"ssm_state_{self.block_loc}\"\n    X_norm = self.norm(X)\n    \n    if cache_key in Z:\n        state = Z[cache_key]\n    else:\n        state, Z = self.ssm(X_norm, **Z)\n        Z[cache_key] = state\n        \n    Y, Z = self.dual_proj(state, **Z)\n    Y = self.dropout(Y) + X\n    return Y, Z\n```\n\n2. **Configuration Flexibility**\nAdd configuration options for SSM and projection parameters:\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Add configuration parameters\n    ssm_config = kwargs.get('ssm_config', {})\n    proj_config = kwargs.get('proj_config', {})\n    \n    self.ssm = SelectiveSSM(embed_dim=self.embed_dim, \n                           **ssm_config,\n                           **self.factory_kwargs)\n    self.dual_proj = DualProjection(embed_dim=self.embed_dim,\n                                   **proj_config,\n                                   **self.factory_kwargs)\n```\n\n3. **Performance Optimization**\n   - Consider adding gradient checkpointing for memory efficiency\n   - Implement parallel computation of SSM states where possible\n   - Add optional fast path for inference\n\n## Innovation and Impact\n\n### Innovative Aspects\n1. **Selective SSM Integration**\n   - Novel combination of selective state space modeling with dual projection\n   - Potential for improved efficiency in capturing long-range dependencies\n\n2. **Dual Projection Mechanism**\n   - Innovative use of state space duality for embedding projection\n   - Could lead to better representation learning\n\n### Potential Impact\n1. **Scalability**\n   - Linear complexity enables processing of longer sequences\n   - Efficient memory usage through selective state processing\n\n2. **Model Performance**\n   - Improved capture of global dependencies\n   - Better handling of long-range relationships in text\n\n## Integration Considerations\n\n1. **Child Unit Dependencies**\n   - Need to implement SelectiveSSM and DualProjection units\n   - Consider providing default implementations for testing\n\n2. **Memory Management**\n   - Monitor state size growth in long sequences\n   - Implement state pruning mechanism if needed\n\n## Recommendations for the Coder\n\n1. **Implementation Priority**\n```python\n# Priority 1: Implement child units\nclass SelectiveSSM(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n\nclass DualProjection(GAUBase):\n    def __init__(self, embed_dim, **kwargs):\n        # Implementation needed\n        pass\n```\n\n2. **Add Unit Tests**\n```python\n@gau_test\ndef test_slow_stream_unit(device=None, dtype=None):\n    # Test initialization\n    unit = SlowStreamUnit(embed_dim=512, block_loc=(0,0), \n                         kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test forward pass\n    batch_size, seq_len = 2, 128\n    X = torch.randn(batch_size, seq_len, 512, \n                   device=device, dtype=dtype)\n    Y, Z = unit(X)\n    \n    # Assert output shape\n    assert Y.shape == X.shape\n    \n    # Test state persistence\n    Y2, Z = unit(X, **Z)\n    assert Y2.shape == X.shape\n```\n\n3. **Documentation Updates**\n   - Add performance characteristics and memory usage patterns\n   - Document expected behavior with different sequence lengths\n   - Provide examples of configuration options\n\n4. **Optimization Opportunities**\n   - Implement parallel state computation\n   - Add gradient checkpointing option\n   - Consider sparse state updates\n\n## Additional Suggestions\n\n1. **Error Handling**\n```python\ndef _forward(self, X, **Z):\n    if X.dim() != 3:\n        raise ValueError(f\"Expected 3D input, got {X.dim()}D\")\n    if X.size(-1) != self.embed_dim:\n        raise ValueError(f\"Expected embed_dim={self.embed_dim}, got {X.size(-1)}\")\n    # ... rest of implementation\n```\n\n2. **Configuration Validation**\n```python\ndef __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n    device=None, dtype=None, **kwargs):\n    # Validate configurations\n    if embed_dim <= 0:\n        raise ValueError(f\"embed_dim must be positive, got {embed_dim}\")\n    if kwargs.get('dropout', 0.1) < 0 or kwargs.get('dropout', 0.1) > 1:\n        raise ValueError(\"dropout must be between 0 and 1\")\n```\n\n3. **Performance Monitoring**\n   - Add hooks for tracking state size and computation time\n   - Implement optional logging of performance metrics\n\nThe implementation shows promise in advancing language model capabilities through efficient handling of global dependencies. Focus on implementing the child units and adding comprehensive tests to ensure robust performance across different scenarios.",
                        "requirements": "N/A",
                        "reuse_from": "dualstategpt.DualStateProcessor",
                        "desc": null,
                        "gautests": {
                            "test_slow_stream_unit": "@gau_test\ndef test_SlowStreamUnit_test_slow_stream_unit(device=None, dtype=None):\n    \"\"\"Unit test for SlowStreamUnit\"\"\"\n    batch_size = 2\n    seq_len = 128\n    embed_dim = 256\n    unit = SlowStreamUnit(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all=\n        {'dropout': 0.1}, device=device, dtype=dtype)\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    Y, Z = unit(X, **Z)\n    assert Y.shape == X.shape, f\"Output shape {Y.shape} doesn't match input shape {X.shape}\"\n    assert Y.dtype == X.dtype, f\"Output dtype {Y.dtype} doesn't match input dtype {X.dtype}\"\n    assert Y.device == X.device, f\"Output device {Y.device} doesn't match input device {X.device}\"\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    for seq_len in [64, 256]:\n        X = torch.randn(batch_size, seq_len, embed_dim, device=device,\n            dtype=dtype)\n        Y, Z = unit(X, **Z)\n        assert Y.shape == X.shape, f'Failed for sequence length {seq_len}'\n    print('All tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional, Tuple\nimport math\n\n\nclass SlowStreamUnit(GAUBase):\n    \"\"\"\n    SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\n\n    This unit processes input sequences through a selective State Space Model (SSM) and applies\n    state space duality via a dual projection to capture long-range dependencies efficiently.\n    It serves as the \"Slow Stream\" in HydraLM's dual-stream architecture.\n\n    **Core Components:**\n\n    - `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\n    - `DualProjection`: Projects SSM states to output embeddings using state space duality\n\n    **Mathematical Formulation:**\n\n    1. State Space Model Processing:\n       .. math::\n          h_t = A h_{t-1} + B x_t\n          y_t = C h_t + D x_t\n\n       where h_t is the hidden state, x_t is input, and y_t is output at time t.\n       A, B, C, D are learnable parameters.\n\n    2. Dual Projection:\n       .. math::\n          Y = \text{DualProj}(h) + X\n\n       where h represents the SSM states and X is the residual connection.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Dictionary of intermediate variables\n\n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n\n    Example:\n        slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\n        x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\n        y, z = slow_stream(x)\n\n    Note:\n        This unit is designed to work efficiently with long sequences by maintaining\n        linear computational complexity through the use of SSMs.\n\n    References:\n        [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\n        [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \n            Algorithms Through Structured State Space Duality.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ssm = SelectiveSSM(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.dual_proj = DualProjection(embed_dim=self.embed_dim, block_loc\n            =self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n        self.dropout = nn.Dropout(kwargs.get('dropout', 0.1))\n\n    def _forward(self, X, **Z):\n        X_norm = self.norm(X)\n        state, Z = self.ssm(X_norm, **Z)\n        Y, Z = self.dual_proj(state, **Z)\n        Y = self.dropout(Y) + X\n        return Y, Z\n",
                        "rating": 4.3,
                        "spec": "{\"unitname\":\"SlowStreamUnit\",\"document\":\"SlowStreamUnit for HydraLM that models global dependencies using State Space Models (SSMs).\\n\\nThis unit processes input sequences through a selective State Space Model (SSM) and applies\\nstate space duality via a dual projection to capture long-range dependencies efficiently.\\nIt serves as the \\\"Slow Stream\\\" in HydraLM's dual-stream architecture.\\n\\n**Core Components:**\\n\\n- `SelectiveSSM`: Processes input through a selective State Space Model to capture global context\\n- `DualProjection`: Projects SSM states to output embeddings using state space duality\\n\\n**Mathematical Formulation:**\\n\\n1. State Space Model Processing:\\n   .. math::\\n      h_t = A h_{t-1} + B x_t\\n      y_t = C h_t + D x_t\\n\\n   where h_t is the hidden state, x_t is input, and y_t is output at time t.\\n   A, B, C, D are learnable parameters.\\n\\n2. Dual Projection:\\n   .. math::\\n      Y =   ext{DualProj}(h) + X\\n\\n   where h represents the SSM states and X is the residual connection.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Dictionary of intermediate variables\\n\\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\\n\\nExample:\\n    slow_stream = SlowStreamUnit(embed_dim=512, block_loc=(0, 0), kwarg_all={})\\n    x = torch.randn(2, 128, 512)  # (batch_size, seq_len, embed_dim)\\n    y, z = slow_stream(x)\\n\\nNote:\\n    This unit is designed to work efficiently with long sequences by maintaining\\n    linear computational complexity through the use of SSMs.\\n\\nReferences:\\n    [1] Gu, A., et al. (2021). Efficiently Modeling Long Sequences with Structured State Spaces.\\n    [2] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient \\n        Algorithms Through Structured State Space Duality.\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "SelectiveSSM",
                            "DualProjection"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "HydraLMBlock": {
                        "review": "# Comprehensive Review of HydraLMBlock Implementation\n\n## Overall Rating\n```rating 4.5```\n\n## Strengths\n\n1. **Clear Architecture Implementation**\n- Successfully implements the proposed dual-stream architecture with Fast and Slow processing paths\n- Clean separation of concerns between components\n- Well-structured initialization and forward pass logic\n\n2. **Code Quality**\n- Excellent docstring documentation following best practices\n- Clear variable naming and logical code organization\n- Proper use of PyTorch modules and tensor operations\n\n3. **Technical Implementation**\n- Efficient use of nn.Sequential for the gating mechanism\n- Proper handling of device and dtype through factory_kwargs\n- Good use of LayerNorm for stabilizing outputs\n\n4. **Modularity**\n- Clean separation between Fast and Slow streams\n- Modular design allows for easy modification or replacement of components\n- Well-structured inheritance from GAUBase\n\n## Areas for Improvement\n\n1. **State Management**\n- Consider adding state persistence mechanisms for the Slow Stream\n- Could benefit from explicit state initialization in __init__\n\n```python\ndef __init__(self, ...):\n    # ... existing code ...\n    self.register_buffer('hidden_state', torch.zeros(embed_dim))\n```\n\n2. **Error Handling**\n- Add input validation for dimensions and types\n- Include checks for numerical stability\n\n```python\ndef _forward(self, X, **Z):\n    if torch.isnan(X).any():\n        raise ValueError(\"Input contains NaN values\")\n    # ... rest of implementation\n```\n\n3. **Memory Efficiency**\n- Consider implementing gradient checkpointing for memory efficiency\n- Add optional memory-efficient attention variants\n\n4. **Performance Optimization**\n- Consider adding JIT compilation support\n- Implement optional fused operations for better performance\n\n## Innovation Assessment\n\n### Novel Aspects\n1. **Adaptive Gating Mechanism**\n- The implementation successfully realizes the proposed dynamic integration of Fast and Slow streams\n- The gating mechanism is efficiently implemented using nn.Sequential\n\n2. **Dual Stream Processing**\n- Clean implementation of parallel processing streams\n- Effective combination of local and global feature extraction\n\n### Potential Impact\n1. **Scalability**\n- The implementation should scale well with sequence length due to linear complexity\n- Modular design allows for easy scaling of model capacity\n\n2. **Performance**\n- The dual-stream architecture could provide better performance on both short and long-range dependencies\n- Adaptive gating mechanism should help in efficient resource utilization\n\n## Integration and Scalability Recommendations\n\n1. **Stream Synchronization**\n```python\ndef _forward(self, X, **Z):\n    # Add synchronization point\n    torch.cuda.synchronize() if X.is_cuda else None\n    H_fast, Z = self.fast_stream(X, **Z)\n    H_slow, Z = self.slow_stream(X, **Z)\n    # ... rest of implementation\n```\n\n2. **Memory Management**\n```python\n@torch.cuda.amp.autocast()\ndef _forward(self, X, **Z):\n    # Enable automatic mixed precision\n    # ... existing implementation\n```\n\n3. **Batch Processing Optimization**\n```python\ndef _forward(self, X, **Z):\n    # Add batch dimension handling\n    B, L, D = X.shape\n    if B > 1:\n        # Process in chunks if needed\n        chunk_size = min(B, 32)\n        results = []\n        for i in range(0, B, chunk_size):\n            chunk = X[i:i+chunk_size]\n            # Process chunk\n            # ... existing implementation\n```\n\n## Recommendations for the Coder\n\n1. **Implementation Priorities**\n- Add comprehensive unit tests covering edge cases\n- Implement memory-efficient variants for large-scale deployment\n- Add support for gradient checkpointing\n\n2. **Code Structure**\n```python\nclass HydraLMBlock(GAUBase):\n    def __init__(self, ...):\n        # Add configuration validation\n        assert embed_dim > 0, \"embed_dim must be positive\"\n        # ... rest of implementation\n\n    @torch.jit.script_method\n    def _forward(self, X, **Z):\n        # Add JIT support\n        # ... existing implementation\n```\n\n3. **Documentation**\n- Add more detailed documentation about the expected shapes and types of intermediate tensors\n- Include performance characteristics and memory requirements\n- Document any assumptions about input distributions or constraints\n\n4. **Testing**\n```python\n@gau_test\ndef test_hydralm_block(device=None, dtype=None):\n    # Add comprehensive tests\n    block = HydraLMBlock(embed_dim=256, block_loc=(0,0), \n                        kwarg_all={}, device=device, dtype=dtype)\n    \n    # Test various sequence lengths\n    for seq_len in [128, 256, 512]:\n        X = torch.randn(2, seq_len, 256, device=device, dtype=dtype)\n        Y, Z = block(X)\n        assert not torch.isnan(Y).any(), f\"Output contains NaN values for seq_len={seq_len}\"\n        assert Y.shape == X.shape, f\"Output shape mismatch for seq_len={seq_len}\"\n```\n\n5. **Performance Optimization**\n- Profile the implementation to identify bottlenecks\n- Consider implementing custom CUDA kernels for critical operations\n- Add support for quantization and pruning\n\n## Final Notes\n\nThe implementation is strong and closely follows the proposed design. The dual-stream architecture with adaptive gating is well-implemented, and the code is clean and well-documented. The main areas for improvement are around optimization, testing, and advanced features like gradient checkpointing and quantization support.\n\nThe high rating (4.5) reflects the solid implementation of the core architecture while leaving room for optimization and advanced features. The code provides a strong foundation for further development and optimization.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_hydralm_block": "@gau_test\ndef test_HydraLMBlock_test_hydralm_block(device=None, dtype=None):\n    batch_size = 2\n    seq_len = 16\n    embed_dim = 32\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=dtype)\n    Z = {}\n    block = HydraLMBlock(embed_dim=embed_dim, block_loc=(0, 0), kwarg_all={\n        }, device=device, dtype=dtype)\n    Y, Z = block(X, **Z)\n    assert Y.shape == X.shape, f'Output shape {Y.shape} does not match input shape {X.shape}'\n    assert torch.isfinite(Y).all(), 'Output contains non-finite values'\n    X_long = torch.randn(batch_size, seq_len * 2, embed_dim, device=device,\n        dtype=dtype)\n    Y_long, Z = block(X_long, **Z)\n    assert Y_long.shape == X_long.shape, 'Failed to handle longer sequence'\n    print('HydraLMBlock tests passed!')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HydraLMBlock(GAUBase):\n    \"\"\"\n    HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\n    \n    This block processes input sequences through two parallel streams:\n    1. Fast Stream: Captures local patterns using Toeplitz Convolution\n    2. Slow Stream: Models global dependencies using State Space Models\n    \n    The outputs of both streams are combined using an adaptive gating mechanism.\n\n    Args:\n        embed_dim (int): Dimension of input embeddings\n        block_loc (tuple): Location of block in network as (layer_idx, n_block)\n        kwarg_all (dict): Additional arguments passed to child units\n        device: Device to place tensors on\n        dtype: Data type of tensors\n\n    Inputs:\n        X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\n        \n    Outputs:\n        Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\n        Z (dict): Updated intermediate variables\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.fast_stream = FastStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.slow_stream = SlowStreamUnit(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **\n            self.factory_kwargs, **self.kwarg_all)\n        self.gate = nn.Sequential(nn.Linear(2 * embed_dim, embed_dim, **\n            self.factory_kwargs), nn.Sigmoid())\n        self.output_proj = nn.Linear(embed_dim, embed_dim, **self.\n            factory_kwargs)\n        self.norm = nn.LayerNorm(embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        H_fast, Z = self.fast_stream(X, **Z)\n        H_slow, Z = self.slow_stream(X, **Z)\n        H_concat = torch.cat([H_fast, H_slow], dim=-1)\n        G = self.gate(H_concat)\n        Y = G * H_fast + (1 - G) * H_slow\n        Y = self.output_proj(Y)\n        Y = self.norm(Y)\n        return Y, Z\n",
                        "rating": 4.5,
                        "spec": "{\"unitname\":\"HydraLMBlock\",\"document\":\"HydraLM block that integrates Fast and Slow processing streams through adaptive gating mechanisms.\\n\\nThis block processes input sequences through two parallel streams:\\n1. Fast Stream: Captures local patterns using Toeplitz Convolution\\n2. Slow Stream: Models global dependencies using State Space Models\\n\\nThe outputs of both streams are combined using an adaptive gating mechanism.\\n\\nArgs:\\n    embed_dim (int): Dimension of input embeddings\\n    block_loc (tuple): Location of block in network as (layer_idx, n_block)\\n    kwarg_all (dict): Additional arguments passed to child units\\n    device: Device to place tensors on\\n    dtype: Data type of tensors\\n\\nInputs:\\n    X (Tensor): Input sequence of shape (batch_size, seq_len, embed_dim)\\n    \\nOutputs:\\n    Y (Tensor): Processed sequence of shape (batch_size, seq_len, embed_dim)\\n    Z (dict): Updated intermediate variables\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                        "children": [
                            "FastStreamUnit",
                            "SlowStreamUnit"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    }
                },
                "rating": 0,
                "declares": {
                    "SelectiveSSM": "{\"unitname\":\"SelectiveSSM\",\"requirements\":\"Implements a selective State Space Model that processes input sequences to capture global dependencies. Should maintain states across time steps and apply selective updates based on input patterns.\",\"inputs\":[\"X\"],\"outputs\":[\"state\"]}",
                    "DualProjection": "{\"unitname\":\"DualProjection\",\"requirements\":\"Projects SSM states to output embeddings using state space duality principles. Should transform state representations while preserving temporal dependencies.\",\"inputs\":[\"state\"],\"outputs\":[\"Y\"]}",
                    "FastStreamUnit": "{\"unitname\":\"FastStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "SlowStreamUnit": "{\"unitname\":\"SlowStreamUnit\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}",
                    "HydraLMBlock": "{\"unitname\":\"HydraLMBlock\",\"requirements\":\"N/A\",\"inputs\":[\"N/A\"],\"outputs\":[\"N/A\"]}"
                },
                "proposal_traces": [],
                "suggestions": "",
                "name": "hydralm"
            },
            "user_input": "",
            "status": "initial_pass",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "o1_preview",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "claude3.5_sonnet",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "claude3.5_sonnet",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0.0,
                "IMPLEMENTATION_PLANNER": 9.473385,
                "IMPLEMENTATION_CODER": 5.485373999999999,
                "PROPOSAL_REVIEWER": 0.0,
                "IMPLEMENTATION_OBSERVER": 8.297925,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}