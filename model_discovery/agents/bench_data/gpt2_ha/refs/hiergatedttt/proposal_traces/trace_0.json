{
    "variantname": "HierarchicalGatedFastTTTLinear",
    "review": "The HierarchicalGatedFastTTTLinear proposal presents an innovative approach to enhancing FastTTTLinear through hierarchical gating mechanisms and selective state tracking. After thorough analysis and comparison with existing research, here is a comprehensive review:\n\nCLARITY (4.5/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n- Well-defined objectives for improving state tracking and efficiency\n- Detailed mathematical formulations and implementation guidelines\n- Clear explanation of hierarchical gating mechanisms\n- Comprehensive pseudo-code and integration steps\n\nINNOVATION (4.2/5):\nThe proposal demonstrates significant innovation through:\n- Novel integration of hierarchical gating with bounded forget gates\n- Unique approach to selective state tracking\n- Distinguished from existing work through layer-wise information flow\nHowever:\n- Some components (like bounded forget gates) have been explored in HGRN\n- The integration strategy builds on existing techniques from Griffin\n\nFEASIBILITY (3.8/5):\nThe implementation appears feasible, supported by:\n- Detailed pseudo-code and mathematical formulations\n- Clear integration guidelines\n- Modular design approach\nConcerns include:\n- Complex interaction between multiple gating mechanisms\n- Potential challenges in maintaining test-time training capabilities\n- Need for careful parameter tuning and initialization\n\nSCALABILITY (4.0/5):\nThe design shows strong potential for scalability:\n- Hierarchical structure enables efficient processing of long sequences\n- Layer-wise information flow reduces memory requirements\n- Selective state tracking improves efficiency\nHowever:\n- Need for more detailed analysis of memory scaling with sequence length\n- Potential bottlenecks in very large models need investigation\n\nACCURACY AND ROBUSTNESS (4.2/5):\nThe proposal addresses performance considerations:\n+ Enhanced state tracking through hierarchical gating\n+ Improved information flow across layers\n+ Selective retention of relevant information\n+ Bounded forget gates for stable gradient flow\n- Need for empirical validation of robustness claims\n\nEFFICIENCY (4.0/5):\nStrong emphasis on efficiency improvements:\n+ Hierarchical structure reduces computational overhead\n+ Selective state tracking optimizes memory usage\n+ Layer-wise information flow improves gradient propagation\n- Additional overhead from multiple gating mechanisms needs quantification\n\nSTRENGTHS:\n1. Clear and comprehensive design with detailed implementation guidelines\n2. Novel integration of hierarchical gating with bounded forget gates\n3. Improved state tracking capabilities\n4. Efficient handling of long sequences\n5. Well-thought-out information flow architecture\n\nCONCERNS:\n1. Complex interaction between multiple gating mechanisms\n2. Potential training stability issues\n3. Memory scaling with sequence length\n4. Need for careful parameter tuning\n5. Impact on test-time training capabilities requires validation",
    "search_stack": [
        {
            "ready": false,
            "query": "state space models neural networks",
            "detail": "Extract technical details about state space models (SSMs) in neural networks, focusing on their architecture, advantages, limitations and comparison with attention mechanisms.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nExtract technical details about state space models (SSMs) in neural networks, focusing on their architecture, advantages, limitations and comparison with attention mechanisms.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 1.00)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 2/21 (Score: 1.00)*\n\n```\nWhile state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5\\% accuracy improvement on public benchmarks. code is avalaible at: https://github.com/ WailordHe/DenseSSM. ## 1. Introduction\n\nSince the release of ChatGPT (OpenAI, 2023), large language models have entered a new epoch, showcasing outstanding abilities in language comprehension, dialogue, and logical reasoning. Over the past year, the industry has witnessed the emergence of numerous large language models, such as LLaMA (Touvron et al., 2023) and ChatGLM (Zeng et al., 2023). These large language models have given rise to a plethora of practical applications, including conversational bots, code assistants, and AI agents. The foundation of large language models lies in the Transformer network\n\n[^0]structure (Vaswani et al., 2017), primarily utilizing a multihead self-attention module for modeling relationships between tokens and a Feed-forward network for non-linear feature transformations. The scaling law (Kaplan et al., 2020) based on the Transformer structure has propelled the continuous development and expansion of large language models. In the Transformer network, multi-head self-attention (MHSA) plays a crucial role, but it comes with significant computational demands and memory requirements during inference. In terms of computational complexity, for an input sentence of length $N$, the calculation of selfattention has a complexity of $O\\left(N^{2}\\right)$ during training and inference. Regarding memory usage, previously encountered keys and values are stored, leading to a memory occupation of $O(N D)$. As a result, recent efforts on network architectures have focused on simplifying Transformer by reducing its computation and space complexity. This includes various approaches, notably convolutional language models (Poli et al., 2023), recurrent unit (Lei, 2021), long context models (Ding et al., 2023), and state space models (SSMs) (Gu et al., 2021; Gu \\& Dao, 2023). These new models have provided strong alternatives to Transformer for building efficient LLMs. SSMs propose modeling sequences by introducing an appropriate design of hidden states for handling long-range dependencies with both training parallelizability and inference efficiency. Starting from the continuous mapping system, SSMs are discretized to process discrete inputs in deep learning such as language sequence. The discretized SSMs can be computed in both linear recurrence and global convolution modes. Commonly, convolution mode is used during training to achieve parallel acceleration, while recurrence mode is used during autoregressive inference because it has lower computational complexity. The core distinction of SSMs from other neural networks, such as fully-connected neural networks, lies in the design of hidden states. Hidden states enable information to be propagated along the temporal dimension, while avoiding the computation complexity of accessing historical tokens at each step. Through state transition parameters $A$, hidden states transfer the hidden information from the previous time\nsteps to the current time step, allowing for autoregressive prediction of the next token. Hidden states play a crucial role in SSMs, but have not received sufficient investigation in the past. Weights and hidden features in different layers contain information at various levels from fine-grained to coarsegrained (Gu et al., 2021). However, in previous versions of SSMs, hidden states only flowed within the current layer and could not transmit more information to deeper layers, thus failing to capture more hierarchical information. In this paper, we propose DenseSSM to facilitate a more comprehensive flow of hidden information between layers in state space models. We first analyze the hidden state degradation in conventional SSMs which will prevent hidden information flow from low levels to high levels. By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information that is useful for the final output. The proposed method is applicable to different types of SSMs, such as RetNet (Sun et al., 2023) and Mamba (Gu \\& Dao, 2023). Our approach maintains the training parallelizability and inference efficiency of SSMs, while achieving a significant improvement with only a slight increase in the number of parameters. For instance, our DenseRetNet model outperforms traditional RetNet with up to 5\\% accuracy improvement on public benchmarks.\n```\n\n##### *Relevant Chunk: No. 3/21 (Score: 1.00)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 1.00)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 2/86 (Score: 1.00)*\n\n```\n## 1 Introduction\n\nTransformers, in particular decoder-only models (e.g. GPT (Brown et al. 2020), Llama (Touvron, Lavril, et al. 2023)) which process input sequences in a causal fashion, are one of the main drivers of modern deep learning's success. Numerous approaches attempt to approximate the core attention layer to address its efficiency issues (Tay et al. 2022), such as scaling quadratically in sequence length during training and requiring a cache of size linear in sequence length during autoregressive generation. In parallel, a class of alternative sequence models, structured state-space models (SSMs), have emerged with linear scaling in sequence length during training and constant state size during generation. They show strong performance on long-range tasks (e.g. S4 (Gu, Goel, and R\u00e9 2022)) and recently matched or beat Transformers on language modeling (e.g. Mamba (Gu and Dao 2023)) at small to moderate scale. However, the development of SSMs have appeared disjoint from the community's collective effort to improve Transformers, such as understanding them theoretically as well as optimizing them on modern hardware. As a result, it is more difficult to understand and experiment with SSMs compared to Transformers, and it remains challenging to train SSMs as efficiently as Transformers from both an algorithmic and systems perspective. Our main goal is to develop a rich body of theoretical connections between structured SSMs and variants of attention. This will allow us to transfer algorithmic and systems optimizations originally developed for Transformers to SSMs, towards the goal of building foundation models that perform better than Transformers while scaling more efficiently in sequence length. A milestone contribution in this direction was the Linear Attention (LA) framework (Katharopoulos et al. 2020), which derived a connection between autoregressive attention and linear RNNs by showing the equivalence between \"dual forms\" of quadratic kernelized attention and a particular linear recurrence. This duality allows new capabilities such as the ability to have both efficient parallelizable training and efficient autoregressive inference. In the same spirit, this paper provides multiple viewpoints connecting linear-complexity SSMs with quadratic-complexity forms to combine the strengths of SSMs and attention. ${ }^{1}$\n\n[^0]State Space Duality. Our framework connecting structured SSMs and variants of attention, which we call structured state space duality (SSD), is made through the abstractions of structured matrices: matrices with subquadratic parameters and multiplication complexity. We develop two broad frameworks for representing sequence models, one as matrix transformations and one as tensor contractions, which each reveal different perspectives of the duality. Our technical contributions include:\n\n- We show an equivalence between state space models and a well-studied family of structured matrices called semiseparable matrices (Section 3). This connection is at the heart our framework, revealing new properties and algorithms for SSMs. A central message of this paper is that different methods of computing state space models can be reframed as various matrix multiplication algorithms on structured matrices. - We significantly improve the theory of linear attention (Katharopoulos et al. 2020). We first provide an incisive proof of its recurrent form through the language of tensor contractions, and then generalize it to a new family of structured masked attention (SMA) (Section 4). - We connect SSMs and SMA, showing that they have a large intersection that are duals of each other, possessing both SSM-like linear and attention-like quadratic forms (Section 5). We also prove that any kernel attention method possessing a fast recurrent form must be an SSM. ![](https://cdn.mathpix.com/cropped/2024_09_12_4f7a89c99c4204d1f9c3g-02.jpg?height=887&width=831&top_left_y=261&top_left_x=1124)\n\nFigure 1: (Structured State-Space Duality.) This paper fleshes out the relationship between state space models and attention through the bridge of structured matrices.\n```\n\n#### 4. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 1.00)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 1.00)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: state space models neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: state space models neural networks*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 2. Liquid Structural State-Space Models\n\n*From Search Query: state space models neural networks*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 3. Resurrecting Recurrent Neural Networks for Long Sequences\n\n*From Search Query: state space models neural networks*\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 189  (*Influential: 32*)\n\n#### 4. Continuous-time identification of dynamic state-space models by deep subspace encoding\n\n*From Search Query: state space models neural networks*\n\n*G. Beintema, M. Schoukens, R. T'oth*\n\n**TL;DR:** It is proved that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and it is shown that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.\n\n**Abstract:** Continuous-time (CT) modeling has proven to provide improved sample efficiency and interpretability in learning the dynamical behavior of physical systems compared to discrete-time (DT) models. However, even with numerous recent developments, the CT nonlinear state-space (NL-SS) model identification problem remains to be solved in full, considering common experimental aspects such as the presence of external inputs, measurement noise, latent states, and general robustness. This paper presents a novel estimation method that addresses all these aspects and that can obtain state-of-the-art results on multiple benchmarks with compact fully connected neural networks capturing the CT dynamics. The proposed estimation method called the subspace encoder approach (SUBNET) ascertains these results by efficiently approximating the complete simulation loss by evaluating short simulations on subsections of the data, by using an encoder function to estimate the initial state for each subsection and a novel state-derivative normalization to ensure stability and good numerical conditioning of the training process. We prove that the use of subsections increases cost function smoothness together with the necessary requirements for the existence of the encoder function and we show that the proposed state-derivative normalization is essential for reliable estimation of CT NL-SS models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 5. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers\n\n*From Search Query: state space models neural networks*\n\n*Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, A. Rudra, Christopher R'e*\n\n**TL;DR:** A simple sequence model inspired by control systems that generalizes RNN heuristics, temporal convolutions, and neural differential equations while addressing their shortcomings, and introduces a trainable subset of structured matrices that endow LSSLs with long-range memory.\n\n**Abstract:** Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 357  (*Influential: 22*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Thermal Neural Networks: Lumped-Parameter Thermal Modeling With State-Space Machine Learning\n\n*From Search Query: state space models neural networks*\n\n*Joachim B\u00f6cker, Oliver Wallscheid, Wilhelm Kirchg\u00e4ssner*\n\n**Abstract:** With electric power systems becoming more compact and increasingly powerful, the relevance of thermal stress especially during overload operation is expected to increase ceaselessly. Whenever critical temperatures cannot be measured economically on a sensor base, a thermal model lends itself to estimate those unknown quantities. Thermal models for electric power systems are usually required to be both, real-time capable and of high estimation accuracy. Moreover, ease of implementation and time to production play an increasingly important role. In this work, the thermal neural network (TNN) is introduced, which unifies both, consolidated knowledge in the form of heat-transfer-based lumped-parameter models, and data-driven nonlinear function approximation with supervised machine learning. A quasi-linear parameter-varying system is identified solely from empirical data, where relationships between scheduling variables and system matrices are inferred statistically and automatically. At the same time, a TNN has physically interpretable states through its state-space representation, is end-to-end trainable -- similar to deep learning models -- with automatic differentiation, and requires no material, geometry, nor expert knowledge for its design. Experiments on an electric motor data set show that a TNN achieves higher temperature estimation accuracies than previous white-/grey- or black-box models with a mean squared error of $3.18~\\text{K}^2$ and a worst-case error of $5.84~\\text{K}$ at 64 model parameters.\n\n**Published:** 2021-03-30\n\n\n\n#### 2. Modeling Multivariate Biosignals With Graph Neural Networks and Structured State Space Models\n\n*From Search Query: state space models neural networks*\n\n*Tina Baykaner, Daniel L. Rubin, Christopher Lee-Messer, Khaled K. Saab, Liangqiong Qu, Jared A. Dunnmon, Siyi Tang*\n\n**Abstract:** Multivariate biosignals are prevalent in many medical domains, such as electroencephalography, polysomnography, and electrocardiography. Modeling spatiotemporal dependencies in multivariate biosignals is challenging due to (1) long-range temporal dependencies and (2) complex spatial correlations between the electrodes. To address these challenges, we propose representing multivariate biosignals as time-dependent graphs and introduce GraphS4mer, a general graph neural network (GNN) architecture that improves performance on biosignal classification tasks by modeling spatiotemporal dependencies in biosignals. Specifically, (1) we leverage the Structured State Space architecture, a state-of-the-art deep sequence model, to capture long-range temporal dependencies in biosignals and (2) we propose a graph structure learning layer in GraphS4mer to learn dynamically evolving graph structures in the data. We evaluate our proposed model on three distinct biosignal classification tasks and show that GraphS4mer consistently improves over existing models, including (1) seizure detection from electroencephalographic signals, outperforming a previous GNN with self-supervised pre-training by 3.1 points in AUROC; (2) sleep staging from polysomnographic signals, a 4.1 points improvement in macro-F1 score compared to existing sleep staging models; and (3) 12-lead electrocardiogram classification, outperforming previous state-of-the-art models by 2.7 points in macro-F1 score.\n\n**Published:** 2022-11-21\n\n\n\n#### 3. Progressive Neural Architecture Search\n\n*From Search Query: state space models neural networks*\n\n*Li Fei-Fei, Li-Jia Li, Jonathon Shlens, Kevin Murphy, Wei Hua, Maxim Neumann, Jonathan Huang, Barret Zoph, Alan Yuille, Chenxi Liu*\n\n**Abstract:** We propose a new method for learning the structure of convolutional neural\nnetworks (CNNs) that is more efficient than recent state-of-the-art methods\nbased on reinforcement learning and evolutionary algorithms. Our approach uses\na sequential model-based optimization (SMBO) strategy, in which we search for\nstructures in order of increasing complexity, while simultaneously learning a\nsurrogate model to guide the search through structure space. Direct comparison\nunder the same search space shows that our method is up to 5 times more\nefficient than the RL method of Zoph et al. (2018) in terms of number of models\nevaluated, and 8 times faster in terms of total compute. The structures we\ndiscover in this way achieve state of the art classification accuracies on\nCIFAR-10 and ImageNet.\n\n**Conference:** progressive-neural-architecture-search-1\n\n**Published:** 2017-12-02\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by incorporating state space models (SSMs) and addressing the limitations and advantages of these models, here are some key technical details and insights:\n\n## Architecture of State Space Models in Neural Networks\n\n### Integration with Neural Networks\nState space models can be integrated into neural networks to leverage their strengths in modeling temporal dynamics and handling sequential data. For instance, the paper \"State-space models can learn in-context by gradient descent\" shows how deep state-space models (Deep SSMs) can be augmented with local self-attention to perform gradient-based learning, similar to transformers. This integration allows SSMs to capture complex temporal dependencies and learn in-context, which is crucial for autoregressive tasks.\n\n### State Space Formulation\nIn the context of graph neural networks, state space models can be used to represent the temporal dynamics of graphs. Here, the state of a graph at a given time step is represented by latent variables that evolve according to state transition equations. This formulation aligns with the internal workings of graph neural networks, where the hidden representations of nodes can be seen as state variables. This approach can inspire novel graph neural network layers that explicitly model temporal evolution.\n\n### Efficient and Scalable Design\nThe SC-MAMBA2 model integrates state-space models with the MAMBA framework, enabling efficient and scalable modeling of extensive gene sequences. This approach leverages the near-linear scalability of SSMs concerning sequence length, which is beneficial for handling long sequences in autoregressive language models. The use of state space duality and selective SSMs simplifies the matrix transformations, making the model more efficient and adaptable.\n\n## Advantages\n\n### Temporal Dynamics and State Tracking\nSSMs are particularly effective in modeling temporal dynamics and state tracking. By using a state space formulation, these models can capture the evolution of states over time, which is essential for autoregressive tasks. The inclusion of local self-attention and multiplicative interactions enhances the expressive power of these models, making them competitive with modern deep learning architectures.\n\n### Efficiency and Scalability\nSSMs offer near-linear scalability concerning sequence length, which is a significant advantage over traditional transformer-based architectures. This scalability is crucial for handling ultra-long sequences efficiently, reducing computational overhead.\n\n### Robustness and Handling Complex Data\nSSMs can handle various challenges such as missing data, noisy observations, and temporal dependencies more robustly due to their rich theoretical foundations. This robustness is beneficial for real-world data, which often exhibits complex, nonlinear, and non-Gaussian characteristics.\n\n## Limitations\n\n### Computational Complexity and Assumptions\nTraditional state space models often rely on assumptions such as linearity and Gaussianity, which may not align well with the complex nature of real-world data. Addressing these limitations requires further advancements in the theory and implementation of SSMs for graph-based and sequential data.\n\n### State Tracking Capabilities\nSSMs have fundamental limitations in expressing computation outside the complexity class TC^0, which means they cannot solve simple state-tracking problems like permutation composition. This limitation highlights the need for hybrid approaches that combine SSMs with other architectures, such as attention mechanisms.\n\n## Comparison with Attention Mechanisms\n\n### Hybrid Approaches\nCombining SSMs with attention mechanisms can overcome the limitations of both architectures. For example, augmenting SSMs with local self-attention can enhance their expressive power and ability to learn in-context. This hybrid approach can leverage the strengths of both models, such as the efficient state tracking of SSMs and the contextual understanding of attention mechanisms.\n\n### Staircase Attention\nStaircase attention, which is designed for recurrent processing of sequences, shows promise in tasks that involve tracking, outperforming conventional transformers. Integrating such attention mechanisms with SSMs could provide a more robust and efficient architecture for autoregressive language models.\n\n## Future Directions\n\n### Exploring Hybrid Architectures\nThere is a need to investigate hybrid architectures that combine the strengths of SSMs and attention mechanisms. This includes exploring how different types of attention (e.g., staircase attention, sparse attention) can be integrated with SSMs to enhance state tracking and overall performance.\n\n### Theoretical Understanding\nFurther research is needed to develop a better theoretical understanding of state tracking capabilities in both SSMs and attention-based models. This understanding will help in designing more effective hybrid architectures that can handle complex sequential data efficiently.\n\nBy addressing these points, researchers can develop more robust, efficient, and scalable autoregressive language models that leverage the strengths of state space models and attention mechanisms.\n",
            "analysis": "Initial Analysis Note\n\n1. Summary of Analysis\n\nThe seed design FastTTTLinear and its siblings focus on enhancing efficiency through attention mechanisms and compression techniques. However, recent research suggests fundamental limitations in both transformer-based and state-space architectures for state tracking. This presents an opportunity to explore hybrid approaches that combine the strengths of different architectures while addressing their core limitations.\n\nKey observations:\n- FastTTTLinear uses Gated Linear Attention for efficiency\n- Siblings focus on sparse attention and hierarchical compression\n- Recent work questions fundamental capabilities of both transformers and SSMs\n- Need to explore alternative architectures that can better handle state tracking\n\n2. Useful References with Excerpts\n\nFrom \"The Illusion of State in State-Space Models\":\n\"SSMs cannot express computation outside the complexity class TC^0. In particular, this means they cannot solve simple state-tracking problems like permutation composition.\"\n\nFrom \"Simplified State Space Layers\":\n\"S5 layer uses one multi-input, multi-output SSM... can leverage efficient and widely implemented parallel scans, allowing S5 to match the computational efficiency of S4.\"\n\nFrom \"Staircase Attention\":\n\"Staircase attention is shown to be able to solve tasks that involve tracking that conventional Transformers cannot... yielding significant perplexity gains.\"\n\n3. Key Insights and Analysis\n\na) Architectural Limitations:\n- Both transformers and SSMs have fundamental limitations in state tracking\n- Need to consider hybrid approaches that can overcome these limitations\n- Staircase attention shows promise for tracking capabilities\n\nb) Efficiency Considerations:\n- S5 demonstrates efficient parallel implementation possibilities\n- Need to balance computational efficiency with expressiveness\n- Memory management remains a critical challenge\n\nc) Research Gaps:\n- Limited exploration of hybrid SSM-attention architectures\n- Need for better theoretical understanding of state tracking capabilities\n- Opportunity to combine strengths of different approaches\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Hybrid architectures combining SSMs and attention\n2. Alternative approaches to state tracking\n3. Recent advances in recurrent architectures\n\n5. References\n\n1. Merrill, W., Petty, J., & Sabharwal, A. (2024). The Illusion of State in State-Space Models. arXiv.org.\n\n2. Smith, J., Warrington, A., & Linderman, S. W. (2022). Simplified State Space Layers for Sequence Modeling. International Conference on Learning Representations.\n\n3. Ju, D., Roller, S., Sukhbaatar, S., & Weston, J. (2021). Staircase Attention for Recurrent Processing of Sequences. Neural Information Processing Systems.\n\n4. Han, C., Wang, Q., Peng, H., Xiong, W., Chen, Y., Ji, H., & Wang, S. (2023). LM-Infinite: Zero-Shot Extreme Length Generalization for Large Language Models. North American Chapter of the Association for Computational Linguistics."
        },
        {
            "ready": false,
            "query": "hybrid attention state space models neural networks",
            "detail": "Find technical details about hybrid architectures combining attention mechanisms with state space models, focusing on implementations and performance comparisons.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about hybrid architectures combining attention mechanisms with state space models, focusing on implementations and performance comparisons.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences (Avg. Score: 1.00)\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n##### *Relevant Chunk: No. 2/32 (Score: 1.00)*\n\n```\nLi ${ }^{1}$\n\n\n#### Abstract\n\nTo mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favourable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method. ## 1. Introduction\n\nTransformer models have demonstrated remarkable performance on a range of natural language processing tasks (Vaswani et al., 2017), such as language modeling (De-\n\n[^0]vlin et al., 2019), visual signal processing (Dosovitskiy et al., 2021; Liu et al., 2022; Li et al., 2023; Liu et al., 2023), and speech understanding (Gulati et al., 2020). These models use the attention mechanism, which calculates a dependency score for each pair of tokens in an input sequence. Consequently, full attention has a quadratic time and space complexity relative to the sequence length. This complexity, however, becomes computationally prohibitive for tasks that involve long sequences (Lin et al., 2022). It is worth mentioning that Transformer models equipped with full attention tend to overfit. This is because the attention mechanism does not make any assumptions about the structure of the inputs, which leads to the absence of structural biases. To train a Transformer model, even the order information has to be included. Therefore, the full attention is too flexible to overfit to noise. This limitation restricts the practicality of these models in long sequence modeling, where the dependency signal is often weak and the signal-to-noise ratio is low. To solve this, recent studies have designed hybrid models (Ma et al., 2022; Zuo et al., 2023) by combining efficient state space models (SSMs) (Gu et al., 2021; 2020a; 2022; Hasani et al., 2022; Smith et al., 2023), with expressive attention variants for modeling long sequences from perspectives in structured and flexible patterns, achieving promising results.\n```\n\n#### 2. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 1.00)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 1/29 (Score: 1.00)*\n\n```\n# Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks \n\nJerome Sieber*<br>ETH Zurich<br>Zurich, Switzerland<br>jsieber@ethz.ch\n\nCarmen Amo Alonso*<br>ETH Zurich<br>Zurich, Switzerland<br>camoalonso@ethz.ch\n\nAlexandre Didier<br>ETH Zurich<br>Zurich, Switzerland<br>adidier@ethz.ch\n\nMelanie N. Zeilinger<br>ETH Zurich<br>Zurich, Switzerland<br>mzeilinger@ethz.ch\n\nAntonio Orvieto<br>ELLIS Institute T\u00fcbingen<br>T\u00fcbingen, Germany<br>antonio@tue.ellis.eu\n\n\n#### Abstract\n\nSoftmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models. ## 1 Introduction\n\nFoundation models serve as the backbone for a wide range of tasks across Artificial Intelligence due to their ability to learn complex interactions in large datasets [Bommasani et al., 2021]. In recent years, the attention mechanism [Vaswani et al. 2017] has been the dominating token-mixing strategy in foundation models. However, its major computational bottleneck, i.e., the quadratic complexity with context length, has posed a challenge to scaling and deploying these models beyond moderate context lengths [Tay et al. 2021]. In order to mitigate these issues, attention-free architectures have been proposed: prominent examples of these are the novel State Space Models (SSMs) Gu et al., 2022b, Smith et al., 2023, Orvieto et al., 2023, Gu and Dao, 2023, Dao and Gu, 2024, as well as recent\n\n[^0]efforts to enhance Recurrent Neural Networks (RNNs) Stani\u0107 et al., 2023, De et al., 2024, Qin et al., 2024, Beck et al., 2024]. Although these models show great promise in boosting efficiency, current comparisons with attention are merely empirical. Despite the prevalence and ubiquity of foundation models, a principled understanding of the similarities and differences among these different design strategies is currently lacking. In order to close this gap, we introduce the Dynamical Systems Framework (DSF), a theoretical framework that allows to evaluate the similarities and differences between different foundation models in a principled manner. This framework spans most current architectures and allows for direct comparisons, theoretical and computational, across attention, SSMs, and RNNs. The DSF provides new insights on the most relevant features found in current architectures, and can inform a systematic development of future hybrid models. Specifically, in this paper we answer the following questions:\n\n## - How are attention, SSMs, and RNNs related? $T L ; D R$ : All three model classes can be represented as recurrent models that can directly be compared using the proposed DSF. - Can softmax attention be expressed as a recurrent model? $T L ; D R$ : Softmax attention translates to a recurrent model within the DSF, however the hidden state dimension needs to be infinite. - Why does state expansion help to improve performance of RNNs and SSMs? $T L ; D R$ : This is related to the second question: state expansion increases the dimension of the hidden state thus allowing for an increased expressivity of the model (Lemma 2). - How closely are linear attention and S6 (i.e. Mamba) related? $T L ; D R$ : The common feature is the coupling of state transition and input matrix via a single (normalization) parameter in recurrent representation. However, the two models differ in the parameterization of this parameter, which we analyze experimentally. - What do selective SSMs teach us about improving RNN architectures? $T L ; D R$ : Replacing the state transition in a RNN variant - qLSTM - with the state transition of S6 improves performance of the RNN. Furthermore, it is important to highlight that, for the models studied here, some model classes are natively stated in recurrent form (i.e. SSMs, RNNs), while others are stated in convolutional (matrix) form (i.e. attention). The DSF allows to switch between these model classes and leverage computational tools developed for other classes. For instance, the recurrent form is efficiently implemented via scan algorithms [Blelloch, 1990], e.g., selective scan [Gu and Dao, 2023], parallel scan [Smith et al., 2023, Orvieto et al., 2023], and accelerated scan [Kyrylov, 2024]. The same holds for the convolutional form via, e.g., flash attention [Dao, 2023], flash linear attention [Yang and Zhang, 2024], and structured masked attention [Dao and Gu, 2024]. Given that the structural requirements on the model parameterization of the algorithm is met, the DSF allows to identify existing algorithms to apply to a new model even if the algorithm was designed for another model class. Notation: We use Latin letters in the following way: $N$ is the size of the hidden state in the DSF, $n$ the state expansion, $d$ the embedding size or model size, and $L$ the sequence length.\n```\n\n#### 3. Long Range Language Modeling via Gated State Spaces (Avg. Score: 0.99)\n\n*Harsh Mehta, Ankit Gupta, Ashok Cutkosky, Behnam Neyshabur*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 134  (*Influential: 17*)\n\n**TL;DR:** This work proposes a new layer named Gated State Space (GSS) and shows that it trains significantly faster than the diagonal version of S4 on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement.\n\n**Abstract:** State space models have shown to be effective at modeling long range dependencies, specially on sequence classification tasks. In this work we focus on autoregressive sequence modeling over English books, Github source code and ArXiv mathematics articles. Based on recent developments around the effectiveness of gated activation functions, we propose a new layer named Gated State Space (GSS) and show that it trains significantly faster than the diagonal version of S4 (i.e. DSS) on TPUs, is fairly competitive with several well-tuned Transformer-based baselines and exhibits zero-shot generalization to longer inputs while being straightforward to implement. Finally, we show that leveraging self-attention to model local dependencies improves the performance of GSS even further.\n\n##### *Relevant Chunk: No. 2/28 (Score: 0.99)*\n\n```\nwhere decoding every token requires attending to the whole past. The ideal model is parallelizable at training time but incurs a small constant cost (per decoded token) at inference time. This brings us to the final point. Due to the inherent convolution-recurrence equivalence of the state space model, it can be made to accumulate state and unroll like an RNN at inference time without any approximations. Despite these attractive properties, we found that current state space models (such as S4, DSS) run slower than we expected at training time on TPUs, our accelerator of choice. We take this opportunity to modify the architecture to reduce dimensionality of specific operations which we found to be bottlenecks. Our proposed changes borrow from a well-supported empirical observation around the effectiveness of gating units [Shazeer, 2020]. Specifically, Hua et al. [2022] observed that replacing the typical Feed-Forward layer in the Transformer with gating units allows for a reduced dimensionality when mixing tokens along the length dimension using self-attention. We extend the use of gating units to state space model family and observe that, even in our context, the use of gating units allows for a reduction in dimensionality when performing FFT operations, which we observed to be the main bottleneck behind slow training. Furthermore, somewhat contrary to observations made by S4 and DSS authors, we found the performance of the model on language modeling tasks to be much less sensitive to initialization. We found that only the scale and structural aspects of initialization of state space variables were important and not the exact values. We were able to successfully train the model while initializing the state space variables randomly. This departs significantly, at least in understanding, from the reliance of the design on the theory of HiPPO matrices, which led the S 4 model to employ several numerical linear algebra tricks to able to make it work. Combining both of these contributions, we propose a layer named Gated State Space (GSS) (Figure 1), which we empirically verified to be $2-3 \\times$ faster than DSS while keeping the perplexity on several language modeling benchmarks (Table 1). Going one step further, we also perform an apples-to-apples comparison with well-tuned and performant baselines reported in Block Recurrent Transformers [Hutchins et al., 2022], on several long range language modeling benchmarks over modalities such as English books, raw source code from Github and LaTeX source of ArXiv mathematics articles. As detailed in Table 2, while our GSS model currently lags behind on some tasks when compared in the fixed-parameter setting, it is fairly competitive in the fixed-compute setting where we measure compute as the exact amount of TPUv4 hours spent on a training run and serves as a fairly accurate proxy to the realistic cost of training that model. Furthermore, we also experimented with a hybrid model in which we sparingly interleave Transformer layers (having local attention) in a GSS stack to allow for a richer modeling of short range interactions. To our delight, this further improves performance at (roughly) no extra training cost, both in terms of parameters and compute. While in our experiments we train on sequences of length at most 4 k , we evaluated our GSS variants on a wide range of sequence lengths upto 65 k and found consistent generalization to longer inputs. Not only the performance doesn't degrade as the sequence length is increased but it gets significantly better, suggesting that GSS is effective at utilizing the extra context even though it was not trained with that much amount of context. At inference time, state space models including GSS are fairly efficient since decoding can happen in recurrent mode (as much as $60 \\times$ better in the case of $S 4$ [Gu et al., 2022a]). Though, the hybrid model which also uses local attention complicates this advantage a bit. In summary, we propose GSS, an alternative to $S 4$ and DSS which trains $2-3 \\times$ faster, is simple to implement and fairly competitive with well-tuned Transformer-based baselines on several long range language modeling benchmarks. ## 2 Related Work\n\nIn recent years, attention-based models have emerged as a dominant technique for sequence modeling, achieving remarkable improvements in a wide range of tasks, starting in NLP [Vaswani et al., 2017, Devlin et al., 2019, Radford et al., 2019, Liu et al., 2019], then moving to other classical machine learning areas such as computer vision [Dosovitskiy et al., 2021] and now to the physical sciences [Avsec et al., 2021, Jumper et al., 2021].\n```\n\n#### 4. LOCOST: State-Space Models for Long Document Abstractive Summarization (Avg. Score: 0.99)\n\n*Florian Le Bronnec, Song Duong, Mathieu Ravaut, Alexandre Allauzen, Nancy F. Chen, Vincent Guigue, Alberto Lumbreras, Laure Soulier, Patrick Gallinari*\n\n**Published in:** Conference of the European Chapter of the Association for Computational Linguistics (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This work proposes LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs that effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n**Abstract:** State-space models are a low-complexity alternative to transformers for encoding long sequences and capturing long-term dependencies. We propose LOCOST: an encoder-decoder architecture based on state-space models for conditional text generation with long context inputs. With a computational complexity of \\mathcal{O}(L \\log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns. We evaluate our model on a series of long document abstractive summarization tasks. The model reaches a performance level that is 93-96% comparable to the top-performing sparse transformers of the same size while saving up to 50% memory during training and up to 87% during inference. Additionally, LOCOST effectively handles input texts exceeding 600K tokens at inference time, setting new state-of-the-art results on full-book summarization and opening new perspectives for long input processing.\n\n##### *Relevant Chunk: No. 2/30 (Score: 0.99)*\n\n```\nAs key examples, Guo et al. (2022) and Zaheer et al. (2020) extended the context capacity of encoderdecoder models (Raffel et al., 2020; Zhang et al., 2020) and showed drastic increases in the performance on long text summarization, motivating the quest to incorporate longer contexts. However, in practice, even the best sparse-transformers need heavy computational resources to handle sequences of length larger than 8 K tokens (see Figure 4). Deep state-space models (SSMs) (Gu et al., 2022b) have been proposed for sequence processing, with complexity $\\mathcal{O}(L \\log L)$, initially for computer vision and audio and more recently for text. Their recurrent architectures are designed for capturing long-range dependencies (Gu et al., 2020). Up to now, their applications have been restrained to either unconditional autoregressive generation, i.e., with a decoder-only (Fu et al., 2023; Goel et al., 2022) ; or sequence classification, i.e., with an encoder-only (Gu et al., 2022b,a; Nguyen et al., 2022). Tackling conditional text generation with SSMs as required e.g. for summarization remains yet unexplored. In this paper, we propose LOCOST an encoder-\ndecoder architecture to explore the performance of SSMs for conditional text generation tasks, through the lens of abstractive summarization. We demonstrate that SSMs can be competitive with transformer-based models while drastically reducing their memory requirements. We opt for a lightweight architecture design, comparable to the average base transformers (roughly 250M parameters) in order to process extremely long sequences on standard compute resources. Our experimentations with extremely long sequences yield stateof-the-art results on the challenging BookSumBook. With an increase of up to 2 points in average ROUGE score compared to sparse attention baselines, our model is able to process entire books, without truncation, and on a single GPU. Our contributions are threefold:\n\n- We propose a new encoder-decoder architecture based on state-space models. By bypassing the self-attention mechanism used in transformers, the model enjoys a complexity of $\\mathcal{O}(L \\log L)$ instead of $\\mathcal{O}\\left(L^{2}\\right)$ as in traditional transformers. - Compared with the best-performing sparse transformers of the same size, the model achieves $93-96 \\%$ of the best performance on various long document abstractive summarization while being up to $50 \\%$ more memory-efficient during training and up to $87 \\%$ at inference time, see Figure 1. - The model is able to process entire input sequences of up to 600 K tokens, a length far out of reach for sparse transformers. This allows the model to achieve a new state-of-the-art on a challenging full-book summarization task. To the best of our knowledge, this is the first encoder-decoder that performs competitively with sparse transformers with no attention in the encoder. Furthermore, this work represents the first successful attempt at processing extremely long texts e.g. entire books without any truncation, all in a single pass. The proposed model opens new perspectives for addressing long texts with lesser resources.*\n\n## 2 Related Work\n\nIn this section, we first review memory-efficient transformers and existing alternatives to the attention mechanism. Then, we discuss recent literature on state-space models. [^1]Memory efficiency for transformers. Reducing the memory consumption of transformers is an active research field. Optimization at the hardware level (Dao et al., 2022) helped to improve the scaling of the attention computation on recent GPUs. A line of work considers retrieving-augmented transformers, like (Borgeaud et al., 2022; Wang et al., 2023), that use additional modules to enhance the language modeling backbone. While crucial in developing memory-efficient architectures, we consider these last two topics as being orthogonal to our work that focuses on the models' architecture. Profuse literature focuses on tailoring the models' architecture for long inputs. Since the computational complexity of attention comes from the computation of the self-attention matrix, a straightforward way to reduce its cost is to approximate it using sparse-attention patterns. These patterns typically incorporate a combination of local attention and a set of carefully selected tokens. For instance, in addition to global tokens, BigBird (Zaheer et al., 2020) considers random tokens, while LSG (Condevaux and Harispe, 2023) considers sparse tokens through various strategy of sparsification. LongT5 (Guo et al., 2022) chunks the sequence into blocks and averages their representations, which gives a number of global tokens equal to the number of blocks. An overview of the complexity of various sparse-transformers can be found in Table 1. In contrast, we propose an alternative, computationally efficient architecture, without the need of costly self-attention blocks nor sparse-attention patterns. Attention-free transformers. Some variants of transformers already avoid the standard attention mechanism. For example Katharopoulos et al. (2020); Hua et al. (2022) approximate the softmax similarity in the attention by a more efficient computation. More recently, mixing architectures were introduced in (Liu et al., 2021). They are the main component of the FNet (Lee-Thorp et al., 2022) model, an encoder that replaces self-attention with a Discrete Fourier Transform (DFT). FNet has a complexity of $\\mathcal{O}(L \\log L)$ and is an encoder-only model, thus restricted to classification and regression tasks. Our proposed model also bypasses attention in the encoder, reaching the same computational complexity as encoders such as FNet, while being a much more versatile model, specifically designed for conditional text generation. | Encoder architecture | Complexity per layer |\n| :--- | :---: |\n| Transformer (full) | $\\mathcal{O}\\left(L^{2}\\right)$ |\n| LED | $\\mathcal{O}(L w)$ |\n| BigBird | $\\mathcal{O}(L w+L(g+r))$ |\n| LSG | $\\mathcal{O}(L w+L(g+s))$ |\n| LongT5 (TGlobal) | $\\mathcal{O}(L w+L\\lfloor L / c\\rfloor)$ |\n| LOCOST | $\\mathcal{O}(L \\log (L))$ |\n\nTable 1: Computational complexity per encoder layer as a function of the input length $L$, the local window size $w$ (typically set to 256 tokens), the number of global tokens $g$, random tokens $r$, sparse tokens $s$ and the chunk size $c$.\n```\n\n#### 5. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hybrid attention state space models neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 271  (*Influential: 21*)\n\n#### 4. Liquid Structural State-Space Models\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 5. Resurrecting Recurrent Neural Networks for Long Sequences\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, Soham De*\n\n**TL;DR:** This paper shows that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, whileAlso introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Abstract:** Recurrent Neural Networks (RNNs) offer fast inference on long sequences but are hard to optimize and slow to train. Deep state-space models (SSMs) have recently been shown to perform remarkably well on long sequence modeling tasks, and have the added benefits of fast parallelizable training and RNN-like fast inference. However, while SSMs are superficially similar to RNNs, there are important differences that make it unclear where their performance boost over RNNs comes from. In this paper, we show that careful design of deep RNNs using standard signal propagation arguments can recover the impressive performance of deep SSMs on long-range reasoning tasks, while also matching their training speed. To achieve this, we analyze and ablate a series of changes to standard RNNs including linearizing and diagonalizing the recurrence, using better parameterizations and initializations, and ensuring proper normalization of the forward pass. Our results provide new insights on the origins of the impressive performance of deep SSMs, while also introducing an RNN block called the Linear Recurrent Unit that matches both their performance on the Long Range Arena benchmark and their computational efficiency.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 189  (*Influential: 32*)\n\n### 3 related papers from Papers with Code\n\n#### 1. TransBTSV2: Towards Better and More Efficient Volumetric Segmentation of Medical Images\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Jing Wang, Hong Yu, Sen Zha, Tianxiang Zhang, Chen Chen, Wenxuan Wang, Jiangyun Li*\n\n**Abstract:** Transformer, benefiting from global (long-range) information modeling using self-attention mechanism, has been successful in natural language processing and computer vision recently. Convolutional Neural Networks, capable of capturing local features, are difficult to model explicit long-distance dependencies from global feature space. However, both local and global features are crucial for dense prediction tasks, especially for 3D medical image segmentation. In this paper, we present the further attempt to exploit Transformer in 3D CNN for 3D medical image volumetric segmentation and propose a novel network named TransBTSV2 based on the encoder-decoder structure. Different from TransBTS, the proposed TransBTSV2 is not limited to brain tumor segmentation (BTS) but focuses on general medical image segmentation, providing a stronger and more efficient 3D baseline for volumetric segmentation of medical images. As a hybrid CNN-Transformer architecture, TransBTSV2 can achieve accurate segmentation of medical images without any pre-training, possessing the strong inductive bias as CNNs and powerful global context modeling ability as Transformer. With the proposed insight to redesign the internal structure of Transformer block and the introduced Deformable Bottleneck Module to capture shape-aware local details, a highly efficient architecture is achieved with superior performance. Extensive experimental results on four medical image datasets (BraTS 2019, BraTS 2020, LiTS 2017 and KiTS 2019) demonstrate that TransBTSV2 achieves comparable or better results compared to the state-of-the-art methods for the segmentation of brain tumor, liver tumor as well as kidney tumor. Code will be publicly available at https://github.com/Wenxuan-1119/TransBTS.\n\n**Published:** 2022-01-30\n\n\n\n#### 2. MedMamba: Vision Mamba for Medical Image Classification\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Zhenzhang Li, Yubiao Yue*\n\n**Abstract:** Since the era of deep learning, convolutional neural networks (CNNs) and vision transformers (ViTs) have been extensively studied and widely used in medical image classification tasks. Unfortunately, CNN's limitations in modeling long-range dependencies result in poor classification performances. In contrast, ViTs are hampered by the quadratic computational complexity of their self-attention mechanism, making them difficult to deploy in real-world settings with limited computational resources. Recent studies have shown that state space models (SSMs) represented by Mamba can effectively model long-range dependencies while maintaining linear computational complexity. Inspired by it, we proposed MedMamba, the first Vision Mamba for generalized medical image classification. Concretely, we introduced a novel hybrid basic block named SS-Conv-SSM, which purely integrates the convolutional layers for extracting local features with the abilities of SSM to capture long-range dependencies, aiming to model medical images from different image modalities efficiently. By employing the grouped convolution strategy and channel-shuffle operation, MedMamba successfully provides fewer model parameters and a lower computational burden for efficient applications without sacrificing accuracy. We thoroughly evaluated MedMamba using 16 datasets containing ten imaging modalities and 411,007 images. Experimental results show that MedMamba demonstrates competitive performance on most tasks compared with the state-of-the-art methods. This work aims to explore the potential of Vision Mamba and establish a new baseline for medical image classification, thereby providing valuable insights for developing more powerful Mamba-based artificial intelligence algorithms and applications in medicine. The source codes and all pre-trained weights of MedMamba are available at https://github.com/YubiaoYue/MedMamba.\n\n**Published:** 2024-03-06\n\n\n\n#### 3. CHAMELEON: A Deep Learning Meta-Architecture for News Recommender Systems [Phd. Thesis]\n\n*From Search Query: hybrid attention state space models neural networks*\n\n*Gabriel de Souza Pereira Moreira*\n\n**Abstract:** Recommender Systems (RS) have became a popular research topic and, since 2016, Deep Learning methods and techniques have been increasingly explored in this area. News RS are aimed to personalize users experiences and help them discover relevant articles from a large and dynamic search space. The main contribution of this research was named CHAMELEON, a Deep Learning meta-architecture designed to tackle the specific challenges of news recommendation. It consists of a modular reference architecture which can be instantiated using different neural building blocks. As information about users' past interactions is scarce in the news domain, the user context can be leveraged to deal with the user cold-start problem. Articles' content is also important to tackle the item cold-start problem. Additionally, the temporal decay of items (articles) relevance is very accelerated in the news domain. Furthermore, external breaking events may temporally attract global readership attention, a phenomenon generally known as concept drift in machine learning. All those characteristics are explicitly modeled on this research by a contextual hybrid session-based recommendation approach using Recurrent Neural Networks. The task addressed by this research is session-based news recommendation, i.e., next-click prediction using only information available in the current user session. A method is proposed for a realistic temporal offline evaluation of such task, replaying the stream of user clicks and fresh articles being continuously published in a news portal. Experiments performed with two large datasets have shown the effectiveness of the CHAMELEON for news recommendation on many quality factors such as accuracy, item coverage, novelty, and reduced item cold-start problem, when compared to other traditional and state-of-the-art session-based recommendation algorithms.\n\n**Published:** 2019-12-29\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by combining hybrid architectures of attention mechanisms with state space models (SSMs), here are some key technical details and performance comparisons from the provided sources and additional insights:\n\n## Hybrid Architectures Combining Attention and SSMs\n\n### State Space Models with Attention\nThe paper \"Longhorn: State Space Models are Amortized Online Learners\" introduces a novel deep SSM architecture that incorporates online learning objectives, which can be linked to attention mechanisms. It discusses how state space models, particularly those with structured matrices and local self-attention, can efficiently process sequences and maintain state information. For example, models like Gated Linear Attention (GLA) and Retention Network (RetNet) use linear attention variants to enhance state tracking and sequence processing efficiency.\n\n### Online Learning Perspective\nThe Longhorn paper proposes treating SSMs as meta-modules for specific online learning problems, which can be integrated with attention mechanisms to optimize state tracking. This approach streamlines SSM design and improves performance on sequence modeling benchmarks and language modeling tasks. The use of implicit updates for optimizing online regression objectives is a key innovation here.\n\n### Structured Matrices and Duality\nThe concept of state space duality (SSD) mentioned in the analysis note allows for designing architectures that refine SSMs, making them competitive with Transformers while being faster. This involves using structured matrices to enhance the efficiency and state tracking capabilities of SSMs, which can be combined with attention mechanisms to improve overall performance [Analysis Note #1].\n\n## Enhancing State Tracking\n\n### Dense Connections\nThe \"DenseMamba\" model introduces dense hidden connections between layers, which improves information flow along the temporal dimension. This approach can be combined with attention mechanisms to enhance state tracking and maintain efficiency, addressing one of the key limitations of traditional SSMs [Analysis Note #1].\n\n### Selective Mechanisms\nModels like Mamba, which use selective state spaces, can be refined to include attention mechanisms. This selective approach enhances efficiency while maintaining competitive performance with Transformers, as seen in the \"Transformers are SSMs\" framework [Analysis Note #1].\n\n## Performance Comparisons\n\n### Efficiency and Performance\nThe Longhorn model, which combines SSMs with online learning objectives and attention-like mechanisms, outperforms state-of-the-art SSMs, including Mamba, on standard sequence modeling benchmarks and language modeling tasks. This model achieves a 1.8x speedup, indicating improved efficiency and performance.\n\n### Comparison with Transformers\nThe SSD framework allows for designing SSM architectures that are 2-8X faster than traditional SSMs while remaining competitive with Transformers. This suggests that hybrid architectures combining SSMs with attention mechanisms can offer significant performance and efficiency advantages over pure Transformer models [Analysis Note #1].\n\n## Implementation Considerations\n\n### Memory Efficiency\nImplementing attention mechanisms within SSMs requires careful memory management to maintain efficiency. Techniques such as using structured matrices and selective state spaces can help reduce memory requirements while preserving performance.\n\n### Hardware Efficiency\nThe design of these hybrid models should also consider hardware efficiency, ensuring that the models can be trained and deployed efficiently on available hardware. The parallelizable nature of SSMs, as highlighted in the Longhorn paper, is particularly beneficial in this regard.\n\nIn summary, combining attention mechanisms with state space models through hybrid architectures offers significant potential for improving autoregressive language model design. Key strategies include using structured matrices, dense connections, and selective mechanisms to enhance state tracking and efficiency, while ensuring memory and hardware efficiency.\n",
            "analysis": "Analysis Note #1\n\n1. Summary of Analysis\n\nThe initial search reveals several key insights about state space models (SSMs) and their relationship with attention mechanisms:\n\n- SSMs offer linear complexity and efficient processing of long sequences but have fundamental limitations in state tracking\n- Recent work shows surprising connections between SSMs and attention through structured matrices\n- Hybrid approaches combining SSMs with other mechanisms show promise\n- There are opportunities to improve the FastTTTLinear architecture by addressing state tracking limitations\n\n2. Useful References with Excerpts\n\nFrom \"The Illusion of State in State Space Models\":\n\"SSMs cannot express computation outside the complexity class TC^0. In particular, this means they cannot solve simple state-tracking problems like permutation composition.\"\n\nFrom \"Transformers are SSMs\":\n\"Our state space duality (SSD) framework allows us to design a new architecture whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers.\"\n\nFrom \"DenseMamba\":\n\"Hidden states enable information to be propagated along the temporal dimension... However, in previous versions of SSMs, hidden states only flowed within the current layer and could not transmit more information to deeper layers.\"\n\n3. Key Insights and Analysis\n\na) State Tracking Limitations:\n- SSMs struggle with certain types of state tracking that are important for language modeling\n- Need to find ways to enhance state tracking while maintaining efficiency\n- Potential to leverage structured matrices and duality principles\n\nb) Architectural Opportunities:\n- Dense connections between layers could improve information flow\n- Selective mechanisms can enhance efficiency\n- Hybrid approaches combining SSM strengths with other architectures show promise\n\nc) Implementation Considerations:\n- Need to maintain test-time training capabilities\n- Hardware efficiency is crucial\n- Memory management remains important\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in hybrid SSM-attention architectures\n2. Techniques for enhancing state tracking in neural networks\n3. Memory-efficient implementations of attention mechanisms\n\n5. References\n\n1. Merrill, W., Petty, J., & Sabharwal, A. (2024). The Illusion of State in State Space Models. arXiv.org.\n\n2. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv.org.\n\n3. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv.org.\n\n4. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.org.\n\n5. Ju, D., Roller, S., Sukhbaatar, S., & Weston, J. (2021). Staircase Attention for Recurrent Processing of Sequences. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "gating mechanisms hybrid neural networks",
            "detail": "Find technical details about gating mechanisms in neural networks, focusing on their use in hybrid architectures and their impact on model performance.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about gating mechanisms in neural networks, focusing on their use in hybrid architectures and their impact on model performance.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models (Avg. Score: 0.95)\n\n*Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, S. Srinivasan, Guillaume Desjardins, Arnaud Doucet, D. Budden, Y. W. Teh, Razvan Pascanu, Nando de Freitas, Caglar Gulcehre*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 48  (*Influential: 9*)\n\n**TL;DR:** Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention are proposed, and it is shown that Griffin can extrapolate on sequences significantly longer than those seen during training.\n\n**Abstract:** Recurrent neural networks (RNNs) have fast inference and scale efficiently on long sequences, but they are difficult to train and hard to scale. We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention. Hawk exceeds the reported performance of Mamba on downstream tasks, while Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens. We also show that Griffin can extrapolate on sequences significantly longer than those seen during training. Our models match the hardware efficiency of Transformers during training, and during inference they have lower latency and significantly higher throughput. We scale Griffin up to 14B parameters, and explain how to shard our models for efficient distributed training.\n\n##### *Relevant Chunk: No. 51/56 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2401.09417, 2024. ## A. RG-LRU Recurrence Gate\n\nIn Figure 7, we demonstrate the behavior of different gating mechanisms applied on the recurrent weight $a$. ![](https://cdn.mathpix.com/cropped/2024_09_12_235c9496c10750230447g-19.jpg?height=616&width=1610&top_left_y=465&top_left_x=221)\n\nFigure 7 | The behaviour of different gating mechanisms applied on the recurrent weight $a$ (note that in the Mamba's notations this is $-A$ ). Implementation We implement our recurrence gate, as defined in Section 2.4, in a slightly different, but mathematically equivalent form, for numerical stability. In particular, we compute the logarithm of $a_{t}$ and then we exponentiate it, instead of computing a sigmoid and then taking a power:\n\n$$\n\\log a_{t}=\\log a^{c r_{t}}=\\log \\sigma(\\Lambda)^{c r_{t}}=-\\operatorname{csoftplus}(\\Lambda) \\odot r_{t}\n$$\n\nGate behaviour Our gate is quite different than other standard gates in the literature. In particular, most gating mechanisms, like the one used in Mamba and GRU, allow through the gate to interpolate fully between the hidden state and the new observation. Ours on the other hand is biased towards retaining information, and does not allow to fully discard the contribution of $h_{t-1}$ (this depends, however, on the value of $\\Lambda$ ). To demonstrate this, we analyze the relative weight of $x_{t}$ compare to $h_{t-1}$ in the output $y_{t}$. For a general recurrence we define this as:\n\n$$\nh_{t}=\\alpha\\left(r_{t}\\right) h_{t-1}+\\beta\\left(r_{t}\\right) x_{t}\n$$\n\nFor our model we have $\\alpha\\left(r_{t}\\right)=a_{t}=a^{c r_{t}}$ and $\\beta\\left(r_{t}\\right)=\\sqrt{1-\\alpha\\left(r_{t}\\right)^{2}}$. For a standard GRU style gating we have $\\alpha\\left(r_{t}\\right)=1-r_{t}$ and $\\beta\\left(r_{t}\\right)=r_{t}$. For Mamba, assuming in their notation $B=1, C=1$, then $\\alpha\\left(r_{t}\\right)=\\left(1-r_{t}\\right)^{-A}$ and $\\beta\\left(r_{t}\\right)=(1-\\alpha) / A$. The behaviour of the different gating mechanisms is depicted in Figure 7, where for clarity we have also included the update value of the LRU (Orvieto et al., 2023b), which has no gating. As can be seen, the Mamba gating is almost identical to the GRU for values of $A$ close to 1 , with minor deviations at smaller values. On the other hand, our gating mechanism performs a very different non-linear interpolation between fully discarding the input $x_{t}$ and the update of the LRU. ## B. Complex-Gated Linear Recurrent Unit (CG-LRU)\n\nIn Section 2.4 we have defined our recurrent layer, however it can be further extended to use complex numbers. To achieve this we first parameterize a complex diagonal recurrence via $\\tilde{a}=\\sigma(\\Lambda) e^{i \\theta}$, where $\\theta$ is a learnable parameter. In addition, we split the input $x_{t}$ along its channel dimensions, and interpret\nits first half as the real part of a complex vector, and the second part as the imaginary part of the same complex vector:\n\n$$\n\\begin{aligned}\nx_{t} & =\\left[\\begin{array}{l}\nx_{t}^{1} \\\\\nx_{t}^{2}\n\\end{array}\\right] \\\\\n\\tilde{x}_{t} & =x_{t}^{1}+i x_{t}^{2}\n\\end{aligned}\n$$\n\nWith this we rewrite the equations for the LRU (see eq. 4) as:\n\n$$\n\\begin{aligned}\nr_{t} & =\\sigma\\left(W_{a} x_{t}+b_{a}\\right), \\quad \\text { recurrence gate } \\\\\ni_{t} & =\\sigma\\left(W_{x} x_{t}+b_{x}\\right), \\quad \\text { input gate } \\\\\n\\tilde{a}_{t} & =\\tilde{a}^{c r_{t}}, \\\\\n\\tilde{h}_{t} & =\\tilde{a}_{t} \\odot \\tilde{h}_{t-1}+\\sqrt{1-\\left|\\tilde{a}_{t}\\right|^{2}} \\odot\\left(i_{t} \\odot \\tilde{x}_{t}\\right)\n\\end{aligned}\n$$\n\nWe mark all complex variables with $\\stackrel{\\sim}{ }$ for clarity.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.94)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 9/74 (Score: 0.99)*\n\n```\nSun et al. 2023). ### 3.5 Properties of Selection Mechanisms\n\nThe selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs or CNNs, to different parameters (e.g. A in Algorithm 2), or using different transformations $s(x)$. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-08.jpg?height=619&width=1548&top_left_y=238&top_left_x=321)\n\nFigure 3: (Architecture.) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with the ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block homogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to the MLP block, Mamba adds an SSM to the main branch. For $\\sigma$ we use the SiLU / Swish activation (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017). ### 3.5.1 Connection to Gating Mechanisms\n\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection mechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of Gu , Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in Appendix C). More broadly, $\\Delta$ in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms. Theorem 1. When $N=1, \\boldsymbol{A}=-1, \\boldsymbol{B}=1, s_{\\Delta}=\\operatorname{Linear}(x)$, and $\\tau_{\\Delta}=$ softplus, then the selective SSM recurrence (Algorithm 2) takes the form\n\n$$\n\\begin{aligned}\n& g_{t}=\\sigma\\left(\\operatorname{Linear}\\left(x_{t}\\right)\\right) \\\\\n& h_{t}=\\left(1-g_{t}\\right) h_{t-1}+g_{t} x_{t}\n\\end{aligned}\n$$\n\nAs mentioned in Section 3.2, our specific choices of $s_{\\Delta}, \\tau_{\\Delta}$ is from this connection. In particular, note that if a given input $x_{t}$ should be completely ignored (as necessary in the synthetic tasks), all $D$ channels should ignore it, and so we project the input down to 1 dimension before repeating/broadcasting with $\\Delta$. ### 3.5.2 Interpretation of Selection Mechanisms\n\nWe elaborate on three particular mechanistic effects of selection. Variable Spacing. Selectivity allows filtering out irrelevant noise tokens that may occur between inputs of interest. This is exemplified by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for discrete data - for example the presence of language fillers such as \"um\". This property arises because the model can mechanistically filter out any particular input $x_{t}$, for example in the gated RNN case (Theorem 1 ) when $g_{t} \\rightarrow 0$. Filtering Context. It has been empirically observed that many sequence models do not improve with longer context ( F . Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation is that many sequence models cannot effectively ignore irrelevant context when necessary; an intuitive example are global convolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.\n```\n\n##### *Relevant Chunk: No. 59/74 (Score: 0.88)*\n\n```\n- RWKV (B. Peng et al. 2023) is another recent RNN designed for language modeling. It is based on AFT (attention-free Transformer (S. Zhai et al. 2021)), another variant of linear attention. Its main \"WKV\" mechanism involves LTI recurrences and can be seen as the ratio of two SSMs. We also highlight the gated attention unit (GAU) from Hua et al. (2022), which was motivated by combining the Transformer's MHA and MLP blocks together and was an inspiration for our architecture (Section 3.4) combining the H3 and MLP blocks. ## B. 3 Relationship to RNNs\n\nRNNs and SSMs are broadly related, as they both involve the concepts of recurrence on a latent state. Several older RNNs such as the strongly typed RNN (Balduzzi and Ghifary 2016), quasi-RNN (QRNN) (Bradbury et al. 2016), and simple recurrent unit (SRU) (Lei 2021; Lei et al. 2017) involve forms of gated RNNs without time-wise nonlinearities. Because of the connections of gating mechanisms and selection mechanisms, these can be viewed as cases of selective SSMs, and are thus more powerful in a sense than the family of LTI structured SSMs above. The main differences are:\n\n- They do not use state expansion $(N=1)$ or selective $B, C$ parameters, both of which are important for performance (Section 4.6). - They use a heuristic gating mechanism, which we generalize as a consequence of the selection mechanism + discretization (Theorem 1). The connections to principled SSM theory provides better parameterizations and initializations (Section 3.6). Additionally, older RNNs famously suffered from efficiency issues and the vanishing gradients problem (Hochreiter 1991; Hochreiter, Bengio, et al. 2001; Pascanu, Mikolov, and Bengio 2013), both caused by their sequential nature. The former could be solved for some of the above RNNs by leveraging the parallel scan (Martin and Cundy 2018), but the latter was difficult without theory later developed for SSMs. For example, modern structured SSMs differ in more careful parameterization of the recurrent dynamics inspired by classical SSM theory (e.g.\n```\n\n#### 3. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.79)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 10/49 (Score: 0.79)*\n\n```\nPerceivers (Jaegle et al. 2021, Hawthorne et al. 2022) use cross-attention to attend to long sequences, and compute self-attention over only a narrow stack of 'latents'. Transformer-VQ differs from Perceivers in that it computes dense self-attention in linear time, instead of just cross-attention. Thus, while Perceivers' long-range layers incur a quadratic time complexity during sampling, TransformerVQ generates sequences in linear time. ### 4.4 Gated SEQUENCE MODELS\n\nGated attention was introduced in FLASH (Hua et al., 2022) as a fusion of attention sublayers (Vaswani et al., 2017) and GLU-based MLP sublayers (Shazeer, 2020). Various gating mechanisms have previously been used to stabilize training of transformers (Parisotto et al., 2019) and other sequence models including S4 (Gu et al.\n```\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training (Avg. Score: 0.75)\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 43  (*Influential: 9*)\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n##### *Relevant Chunk: No. 5/51 (Score: 0.75)*\n\n```\n$w / \\mathrm{m}$. and $w / o m$. denotes using FLASHLINEARATTENTION with or without materialization of hidden states in HBM. and a pure PyTorch (i.e., I/O-unaware) implementation of chunkwise linear attention, showing the benefits of I/O-awareness. ## 4 Gated Linear Attention\n\nThe linear recurrence in Eq. 1 does not have a decay term or a forget gate, which has been shown to be crucial in RNNs (Hochreiter \\& Schmidhuber, 1997; Cho et al., 2014; van der Westhuizen \\& Lasenby, 2018). The lack of a decay term makes it difficult for a model to \"forget\" information, and has been hypothesized to be partially responsible for the instability of linear attention in long-context tasks (Buckman \\& Gelada, 2024). Recent works (Sun et al., 2023a; Qin et al., 2023b) obtain better performance through incorporating a global, non-data-dependent decay factor ${ }^{3} \\gamma \\in(0,1)$ into linear attention: $\\mathbf{S}_{t}=\\gamma \\mathbf{S}_{t-1}+\\boldsymbol{k}_{t}^{\\top} \\boldsymbol{v}_{t}$. The use of a single $\\gamma$ is designed to preserve the attention-style parallel form for efficient training. In this work, we consider a data-dependent gating mechanism for linear attention. We show that despite having a more expressive gating factor, the resulting gated linear attention (GLA) layer still admits a hardware-efficient chunkwise form for efficient training. ### 4.1 Recurrent and Parallel Form of GLA\n\nRecurrent form. GLA has a 2D forget gate $\\mathbf{G}_{t} \\in(0,1)^{d_{k} \\times d_{v}}$ that varies over time:\n\n$$\n\\mathbf{S}_{t}=\\mathbf{G}_{t} \\odot \\mathbf{S}_{t-1}+\\boldsymbol{k}_{t}^{\\top} \\boldsymbol{v}_{t}\n$$\n\nwhere we now allow the hidden state to have varying dimensions. This Hadamard product-based recurrent form is very general and encompasses many recent RNNs with 2D hidden states, as listed in Table 1. Central to the design of gated linear attention is the parameterization of $\\mathbf{G}_{t}$ which requires a balance between parameter-efficiency, state size, and training efficiency. A\n\n[^2]| Model | Parameterization | Learnable parameters |  |\n| :--- | :--- | :--- | :--- |\n| Mamba (Gu \\& Dao, 2023) | $\\mathbf{G}_{t}=\\exp \\left(-\\left(\\mathbf{1}^{\\top} \\boldsymbol{\\alpha}_{t}\\right) \\odot \\exp (\\boldsymbol{A})\\right), \\quad \\boldsymbol{\\alpha}_{t}=\\operatorname{softplus}\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{1}} \\boldsymbol{W}_{\\alpha_{2}}\\right)$ | $\\boldsymbol{A} \\in \\mathbb{R}^{d_{k} \\times d_{v}}, \\quad \\boldsymbol{W}_{\\alpha_{1}} \\in \\mathbb{R}^{d \\times \\frac{d}{16}}, \\quad \\boldsymbol{W}_{\\alpha_{2}} \\in \\mathbb{R}^{\\frac{d}{16} \\times d_{v}}$ |  |\n| Mamba-2 (Dao \\& Gu, 2024) | $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{\\top} \\mathbf{1}, \\quad \\gamma_{t}=\\exp \\left(-\\operatorname{softplus}\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\gamma}\\right) \\exp (a)\\right)$ | $\\boldsymbol{W}_{\\gamma} \\in \\mathbb{R}^{d \\times 1}, \\quad a \\in \\mathbb{R}^{\\top}$ |  |\n| mLSTM (Beck et al., 2024; Peng et al., 2021) | $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{\\top} \\mathbf{1}, \\quad \\gamma_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\gamma}\\right)$ | $\\boldsymbol{W}_{\\gamma} \\in \\mathbb{R}^{d \\times 1}$, |  |\n| Gated Retention (Sun et al., 2024) | $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{\\top} \\mathbf{1}, \\quad \\gamma_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\gamma}\\right)^{\\frac{1}{\\tau}}$ | $\\boldsymbol{W}_{\\gamma} \\in \\mathbb{R}^{d \\times 1}$ |  |\n| DFW (Mao, 2022; Pramanik et al., 2023) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\boldsymbol{\\beta}_{t}, \\quad \\boldsymbol{\\alpha}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha}\\right), \\quad \\boldsymbol{\\beta}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\beta}\\right)$ | $\\boldsymbol{W}_{\\alpha} \\in \\mathbb{R}^{d \\times d_{k}}, \\quad \\boldsymbol{W}_{\\beta} \\in \\mathbb{R}^{d \\times d_{v}}$ |  |\n| GateLoop (Katsch, 2023) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\quad \\boldsymbol{\\alpha}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{1}}\\right) \\exp \\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{2}} \\mathbf{i}\\right)$ | $\\boldsymbol{W}_{\\alpha_{1}} \\in \\mathbb{R}^{d \\times d_{k}}, \\quad \\boldsymbol{W}_{\\alpha_{2}} \\in \\mathbb{R}^{d \\times d_{k}}$ |  |\n| HGRN-2 (Qin et al., 2024b) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\boldsymbol{\\alpha}_{t}=\\boldsymbol{\\gamma}+(1-\\boldsymbol{\\gamma}) \\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha}\\right)$ | $\\boldsymbol{W}_{\\alpha} \\in \\mathbb{R}^{d \\times d_{k}}, \\quad \\boldsymbol{\\gamma} \\in(0,1)^{d_{k}}$ |  |\n| RWKV-6 (Peng et al., 2024) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\boldsymbol{\\alpha}_{t}=\\exp \\left(-\\exp \\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha}\\right)\\right)$ | $\\boldsymbol{W}_{\\alpha} \\in \\mathbb{R}^{d \\times d_{k}}$, | $\\boldsymbol{W}_{\\alpha_{1}} \\in \\mathbb{R}^{d \\times 16}, \\quad \\boldsymbol{W}_{\\alpha_{2}} \\in \\mathbb{R}^{16 \\times d_{k}}$ |\n| Gated Linear Attention (GLA) | $\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\mathbf{1}, \\boldsymbol{\\alpha}_{t}=\\sigma\\left(\\boldsymbol{x}_{t} \\boldsymbol{W}_{\\alpha_{1}} \\boldsymbol{W}_{\\alpha_{2}}\\right)^{\\frac{1}{\\tau}}$ |  |  |\n\nTable 1: Gated linear attention formulation of recent models, which vary in their parameterization of $\\mathbf{G}_{t}$. The bias terms are omitted. na\u00efve mapping $\\boldsymbol{x}_{t} \\mapsto \\mathbf{G}_{t}$ to obtain a data-dependent gating matrix would require a matrix of size $d \\cdot d_{k} \\cdot d_{v}$, which would be parameter-inefficient. Mao (2022) propose a more efficient outer-product-based low-rank parameterization $\\left(\\mathbf{G}_{t}=\\boldsymbol{\\alpha}_{t}^{\\top} \\boldsymbol{\\beta}_{t}\\right)$, which requires $d \\cdot d_{v}+d \\cdot d_{k}$ parameters. ${ }^{4}$\n\nIn Mamba (Gu \\& Dao, 2023), $\\mathbf{G}_{t}$ is obtained by combining a data-independent learnable matrix $\\boldsymbol{A}$ with a data-dependent vector $\\boldsymbol{\\alpha}_{t}$, which allows the matrix to be full rank. However, this prevents the use of tensor cores because it cannot be reformulated into a matrix-multiply format, as discussed in Dao \\& Gu (2024). The lack of a compact matrix-multiply form necessitates the materialization of each time step's hidden states. To reduce high I/O costs, Gu \\& Dao (2023) develop a hardware-aware algorithm that materializes the hidden states exclusively in SRAM rather than in HBM. Due to limited SRAM capacity, this approach cannot scale to larger hidden states, which, as we will show in our experiments, results in suboptimal performance on recall-intensive tasks. Mamba-2 (Dao \\& Gu, 2024) addresses this limitation with a more restricted gating mechanism: $\\mathbf{G}_{t}=\\gamma_{t} \\mathbf{1}^{T} \\mathbf{1}$, where $\\gamma_{t} \\in(0,1)$ is a scalar, which makes it possible to to reformulate the recurrence in matrix-multiply form, enabling the use of tensor cores and larger state sizes. This scalar data-dependent gating is also used in Peng et al.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: gating mechanisms hybrid neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Improving the Gating Mechanism of Recurrent Neural Networks\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, T. Paine, Matthew W. Hoffman, Razvan Pascanu*\n\n**TL;DR:** Two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation robustly improve the performance of recurrent models on a range of applications.\n\n**Abstract:** Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 2. Learning to Time-Decode in Spiking Neural Networks Through the Information Bottleneck\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*N. Skatchkovsky, O. Simeone, Hyeryung Jang*\n\n**TL;DR:** A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients and demonstrates the applicability of the technique in an experimental settings on various tasks, including real-life datasets.\n\n**Abstract:** One of the key challenges in training Spiking Neural Networks (SNNs) is that target outputs typically come in the form of natural signals, such as labels for classification or images for generative models, and need to be encoded into spikes. This is done by handcrafting target spiking signals, which in turn implicitly fixes the mechanisms used to decode spikes into natural signals, e.g., rate decoding. The arbitrary choice of target signals and decoding rule generally impairs the capacity of the SNN to encode and process information in the timing of spikes. To address this problem, this work introduces a hybrid variational autoencoder architecture, consisting of an encoding SNN and a decoding Artificial Neural Network (ANN). The role of the decoding ANN is to learn how to best convert the spiking signals output by the SNN into the target natural signal. A novel end-to-end learning rule is introduced that optimizes a directed information bottleneck training criterion via surrogate gradients. We demonstrate the applicability of the technique in an experimental settings on various tasks, including real-life datasets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 3. Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*Minmin Chen, Jeffrey Pennington, S. Schoenholz*\n\n**TL;DR:** A theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory is developed and it is shown that this theory predicts trainability for both recurrent architectures.\n\n**Abstract:** Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improvement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 112  (*Influential: 10*)\n\n#### 4. Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen*\n\n**TL;DR:** This study proposes a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate and introduces novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.\n\n**Abstract:** Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 89  (*Influential: 9*)\n\n#### 5. Can recurrent neural networks warp time?\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*Corentin Tallec, Y. Ollivier*\n\n**TL;DR:** It is proved that learnable gates in a recurrent model formally provide quasi- invariance to general time transformations in the input data, which leads to a new way of initializing gate biases in LSTMs and GRUs.\n\n**Abstract:** Successful recurrent models such as long short-term memories (LSTMs) and gated recurrent units (GRUs) use ad hoc gating mechanisms. Empirically these models have been found to improve the learning of medium to long term temporal dependencies and to help with vanishing gradient issues. We prove that learnable gates in a recurrent model formally provide quasi- invariance to general time transformations in the input data. We recover part of the LSTM architecture from a simple axiomatic approach. This result leads to a new way of initializing gate biases in LSTMs and GRUs. Ex- perimentally, this new chrono initialization is shown to greatly improve learning of long term dependencies, with minimal implementation effort.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2018\n\n**Citations:** 128  (*Influential: 13*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*ChengXiang Zhai, Chenguang Zhu, Yichong Xu, Shuohang Wang, Yang Liu, Liliang Ren*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with self-attention mechanisms have demonstrated impressive results across a range of sequence modeling tasks. However, current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. To address this limitation, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption of neural networks at both training and inference stages. To validate the effectiveness of SMA on sequence modeling, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including long sequence modeling, speech classification and language modeling, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity, and reveals the amount of attention needed for each task through the learned sparse activation patterns. Our code is publicly available at https://github.com/renll/SeqBoat.\n\n**Conference:** sparse-modular-activation-for-efficient\n\n**Published:** 2023-06-19\n\n\n\n#### 2. One-shot Learning for Question-Answering in Gaokao History Challenge\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*Zhuosheng Zhang, Hai Zhao*\n\n**Abstract:** Answering questions from university admission exams (Gaokao in Chinese) is a\nchallenging AI task since it requires effective representation to capture\ncomplicated semantic relations between questions and answers. In this work, we\npropose a hybrid neural model for deep question-answering task from history\nexaminations. Our model employs a cooperative gated neural network to retrieve\nanswers with the assistance of extra labels given by a neural turing machine\nlabeler. Empirical study shows that the labeler works well with only a small\ntraining dataset and the gated mechanism is good at fetching the semantic\nrepresentation of lengthy answers. Experiments on question answering\ndemonstrate the proposed model obtains substantial performance gains over\nvarious neural model baselines in terms of multiple evaluation metrics.\n\n**Conference:** one-shot-learning-for-question-answering-in-2\n\n**Published:** 2018-06-24\n\n\n\n#### 3. Leveraging Local and Global Patterns for Self-Attention Networks\n\n*From Search Query: gating mechanisms hybrid neural networks*\n\n*Yue Zhang, Lidia S. Chao, Baosong Yang, Mingzhou Xu, Derek F. Wong*\n\n**Abstract:** Self-attention networks have received increasing research attention. By default, the hidden states of each word are hierarchically calculated by attending to all words in the sentence, which assembles global information. However, several studies pointed out that taking all signals into account may lead to overlooking neighboring information (e.g. phrase pattern). To address this argument, we propose a hybrid attention mechanism to dynamically leverage both of the local and global information. Specifically, our approach uses a gating scalar for integrating both sources of the information, which is also convenient for quantifying their contributions. Experiments on various neural machine translation tasks demonstrate the effectiveness of the proposed method. The extensive analyses verify that the two types of contexts are complementary to each other, and our method gives highly effective improvements in their integration.\n\n**Proceeding:** acl-2019-7\n\n**Published:** 2019-07-01\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the use of gating mechanisms in hybrid architectures, here are some key technical details and insights:\n\n## Gating Mechanisms in Recurrent Neural Networks\nGating mechanisms are crucial in recurrent neural networks (RNNs) for managing the flow of information over time. In models like Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), these mechanisms help in retaining relevant information and discarding irrelevant data.\n\n- **LSTM Networks**: LSTMs use three types of gates: input gates, forget gates, and output gates. These gates control the flow of information into and out of the cell state, allowing the model to learn long-term dependencies effectively.\n- **GRU Networks**: GRUs use two gates: reset gates and update gates. These gates help in determining how much of the previous hidden state to forget and how much new information to add to the current hidden state.\n\n## Hybrid Architectures\nHybrid models that combine different types of neural networks can leverage the strengths of each component, including the gating mechanisms.\n\n- **Combining LSTM, CNN, and Transformers**: In a hybrid model integrating LSTM, CNN, and Transformer layers, the LSTM layer uses its gating mechanisms to capture long-term dependencies. The CNN layer captures local patterns, and the Transformer layer uses self-attention to focus on relationships across the entire sequence. This combination allows the model to handle complex temporal, spatial, and long-range dependencies.\n\n## Impact on Model Performance\nThe use of gating mechanisms in hybrid architectures can significantly improve model performance in several ways:\n\n- **Selective Information Flow**: Gating mechanisms enable the model to selectively retain or discard information, which helps in maintaining relevant context and reducing the impact of noise or irrelevant data.\n- **Handling Long Sequences**: By effectively managing long-term dependencies, gating mechanisms in LSTMs and GRUs help in handling long sequences, which is crucial for tasks like language modeling and time series forecasting.\n- **Expressiveness and Efficiency**: Hybrid models that incorporate gating mechanisms can achieve a balance between expressiveness and computational complexity. For example, using short-long convolutions and hierarchical processing can maintain efficiency while enhancing expressiveness.\n\n## Memory Management and Hardware Optimization\nFor practical deployment, especially with long sequences, memory management and hardware optimization are critical.\n\n- **Memory-Efficient State Tracking**: Techniques like selective state spaces and hierarchical processing can help in managing memory efficiently, allowing the model to handle longer sequences without significant computational overhead.\n- **Hardware-Efficient Implementations**: Approaches such as using short-long convolutions and divide-and-conquer methods for linear attention can help in maintaining linear complexity and optimizing hardware usage.\n\n## Future Directions\nTo further improve autoregressive language models, researchers can explore:\n\n- **Advanced Gating Mechanisms**: Investigating novel gating mechanisms or enhancing existing ones to better handle complex dependencies and context ranges.\n- **Integration with Attention Mechanisms**: Combining gating mechanisms with self-attention to leverage both local and global context effectively.\n- **Hardware Optimization**: Developing hardware-efficient implementations of hybrid models to ensure practical deployment.\n\nBy integrating these insights, researchers can design more robust, efficient, and scalable autoregressive language models that leverage the strengths of various neural network architectures.\n",
            "analysis": "Analysis Note #2\n\n1. Summary of Analysis\n\nThe second round of searches revealed important insights about hybrid architectures combining SSMs with attention mechanisms, particularly focusing on:\n- Novel approaches to selective state tracking\n- Hardware-efficient implementations\n- Techniques for combining local and global context\n- Methods for maintaining linear complexity while enhancing expressiveness\n\n2. Useful References with Excerpts\n\nFrom \"CHELA: Short-Long Convolutions Help Hardware-Efficient Linear Attention\":\n\"We leverage the thought of tiling and hierarchy to propose CHELA, which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\"\n\nFrom \"Understanding the differences in Foundation Models\":\n\"Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent.\"\n\nFrom \"Long Range Language Modeling via Gated State Spaces\":\n\"Leveraging self-attention to model local dependencies improves the performance of GSS even further... combining both of these contributions, we propose a layer named Gated State Space (GSS).\"\n\nFrom \"LOCOST: State-Space Models for Long Document\":\n\"With a computational complexity of O(L log L), this architecture can handle significantly longer sequences than state-of-the-art models that are based on sparse attention patterns.\"\n\n3. Key Insights and Analysis\n\na) Architectural Innovations:\n- Short-long convolutions can effectively replace SSMs while maintaining efficiency\n- Divide-and-conquer approaches for linear attention implementation\n- Gated mechanisms improve expressiveness without sacrificing complexity\n- Hierarchical processing enables better handling of different context ranges\n\nb) Performance Considerations:\n- Hardware-efficient implementations are crucial for practical deployment\n- Trade-offs between expressiveness and computational complexity\n- Importance of maintaining linear complexity for long sequences\n- Balance between local and global context processing\n\nc) Integration Strategies:\n- Selective activation mechanisms for dynamic computation\n- Hierarchical processing for different context ranges\n- Gating mechanisms for controlled information flow\n- Memory-efficient state tracking\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of gating mechanisms in hybrid models\n2. Memory management techniques for long sequences\n3. Hardware optimization strategies for hybrid architectures\n\n5. References\n\n1. Liu, Z., Li, S., Wang, L., Wang, Z., Liu, Y., & Li, S. Z. (2024). Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences. arXiv.org.\n\n2. Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv.org.\n\n3. Mehta, H., Gupta, A., Cutkosky, A., & Neyshabur, B. (2022). Long Range Language Modeling via Gated State Spaces. International Conference on Learning Representations.\n\n4. Le Bronnec, F., et al. (2024). LOCOST: State-Space Models for Long Document Abstractive Summarization. Conference of the European Chapter of the Association for Computational Linguistics.\n\n5. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv.org."
        },
        {
            "ready": false,
            "query": "hierarchical gating mechanisms neural networks",
            "detail": "Find technical details about hierarchical gating mechanisms in neural networks, focusing on their implementation and impact on model performance.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about hierarchical gating mechanisms in neural networks, focusing on their implementation and impact on model performance.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.98)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 1/30 (Score: 0.98)*\n\n```\n# Hierarchically Gated Recurrent Neural Network for Sequence Modeling \n\n${ }^{1}$ Zhen Qin ${ }^{\\star}, \\quad{ }^{2}$ Songlin Yang ${ }^{\\star}, \\quad{ }^{1}$ Yiran Zhong ${ }^{\\boxtimes}$<br>${ }^{1}$ OpenNLPLab, Shanghai Artificial Intelligence Laboratory, ${ }^{2}$ MIT CSAIL<br>https://github.com/OpenNLPLab/HGRN\n\n\n#### Abstract\n\nTransformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value.\n```\n\n#### 2. HGRN2: Gated Linear RNNs with State Expansion (Avg. Score: 0.95)\n\n*Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 11  (*Influential: 2*)\n\n**TL;DR:** This work introduces a simple outer-product-based state expansion mechanism so that the recurrent state size of HGRN can be significantly enlarged without introducing any additional parameters, and allows for hardware-efficient training.\n\n**Abstract:** Hierarchically gated linear RNN (HGRN,Qin et al. 2023) has demonstrated competitive training speed and performance in language modeling, while offering efficient inference. However, the recurrent state size of HGRN remains relatively small, which limits its expressiveness.To address this issue, inspired by linear attention, we introduce a simple outer-product-based state expansion mechanism so that the recurrent state size can be significantly enlarged without introducing any additional parameters. The linear attention form also allows for hardware-efficient training.Our extensive experiments verify the advantage of HGRN2 over HGRN1 in language modeling, image classification, and Long Range Arena.Our largest 3B HGRN2 model slightly outperforms Mamba and LLaMa Architecture Transformer for language modeling in a controlled experiment setting; and performs competitively with many open-source 3B models in downstream evaluation while using much fewer total training tokens.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.95)*\n\n```\n# HGRN2: Gated Linear RNNs with State Expansion \n\n${ }^{1}$ Zhen Qin ${ }^{\\dagger},{ }^{2}$ Songlin Yang ${ }^{\\dagger},{ }^{3}$ Weixuan Sun, ${ }^{3}$ Xuyang Shen, ${ }^{3}$ Dong Li, ${ }^{3}$ Weigao Sun,<br>${ }^{3}$ Yiran Zhong*<br>${ }^{1}$ TapTap ${ }^{2}$ MIT CSAIL ${ }^{3}$ OpenNLPLab, Shanghai AI Lab<br>https://github.com/OpenNLPLab/HGRN2\n\n\n#### Abstract\n\nHierarchically gated linear RNN (HGRN, Qin et al.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.95)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 9/74 (Score: 0.95)*\n\n```\nSun et al. 2023). ### 3.5 Properties of Selection Mechanisms\n\nThe selection mechanism is a broader concept that can be applied in different ways, such as to more traditional RNNs or CNNs, to different parameters (e.g. A in Algorithm 2), or using different transformations $s(x)$. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-08.jpg?height=619&width=1548&top_left_y=238&top_left_x=321)\n\nFigure 3: (Architecture.) Our simplified block design combines the H3 block, which is the basis of most SSM architectures, with the ubiquitous MLP block of modern neural networks. Instead of interleaving these two blocks, we simply repeat the Mamba block homogenously. Compared to the H3 block, Mamba replaces the first multiplicative gate with an activation function. Compared to the MLP block, Mamba adds an SSM to the main branch. For $\\sigma$ we use the SiLU / Swish activation (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017). ### 3.5.1 Connection to Gating Mechanisms\n\nWe highlight the most important connection: the classical gating mechanism of RNNs is an instance of our selection mechanism for SSMs. We note that the connection between RNN gating and the discretization of continuous-time systems is well established (Funahashi and Nakamura 1993; Tallec and Ollivier 2018). In fact, Theorem 1 is an improvement of Gu , Johnson, Goel, et al. (2021, Lemma 3.1) generalizing to the ZOH discretization and input-dependent gates (proof in Appendix C). More broadly, $\\Delta$ in SSMs can be seen to play a generalized role of the RNN gating mechanism. In line with prior work, we adopt the view that discretization of SSMs is the principled foundation of heuristic gating mechanisms. Theorem 1. When $N=1, \\boldsymbol{A}=-1, \\boldsymbol{B}=1, s_{\\Delta}=\\operatorname{Linear}(x)$, and $\\tau_{\\Delta}=$ softplus, then the selective SSM recurrence (Algorithm 2) takes the form\n\n$$\n\\begin{aligned}\n& g_{t}=\\sigma\\left(\\operatorname{Linear}\\left(x_{t}\\right)\\right) \\\\\n& h_{t}=\\left(1-g_{t}\\right) h_{t-1}+g_{t} x_{t}\n\\end{aligned}\n$$\n\nAs mentioned in Section 3.2, our specific choices of $s_{\\Delta}, \\tau_{\\Delta}$ is from this connection. In particular, note that if a given input $x_{t}$ should be completely ignored (as necessary in the synthetic tasks), all $D$ channels should ignore it, and so we project the input down to 1 dimension before repeating/broadcasting with $\\Delta$. ### 3.5.2 Interpretation of Selection Mechanisms\n\nWe elaborate on three particular mechanistic effects of selection. Variable Spacing. Selectivity allows filtering out irrelevant noise tokens that may occur between inputs of interest. This is exemplified by the Selective Copying task, but occurs ubiquitously in common data modalities, particularly for discrete data - for example the presence of language fillers such as \"um\". This property arises because the model can mechanistically filter out any particular input $x_{t}$, for example in the gated RNN case (Theorem 1 ) when $g_{t} \\rightarrow 0$. Filtering Context. It has been empirically observed that many sequence models do not improve with longer context ( F . Shi et al. 2023), despite the principle that more context should lead to strictly better performance. An explanation is that many sequence models cannot effectively ignore irrelevant context when necessary; an intuitive example are global convolutions (and general LTI models). On the other hand, selective models can simply reset their state at any time to remove extraneous history, and thus their performance in principle improves monotonicly with context length (e.g.\n```\n\n#### 4. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.91)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 39/63 (Score: 0.91)*\n\n```\nSun, and Y. Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [57] Z. Qin, S. Yang, and Y. Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [58] J. Sakarovitch. Elements of automata theory. Cambridge university press, 2009. [59] C. Sanford, D. J. Hsu, and M. Telgarsky. Representational strengths and limitations of transformers. Advances in Neural Information Processing Systems, 36, 2024.\n```\n\n#### 5. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.78)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.78)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: hierarchical gating mechanisms neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Focused Hierarchical RNNs for Conditional Sequence Processing\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Nan Rosemary Ke, Konrad Zolna, Alessandro Sordoni, Zhouhan Lin, Adam Trischler, Yoshua Bengio, Joelle Pineau, Laurent Charlin, C. Pal*\n\n**TL;DR:** A mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed and generalize significantly better on synthetic tasks as compared to the baselines is presented.\n\n**Abstract:** Recurrent Neural Networks (RNNs) with attention mechanisms have obtained state-of-the-art results for many sequence processing tasks. Most of these models use a simple form of encoder with attention that looks over the entire sequence and assigns a weight to each token independently. We present a mechanism for focusing RNN encoders for sequence modelling tasks which allows them to attend to key parts of the input as needed. We formulate this using a multi-layer conditional sequence encoder that reads in one token at a time and makes a discrete decision on whether the token is relevant to the context or question being asked. The discrete gating mechanism takes in the context embedding and the current hidden state as inputs and controls information flow into the layer above. We train it using policy gradient methods. We evaluate this method on several types of tasks with different attributes. First, we evaluate the method on synthetic tasks which allow us to evaluate the model for its generalization ability and probe the behavior of the gates in more controlled settings. We then evaluate this approach on large scale Question Answering tasks including the challenging MS MARCO and SearchQA tasks. Our models shows consistent improvements for both tasks over prior work and our baselines. It has also shown to generalize significantly better on synthetic tasks as compared to the baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 24  (*Influential: 0*)\n\n#### 2. Improving the Gating Mechanism of Recurrent Neural Networks\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, T. Paine, Matthew W. Hoffman, Razvan Pascanu*\n\n**TL;DR:** Two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation robustly improve the performance of recurrent models on a range of applications.\n\n**Abstract:** Gating mechanisms are widely used in neural network models, where they allow gradients to backpropagate more easily through depth or time. However, their saturation property introduces problems of its own. For example, in recurrent models these gates need to have outputs near 1 to propagate information over long time-delays, which requires them to operate in their saturation regime and hinders gradient-based learning of the gate mechanism. We address this problem by deriving two synergistic modifications to the standard gating mechanism that are easy to implement, introduce no additional hyperparameters, and improve learnability of the gates when they are close to saturation. We show how these changes are related to and improve on alternative recently proposed gating mechanisms such as chrono initialization and Ordered Neurons. Empirically, our simple gating mechanisms robustly improve the performance of recurrent models on a range of applications, including synthetic memorization tasks, sequential image classification, language modeling, and reinforcement learning, particularly when long-term dependencies are involved.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. GPT-ST: Generative Pre-Training of Spatio-Temporal Graph Neural Networks\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Zhonghang Li, Lianghao Xia, Yong Xu, Chao Huang*\n\n**TL;DR:** This work introduces a spatio-temporal pre- training framework that seamlessly integrates with downstream baselines and enhances their performance, and introduces an adaptive mask strategy as part of the pre-training mechanism.\n\n**Abstract:** In recent years, there has been a rapid development of spatio-temporal prediction techniques in response to the increasing demands of traffic management and travel planning. While advanced end-to-end models have achieved notable success in improving predictive performance, their integration and expansion pose significant challenges. This work aims to address these challenges by introducing a spatio-temporal pre-training framework that seamlessly integrates with downstream baselines and enhances their performance. The framework is built upon two key designs: (i) We propose a spatio-temporal mask autoencoder as a pre-training model for learning spatio-temporal dependencies. The model incorporates customized parameter learners and hierarchical spatial pattern encoding networks. These modules are specifically designed to capture spatio-temporal customized representations and intra- and inter-cluster region semantic relationships, which have often been neglected in existing approaches. (ii) We introduce an adaptive mask strategy as part of the pre-training mechanism. This strategy guides the mask autoencoder in learning robust spatio-temporal representations and facilitates the modeling of different relationships, ranging from intra-cluster to inter-cluster, in an easy-to-hard training manner. Extensive experiments conducted on representative benchmarks demonstrate the effectiveness of our proposed method. We have made our model implementation publicly available at https://github.com/HKUDS/GPT-ST.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 4. Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Minmin Chen, Jeffrey Pennington, S. Schoenholz*\n\n**TL;DR:** A theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory is developed and it is shown that this theory predicts trainability for both recurrent architectures.\n\n**Abstract:** Recurrent neural networks have gained widespread use in modeling sequence data across various domains. While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood. We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory. To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs. Our theory allows us to define a maximum timescale over which RNNs can remember an input. We show that this theory predicts trainability for both recurrent architectures. We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings. Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs. We show that this results in significantly improvement in training dynamics. Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 112  (*Influential: 10*)\n\n#### 5. Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen*\n\n**TL;DR:** This study proposes a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate and introduces novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.\n\n**Abstract:** Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 89  (*Influential: 9*)\n\n### 3 related papers from Papers with Code\n\n#### 1. X-LoRA: Mixture of Low-Rank Adapter Experts, a Flexible Framework for Large Language Models with Applications in Protein Mechanics and Molecular Design\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Markus J. Buehler, Eric L. Buehler*\n\n**Abstract:** We report a mixture of expert strategy to create fine-tuned large language models using a deep layer-wise token-level approach based on low-rank adaptation (LoRA). Starting with a set of pre-trained LoRA adapters, our gating strategy uses the hidden states to dynamically mix adapted layers, allowing the resulting X-LoRA model to draw upon different capabilities and create never-before-used deep layer-wise combinations to solve tasks. The design is inspired by the biological principles of universality and diversity, where neural network building blocks are reused in different hierarchical manifestations. Hence, the X-LoRA model can be easily implemented for any existing large language model (LLM) without a need for modifications of the underlying structure. We develop a tailored X-LoRA model that offers scientific capabilities including forward/inverse analysis tasks and enhanced reasoning capability, focused on biomaterial analysis, protein mechanics and design. The impact of this work include access to readily expandable and adaptable models with strong domain knowledge and the capability to integrate across areas of knowledge. Featuring experts in biology, mathematics, reasoning, bio-inspired materials, mechanics and materials, chemistry, protein biophysics, mechanics and quantum-mechanics based molecular properties, we conduct a series of physics-focused case studies. We examine knowledge recall, protein mechanics forward/inverse tasks, protein design, adversarial agentic modeling including ontological knowledge graph construction, as well as molecular design. The model is capable not only of making quantitative predictions of nanomechanical properties of proteins or quantum mechanical molecular properties, but also reasons over the results and correctly predicts likely mechanisms that explain distinct molecular behaviors.\n\n**Published:** 2024-02-11\n\n\n\n#### 2. Hierarchical Attentional Hybrid Neural Networks for Document Classification\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Cleber Zanchettin, David Mac\u00eado, Luis Fred, Jader Abreu*\n\n**Abstract:** Document classification is a challenging task with important applications. The deep learning approaches to the problem have gained much attention recently. Despite the progress, the proposed models do not incorporate the knowledge of the document structure in the architecture efficiently and not take into account the contexting importance of words and sentences. In this paper, we propose a new approach based on a combination of convolutional neural networks, gated recurrent units, and attention mechanisms for document classification tasks. The main contribution of this work is the use of convolution layers to extract more meaningful, generalizable and abstract features by the hierarchical representation. The proposed method in this paper improves the results of the current attention-based approaches for document classification.\n\n**Published:** 2019-01-20\n\n\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling\n\n*From Search Query: hierarchical gating mechanisms neural networks*\n\n*Anonymous*\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling.\nRecently, there has been a renewed interest in using linear RNNs for efficient sequence modeling.\nThese linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-09-21\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design by incorporating hierarchical gating mechanisms, here are some key technical details and insights from the available sources and the analysis provided:\n\n## Hierarchical Gating Mechanisms\n\n### Implementation Details\n\n1. **Combining Local and Global Context**:\n   Hierarchical gating mechanisms can be implemented by combining local and global context. For example, models like Griffin propose mixing gated linear recurrences with local attention. This approach allows the model to capture both local dependencies and global context efficiently.\n\n   ```python\n   log a_t = log a^(cr_t) = log \u03c3(\u039b)^(cr_t) = -csoftplus(\u039b) \u2299 r_t\n   ```\n   This formulation shows how gated mechanisms can be integrated with attention to manage information flow effectively.\n\n2. **Data-Dependent Gates**:\n   Models such as Gated Linear Attention Transformers (GLA) use data-dependent gates for selective information processing. The 2D forget gate \\(G_t \\in (0,1)^{d_k \\times d_v}\\) varies over time, allowing the model to dynamically adjust the information flow based on the input data.\n\n3. **Efficient Parameter Sharing**:\n   To balance expressiveness with computational efficiency, hierarchical gating mechanisms can implement efficient parameter sharing. For instance, using structured matrices and parallel scan operations can optimize the computation and memory usage.\n\n### Impact on Model Performance\n\n1. **Improved State Tracking**:\n   Hierarchical gating mechanisms enhance the state tracking capabilities of the model. By managing the flow of information through multiple layers, these mechanisms help in maintaining relevant context over long sequences, which is crucial for autoregressive language models.\n\n2. **Efficient Information Flow**:\n   The use of hierarchical gating ensures that the model can selectively process and retain relevant information. This selective processing reduces the interference of irrelevant information and improves the overall performance of the model.\n\n3. **Computational Efficiency**:\n   Implementing hierarchical gating mechanisms with hardware-efficient designs, such as using residual connections and multi-attention mechanisms, can significantly reduce computational overhead. For example, the Multi-Hierarchical Complementary Feature Interaction Network (MHCFIN) demonstrates improved performance with reduced computational complexity by leveraging residual learning and multi-attention mechanisms.\n\n## Architectural Innovations\n\n1. **Hybrid Designs**:\n   Combining recurrent and attention mechanisms in a hybrid architecture can leverage the strengths of both. Models like Griffin and GLA show that such hybrid designs can match or even surpass the performance of state-of-the-art models while being more efficient.\n\n2. **Selective Activation**:\n   Hierarchical gating mechanisms can be used to implement selective activation, where only relevant parts of the network are activated based on the input. This approach can lead to more efficient processing and better performance on long sequences.\n\n3. **Memory-Efficient State Management**:\n   By using hierarchical gating to manage state information, models can achieve memory-efficient state tracking. This is particularly important for autoregressive language models that need to process long sequences without significant memory overhead.\n\n## Future Directions\n\n1. **Specific Implementations**:\n   Investigating specific implementations of hierarchical gating mechanisms, such as those in Griffin and GLA, can provide detailed insights into how these mechanisms can be optimized for autoregressive language models.\n\n2. **Hardware Optimization**:\n   Exploring hardware optimization techniques, such as using structured matrices and parallel scan operations, can help in making these models more efficient for real-world applications.\n\n3. **Memory Management**:\n   Developing memory management strategies that leverage hierarchical gating mechanisms will be crucial for handling long sequences efficiently.\n\nBy focusing on these aspects, researchers can design more advanced autoregressive language models that combine the benefits of hierarchical gating mechanisms with efficient computational and memory management strategies.\n",
            "analysis": "Analysis Note #3\n\n1. Summary of Analysis\n\nAfter three rounds of searches, a clear direction for improving FastTTTLinear emerges by focusing on advanced gating mechanisms and hybrid architectures. The research reveals opportunities to enhance state tracking capabilities while maintaining computational efficiency through:\n- Novel gating mechanisms that combine local and global context\n- Hardware-efficient implementations of hybrid attention\n- Improved information flow between layers\n- Memory-efficient state management\n\n2. Useful References with Excerpts\n\nFrom \"Griffin: Mixing Gated Linear Recurrences with Local Attention\":\n\"We propose Hawk, an RNN with gated linear recurrences, and Griffin, a hybrid model that mixes gated linear recurrences with local attention... Griffin matches the performance of Llama-2 despite being trained on over 6 times fewer tokens.\"\n\nKey implementation detail:\n```python\nlog a_t = log a^(cr_t) = log \u03c3(\u039b)^(cr_t) = -csoftplus(\u039b) \u2299 r_t\n```\n\nFrom \"Mamba: Linear-Time Sequence Modeling\":\n\"The selection mechanism is a broader concept that can be applied in different ways... Variable Spacing allows filtering out irrelevant noise tokens that may occur between inputs of interest.\"\n\nFrom \"Gated Linear Attention Transformers\":\n\"GLA has a 2D forget gate G_t \u2208 (0,1)^(d_k \u00d7 d_v) that varies over time... Central to the design of gated linear attention is the parameterization of G_t which requires a balance between parameter-efficiency, state size, and training efficiency.\"\n\n3. Key Insights and Analysis\n\na) Advanced Gating Mechanisms:\n- Combining local and global context through hierarchical gating\n- Using data-dependent gates for selective information processing\n- Implementing efficient parameter sharing in gating mechanisms\n- Balancing expressiveness with computational efficiency\n\nb) Hardware Optimization:\n- Leveraging structured matrices for efficient computation\n- Using parallel scan operations for faster processing\n- Implementing memory-efficient state management\n- Optimizing for different hardware architectures\n\nc) Architectural Innovations:\n- Hybrid designs combining recurrent and attention mechanisms\n- Selective activation for efficient processing\n- Hierarchical information flow between layers\n- Memory-efficient state tracking\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific implementations of hierarchical gating mechanisms\n2. Hardware optimization techniques for hybrid architectures\n3. Memory management strategies for long sequences\n\n5. References\n\n1. De, S., Smith, S. L., Fernando, A., Botev, A., Cristian-Muraru, G., Gu, A., ... & Gulcehre, C. (2024). Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models. arXiv preprint arXiv:2401.09417.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n3. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint arXiv:2302.16891.\n\n4. Chen, M., Pennington, J., & Schoenholz, S. (2018). Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. International Conference on Machine Learning.\n\n5. Tallec, C., & Ollivier, Y. (2018). Can recurrent neural networks warp time? International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "\"test-time training hierarchical\", \"bounded forget gates implementation\", \"gated architecture stability\"",
            "detail": "Search for papers and technical documents discussing:\n1. Test-time training methods specifically designed for or applied to hierarchical neural architectures\n2. Implementation details and analysis of bounded forget gates in neural networks\n3. Stability analysis and training techniques for hierarchical gated architectures\n4. Methods for combining test-time adaptation with gated structures\n5. Theoretical analysis of hierarchical gating mechanisms",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n1. Test-time training methods specifically designed for or applied to hierarchical neural architectures\n2. Implementation details and analysis of bounded forget gates in neural networks\n3. Stability analysis and training techniques for hierarchical gated architectures\n4. Methods for combining test-time adaptation with gated structures\n5. Theoretical analysis of hierarchical gating mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.25)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 1/30 (Score: 0.39)*\n\n```\n# Hierarchically Gated Recurrent Neural Network for Sequence Modeling \n\n${ }^{1}$ Zhen Qin ${ }^{\\star}, \\quad{ }^{2}$ Songlin Yang ${ }^{\\star}, \\quad{ }^{1}$ Yiran Zhong ${ }^{\\boxtimes}$<br>${ }^{1}$ OpenNLPLab, Shanghai Artificial Intelligence Laboratory, ${ }^{2}$ MIT CSAIL<br>https://github.com/OpenNLPLab/HGRN\n\n\n#### Abstract\n\nTransformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value.\n```\n\n##### *Relevant Chunk: No. 12/30 (Score: 0.11)*\n\n```\nZenodo, Sept. 2021. [17] Felix A. Gers, J\u00fcrgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000. [18] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571-575, 2021. [19] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, and J\u00fcrgen Schmidhuber. Lstm: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:2222-2232, 2015. [20] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [23] Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3800-3809. PMLR, 2020. [24] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572-585, 2021. [25] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers, 2021. [26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [27] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns. CoRR, abs/2212.00768, 2022. [28] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyu He and Marko Kabic. A unified view of long-sequence models towards modeling million-scale dependencies. CoRR, abs/2302.06218, 2023. [30] Sepp Hochreiter and Yoshua Bengio. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n```\n\n#### 2. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.20)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.20)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n#### 3. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.12)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.12)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 4. Mechanistic Design and Scaling of Hybrid Architectures (Avg. Score: 0.11)\n\n*Michael Poli, Armin W. Thomas, Eric Nguyen, Pragaash Ponnusamy, Bjorn Deiseroth, K. Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R'e, Ce Zhang, Stefano Massaroli*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** Results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n**Abstract:** The development of deep learning architectures is a resource-demanding process, due to a vast design space, long prototyping times, and high compute costs associated with at-scale model training and evaluation. We set out to simplify this process by grounding it in an end-to-end mechanistic architecture design (MAD) pipeline, encompassing small-scale capability unit tests predictive of scaling laws. Through a suite of synthetic token manipulation tasks such as compression and recall, designed to probe capabilities, we identify and test new hybrid architectures constructed from a variety of computational primitives. We experimentally validate the resulting architectures via an extensive compute-optimal and a new state-optimal scaling law analysis, training over 500 language models between 70M to 7B parameters. Surprisingly, we find MAD synthetics to correlate with compute-optimal perplexity, enabling accurate evaluation of new architectures via isolated proxy tasks. The new architectures found via MAD, based on simple ideas such as hybridization and sparsity, outperform state-of-the-art Transformer, convolutional, and recurrent architectures (Transformer++, Hyena, Mamba) in scaling, both at compute-optimal budgets and in overtrained regimes. Overall, these results provide evidence that performance on curated synthetic tasks can be predictive of scaling laws, and that an optimal architecture should leverage specialized layers via a hybrid topology.\n\n##### *Relevant Chunk: No. 19/40 (Score: 0.11)*\n\n```\non pp. 2, 3, 5, 16). [18] Ekin Aky\u00fcrek et al. \"In-Context Language Learning: Architectures and Algorithms\". In: arXiv preprint arXiv:2401.12973 (2024) (cit. on pp. 2, 16). [19] Noam Shazeer et al. \"Outrageously large neural networks: The sparsely-gated mixture-of-experts layer\". In: arXiv preprint arXiv:1701.06538 (2017) (cit. on pp. 2, 7). [20] Xuezhe Ma et al. \"Mega: moving average equipped gated attention\". In: arXiv preprint arXiv:2209.10655 (2022) (cit.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"test-time training hierarchical\", \"bounded forget gates implementation\", \"gated architecture stability\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Test-Time Amendment with a Coarse Classifier for Fine-Grained Classification\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Kanishk Jain, Shyamgopal Karthik, Vineet Gandhi*\n\n**TL;DR:** A novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse- grained predictions to significantly reduce mistake severity.\n\n**Abstract:** We investigate the problem of reducing mistake severity for fine-grained classification. Fine-grained classification can be challenging, mainly due to the requirement of domain expertise for accurate annotation. However, humans are particularly adept at performing coarse classification as it requires relatively low levels of expertise. To this end, we present a novel approach for Post-Hoc Correction called Hierarchical Ensembles (HiE) that utilizes label hierarchy to improve the performance of fine-grained classification at test-time using the coarse-grained predictions. By only requiring the parents of leaf nodes, our method significantly reduces avg. mistake severity while improving top-1 accuracy on the iNaturalist-19 and tieredImageNet-H datasets, achieving a new state-of-the-art on both benchmarks. We also investigate the efficacy of our approach in the semi-supervised setting. Our approach brings notable gains in top-1 accuracy while significantly decreasing the severity of mistakes as training data decreases for the fine-grained classes. The simplicity and post-hoc nature of HiE renders it practical to be used with any off-the-shelf trained model to improve its predictions further.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Hierarchical Decomposition of Prompt-Based Continual Learning: Rethinking Obscured Sub-optimality\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Liyuan Wang, Jingyi Xie, Xingxing Zhang, Mingyi Huang, Hang Su, Jun Zhu*\n\n**TL;DR:** Hierarchical Decomposition (HiDe-)Prompt is proposed, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy.\n\n**Abstract:** Prompt-based continual learning is an emerging direction in leveraging pre-trained knowledge for downstream continual learning, and has almost reached the performance pinnacle under supervised pre-training. However, our empirical research reveals that the current strategies fall short of their full potential under the more realistic self-supervised pre-training, which is essential for handling vast quantities of unlabeled data in practice. This is largely due to the difficulty of task-specific knowledge being incorporated into instructed representations via prompt parameters and predicted by uninstructed representations at test time. To overcome the exposed sub-optimality, we conduct a theoretical analysis of the continual learning objective in the context of pre-training, and decompose it into hierarchical components: within-task prediction, task-identity inference, and task-adaptive prediction. Following these empirical and theoretical insights, we propose Hierarchical Decomposition (HiDe-)Prompt, an innovative approach that explicitly optimizes the hierarchical components with an ensemble of task-specific prompts and statistics of both uninstructed and instructed representations, further with the coordination of a contrastive regularization strategy. Our extensive experiments demonstrate the superior performance of HiDe-Prompt and its robustness to pre-training paradigms in continual learning (e.g., up to 15.01% and 9.61% lead on Split CIFAR-100 and Split ImageNet-R, respectively). Our code is available at \\url{https://github.com/thu-ml/HiDe-Prompt}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 9*)\n\n#### 3. Hierarchical Sketch Induction for Paraphrase Generation\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Tom Hosking, Hao Tang, Mirella Lapata*\n\n**TL;DR:** Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity, is introduced.\n\n**Abstract:** We propose a generative model of paraphrase generation, that encourages syntactic diversity by conditioning on an explicit syntactic sketch. We introduce Hierarchical Refinement Quantized Variational Autoencoders (HRQ-VAE), a method for learning decompositions of dense encodings as a sequence of discrete latent variables that make iterative refinements of increasing granularity. This hierarchy of codes is learned through end-to-end training, and represents fine-to-coarse grained information about the input. We use HRQ-VAE to encode the syntactic form of an input sentence as a path through the hierarchy, allowing us to more easily predict syntactic sketches at test time. Extensive experiments, including a human evaluation, confirm that HRQ-VAE learns a hierarchical representation of the input space, and generates paraphrases of higher quality than previous systems.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 29  (*Influential: 8*)\n\n#### 4. Optimized Covariance Design for AB Test on Social Network under Interference\n\n*From Search Query: \"bounded forget gates implementation\"*\n\n*Qianyi Chen, Bo Li, Lu Deng, Yong Wang*\n\n**TL;DR:** This paper derives the bias and variance of the standard HT estimator and reveals their relation to the network topological structure and the covariance of the treatment assignment vector, and proposes to formulate the experimental design problem to optimize the covariANCE matrix of thetreatment assignment vector to achieve the biases and variance balance.\n\n**Abstract:** Online A/B tests have become increasingly popular and important for social platforms. However, accurately estimating the global average treatment effect (GATE) has proven to be challenging due to network interference, which violates the Stable Unit Treatment Value Assumption (SUTVA) and poses a great challenge to experimental design. Existing network experimental design research was mostly based on the unbiased Horvitz-Thompson (HT) estimator with substantial data trimming to ensure unbiasedness at the price of high resultant estimation variance. In this paper, we strive to balance the bias and variance in designing randomized network experiments. Under a potential outcome model with 1-hop interference, we derive the bias and variance of the standard HT estimator and reveal their relation to the network topological structure and the covariance of the treatment assignment vector. We then propose to formulate the experimental design problem to optimize the covariance matrix of the treatment assignment vector to achieve the bias and variance balance by minimizing a well-crafted upper bound of the mean squared error (MSE) of the estimator, which allows us to decouple the unknown interference effect component and the experimental design component. An efficient projected gradient descent algorithm is presented to implement the desired randomization scheme. Finally, we carry out extensive simulation studies 2 to demonstrate the advantages of our proposed method over other existing methods in many settings, with different levels of model misspecification.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Uncertainty-based Continual Learning with Adaptive Regularization\n\n*From Search Query: \"bounded forget gates implementation\"*\n\n*Hongjoon Ahn, Donggyu Lee, Sungmin Cha, Taesup Moon*\n\n**TL;DR:** This paper introduces a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference and proposes the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization.\n\n**Abstract:** We introduce a new neural network-based continual learning algorithm, dubbed as Uncertainty-regularized Continual Learning (UCL), which builds on traditional Bayesian online learning framework with variational inference. We focus on two significant drawbacks of the recently proposed regularization-based methods: a) considerable additional memory cost for determining the per-weight regularization strengths and b) the absence of gracefully forgetting scheme, which can prevent performance degradation in learning new tasks. In this paper, we show UCL can solve these two problems by introducing a fresh interpretation on the Kullback-Leibler (KL) divergence term of the variational lower bound for Gaussian mean-field approximation. Based on the interpretation, we propose the notion of node-wise uncertainty, which drastically reduces the number of additional parameters for implementing per-weight regularization. Moreover, we devise two additional regularization terms that enforce stability by freezing important parameters for past tasks and allow plasticity by controlling the actively learning parameters for a new task. Through extensive experiments, we show UCL convincingly outperforms most of recent state-of-the-art baselines not only on popular supervised learning benchmarks, but also on challenging lifelong reinforcement learning tasks. The source code of our algorithm is available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 178  (*Influential: 14*)\n\n#### 6. Accelerating Transformer Pre-Training with 2: 4 Sparsity\n\n*From Search Query: \"gated architecture stability\"*\n\n*Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu*\n\n**TL;DR:** This work comprehensively investigates the feasibility of accelerating feed-forward networks of transformers in pre-training and devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss.\n\n**Abstract:** Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 7. Catformer: Designing Stable Transformers via Sensitivity Analysis\n\n*From Search Query: \"gated architecture stability\"*\n\n*Jared Davis, Albert Gu, K. Choromanski, Tri Dao, C. R\u00e9, Chelsea Finn, Percy Liang*\n\n**TL;DR:** It is proved that Catformers are less sensitive than other Transformer variants and demonstrate that this leads to more stable training, and on DMLab30, a suite of high-dimension reinforcement tasks, Catformer outperforms other transformers, including Gated Transformer-XL.\n\n**Abstract:** Transformer architectures are widely used, but training them is non-trivial, requiring custom learning rate schedules, scaling terms, residual connections, careful placement of submodules such as normalization, and so on. In this paper, we improve upon recent analysis of Transformers and formalize a notion of sensitivity to capture the difficulty of training. Sensitivity characterizes how the variance of activation and gradient norms change in expectation when parameters are randomly perturbed. We analyze the sensitivity of previous Transformer architectures and design a new architecture, the Catformer, which replaces residual connections or RNN-based gating mechanisms with concatenation. We prove that Catformers are less sensitive than other Transformer variants and demonstrate that this leads to more stable training. On DMLab30, a suite of high-dimension reinforcement tasks, Catformer outperforms other transformers, including Gated Transformer-XL\u2014the state-of-the-art architecture designed to address stability\u2014by 13%.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 12  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. Learning Syntax Without Planting Trees: Understanding When and Why\n  Transformers Generalize Hierarchically\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Kabir Ahuja, Vidhisha Balachandran, Madhur Panwar, Tianxing He, Noah A. Smith, Navin Goyal, Yulia Tsvetkov*\n\n**Abstract:** Transformers trained on natural language data have been shown to learn its\nhierarchical structure and generalize to sentences with unseen syntactic\nstructures without explicitly encoding any structural bias. In this work, we\ninvestigate sources of inductive bias in transformer models and their training\nthat could cause such generalization behavior to emerge. We extensively\nexperiment with transformer models trained on multiple synthetic datasets and\nwith different training objectives and show that while other objectives e.g.\nsequence-to-sequence modeling, prefix language modeling, often failed to lead\nto hierarchical generalization, models trained with the language modeling\nobjective consistently learned to generalize hierarchically. We then conduct\npruning experiments to study how transformers trained with the language\nmodeling objective encode hierarchical structure. When pruned, we find joint\nexistence of subnetworks within the model with different generalization\nbehaviors (subnetworks corresponding to hierarchical structure and linear\norder). Finally, we take a Bayesian perspective to further uncover\ntransformers' preference for hierarchical generalization: We establish a\ncorrelation between whether transformers generalize hierarchically on a dataset\nand whether the simplest explanation of that dataset is provided by a\nhierarchical grammar compared to regular grammars exhibiting linear\ngeneralization.\n\n**Published:** 2024-04-25T07:10:29Z  (*Updated: 2024-05-31T23:47:15Z*)\n\n\n\n#### 2. HIBERT: Document Level Pre-training of Hierarchical Bidirectional\n  Transformers for Document Summarization\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Xingxing Zhang, Furu Wei, Ming Zhou*\n\n**Abstract:** Neural extractive summarization models usually employ a hierarchical encoder\nfor document encoding and they are trained using sentence-level labels, which\nare created heuristically using rule-based methods. Training the hierarchical\nencoder with these \\emph{inaccurate} labels is challenging. Inspired by the\nrecent work on pre-training transformer sentence encoders\n\\cite{devlin:2018:arxiv}, we propose {\\sc Hibert} (as shorthand for {\\bf\nHI}erachical {\\bf B}idirectional {\\bf E}ncoder {\\bf R}epresentations from {\\bf\nT}ransformers) for document encoding and a method to pre-train it using\nunlabeled data. We apply the pre-trained {\\sc Hibert} to our summarization\nmodel and it outperforms its randomly initialized counterpart by 1.25 ROUGE on\nthe CNN/Dailymail dataset and by 2.0 ROUGE on a version of New York Times\ndataset. We also achieve the state-of-the-art performance on these two\ndatasets.\n\n**Published:** 2019-05-16T07:20:21Z  (*Updated: 2019-05-16T07:20:21Z*)\n\n\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling\n\n*From Search Query: \"bounded forget gates implementation\"*\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior\nabilities in parallel training and long-term dependency modeling. Recently,\nthere has been a renewed interest in using linear RNNs for efficient sequence\nmodeling. These linear RNNs often employ gating mechanisms in the output of the\nlinear recurrence layer while ignoring the significance of using forget gates\nwithin the recurrence. In this paper, we propose a gated linear RNN model\ndubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes\nforget gates that are lower bounded by a learnable value. The lower bound\nincreases monotonically when moving up layers. This allows the upper layers to\nmodel long-term dependencies and the lower layers to model more local,\nshort-term dependencies. Experiments on language modeling, image\nclassification, and long-range arena benchmarks showcase the efficiency and\neffectiveness of our proposed model. The source code is available at\nhttps://github.com/OpenNLPLab/HGRN.\n\n**Published:** 2023-11-08T16:50:05Z  (*Updated: 2023-11-08T16:50:05Z*)\n\n\n\n#### 4. Gated Slot Attention for Efficient Linear-Time Sequence Modeling\n\n*From Search Query: \"bounded forget gates implementation\"*\n\n*Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, Guohong Fu*\n\n**Abstract:** Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing\ncontext-aware memory reading and adaptive forgetting to improve memory capacity\nwhile maintaining compact recurrent state size. This design greatly enhances\nboth training and inference efficiency through GLA's hardware-efficient\ntraining algorithm and reduced state size. Additionally, retaining the softmax\noperation is particularly beneficial in \"finetuning pretrained Transformers to\nRNNs\" (T2R) settings, reducing the need for extensive training from scratch.\nExtensive experiments confirm GSA's superior performance in scenarios requiring\nin-context recall and in T2R settings.\n\n**Published:** 2024-09-11T09:49:50Z  (*Updated: 2024-09-11T09:49:50Z*)\n\n\n\n#### 5. Megalodon: Efficient LLM Pretraining and Inference with Unlimited\n  Context Length\n\n*From Search Query: \"gated architecture stability\"*\n\n*Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou*\n\n**Abstract:** The quadratic complexity and weak length extrapolation of Transformers limits\ntheir ability to scale to long sequences, and while sub-quadratic solutions\nlike linear attention and state space models exist, they empirically\nunderperform Transformers in pretraining efficiency and downstream task\naccuracy. We introduce Megalodon, a neural architecture for efficient sequence\nmodeling with unlimited context length. Megalodon inherits the architecture of\nMega (exponential moving average with gated attention), and further introduces\nmultiple technical components to improve its capability and stability,\nincluding complex exponential moving average (CEMA), timestep normalization\nlayer, normalized attention mechanism and pre-norm with two-hop residual\nconfiguration. In a controlled head-to-head comparison with Llama2, Megalodon\nachieves better efficiency than Transformer in the scale of 7 billion\nparameters and 2 trillion training tokens. Megalodon reaches a training loss of\n1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code:\nhttps://github.com/XuezheMax/megalodon\n\n**Published:** 2024-04-12T20:28:14Z  (*Updated: 2024-04-16T07:27:58Z*)\n\n\n\n#### 6. Stabilising and accelerating light gated recurrent units for automatic\n  speech recognition\n\n*From Search Query: \"gated architecture stability\"*\n\n*Adel Moumen, Titouan Parcollet*\n\n**Abstract:** The light gated recurrent units (Li-GRU) is well-known for achieving\nimpressive results in automatic speech recognition (ASR) tasks while being\nlighter and faster to train than a standard gated recurrent units (GRU).\nHowever, the unbounded nature of its rectified linear unit on the candidate\nrecurrent gate induces an important gradient exploding phenomenon disrupting\nthe training process and preventing it from being applied to famous datasets.\nIn this paper, we theoretically and empirically derive the necessary conditions\nfor its stability as well as engineering mechanisms to speed up by a factor of\nfive its training time, hence introducing a novel version of this architecture\nnamed SLi-GRU. Then, we evaluate its performance both on a toy task\nillustrating its newly acquired capabilities and a set of three different ASR\ndatasets demonstrating lower word error rates compared to more complex\nrecurrent neural networks.\n\n**Published:** 2023-02-16T16:18:58Z  (*Updated: 2023-02-16T16:18:58Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. Focal Modulation Networks\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Jianfeng Gao, Lu Yuan, Xiyang Dai, Chunyuan Li, Jianwei Yang*\n\n**Abstract:** We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3. Code and checkpoints are available at https://github.com/microsoft/FocalNet.\n\n**Published:** 2022-03-22\n\n\n\n#### 2. A Survey of Unsupervised Deep Domain Adaptation\n\n*From Search Query: \"test-time training hierarchical\"*\n\n*Garrett Wilson, Diane J. Cook*\n\n**Abstract:** Deep learning has produced state-of-the-art results for a variety of tasks. While such approaches for supervised learning have performed well, they assume that training and testing data are drawn from the same distribution, which may not always be the case. As a complement to this challenge, single-source unsupervised domain adaptation can handle situations where a network is trained on labeled data from a source domain and unlabeled data from a related but different target domain with the goal of performing well at test-time on the target domain. Many single-source and typically homogeneous unsupervised deep domain adaptation approaches have thus been developed, combining the powerful, hierarchical representations from deep learning with domain adaptation to reduce reliance on potentially-costly target data labels. This survey will compare these approaches by examining alternative methods, the unique and common elements, results, and theoretical insights. We follow this with a look at application areas and open research directions.\n\n**Published:** 2018-12-06\n\n\n\n#### 3. xLSTM: Extended Long Short-Term Memory\n\n*From Search Query: \"gated architecture stability\"*\n\n*Sepp Hochreiter, Johannes Brandstetter, G\u00fcnter Klambauer, Michael Kopp, Oleksandra Prudnikova, Andreas Auer, Markus Spanring, Korbinian P\u00f6ppel, Maximilian Beck*\n\n**Abstract:** In the 1990s, the constant error carousel and gating were introduced as the central ideas of the Long Short-Term Memory (LSTM). Since then, LSTMs have stood the test of time and contributed to numerous deep learning success stories, in particular they constituted the first Large Language Models (LLMs). However, the advent of the Transformer technology with parallelizable self-attention at its core marked the dawn of a new era, outpacing LSTMs at scale. We now raise a simple question: How far do we get in language modeling when scaling LSTMs to billions of parameters, leveraging the latest techniques from modern LLMs, but mitigating known limitations of LSTMs? Firstly, we introduce exponential gating with appropriate normalization and stabilization techniques. Secondly, we modify the LSTM memory structure, obtaining: (i) sLSTM with a scalar memory, a scalar update, and new memory mixing, (ii) mLSTM that is fully parallelizable with a matrix memory and a covariance update rule. Integrating these LSTM extensions into residual block backbones yields xLSTM blocks that are then residually stacked into xLSTM architectures. Exponential gating and modified memory structures boost xLSTM capabilities to perform favorably when compared to state-of-the-art Transformers and State Space Models, both in performance and scaling.\n\n**Published:** 2024-05-07\n\n\n\n#### 4. Stabilizing Transformers for Reinforcement Learning\n\n*From Search Query: \"gated architecture stability\"*\n\n*Raia Hadsell, Siddhant M. Jayakumar, Seb Noury, Matthew M. Botvinick, Nicolas Heess, Emilio Parisotto, Raphael Lopez Kaufman, Jack W. Rae, H. Francis Song, Razvan Pascanu, Max Jaderberg, Caglar Gulcehre, Aidan Clark*\n\n**Abstract:** Owing to their ability to both effectively integrate information over long time horizons and scale to massive amounts of data, self-attention architectures have recently shown breakthrough success in natural language processing (NLP), achieving state-of-the-art results in domains such as language modeling and machine translation. Harnessing the transformer's ability to process long time horizons of information could provide a similar performance boost in partially observable reinforcement learning (RL) domains, but the large-scale transformers used in NLP have yet to be successfully applied to the RL setting. In this work we demonstrate that the standard transformer architecture is difficult to optimize, which was previously observed in the supervised learning setting but becomes especially pronounced with RL objectives. We propose architectural modifications that substantially improve the stability and learning speed of the original Transformer and XL variant. The proposed architecture, the Gated Transformer-XL (GTrXL), surpasses LSTMs on challenging memory environments and achieves state-of-the-art results on the multi-task DMLab-30 benchmark suite, exceeding the performance of an external memory architecture. We show that the GTrXL, trained using the same losses, has stability and performance that consistently matches or exceeds a competitive LSTM baseline, including on more reactive tasks where memory is less critical. GTrXL offers an easy-to-train, simple-to-implement but substantially more expressive architectural alternative to the standard multi-layer LSTM ubiquitously used for RL agents in partially observable environments.\n\n**Proceeding:** icml-2020-1\n\n**Published:** 2019-10-13\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models, especially focusing on hierarchical architectures, test-time training, bounded forget gates, and stability analysis, here are some key points and references that align with the given details:\n\n## Test-Time Training for Hierarchical Architectures\n\n- The concept of test-time training (TTT) can be adapted to hierarchical architectures by fine-tuning the model on the test data using self-supervised objectives. This approach is similar to what is described in the context of image manipulation localization, where the model adapts to unseen data through self-supervised learning during testing.\n- For hierarchical models, TTT could involve fine-tuning different layers or components based on the hierarchical structure, ensuring that each layer adapts to the test data in a manner that preserves the hierarchical representation.\n\n## Implementation Details of Bounded Forget Gates\n\n- Bounded forget gates are a key component in hierarchically gated recurrent neural networks (HGRNs). These gates are lower bounded by a learnable value, which increases monotonically across layers. This design allows upper layers to model long-term dependencies while lower layers focus on local patterns (referenced in the analysis note).\n- The implementation involves ensuring that the forget gates are constrained to prevent them from becoming too small, which helps in maintaining long-term information in higher layers.\n\n## Stability Analysis and Training Techniques for Hierarchical Gated Architectures\n\n- Stability in hierarchical gated architectures can be improved by using binary-valued gates, which enhance robustness and training stability. Careful initialization and regularization are also crucial to manage the complexity of these models (referenced in the analysis note).\n- Adaptive growth of architectures can help in managing complexity by gradually adding layers or components based on the training progress, ensuring that the model does not become too complex too quickly.\n\n## Combining Test-Time Adaptation with Gated Structures\n\n- Combining test-time training with gated structures involves integrating the self-supervised adaptation process with the hierarchical gating mechanism. For example, during test-time training, the model could adapt the gate values based on the test data, ensuring that the gates learn to retain relevant information dynamically.\n- This integration can be achieved by incorporating the self-supervised objective function into the hierarchical gating mechanism, allowing the model to fine-tune the gate values and the layer-wise representations simultaneously.\n\n## Theoretical Analysis of Hierarchical Gating Mechanisms\n\n- Theoretical analysis of hierarchical gating mechanisms involves understanding how the layer-wise progression of information retention affects the overall performance of the model. This includes analyzing how the bounded forget gates impact the ability of the model to capture long-term and short-term dependencies (referenced in the analysis note).\n- The analysis should also consider the impact of reversible connections and sparse activation patterns on the memory efficiency and computational requirements of the model.\n\nIn summary, the key insights and methods include:\n\n- **Test-Time Training**: Adapt TTT methods to hierarchical architectures by fine-tuning different layers based on self-supervised objectives.\n- **Bounded Forget Gates**: Implement gates with learnable lower bounds to ensure long-term information retention in higher layers (referenced in the analysis note).\n- **Stability Analysis**: Use binary-valued gates, careful initialization, and regularization to ensure stability. Adaptive growth can help manage complexity (referenced in the analysis note).\n- **Combining Test-Time Adaptation and Gated Structures**: Integrate self-supervised adaptation with hierarchical gating mechanisms to dynamically adjust gate values during test-time training.\n- **Theoretical Analysis**: Analyze the impact of hierarchical gating on information retention and model performance, considering memory efficiency and computational requirements (referenced in the analysis note).\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several important aspects relevant to hierarchical gating and memory-efficient architectures:\n\na) Hierarchical Architectures:\n- HGRN demonstrates success with hierarchically gated RNNs using bounded forget gates\n- Adaptive neural trees show promise in combining representation learning with hierarchical structures\n- Hierarchical pooling mechanisms can preserve structural information effectively\n\nb) Memory Efficiency:\n- Reversible architectures offer significant memory savings without performance loss\n- Sparse activation patterns can reduce computational and memory requirements\n- Switch Transformers demonstrate effective scaling with sparse activation\n\nc) Training Stability:\n- Binary-valued gates improve robustness and training stability\n- Careful initialization and regularization are crucial for hierarchical structures\n- Adaptive growth of architectures can help manage complexity\n\n2. Key Useful References with Excerpts:\n\nFrom \"Hierarchically Gated Recurrent Neural Network\":\n\"...includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers, allowing upper layers to model long-term dependencies and lower layers to model local patterns.\"\n\nFrom \"Reversible Column Networks\":\n\"Features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded.\"\n\nFrom \"CoreInfer\":\n\"Core neurons exhibit both stability and similarity in relation to the sentence's semantics... enabling zero-cost sparse inference.\"\n\n3. Key Insights:\n\na) Architectural Design:\n- Hierarchical gating should incorporate bounded forget gates\n- Layer-wise progression of information retention is beneficial\n- Reversible connections can maintain information while reducing memory usage\n\nb) Memory Management:\n- Sparse activation patterns can significantly reduce resource requirements\n- Core neuron identification can enable efficient inference\n- Reversible architectures offer promising memory-computation trade-offs\n\nc) Training Considerations:\n- Binary-valued gates improve stability\n- Hierarchical structures need careful initialization\n- Adaptive growth can help manage model complexity\n\n4. Future Search Plan:\nNeed additional information on:\n1. Integration of test-time training with hierarchical structures\n2. Specific implementations of bounded forget gates\n3. Stability analysis of hierarchical gating mechanisms\n4. Memory-efficient implementations of adaptive components\n\nNext search queries should focus on:\n1. Test-time training methods for hierarchical models\n2. Implementation details of bounded forget gates\n3. Stability analysis techniques for gated architectures\n\n5. References:\n\n[1] Qin, Z., Yang, S., & Zhong, Y. (2023). Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Neural Information Processing Systems, 36.\n\n[2] Cai, Y., Zhou, Y., Han, Q., et al. (2022). Reversible Column Networks. International Conference on Learning Representations.\n\n[3] Wang, Q., Vahidian, S., Ye, H., et al. (2024). CoreInfer: Accelerating Large Language Model Inference with Semantics-Inspired Adaptive Sparse Activation. arXiv preprint.\n\n[4] Tanno, R., Nori, A., Arulkumaran, K., et al. (2018). Adaptive Neural Trees. arXiv preprint.\n\n[5] Shazeer, N., Zoph, B., & Fedus, W. (2021). Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"hierarchical gated neural networks\", \"bounded forget gates\", \"selective state tracking\"",
            "detail": "Search for papers and technical documents discussing:\n1. Hierarchical gating mechanisms in neural networks, particularly those focusing on state tracking and information flow\n2. Implementations and analysis of bounded forget gates and their impact on gradient flow\n3. Architectures combining gating mechanisms with attention or linear transformations\n4. Training stability analysis for networks with multiple gating mechanisms\n5. Memory-efficient implementations of hierarchical neural architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n1. Hierarchical gating mechanisms in neural networks, particularly those focusing on state tracking and information flow\n2. Implementations and analysis of bounded forget gates and their impact on gradient flow\n3. Architectures combining gating mechanisms with attention or linear transformations\n4. Training stability analysis for networks with multiple gating mechanisms\n5. Memory-efficient implementations of hierarchical neural architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.79)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.79)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n#### 2. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.69)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 12/30 (Score: 0.69)*\n\n```\nZenodo, Sept. 2021. [17] Felix A. Gers, J\u00fcrgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000. [18] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571-575, 2021. [19] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, and J\u00fcrgen Schmidhuber. Lstm: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:2222-2232, 2015. [20] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [23] Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3800-3809. PMLR, 2020. [24] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572-585, 2021. [25] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers, 2021. [26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [27] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns. CoRR, abs/2212.00768, 2022. [28] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyu He and Marko Kabic. A unified view of long-sequence models towards modeling million-scale dependencies. CoRR, abs/2302.06218, 2023. [30] Sepp Hochreiter and Yoshua Bengio. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n```\n\n##### *Relevant Chunk: No. 1/30 (Score: 0.69)*\n\n```\n# Hierarchically Gated Recurrent Neural Network for Sequence Modeling \n\n${ }^{1}$ Zhen Qin ${ }^{\\star}, \\quad{ }^{2}$ Songlin Yang ${ }^{\\star}, \\quad{ }^{1}$ Yiran Zhong ${ }^{\\boxtimes}$<br>${ }^{1}$ OpenNLPLab, Shanghai Artificial Intelligence Laboratory, ${ }^{2}$ MIT CSAIL<br>https://github.com/OpenNLPLab/HGRN\n\n\n#### Abstract\n\nTransformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value.\n```\n\n#### 3. Scalable MatMul-free Language Modeling (Avg. Score: 0.52)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.52)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 4. Fully Data-Controlled Linear Recurrence for Sequence Modeling (Avg. Score: 0.43)\n\n*Tobias Katsch*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 12  (*Influential: 1*)\n\n**TL;DR:** GateLoop is developed, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet by employing data-controlled state transitions, and empirically outperforms existing models for auto-regressive language modeling.\n\n**Abstract:** Linear Recurrence has proven to be a powerful tool for modeling long sequences efficiently. In this work, we show that existing models fail to take full advantage of its potential. Motivated by this finding, we develop GateLoop, a foundational sequence model that generalizes linear recurrent models such as S4, S5, LRU and RetNet, by employing data-controlled state transitions. Utilizing this theoretical advance, GateLoop empirically outperforms existing models for auto-regressive language modeling. Our method comes with a low-cost $O(l)$ recurrent mode and an efficient $O(l \\log_{2} l)$ parallel mode making use of highly optimized associative scan implementations. Furthermore, we derive an $O(l^2)$ surrogate attention mode, revealing remarkable implications for Transformer and recently proposed architectures. Specifically, we prove that our approach can be interpreted as providing data-controlled relative-positional information to Attention. While many existing models solely rely on data-controlled cumulative sums for context aggregation, our findings suggest that incorporating data-controlled complex cumulative products may be a crucial step towards more powerful sequence models.\n\n##### *Relevant Chunk: No. 8/15 (Score: 0.43)*\n\n```\n![](https://cdn.mathpix.com/cropped/2024_09_12_539af27a11cae64ab314g-09.jpg?height=413&width=856&top_left_y=1105&top_left_x=626)\n\nFigure 8: We plot the state transitions of the trained model for a random test input batch at layers 0 and 8 . We observe structured patterns in the data-controlled state transition. While we leave interpretability for future work, we point out that these patterns indicate that the trained model deliberately utilizes the data-controlled gating of the state transition (and thus forgetting and retention of memories) by applying large varieties of magnitudes and phases. ## 6 Future Work\n\nWhile our primary focus in this paper is to establish the groundwork for constructing fully datacontrolled linear RNNs, we recognize the multitude of opportunities for future research. One avenue involves exploring the effects of different initialization strategies, amplitude- and phase-activations. Moreover, we suggest that future work should pay focus to the interpretability of the learned state transitions for gaining deeper insights into the model's inner workings. ## 7 CONCLUSION\n\nWe introduce GateLoop, a fully data-controlled linear RNN which generalizes existing linear recurrent models by leveraging data controlled gating of inputs and outputs and state transitions. While our method comes with linear runtime complexity $O(l)$, we derive an efficient parallelizable $O(l \\log l)$ training strategy utilizing parallel scans. Furthermore, GateLoop can be reformulated in an equivalent $O\\left(l^{2}\\right)$ surrogate attention mode which reveals, that its mechanism can be interpreted as providing relative positional information to Attention. Finally we validate empirically, that fully data-controlled linear recurrence is highly performant for autoregressive language modeling. ## REFERENCES\n\nGuy Blelloch. Prefix sums and their applications. Tech. rept. CMU-CS-90-190, School of Computer Science, Carnegie Mellon, 1990. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry hungry hippos: Towards language modeling with state space models, 2023. Sarthak Garg, Stephan Peitz, Udhyakumar Nallasamy, and Matthias Paulik. Jointly learning to align and translate with transformer models, 2019. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces, 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8): 1735-1780, nov 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https: //doi.org/10.1162/neco.1997.9.8.1735\n\nFeiqing Huang, Kexin Lu, Yuxi CAI, Zhen Qin, Yanwen Fang, Guangjian Tian, and Guodong Li. Encoding recurrence into transformers.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical gated neural networks\", \"bounded forget gates\", \"selective state tracking\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Focal Modulation Networks\n\n*From Search Query: \"hierarchical gated neural networks\"*\n\n*Jianwei Yang, Chunyuan Li, Jianfeng Gao*\n\n**TL;DR:** Focal modulation networks (FocalNets in short), where self-attention is completely replaced by a focal modulation mechanism for modeling token interactions in vision, exhibit clear superiority on the tasks of image classification, object detection, and segmentation.\n\n**Abstract:** We propose focal modulation networks (FocalNets in short), where self-attention (SA) is completely replaced by a focal modulation mechanism for modeling token interactions in vision. Focal modulation comprises three components: (i) hierarchical contextualization, implemented using a stack of depth-wise convolutional layers, to encode visual contexts from short to long ranges, (ii) gated aggregation to selectively gather contexts for each query token based on its content, and (iii) element-wise modulation or affine transformation to inject the aggregated context into the query. Extensive experiments show FocalNets outperform the state-of-the-art SA counterparts (e.g., Swin and Focal Transformers) with similar computational costs on the tasks of image classification, object detection, and segmentation. Specifically, FocalNets with tiny and base size achieve 82.3% and 83.9% top-1 accuracy on ImageNet-1K. After pretrained on ImageNet-22K in 224 resolution, it attains 86.5% and 87.3% top-1 accuracy when finetuned with resolution 224 and 384, respectively. When transferred to downstream tasks, FocalNets exhibit clear superiority. For object detection with Mask R-CNN, FocalNet base trained with 1\\times outperforms the Swin counterpart by 2.1 points and already surpasses Swin trained with 3\\times schedule (49.0 v.s. 48.5). For semantic segmentation with UPerNet, FocalNet base at single-scale outperforms Swin by 2.4, and beats Swin at multi-scale (50.5 v.s. 49.7). Using large FocalNet and Mask2former, we achieve 58.5 mIoU for ADE20K semantic segmentation, and 57.9 PQ for COCO Panoptic Segmentation. Using huge FocalNet and DINO, we achieved 64.3 and 64.4 mAP on COCO minival and test-dev, respectively, establishing new SoTA on top of much larger attention-based models like Swinv2-G and BEIT-3. Code and checkpoints are available at https://github.com/microsoft/FocalNet.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 187  (*Influential: 24*)\n\n#### 2. Learning long-range spatial dependencies with horizontal gated-recurrent units\n\n*From Search Query: \"hierarchical gated neural networks\"*\n\n*Drew Linsley, Junkyung Kim, Vijay Veerabadran, Thomas Serre*\n\n**TL;DR:** This work introduces the horizontal gated-recurrent unit (hGRU) to learn intrinsic horizontal connections -- both within and across feature columns, and demonstrates that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures which have orders of magnitude more free parameters.\n\n**Abstract:** Progress in deep learning has spawned great successes in many engineering applications. As a prime example, convolutional neural networks, a type of feedforward neural networks, are now approaching -- and sometimes even surpassing -- human accuracy on a variety of visual recognition tasks. Here, however, we show that these neural networks and their recent extensions struggle in recognition tasks where co-dependent visual features must be detected over long spatial ranges. We introduce the horizontal gated-recurrent unit (hGRU) to learn intrinsic horizontal connections -- both within and across feature columns. We demonstrate that a single hGRU layer matches or outperforms all tested feedforward hierarchical baselines including state-of-the-art architectures which have orders of magnitude more free parameters. We further discuss the biological plausibility of the hGRU in comparison to anatomical data from the visual cortex as well as human behavioral data on a classic contour detection task.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 144  (*Influential: 14*)\n\n#### 3. Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction\n\n*From Search Query: \"hierarchical gated neural networks\"*\n\n*Dan Xu, Wanli Ouyang, Xavier Alameda-Pineda, E. Ricci, Xiaogang Wang, N. Sebe*\n\n**TL;DR:** A hierarchical deep model is introduced which produces more rich and complementary representations and the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed to refine and robustly fuse the representations learned at different scales.\n\n**Abstract:** Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly consider- ing multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 112  (*Influential: 7*)\n\n#### 4. Multistage Fusion with Forget Gate for Multimodal Summarization in Open-Domain Videos\n\n*From Search Query: \"bounded forget gates\"*\n\n*Nayu Liu, Xian Sun, Hongfeng Yu, Wenkai Zhang, Guangluan Xu*\n\n**TL;DR:** A multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module.\n\n**Abstract:** Multimodal summarization for open-domain videos is an emerging task, aiming to generate a summary from multisource information (video, audio, transcript). Despite the success of recent multiencoder-decoder frameworks on this task, existing methods lack fine-grained multimodality interactions of multisource inputs. Besides, unlike other multimodal tasks, this task has longer multimodal sequences with more redundancy and noise. To address these two issues, we propose a multistage fusion network with the fusion forget gate module, which builds upon this approach by modeling fine-grained interactions between the modalities through a multistep fusion schema and controlling the flow of redundant information between multimodal long sequences via a forgetting module. Experimental results on the How2 dataset show that our proposed model achieves a new state-of-the-art performance. Comprehensive analysis empirically verifies the effectiveness of our fusion schema and forgetting module on multiple encoder-decoder architectures. Specially, when using high noise ASR transcripts (WER>30%), our model still achieves performance close to the ground-truth transcript model, which reduces manual annotation cost.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 45  (*Influential: 6*)\n\n#### 5. Logical Languages Accepted by Transformer Encoders with Hard Attention\n\n*From Search Query: \"bounded forget gates\"*\n\n*Pablo Barcel\u00f3, A. Kozachinskiy, A. W. Lin, Vladimir Podolskii*\n\n**TL;DR:** UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates, and it is shown that AHAT encoder can recognize all languages of their logic even when the authors enrich it with counting terms.\n\n**Abstract:** We contribute to the study of formal languages that can be recognized by transformer encoders. We focus on two self-attention mechanisms: (1) UHAT (Unique Hard Attention Transformers) and (2) AHAT (Average Hard Attention Transformers). UHAT encoders are known to recognize only languages inside the circuit complexity class ${\\sf AC}^0$, i.e., accepted by a family of poly-sized and depth-bounded boolean circuits with unbounded fan-ins. On the other hand, AHAT encoders can recognize languages outside ${\\sf AC}^0$), but their expressive power still lies within the bigger circuit complexity class ${\\sf TC}^0$, i.e., ${\\sf AC}^0$-circuits extended by majority gates. We first show a negative result that there is an ${\\sf AC}^0$-language that cannot be recognized by an UHAT encoder. On the positive side, we show that UHAT encoders can recognize a rich fragment of ${\\sf AC}^0$-languages, namely, all languages definable in first-order logic with arbitrary unary numerical predicates. This logic, includes, for example, all regular languages from ${\\sf AC}^0$. We then show that AHAT encoders can recognize all languages of our logic even when we enrich it with counting terms. We apply these results to derive new results on the expressive power of UHAT and AHAT up to permutation of letters (a.k.a. Parikh images).\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 10  (*Influential: 1*)\n\n#### 6. Turn-Level Active Learning for Dialogue State Tracking\n\n*From Search Query: \"selective state tracking\"*\n\n*Zihan Zhang, Meng Fang, Fanghua Ye, Ling Chen, Mohammad-Reza Namazi-Rad*\n\n**TL;DR:** A novel turn-level active learning framework for DST to actively select turns in dialogues to annotate and can effectively achieve comparable DST performance to traditional training approaches with significantly less annotated data.\n\n**Abstract:** Dialogue state tracking (DST) plays an important role in task-oriented dialogue systems. However, collecting a large amount of turn-by-turn annotated dialogue data is costly and inefficient. In this paper, we propose a novel turn-level active learning framework for DST to actively select turns in dialogues to annotate. Given the limited labelling budget, experimental results demonstrate the effectiveness of selective annotation of dialogue turns. Additionally, our approach can effectively achieve comparable DST performance to traditional training approaches with significantly less annotated data, which provides a more efficient way to annotate new dialogue data.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 7. Multimedia Generative Script Learning for Task Planning\n\n*From Search Query: \"selective state tracking\"*\n\n*Qingyun Wang, Manling Li, Hou Pong Chan, Lifu Huang, J. Hockenmaier, Girish V. Chowdhary, Heng Ji*\n\n**TL;DR:** This work proposes a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 5,652 tasks and 79,089 multimedia steps.\n\n**Abstract:** Goal-oriented generative script learning aims to generate subsequent steps to reach a particular goal, which is an essential task to assist robots or humans in performing stereotypical activities. An important aspect of this process is the ability to capture historical states visually, which provides detailed information that is not covered by text and will guide subsequent steps. Therefore, we propose a new task, Multimedia Generative Script Learning, to generate subsequent steps by tracking historical states in both text and vision modalities, as well as presenting the first benchmark containing 5,652 tasks and 79,089 multimedia steps. This task is challenging in three aspects: the multimedia challenge of capturing the visual states in images, the induction challenge of performing unseen tasks, and the diversity challenge of covering different information in individual steps. We propose to encode visual state changes through a selective multimedia encoder to address the multimedia challenge, transfer knowledge from previously observed tasks using a retrieval-augmented decoder to overcome the induction challenge, and further present distinct information at each step by optimizing a diversity-oriented contrastive learning objective. We define metrics to evaluate both generation and inductive quality. Experiment results demonstrate that our approach significantly outperforms strong baselines.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 9  (*Influential: 1*)\n\n### 5 related papers from ArXiv\n\n#### 1. Hierarchically Gated Recurrent Neural Network for Sequence Modeling\n\n*From Search Query: \"hierarchical gated neural networks\"*\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior\nabilities in parallel training and long-term dependency modeling. Recently,\nthere has been a renewed interest in using linear RNNs for efficient sequence\nmodeling. These linear RNNs often employ gating mechanisms in the output of the\nlinear recurrence layer while ignoring the significance of using forget gates\nwithin the recurrence. In this paper, we propose a gated linear RNN model\ndubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes\nforget gates that are lower bounded by a learnable value. The lower bound\nincreases monotonically when moving up layers. This allows the upper layers to\nmodel long-term dependencies and the lower layers to model more local,\nshort-term dependencies. Experiments on language modeling, image\nclassification, and long-range arena benchmarks showcase the efficiency and\neffectiveness of our proposed model. The source code is available at\nhttps://github.com/OpenNLPLab/HGRN.\n\n**Published:** 2023-11-08T16:50:05Z  (*Updated: 2023-11-08T16:50:05Z*)\n\n\n\n#### 2. A Hierarchical Reasoning Graph Neural Network for The Automatic Scoring\n  of Answer Transcriptions in Video Job Interviews\n\n*From Search Query: \"hierarchical gated neural networks\"*\n\n*Kai Chen, Meng Niu, Qingcai Chen*\n\n**Abstract:** We address the task of automatically scoring the competency of candidates\nbased on textual features, from the automatic speech recognition (ASR)\ntranscriptions in the asynchronous video job interview (AVI). The key challenge\nis how to construct the dependency relation between questions and answers, and\nconduct the semantic level interaction for each question-answer (QA) pair.\nHowever, most of the recent studies in AVI focus on how to represent questions\nand answers better, but ignore the dependency information and interaction\nbetween them, which is critical for QA evaluation. In this work, we propose a\nHierarchical Reasoning Graph Neural Network (HRGNN) for the automatic\nassessment of question-answer pairs. Specifically, we construct a\nsentence-level relational graph neural network to capture the dependency\ninformation of sentences in or between the question and the answer. Based on\nthese graphs, we employ a semantic-level reasoning graph attention network to\nmodel the interaction states of the current QA session. Finally, we propose a\ngated recurrent unit encoder to represent the temporal question-answer pairs\nfor the final prediction. Empirical results conducted on CHNAT (a real-world\ndataset) validate that our proposed model significantly outperforms\ntext-matching based benchmark models. Ablation studies and experimental results\nwith 10 random seeds also show the effectiveness and stability of our models.\n\n**Published:** 2020-12-22T12:27:45Z  (*Updated: 2020-12-22T12:27:45Z*)\n\n\n\n#### 3. Gated Slot Attention for Efficient Linear-Time Sequence Modeling\n\n*From Search Query: \"bounded forget gates\"*\n\n*Yu Zhang, Songlin Yang, Ruijie Zhu, Yue Zhang, Leyang Cui, Yiqiao Wang, Bolun Wang, Freda Shi, Bailin Wang, Wei Bi, Peng Zhou, Guohong Fu*\n\n**Abstract:** Linear attention Transformers and their gated variants, celebrated for\nenabling parallel training and efficient recurrent inference, still fall short\nin recall-intensive tasks compared to traditional Transformers and demand\nsignificant resources for training from scratch. This paper introduces Gated\nSlot Attention (GSA), which enhances Attention with Bounded-memory-Control\n(ABC) by incorporating a gating mechanism inspired by Gated Linear Attention\n(GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing\ncontext-aware memory reading and adaptive forgetting to improve memory capacity\nwhile maintaining compact recurrent state size. This design greatly enhances\nboth training and inference efficiency through GLA's hardware-efficient\ntraining algorithm and reduced state size. Additionally, retaining the softmax\noperation is particularly beneficial in \"finetuning pretrained Transformers to\nRNNs\" (T2R) settings, reducing the need for extensive training from scratch.\nExtensive experiments confirm GSA's superior performance in scenarios requiring\nin-context recall and in T2R settings.\n\n**Published:** 2024-09-11T09:49:50Z  (*Updated: 2024-09-11T09:49:50Z*)\n\n\n\n#### 4. Span-Selective Linear Attention Transformers for Effective and Robust\n  Schema-Guided Dialogue State Tracking\n\n*From Search Query: \"selective state tracking\"*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of\na conversation using natural language descriptions of the service schema for\ngeneralization to unseen services. Prior generative approaches which decode\nslot values sequentially do not generalize well to variations in schema, while\ndiscriminative approaches separately encode history and schema and fail to\naccount for inter-slot and intent-slot dependencies. We introduce SPLAT, a\nnovel architecture which achieves better generalization and efficiency than\nprior approaches by constraining outputs to a limited prediction space. At the\nsame time, our model allows for rich attention among descriptions and history\nwhile keeping computation costs constrained by incorporating linear-time\nattention. We demonstrate the effectiveness of our model on the Schema-Guided\nDialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon\nexisting models achieving 85.3 JGA on the SGD dataset. Further, we show\nincreased robustness on the SGD-X benchmark: our model outperforms the more\nthan 30$\\times$ larger D3ST-XXL model by 5.0 points.\n\n**Published:** 2023-06-15T17:59:31Z  (*Updated: 2023-06-15T17:59:31Z*)\n\n\n\n#### 5. Extraction of Medication Names from Twitter Using Augmentation and an\n  Ensemble of Language Models\n\n*From Search Query: \"selective state tracking\"*\n\n*Igor Kulev, Berkay K\u00f6pr\u00fc, Raul Rodriguez-Esteban, Diego Saldana, Yi Huang, Alessandro La Torraca, Elif Ozkirimli*\n\n**Abstract:** The BioCreative VII Track 3 challenge focused on the identification of\nmedication names in Twitter user timelines. For our submission to this\nchallenge, we expanded the available training data by using several data\naugmentation techniques. The augmented data was then used to fine-tune an\nensemble of language models that had been pre-trained on general-domain Twitter\ncontent. The proposed approach outperformed the prior state-of-the-art\nalgorithm Kusuri and ranked high in the competition for our selected objective\nfunction, overlapping F1 score.\n\n**Published:** 2021-11-12T11:18:46Z  (*Updated: 2021-11-12T11:18:46Z*)\n\n\n\n### 3 related papers from Papers with Code\n\n#### 1. Parallel-Data-Free Voice Conversion Using Cycle-Consistent Adversarial Networks\n\n*From Search Query: \"hierarchical gated neural networks\"*\n\n*Hirokazu Kameoka, Takuhiro Kaneko*\n\n**Abstract:** We propose a parallel-data-free voice-conversion (VC) method that can learn a\nmapping from source to target speech without relying on parallel data. The\nproposed method is general purpose, high quality, and parallel-data free and\nworks without any extra data, modules, or alignment procedure. It also avoids\nover-smoothing, which occurs in many conventional statistical model-based VC\nmethods. Our method, called CycleGAN-VC, uses a cycle-consistent adversarial\nnetwork (CycleGAN) with gated convolutional neural networks (CNNs) and an\nidentity-mapping loss. A CycleGAN learns forward and inverse mappings\nsimultaneously using adversarial and cycle-consistency losses. This makes it\npossible to find an optimal pseudo pair from unpaired data. Furthermore, the\nadversarial loss contributes to reducing over-smoothing of the converted\nfeature sequence. We configure a CycleGAN with gated CNNs and train it with an\nidentity-mapping loss. This allows the mapping function to capture sequential\nand hierarchical structures while preserving linguistic information. We\nevaluated our method on a parallel-data-free VC task. An objective evaluation\nshowed that the converted feature sequence was near natural in terms of global\nvariance and modulation spectra. A subjective evaluation showed that the\nquality of the converted speech was comparable to that obtained with a Gaussian\nmixture model-based method under advantageous conditions with parallel and\ntwice the amount of data.\n\n**Published:** 2017-11-30\n\n\n\n#### 2. Efficient Dialogue State Tracking by Selectively Overwriting Memory\n\n*From Search Query: \"selective state tracking\"*\n\n*Sang-Woo Lee, Gyuwan Kim, Sungdong Kim, Sohee Yang*\n\n**Abstract:** Recent works in dialogue state tracking (DST) focus on an open vocabulary-based setting to resolve scalability and generalization issues of the predefined ontology-based approaches. However, they are inefficient in that they predict the dialogue state at every turn from scratch. Here, we consider dialogue state as an explicit fixed-sized memory and propose a selectively overwriting mechanism for more efficient DST. This mechanism consists of two steps: (1) predicting state operation on each of the memory slots, and (2) overwriting the memory with new values, of which only a few are generated according to the predicted state operations. Our method decomposes DST into two sub-tasks and guides the decoder to focus only on one of the tasks, thus reducing the burden of the decoder. This enhances the effectiveness of training and DST performance. Our SOM-DST (Selectively Overwriting Memory for Dialogue State Tracking) model achieves state-of-the-art joint goal accuracy with 51.72% in MultiWOZ 2.0 and 53.01% in MultiWOZ 2.1 in an open vocabulary-based DST setting. In addition, we analyze the accuracy gaps between the current and the ground truth-given situations and suggest that it is a promising direction to improve state operation prediction to boost the DST performance.\n\n**Conference:** efficient-dialogue-state-tracking-by-1\n\n**Published:** 2019-11-10\n\n\n\n#### 3. Continual Dialogue State Tracking via Reason-of-Select Distillation\n\n*From Search Query: \"selective state tracking\"*\n\n*Albert Y. S. Lam, Xiao-Ming Wu, Li-Ming Zhan, Zexin Lu, Xiaoyu Dong, Bo Liu, Yujie Feng*\n\n**Abstract:** An ideal dialogue system requires continuous skill acquisition and adaptation to new tasks while retaining prior knowledge. Dialogue State Tracking (DST), vital in these systems, often involves learning new services and confronting catastrophic forgetting, along with a critical capability loss termed the \"Value Selection Quandary.\" To address these challenges, we introduce the Reason-of-Select (RoS) distillation method by enhancing smaller models with a novel 'meta-reasoning' capability. Meta-reasoning employs an enhanced multi-domain perspective, combining fragments of meta-knowledge from domain-specific dialogues during continual learning. This transcends traditional single-perspective reasoning. The domain bootstrapping process enhances the model's ability to dissect intricate dialogues from multiple possible values. Its domain-agnostic property aligns data distribution across different domains, effectively mitigating forgetting. Additionally, two novel improvements, \"multi-value resolution\" strategy and Semantic Contrastive Reasoning Selection method, significantly enhance RoS by generating DST-specific selection chains and mitigating hallucinations in teachers' reasoning, ensuring effective and reliable knowledge transfer. Extensive experiments validate the exceptional performance and robust generalization capabilities of our method. The source code is provided for reproducibility.\n\n**Published:** 2024-08-19\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design with a focus on hierarchical gated neural networks, bounded forget gates, and selective state tracking, here are some key points and relevant sources:\n\n## Hierarchical Gating Mechanisms and State Tracking\n\n- **Hierarchical Gating**: The concept of hierarchical gating is closely related to the use of multiple layers of gated recurrent units (GRUs) or Long Short-Term Memory (LSTM) networks. These architectures allow for layered processing and state tracking, which can be crucial for maintaining long-term dependencies in sequential data. The Wikipedia articles on RNNs and LSTMs provide a comprehensive overview of these mechanisms.\n\n- **Selective State Tracking**: This involves optimizing the flow of information through the network by selectively updating the state based on the input and previous states. LSTMs, with their input, output, and forget gates, are a prime example of this. The selective state tracking mechanism can be enhanced by hierarchical designs, where each layer can focus on different aspects of the input sequence.\n\n## Bounded Forget Gates and Gradient Flow\n\n- **Bounded Forget Gates**: The forget gate in LSTMs is a key component that decides what information to discard from the previous state. Bounded forget gates ensure that the gradient flow remains stable by preventing the gates from outputting extreme values (0 or 1). This is inherently addressed in the LSTM architecture, where the forget gate's activation is bounded between 0 and 1.\n\n- **Impact on Gradient Flow**: The use of bounded gates helps in preventing the vanishing or exploding gradient problem, which is crucial for training deep neural networks. This is well-documented in the context of LSTMs and GRUs, where these gates help in maintaining a stable gradient flow over long sequences.\n\n## Architectures Combining Gating Mechanisms with Attention or Linear Transformations\n\n- **Combining with Attention**: Architectures like the Transformer model, which use self-attention mechanisms, can be combined with gated recurrent units to enhance state tracking and information flow. For example, hybrid models that integrate LSTM or GRU layers with Transformer layers can leverage the strengths of both architectures.\n\n- **Linear Transformations**: The FastTTTLinear model, which the researchers are building upon, likely involves linear transformations to optimize the information flow. Integrating hierarchical gating mechanisms with these linear transformations can enhance the model's ability to capture long-term dependencies and optimize state tracking.\n\n## Training Stability Analysis\n\n- **Training Stability**: Training stability is a critical concern when dealing with multiple gating mechanisms. Research on LSTMs and GRUs has shown that these models can be stable if properly initialized and regularized. However, hierarchical designs may introduce additional complexity, requiring careful analysis of the gradient flow and the impact of each gate on the overall stability of the network.\n\n## Memory-Efficient Implementations\n\n- **Memory Efficiency**: Hierarchical neural architectures can be memory-intensive due to the need to store multiple layers of states. Research on memory-efficient implementations, such as using sparse attention mechanisms or compressing the state representations, can be beneficial. For example, the paper on \"EfficientSPARSEKFastTTT\" might provide insights into how to optimize memory usage while maintaining performance.\n\n### Specific Research Directions\n\n1. **State Tracking Capabilities**:\n   - Investigate how hierarchical gating mechanisms compare to existing state tracking solutions like LSTMs and GRUs. This can involve analyzing the effectiveness of state preservation and the impact on test-time training capabilities.\n\n2. **Computational Efficiency**:\n   - Examine the computational complexity introduced by the hierarchical gating mechanisms. Research on similar architectures, such as those using sparse attention or compressed state representations, can provide insights into optimizing memory and computational resources.\n\n3. **Architecture Design**:\n   - Optimize the configuration of hierarchical gates and their integration with existing components of the FastTTTLinear model. This involves studying the stability of training with dynamic gates and ensuring compatibility with test-time training.\n\n4. **Comparison with Previous Designs**:\n   - Compare the proposed HierarchicalGatedFastTTTLinear model with previous designs like EfficientSPARSEKFastTTT and LosslessCompressedTTTLinear. This comparison should focus on the novel elements such as hierarchical gating and selective state tracking.\n\n### Relevant Sources\n\n- For detailed explanations of LSTMs, GRUs, and their gating mechanisms: Wikipedia articles on RNNs and LSTMs.\n- For understanding the integration of attention mechanisms with gated recurrent units: Research on hybrid models combining Transformers with LSTMs or GRUs.\n- For memory-efficient implementations: Papers on sparse attention mechanisms and compressed state representations.\n\nBy focusing on these areas and leveraging the insights from existing research on gated recurrent networks, the researchers can develop a more efficient and effective autoregressive language model.\n",
            "analysis": "Initial Analysis of HierarchicalGatedFastTTTLinear Proposal:\n\n1. Key Novel Elements:\n- Integration of hierarchical gating with bounded forget gates\n- Selective state tracking mechanism\n- Layer-wise information flow optimization\n- Combination of HGRN and Griffin concepts for state tracking\n\n2. Areas Requiring Investigation:\na) State Tracking Capabilities:\n   - Effectiveness of hierarchical gating for state preservation\n   - Comparison with existing state tracking solutions\n   - Impact on test-time training capabilities\n\nb) Computational Efficiency:\n   - Overhead from gating mechanisms\n   - Memory requirements for hierarchical structure\n   - Scalability with sequence length\n\nc) Architecture Design:\n   - Optimal configuration of hierarchical gates\n   - Integration with FastTTTLinear's existing components\n   - Stability of training with dynamic gates\n\n3. Investigation Plan:\na) Phase 1: State Tracking & Gating\n   - Research on hierarchical gating mechanisms\n   - Analysis of state tracking approaches\n   - Study of bounded forget gates\n\nb) Phase 2: Efficiency & Scalability\n   - Examine computational complexity\n   - Research on memory-efficient implementations\n   - Study of similar architectures' performance\n\nc) Phase 3: Integration & Training\n   - Investigation of training stability\n   - Analysis of test-time training compatibility\n   - Research on similar hybrid architectures\n\n4. Comparison with Previous Designs:\n- Differs from EfficientSPARSEKFastTTT by focusing on hierarchical gating instead of sparse attention\n- Distinct from LosslessCompressedTTTLinear by emphasizing state tracking over compression\n- Novel combination of HGRN and Griffin concepts not seen in previous proposals\n\n5. Initial Concerns:\n- Training stability with multiple gating mechanisms\n- Potential computational overhead\n- Complexity of implementation\n- Integration with existing FastTTTLinear components\n\nThe proposal appears to offer a novel approach to enhancing FastTTTLinear, particularly in its focus on hierarchical gating and state tracking. Further investigation is needed to validate its effectiveness and efficiency claims."
        },
        {
            "ready": false,
            "query": "\"test-time training neural networks\", \"memory efficient gated architectures\", \"training stability multiple gates\"",
            "detail": "Search for papers and technical documents discussing:\n1. Test-time training methods in neural networks, particularly for sequence models\n2. Memory optimization techniques for hierarchical neural architectures\n3. Training stability analysis for networks with multiple adaptive components\n4. Implementations combining test-time adaptation with gated architectures\n5. Efficient state management in hierarchical models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n1. Test-time training methods in neural networks, particularly for sequence models\n2. Memory optimization techniques for hierarchical neural architectures\n3. Training stability analysis for networks with multiple adaptive components\n4. Implementations combining test-time adaptation with gated architectures\n5. Efficient state management in hierarchical models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Learning to (Learn at Test Time): RNNs with Expressive Hidden States (Avg. Score: 0.84)\n\n*Yu Sun, Xinhao Li, Karan Dalal, Jiarui Xu, Arjun Vikram, Genghan Zhang, Yann Dubois, Xinlei Chen, Xiaolong Wang, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin*\n\n**Published in:**  (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time, and TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n**Abstract:** Self-attention performs well in long context but has quadratic complexity. Existing RNN layers have linear complexity, but their performance in long context is limited by the expressive power of their hidden state. We propose a new class of sequence modeling layers with linear complexity and an expressive hidden state. The key idea is to make the hidden state a machine learning model itself, and the update rule a step of self-supervised learning. Since the hidden state is updated by training even on test sequences, our layers are called Test-Time Training (TTT) layers. We consider two instantiations: TTT-Linear and TTT-MLP, whose hidden state is a linear model and a two-layer MLP respectively. We evaluate our instantiations at the scale of 125M to 1.3B parameters, comparing with a strong Transformer and Mamba, a modern RNN. Both TTT-Linear and TTT-MLP match or exceed the baselines. Similar to Transformer, they can keep reducing perplexity by conditioning on more tokens, while Mamba cannot after 16k context. With preliminary systems optimization, TTT-Linear is already faster than Transformer at 8k context and matches Mamba in wall-clock time. TTT-MLP still faces challenges in memory I/O, but shows larger potential in long context, pointing to a promising direction for future research.\n\n##### *Relevant Chunk: No. 33/51 (Score: 0.84)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. [22] A. Gammerman, V. Vovk, and V. Vapnik. Learning by transduction. In In Uncertainty in Artificial Intelligence, pages 148-155. Morgan Kaufmann, 1998. [23] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei A. Efros. Test-time training with masked autoencoders. Advances in Neural Information Processing Systems, 2022. [24] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. The pile: An 800 gb dataset of diverse text for language modeling, 2020. [25] Xinyang Geng. EasyLM: A Simple And Scalable Training Framework for Large Language Models. https://github.com/young-geng/EasyLM, mar 2023. https://github.com/ young-geng/EasyLM. [26] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [27] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [28] Nicklas Hansen, Rishabh Jangir, Yu Sun, Guillem Aleny\u00e0, Pieter Abbeel, Alexei A Efros, Lerrel Pinto, and Xiaolong Wang. Self-supervised policy adaptation during deployment. arXiv preprint arXiv:2007.04309, 2020. [29] Moritz Hardt and Yu Sun. Test-time training on nearest neighbors for large language models.\n```\n\n#### 2. Scalable MatMul-free Language Modeling (Avg. Score: 0.52)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.52)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 3. Modeling Hierarchical Structures with Continuous Recursive Neural Networks  (Avg. Score: 0.23)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** International Conference on Machine Learning (2021)\t**Cited by** 11  (*Influential: 1*)\n\n**TL;DR:** This work proposes Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations of traditional RvNNs by incorporating a continuous relaxation to the induced structure.\n\n**Abstract:** Recursive Neural Networks (RvNNs), which compose sequences according to their underlying hierarchical syntactic structure, have performed well in several natural language processing tasks compared to similar models without structural biases. However, traditional RvNNs are incapable of inducing the latent structure in a plain text sequence on their own. Several extensions have been proposed to overcome this limitation. Nevertheless, these extensions tend to rely on surrogate gradients or reinforcement learning at the cost of higher bias or variance. In this work, we propose Continuous Recursive Neural Network (CRvNN) as a backpropagation-friendly alternative to address the aforementioned limitations. This is done by incorporating a continuous relaxation to the induced structure. We demonstrate that CRvNN achieves strong performance in challenging synthetic tasks such as logical inference and ListOps. We also show that CRvNN performs comparably or better than prior latent structure models on real-world tasks such as sentiment analysis and natural language inference.\n\n##### *Relevant Chunk: No. 27/40 (Score: 0.23)*\n\n```\nArXiv, abs/1606.08415, 2016. Hochreiter, S. Untersuchungen zu dynamischen neuronalen netzen. diploma thesis. Diploma thesis, TU Munich, 1991. Hochreiter, S. and Schmidhuber, J. Long short-term memory. Neural Comput., 9(8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. Jacob, A. P., Lin, Z., Sordoni, A., and Bengio, Y. Learning hierarchical structures on-the-fly with a recurrentrecursive model for sequences. In Proceedings of The Third Workshop on Representation Learning for NLP, pp. 154-158, Melbourne, Australia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/W183020 . Jang, E., Gu, S., and Poole, B. Categorical reparameterization with gumbel-softmax. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017. Kim, Y., Denton, C., Hoang, L., and Rush, A. M. Structured attention networks. International Conference on Learning Representations, 2017. Kim, Y., Rush, A., Yu, L., Kuncoro, A., Dyer, C., and Melis, G. Unsupervised recurrent neural network grammars. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 1105-1117, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: $10.18653 / \\mathrm{v} 1 / \\mathrm{N} 19-1114$. Kingma, D.\n```\n\n#### 4. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.06)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.06)*\n\n```\narXiv preprint arXiv:1711.05101, 2017. Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. arXiv preprint arXiv:2206.13947, 2022. Martial Mermillod, Aur\u00e9lia Bugaiska, and Patrick Bonin. The stability-plasticity dilemma: Investigating the continuum from catastrophic forgetting to age-limited learning effects, 2013. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 26670-26698. PMLR, 2023. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, Przemys\u0142aw Kazienko, et al. Eagle and finch: Rwkv with matrix-valued states and dynamic recurrence.\n```\n\n#### 5. Attention is All you Need (Avg. Score: 0.06)\n\n*Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin*\n\n**Published in:** Neural Information Processing Systems (2017)\t**Cited by** 98603  (*Influential: 14920*)\n\n**TL;DR:** A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n**Abstract:** The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.\n\n##### *Relevant Chunk: No. 9/19 (Score: 0.06)*\n\n```\narXiv preprint arXiv:1610.02357, 2016. [7] Junyoung Chung, \u00c7aglar G\u00fcl\u00e7ehre, Kyunghyun Cho, and Yoshua Bengio. Empirical evaluation of gated recurrent neural networks on sequence modeling. CoRR, abs/1412.3555, 2014. [8] Chris Dyer, Adhiguna Kuncoro, Miguel Ballesteros, and Noah A. Smith. Recurrent neural network grammars. In Proc. of NAACL, 2016. [9] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. arXiv preprint arXiv:1705.03122v2, 2017. [10] Alex Graves. Generating sequences with recurrent neural networks. arXiv preprint arXiv:1308.0850, 2013. [11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770-778, 2016. [12] Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J\u00fcrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2001. [13] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [14] Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"test-time training neural networks\", \"memory efficient gated architectures\", \"training stability multiple gates\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 2. GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye*\n\n**TL;DR:** GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes, and can also mitigate the degree bias for either self-supervised or supervised GNNs.\n\n**Abstract:** Recent studies have shown that graph neural networks (GNNs) exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes. Existing works tackle this problem by deriving either designated GNN architectures or training strategies specifically for low-degree nodes. Though effective, these approaches unintentionally create an artificial out-of-distribution scenario, where models mainly or even only observe low-degree nodes during the training, leading to a downgraded performance for high-degree nodes that GNNs originally perform well at. In light of this, we propose a test-time augmentation framework, namely GraphPatcher, to enhance test-time generalization of any GNNs on low-degree nodes. Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs. Comprehensive experiments are conducted over seven benchmark datasets and GraphPatcher consistently enhances common GNNs' overall performance by up to 3.6% and low-degree performance by up to 6.5%, significantly outperforming state-of-the-art baselines. The source code is publicly available at https://github.com/jumxglhf/GraphPatcher.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 3. Efficient recurrent architectures through activity sparsity and sparse back-propagation through time\n\n*From Search Query: \"memory efficient gated architectures\"*\n\n*Anand Subramoney, Khaleelulla Khan Nazeer, Mark Sch\u00f6ne, C. Mayr, D. Kappel*\n\n**TL;DR:** It is shown theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network, which means the model achieves efficiency without compromising task performance.\n\n**Abstract:** Recurrent neural networks (RNNs) are well suited for solving sequence tasks in resource-constrained systems due to their expressivity and low computational requirements. However, there is still a need to bridge the gap between what RNNs are capable of in terms of efficiency and performance and real-world application requirements. The memory and computational requirements arising from propagating the activations of all the neurons at every time step to every connected neuron, together with the sequential dependence of activations, contribute to the inefficiency of training and using RNNs. We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well. We base our model on the gated recurrent unit (GRU), extending it with units that emit discrete events for communication triggered by a threshold so that no information is communicated to other units in the absence of events. We show theoretically that the communication between units, and hence the computation required for both the forward and backward passes, scales with the number of events in the network. Our model achieves efficiency without compromising task performance, demonstrating competitive performance compared to state-of-the-art recurrent network models in real-world tasks, including language modeling. The dynamic activity sparsity mechanism also makes our model well suited for novel energy-efficient neuromorphic hardware. Code is available at https://github.com/KhaleelKhan/EvNN/.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 13  (*Influential: 1*)\n\n#### 4. Make Pre-trained Model Reversible: From Parameter to Memory Efficient Fine-Tuning\n\n*From Search Query: \"memory efficient gated architectures\"*\n\n*Baohao Liao, Shaomu Tan, C. Monz*\n\n**TL;DR:** This paper proposes memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training.\n\n**Abstract:** Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs) has emerged as a highly successful approach, with training only a small number of parameters without sacrificing performance and becoming the de-facto learning paradigm with the increasing size of PLMs. However, existing PEFT methods are not memory-efficient, because they still require caching most of the intermediate activations for the gradient calculation, akin to fine-tuning. One effective way to reduce the activation memory is to apply a reversible model, so the intermediate activations are not necessary to be cached and can be recomputed. Nevertheless, modifying a PLM to its reversible variant is not straightforward, since the reversible model has a distinct architecture from the currently released PLMs. In this paper, we first investigate what is a key factor for the success of existing PEFT methods, and realize that it's essential to preserve the PLM's starting point when initializing a PEFT method. With this finding, we propose memory-efficient fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's starting point and making it reversible without additional pre-training. We evaluate MEFT on the GLUE benchmark and five question-answering tasks with various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the activation memory up to 84% of full fine-tuning with a negligible amount of trainable parameters. Moreover, MEFT achieves the same score on GLUE and a comparable score on the question-answering tasks as full fine-tuning. A similar finding is also observed for the image classification task.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n#### 5. Towards Binary-Valued Gates for Robust LSTM Training\n\n*From Search Query: \"training stability multiple gates\"*\n\n*Zhuohan Li, Di He, Fei Tian, Wei Chen, Tao Qin, Liwei Wang, Tie-Yan Liu*\n\n**TL;DR:** A new way for LSTM training is proposed, which pushes the output values of the gates towards 0 or 1, which can better control the information flow and achieve better or comparable performances due to its better generalization ability.\n\n**Abstract:** Long Short-Term Memory (LSTM) is one of the most widely used recurrent structures in sequence modeling. It aims to use gates to control information flow (e.g., whether to skip some information or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal. In this paper, we propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable. Empirical studies show that (1) Although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) The outputs of gates are not sensitive to their inputs: we can easily compress the LSTM unit in multiple ways, e.g., low-rank approximation and low-precision approximation. The compressed models are even better than the baseline models without compression.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 46  (*Influential: 4*)\n\n#### 6. Self-Supervision Enhanced Feature Selection with Correlated Gates\n\n*From Search Query: \"training stability multiple gates\"*\n\n*Changhee Lee, F. Imrie, M. Schaar*\n\n**TL;DR:** A novel deep learning approach to feature selection that addresses both challenges simultaneously, pre-train the network using unlabeled samples within a self-supervised learning framework and tunes the pre-trained network to discover relevant features using labeled samples.\n\n**Abstract:** Discovering relevant input features for predicting a target variable is a key scien-ti\ufb01c question. However, in many domains, such as medicine and biology, feature selection is confounded by a scarcity of labeled samples coupled with signi\ufb01cant correlations among features. In this paper, we propose a novel deep learning approach to feature selection that addresses both challenges simultaneously. First, we pre-train the network using unlabeled samples within a self-supervised learning framework by solving pretext tasks that require the network to learn informative representations from partial feature sets. Then, we \ufb01ne-tune the pre-trained network to discover relevant features using labeled samples. During both training phases, we explicitly account for the correlation structure of the input features by generating correlated gate vectors from a multivariate Bernoulli distribution. Experiments on multiple real-world datasets including clinical and omics demonstrate that our model discovers relevant features that provide superior prediction performance compared to the state-of-the-art benchmarks in practical scenarios where there is often limited labeled data and high correlations among features.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 18  (*Influential: 5*)\n\n#### 7. Make Every Example Count: On Stability and Utility of Self-Influence for Learning from Noisy NLP Datasets\n\n*From Search Query: \"training stability multiple gates\"*\n\n*Irina Bejan, Artem Sokolov, Katja Filippova*\n\n**TL;DR:** The fitness of task-agnostic self-influence scores of training examples for data cleaning are studied, their efficacy in capturing naturally occurring outliers are analyzed, and to what extent self-Influence based data cleaning can improve downstream performance in machine translation, question answering and text classification are investigated.\n\n**Abstract:** Increasingly larger datasets have become a standard ingredient to advancing the state-of-the-art in NLP. However, data quality might have already become the bottleneck to unlock further gains. Given the diversity and the sizes of modern datasets, standard data filtering is not straight-forward to apply, because of the multifacetedness of the harmful data and elusiveness of filtering rules that would generalize across multiple tasks. We study the fitness of task-agnostic self-influence scores of training examples for data cleaning, analyze their efficacy in capturing naturally occurring outliers, and investigate to what extent self-influence based data cleaning can improve downstream performance in machine translation, question answering and text classification, building up on recent approaches to self-influence calculation and automated curriculum learning.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n### 6 related papers from ArXiv\n\n#### 1. Optimizing the Neural Network Training for OCR Error Correction of\n  Historical Hebrew Texts\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet*\n\n**Abstract:** Over the past few decades, large archives of paper-based documents such as\nbooks and newspapers have been digitized using Optical Character Recognition.\nThis technology is error-prone, especially for historical documents. To correct\nOCR errors, post-processing algorithms have been proposed based on natural\nlanguage analysis and machine learning techniques such as neural networks.\nNeural network's disadvantage is the vast amount of manually labeled data\nrequired for training, which is often unavailable. This paper proposes an\ninnovative method for training a light-weight neural network for Hebrew OCR\npost-correction using significantly less manually created data. The main\nresearch goal is to develop a method for automatically generating language and\ntask-specific training data to improve the neural network results for OCR\npost-correction, and to investigate which type of dataset is the most effective\nfor OCR post-correction of historical documents. To this end, a series of\nexperiments using several datasets was conducted. The evaluation corpus was\nbased on Hebrew newspapers from the JPress project. An analysis of historical\nOCRed newspapers was done to learn common language and corpus-specific OCR\nerrors. We found that training the network using the proposed method is more\neffective than using randomly generated errors. The results also show that the\nperformance of the neural network for OCR post-correction strongly depends on\nthe genre and area of the training data. Moreover, neural networks that were\ntrained with the proposed method outperform other state-of-the-art neural\nnetworks for OCR post-correction and complex spellcheckers. These results may\nhave practical implications for many digital humanities projects.\n\n**Published:** 2023-07-30T12:59:06Z  (*Updated: 2023-07-30T12:59:06Z*)\n\n\n\n#### 2. Performance of Transfer Learning Model vs. Traditional Neural Network in\n  Low System Resource Environment\n\n*From Search Query: \"test-time training neural networks\"*\n\n*William Hui*\n\n**Abstract:** Recently, the use of pre-trained model to build neural network based on\ntransfer learning methodology is increasingly popular. These pre-trained models\npresent the benefit of using less computing resources to train model with\nsmaller amount of training data. The rise of state-of-the-art models such as\nBERT, XLNet and GPT boost accuracy and benefit as a base model for transfer\nleanring. However, these models are still too complex and consume many\ncomputing resource to train for transfer learning with low GPU memory. We will\ncompare the performance and cost between lighter transfer learning model and\npurposely built neural network for NLP application of text classification and\nNER model.\n\n**Published:** 2020-10-20T08:12:56Z  (*Updated: 2020-10-20T08:12:56Z*)\n\n\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: \"memory efficient gated architectures\"*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but\ncan simultaneously be formulated as an RNN with 2D (matrix-valued) hidden\nstates, thus enjoying linear-time inference complexity. However, linear\nattention generally underperforms ordinary softmax attention. Moreover, current\nimplementations of linear attention lack I/O-awareness and are thus slower than\nhighly optimized implementations of softmax attention. This work describes a\nhardware-efficient algorithm for linear attention that trades off memory\nmovement against parallelizability. The resulting implementation, dubbed\nFLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a\nstandalone layer even on short sequence lengths (e.g., 1K). We then generalize\nthis algorithm to a more expressive variant of linear attention with\ndata-dependent gates. When used as a replacement for the standard attention\nlayer in Transformers, the resulting gated linear attention (GLA) Transformer\nis found to perform competitively against the LLaMA-architecture Transformer\n(Touvron et al., 2023) as well recent linear-time-inference baselines such as\nRetNet (Sun et al., 2023a) and Mamba (Gu & Dao, 2023) on moderate-scale\nlanguage modeling experiments. GLA Transformer is especially effective at\nlength generalization, enabling a model trained on 2K to generalize to\nsequences longer than 20K without significant perplexity degradations. For\ntraining speed, the GLA Transformer has higher throughput than a\nsimilarly-sized Mamba model.\n\n**Published:** 2023-12-11T18:51:59Z  (*Updated: 2024-08-27T01:27:29Z*)\n\n\n\n#### 4. Read-ME: Refactorizing LLMs as Router-Decoupled Mixture of Experts with\n  System Co-Design\n\n*From Search Query: \"memory efficient gated architectures\"*\n\n*Ruisi Cai, Yeonju Ro, Geon-Woo Kim, Peihao Wang, Babak Ehteshami Bejnordi, Aditya Akella, Zhangyang Wang*\n\n**Abstract:** The proliferation of large language models (LLMs) has led to the adoption of\nMixture-of-Experts (MoE) architectures that dynamically leverage specialized\nsubnetworks for improved efficiency and performance. Despite their benefits,\nMoE models face significant challenges during inference, including inefficient\nmemory management and suboptimal batching, due to misaligned design choices\nbetween the model architecture and the system policies. Furthermore, the\nconventional approach of training MoEs from scratch is increasingly prohibitive\nin terms of cost. In this paper, we propose a novel framework Read-ME that\ntransforms pre-trained dense LLMs into smaller MoE models (in contrast to\n\"upcycling\" generalist MoEs), avoiding the high costs of ground-up training.\nOur approach employs activation sparsity to extract experts. To compose\nexperts, we examine the widely-adopted layer-wise router design and show its\nredundancy, and thus we introduce the pre-gating router decoupled from the MoE\nbackbone that facilitates system-friendly pre-computing and lookahead\nscheduling, enhancing expert-aware batching and caching. Our codesign therefore\naddresses critical gaps on both the algorithmic and system fronts, establishing\na scalable and efficient alternative for LLM inference in resource-constrained\nsettings. Read-ME outperforms other popular open-source dense models of similar\nscales, achieving improvements of up to 10.1% on MMLU, and improving mean\nend-to-end latency up to 6.1%. Codes are available at:\nhttps://github.com/VITA-Group/READ-ME.\n\n**Published:** 2024-10-24T19:48:51Z  (*Updated: 2024-10-24T19:48:51Z*)\n\n\n\n#### 5. Megalodon: Efficient LLM Pretraining and Inference with Unlimited\n  Context Length\n\n*From Search Query: \"training stability multiple gates\"*\n\n*Xuezhe Ma, Xiaomeng Yang, Wenhan Xiong, Beidi Chen, Lili Yu, Hao Zhang, Jonathan May, Luke Zettlemoyer, Omer Levy, Chunting Zhou*\n\n**Abstract:** The quadratic complexity and weak length extrapolation of Transformers limits\ntheir ability to scale to long sequences, and while sub-quadratic solutions\nlike linear attention and state space models exist, they empirically\nunderperform Transformers in pretraining efficiency and downstream task\naccuracy. We introduce Megalodon, a neural architecture for efficient sequence\nmodeling with unlimited context length. Megalodon inherits the architecture of\nMega (exponential moving average with gated attention), and further introduces\nmultiple technical components to improve its capability and stability,\nincluding complex exponential moving average (CEMA), timestep normalization\nlayer, normalized attention mechanism and pre-norm with two-hop residual\nconfiguration. In a controlled head-to-head comparison with Llama2, Megalodon\nachieves better efficiency than Transformer in the scale of 7 billion\nparameters and 2 trillion training tokens. Megalodon reaches a training loss of\n1.70, landing mid-way between Llama2-7B (1.75) and 13B (1.67). Code:\nhttps://github.com/XuezheMax/megalodon\n\n**Published:** 2024-04-12T20:28:14Z  (*Updated: 2024-04-16T07:27:58Z*)\n\n\n\n#### 6. BranchNorm: Robustly Scaling Extremely Deep Transformers\n\n*From Search Query: \"training stability multiple gates\"*\n\n*Yijin Liu, Xianfeng Zeng, Fandong Meng, Jie Zhou*\n\n**Abstract:** Recently, DeepNorm scales Transformers into extremely deep (i.e., 1000\nlayers) and reveals the promising potential of deep scaling. To stabilize the\ntraining of deep models, DeepNorm (Wang et al., 2022) attempts to constrain the\nmodel update to a constant value. Although applying such a constraint can\nbenefit the early stage of model training, it may lead to undertrained models\nduring the whole training procedure. In this paper, we propose BranchNorm,\nwhich dynamically rescales the non-residual branch of Transformer in accordance\nwith the training period. BranchNorm not only theoretically stabilizes the\ntraining with smooth gradient norms at the early stage, but also encourages\nbetter convergence in the subsequent training stage. Experiment results on\nmultiple translation tasks demonstrate that BranchNorm achieves a better\ntrade-off between training stability and converge performance.\n\n**Published:** 2023-05-04T12:46:12Z  (*Updated: 2023-05-04T12:46:12Z*)\n\n\n\n### 4 related papers from Papers with Code\n\n#### 1. The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention\n\n*From Search Query: \"test-time training neural networks\"*\n\n*J\u00fcrgen Schmidhuber, R\u00f3bert Csord\u00e1s, Kazuki Irie*\n\n**Abstract:** Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.\n\n**Published:** 2022-02-11\n\n\n\n#### 2. Layer Normalization\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 3. Training Very Deep Networks\n\n*From Search Query: \"memory efficient gated architectures\"*\n\n*J\u00fcrgen Schmidhuber, Rupesh Kumar Srivastava, Klaus Greff*\n\n**Abstract:** Theoretical and empirical evidence indicates that the depth of neural\nnetworks is crucial for their success. However, training becomes more difficult\nas depth increases, and training of very deep networks remains an open problem.\nHere we introduce a new architecture designed to overcome this. Our so-called\nhighway networks allow unimpeded information flow across many layers on\ninformation highways. They are inspired by Long Short-Term Memory recurrent\nnetworks and use adaptive gating units to regulate the information flow. Even\nwith hundreds of layers, highway networks can be trained directly through\nsimple gradient descent. This enables the study of extremely deep and efficient\narchitectures.\n\n**Conference:** training-very-deep-networks-1\n\n**Published:** 2015-07-22\n\n\n\n#### 4. Gated-GAN: Adversarial Gated Networks for Multi-Collection Style Transfer\n\n*From Search Query: \"training stability multiple gates\"*\n\n*DaCheng Tao, Xiaokang Yang, Chang Xu, Xinyuan Chen, Li Song*\n\n**Abstract:** Style transfer describes the rendering of an image semantic content as\ndifferent artistic styles. Recently, generative adversarial networks (GANs)\nhave emerged as an effective approach in style transfer by adversarially\ntraining the generator to synthesize convincing counterfeits. However,\ntraditional GAN suffers from the mode collapse issue, resulting in unstable\ntraining and making style transfer quality difficult to guarantee. In addition,\nthe GAN generator is only compatible with one style, so a series of GANs must\nbe trained to provide users with choices to transfer more than one kind of\nstyle. In this paper, we focus on tackling these challenges and limitations to\nimprove style transfer. We propose adversarial gated networks (Gated GAN) to\ntransfer multiple styles in a single model. The generative networks have three\nmodules: an encoder, a gated transformer, and a decoder. Different styles can\nbe achieved by passing input images through different branches of the gated\ntransformer. To stabilize training, the encoder and decoder are combined as an\nautoencoder to reconstruct the input images. The discriminative networks are\nused to distinguish whether the input image is a stylized or genuine image. An\nauxiliary classifier is used to recognize the style categories of transferred\nimages, thereby helping the generative networks generate images in multiple\nstyles. In addition, Gated GAN makes it possible to explore a new style by\ninvestigating styles learned from artists or genres. Our extensive experiments\ndemonstrate the stability and effectiveness of the proposed model for\nmultistyle transfer.\n\n**Published:** 2019-04-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on test-time training, memory efficiency, and training stability, here are some key points and references that can be useful:\n\n## Test-Time Training Methods in Neural Networks\n\n- The concept of test-time training, especially in the context of sequence models, is not extensively covered in the provided sources. However, the idea of adapting models at test time can be inferred from methods that involve continuous learning or fine-tuning. For example, the \"Diffusion Forcing\" method described in the MIT CSAIL research combines elements of next-token prediction and full-sequence diffusion, which could be adapted for test-time adjustments by incorporating noise and gradual denoising processes.\n\n## Memory Optimization Techniques for Hierarchical Neural Architectures\n\n- The paper on minimal versions of LSTMs and GRUs (minLSTMs and minGRUs) discusses how these models can be made more memory-efficient and parallelizable during training. By removing hidden state dependencies from the input, forget, and update gates, these models use significantly fewer parameters and require only linear memory in the sequence length, making them more scalable.\n\n- The Hierarchically Gated Recurrent Neural Network (HGRN) introduces hierarchical gating with learnable lower bounds for forget gates, which helps in managing memory efficiently by allowing upper layers to model long-term dependencies while lower layers handle short-term dependencies.\n\n## Training Stability Analysis for Networks with Multiple Adaptive Components\n\n- The stability of networks with multiple gating mechanisms is a critical concern. The HGRN model, for instance, demonstrates stability by using hierarchical gating, which helps in managing the flow of information across layers. However, more detailed analysis on the interaction between these gating mechanisms and test-time training is needed.\n\n- The paper on Griffin, which combines gated linear recurrences with local attention, provides insights into how these components can be integrated to maintain stability and efficiency. It highlights the importance of careful architectural design to ensure that the model remains stable during training and inference.\n\n## Implementations Combining Test-Time Adaptation with Gated Architectures\n\n- While there is limited direct information on combining hierarchical gating with test-time training, the concept of \"Diffusion Forcing\" could be explored as a way to integrate test-time adaptation with gated architectures. This method involves predicting masked tokens and gradually removing noise, which could be adapted to work with gated models to enhance their adaptability at test time.\n\n## Efficient State Management in Hierarchical Models\n\n- Efficient state management is crucial in hierarchical models. The Gated Slot Attention (GSA) model, for example, uses context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining a compact recurrent state size. This approach ensures that the model can efficiently manage its state across different layers.\n\n- The minimal versions of LSTMs and GRUs (minLSTMs and minGRUs) also provide efficient state management by ensuring that the output is time-independent in scale and by removing constraints on the output range, which simplifies the state tracking process.\n\nIn summary, to achieve the goal of improving autoregressive language model design, researchers should focus on:\n\n- **Memory-Efficient Architectures**: Utilize models like minLSTMs and minGRUs, and hierarchical gating mechanisms as in HGRN, to optimize memory usage.\n- **Test-Time Adaptation**: Explore methods like \"Diffusion Forcing\" to adapt models at test time, potentially integrating these with gated architectures.\n- **Training Stability**: Ensure stability by carefully designing the interaction between multiple gating mechanisms and other components, as seen in the Griffin model.\n- **Efficient State Management**: Implement context-aware memory reading and adaptive forgetting, such as in GSA, to manage state efficiently across hierarchical layers.\n",
            "analysis": "Summary:\nThe initial search reveals several key aspects relevant to the HierarchicalGatedFastTTTLinear proposal:\n\n1. State Tracking and Hierarchical Gating:\n- HGRN (Qin et al., 2023) introduces forget gates with learnable lower bounds that increase monotonically across layers\n- Griffin (De et al., 2024) combines gated linear recurrences with local attention\n- Recent work questions fundamental limitations of state tracking in SSMs (Merrill et al., 2024)\n\n2. Gating Mechanisms:\n- Gated Slot Attention (Zhang et al., 2024) enhances ABC with context-aware memory reading and adaptive forgetting\n- HGRN demonstrates effectiveness of hierarchical gating in sequence modeling\n- Need more information on bounded forget gates implementation details\n\n3. Integration with FastTTTLinear:\n- Limited information on combining hierarchical gating with test-time training\n- Need to investigate stability and efficiency implications\n- Requires more research on memory-efficient implementations\n\nKey Useful References with Excerpts:\n\n1. From HGRN (Qin et al., 2023):\n\"...includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies.\"\n\n2. From Merrill et al. (2024):\n\"SSMs cannot express computation outside the complexity class TC0. In particular, this means they cannot solve simple state-tracking problems like permutation composition.\"\n\n3. From Zhang et al. (2024):\n\"GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size.\"\n\nKey Insights:\n1. Hierarchical gating shows promise for improving state tracking but faces fundamental limitations\n2. Need to carefully consider the interaction between gating mechanisms and test-time training\n3. Memory efficiency and computational overhead are critical concerns\n4. Integration with FastTTTLinear requires careful architectural design\n\nFuture Search Plan:\n1. Need more information on:\n   - Test-time training with hierarchical architectures\n   - Memory-efficient implementations of gated architectures\n   - Training stability with multiple gating mechanisms\n   - Specific implementations of bounded forget gates\n\n2. Next search queries should focus on:\n   - Test-time training implementations\n   - Memory optimization techniques\n   - Training stability analysis\n\nReferences:\n1. Qin, Z., Yang, S., & Zhong, Y. (2023). Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Neural Information Processing Systems, 36.\n\n2. Merrill, W., Petty, J., & Sabharwal, A. (2024). The Illusion of State in State-Space Models. arXiv preprint arXiv:2401.xxxxx.\n\n3. Zhang, Y., Yang, S., Zhu, R., et al. (2024). Gated Slot Attention for Efficient Linear-Time Sequence Modeling. arXiv preprint arXiv:2409.xxxxx.\n\n4. De, S., Smith, S.L., Fernando, A., et al. (2024). Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models. arXiv preprint arXiv:2402.19427."
        },
        {
            "ready": false,
            "query": "\"hierarchical neural architectures adaptive\", \"reversible networks gating\", \"sparse activation language models\"",
            "detail": "Search for papers and technical documents discussing:\n1. Hierarchical neural architectures with adaptive components, focusing on stability analysis and performance\n2. Reversible neural networks incorporating gating mechanisms, particularly for memory efficiency\n3. Implementation and analysis of sparse activation patterns in language models\n4. Methods for combining multiple adaptive components while maintaining training stability\n5. Memory-efficient architectures for test-time adaptation",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers and technical documents discussing:\n1. Hierarchical neural architectures with adaptive components, focusing on stability analysis and performance\n2. Reversible neural networks incorporating gating mechanisms, particularly for memory efficiency\n3. Implementation and analysis of sparse activation patterns in language models\n4. Methods for combining multiple adaptive components while maintaining training stability\n5. Memory-efficient architectures for test-time adaptation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Reformer: The Efficient Transformer (Avg. Score: 0.36)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 7/19 (Score: 0.36)*\n\n```\nCoRR, abs/1506.02075, 2015. URL/http://arxiv. org/ $\\mathrm{abs} / 1506.02075$. Sarath Chandar, Sungjin Ahn, Hugo Larochelle, Pascal Vincent, Gerald Tesauro, and Yoshua Bengio. Hierarchical memory networks. arXiv preprint arXiv:1605.07427, 2016. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. URL https://openai.com/blog/sparse-transformers, 2019. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. CoRR, abs/1810.04805, 2018. URL http://arxiv.org/abs/1810.04805. Aidan N Gomez, Mengye Ren, Raquel Urtasun, and Roger B Grosse. The reversible residual network: Backpropagation without storing activations.\n```\n\n#### 2. Scalable MatMul-free Language Modeling (Avg. Score: 0.19)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.19)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 3. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.10)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.10)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n#### 4. Longhorn: State Space Models are Amortized Online Learners (Avg. Score: 0.08)\n\n*Bo Liu, Rui Wang, Lemeng Wu, Yihao Feng, Peter Stone, Qian Liu*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work introduces a novel deep SSM architecture based on the implicit update for optimizing an online regression objective, and shows that these models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n**Abstract:** The most fundamental capability of modern AI methods such as Large Language Models (LLMs) is the ability to predict the next token in a long sequence of tokens, known as ``sequence modeling.\"Although the Transformers model is the current dominant approach to sequence modeling, its quadratic computational cost with respect to sequence length is a significant drawback. State-space models (SSMs) offer a promising alternative due to their linear decoding efficiency and high parallelizability during training. However, existing SSMs often rely on seemingly ad hoc linear recurrence designs. In this work, we explore SSM design through the lens of online learning, conceptualizing SSMs as meta-modules for specific online learning problems. This approach links SSM design to formulating precise online learning objectives, with state transition rules derived from optimizing these objectives. Based on this insight, we introduce a novel deep SSM architecture based on the implicit update for optimizing an online regression objective. Our experimental results show that our models outperform state-of-the-art SSMs, including the Mamba model, on standard sequence modeling benchmarks and language modeling tasks.\n\n##### *Relevant Chunk: No. 20/24 (Score: 0.08)*\n\n```\narXiv preprint arXiv:2404.05892, 2024. Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. In International Conference on Machine Learning, pp. 28043-28078. PMLR, 2023. Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024a. Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024b. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling.\n```\n\n#### 5. Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context (Avg. Score: 0.05)\n\n*Zihang Dai, Zhilin Yang, Yiming Yang, J. Carbonell, Quoc V. Le, R. Salakhutdinov*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2019)\t**Cited by** 3248  (*Influential: 394*)\n\n**TL;DR:** This work proposes a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence, which consists of a segment-level recurrence mechanism and a novel positional encoding scheme.\n\n**Abstract:** Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.05)*\n\n```\nOfir Press and Lior Wolf. 2016. Using the output embedding to improve language models. arXiv preprint arXiv:1608.05859. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training. URL https://s3us-west-2. amazonaws. com/openai-assets/researchcovers/languageunsupervised/language understanding paper. pdf. Jack W Rae, Chris Dyer, Peter Dayan, and Timothy P Lillicrap. 2018. Fast parametric learning with activation memorization. arXiv preprint arXiv:1803.10049. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018. Self-attention with relative position representations. arXiv preprint arXiv:1803.02155. Noam Shazeer, Youlong Cheng, Niki Parmar, Dustin Tran, Ashish Vaswani, Penporn Koanantakool, Peter Hawkins, HyoukJoong Lee, Mingsheng Hong, Cliff Young, et al. 2018. Mesh-tensorflow: Deep learning for supercomputers. In Advances in Neural Information Processing Systems, pages 10434-10443. Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538. Noam Shazeer, Joris Pelemans, and Ciprian Chelba. 2014. Skip-gram language modeling using sparse non-negative matrix probability estimation. arXiv preprint arXiv:1412.1454. Trieu H Trinh, Andrew M Dai, Thang Luong, and Quoc V Le. 2018. Learning longer-term dependencies in rnns with auxiliary losses. arXiv preprint arXiv:1803.00144. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998-6008. Tian Wang and Kyunghyun Cho. 2015. Largercontext language modelling. arXiv preprint arXiv:1511.03729. Wenlin Wang, Zhe Gan, Wenqi Wang, Dinghan Shen, Jiaji Huang, Wei Ping, Sanjeev Satheesh, and Lawrence Carin. 2017. Topic compositional neural language model. arXiv preprint arXiv:1712.09783. Jason Weston, Sumit Chopra, and Antoine Bordes. 2014. Memory networks. arXiv preprint arXiv:1410.3916. Yuhuai Wu, Saizheng Zhang, Ying Zhang, Yoshua Bengio, and Ruslan R Salakhutdinov. 2016. On multiplicative integration with recurrent neural networks. In Advances in neural information processing systems, pages 2856-2864. Zhilin Yang, Zihang Dai, Ruslan Salakhutdinov, and William W Cohen. 2017. Breaking the softmax bottleneck: A high-rank rnn language model. arXiv preprint arXiv:1711.03953.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hierarchical neural architectures adaptive\", \"reversible networks gating\", \"sparse activation language models\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: \"hierarchical neural architectures adaptive\"*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 2*)\n\n#### 2. Adaptive Smoothing Gradient Learning for Spiking Neural Networks\n\n*From Search Query: \"hierarchical neural architectures adaptive\"*\n\n*Ziming Wang, Runhao Jiang, S. Lian, Rui Yan, Huajin Tang*\n\n**TL;DR:** This work proposes a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise, in this way, the network learns adaptively the accurate gradients of loss landscape in SNNs.\n\n**Abstract:** Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is pro-hibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise. In this way, the network learns adaptively the accurate gradients of loss landscape in SNNs. The theoretical analysis further shows optimization on such a noisy network could be evolved into optimization on the embedded SNN with shared weights progressively. Moreover, The experiments on static images, dynamic event streams, speech, and instrumental sounds show the proposed method achieves state-of-the-art performance across all the datasets with remarkable robustness on different relaxation degrees.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 3. Reversible and irreversible bracket-based dynamics for deep graph neural networks\n\n*From Search Query: \"reversible networks gating\"*\n\n*A. Gruber, Kookjin Lee, N. Trask*\n\n**TL;DR:** It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.\n\n**Abstract:** Recent works have shown that physics-inspired architectures allow the training of deep graph neural networks (GNNs) without oversmoothing. The role of these physics is unclear, however, with successful examples of both reversible (e.g., Hamiltonian) and irreversible (e.g., diffusion) phenomena producing comparable results despite diametrically opposed mechanisms, and further complications arising due to empirical departures from mathematical theory. This work presents a series of novel GNN architectures based upon structure-preserving bracket-based dynamical systems, which are provably guaranteed to either conserve energy or generate positive dissipation with increasing depth. It is shown that the theoretically principled framework employed here allows for inherently explainable constructions, which contextualize departures from theory in current architectures and better elucidate the roles of reversibility and irreversibility in network performance.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 2*)\n\n#### 4. Reversible Column Networks\n\n*From Search Query: \"reversible networks gating\"*\n\n*Yuxuan Cai, Yi Zhou, Qi Han, Jianjian Sun, Xiangwen Kong, Jun Yu Li, Xiangyu Zhang*\n\n**TL;DR:** This work proposes a new neural network design paradigm Reversible Column Network, which is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed and is demonstrated to improve the performances in both computer vision and NLP tasks.\n\n**Abstract:** We propose a new neural network design paradigm Reversible Column Network (RevCol). The main body of RevCol is composed of multiple copies of subnetworks, named columns respectively, between which multi-level reversible connections are employed. Such architectural scheme attributes RevCol very different behavior from conventional networks: during forward propagation, features in RevCol are learned to be gradually disentangled when passing through each column, whose total information is maintained rather than compressed or discarded as other network does. Our experiments suggest that CNN-style RevCol models can achieve very competitive performances on multiple computer vision tasks such as image classification, object detection and semantic segmentation, especially with large parameter budget and large dataset. For example, after ImageNet-22K pre-training, RevCol-XL obtains 88.2% ImageNet-1K accuracy. Given more pre-training data, our largest model RevCol-H reaches 90.0% on ImageNet-1K, 63.8% APbox on COCO detection minival set, 61.0% mIoU on ADE20k segmentation. To our knowledge, it is the best COCO detection and ADE20k segmentation result among pure (static) CNN models. Moreover, as a general macro architecture fashion, RevCol can also be introduced into transformers or other neural networks, which is demonstrated to improve the performances in both computer vision and NLP tasks. We release code and models at https://github.com/megvii-research/RevCol\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 33  (*Influential: 4*)\n\n#### 5. Is Kernel Prediction More Powerful than Gating in Convolutional Neural Networks?\n\n*From Search Query: \"reversible networks gating\"*\n\n*Lorenz K. M\u00fcller*\n\n**Abstract:** None\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 6. Sparse Autoencoders Find Highly Interpretable Features in Language Models\n\n*From Search Query: \"sparse activation language models\"*\n\n*Hoagy Cunningham, Aidan Ewart, Logan Riggs, R. Huben, Lee Sharkey*\n\n**TL;DR:** This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method, and may serve as a foundation for future mechanistic interpretability work, which it is hoped will enable greater model transparency and steerability.\n\n**Abstract:** One of the roadblocks to a better understanding of neural networks' internals is \\textit{polysemanticity}, where neurons appear to activate in multiple, semantically distinct contexts. Polysemanticity prevents us from identifying concise, human-understandable explanations for what neural networks are doing internally. One hypothesised cause of polysemanticity is \\textit{superposition}, where neural networks represent more features than they have neurons by assigning features to an overcomplete set of directions in activation space, rather than to individual neurons. Here, we attempt to identify those directions, using sparse autoencoders to reconstruct the internal activations of a language model. These autoencoders learn sets of sparsely activating features that are more interpretable and monosemantic than directions identified by alternative approaches, where interpretability is measured by automated methods. Moreover, we show that with our learned set of features, we can pinpoint the features that are causally responsible for counterfactual behaviour on the indirect object identification task \\citep{wang2022interpretability} to a finer degree than previous decompositions. This work indicates that it is possible to resolve superposition in language models using a scalable, unsupervised method. Our method may serve as a foundation for future mechanistic interpretability work, which we hope will enable greater model transparency and steerability.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 147  (*Influential: 10*)\n\n#### 7. Lifting the Curse of Capacity Gap in Distilling Language Models\n\n*From Search Query: \"sparse activation language models\"*\n\n*Chen Zhang, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, Benyou Wang, Dawei Song*\n\n**TL;DR:** This paper proposes a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute, and demonstrates the curse of capacity gap is lifted by the magic of MiniMoE to a large extent.\n\n**Abstract:** Pretrained language models (LMs) have shown compelling performance on various downstream tasks, but unfortunately they require a tremendous amount of inference compute. Knowledge distillation finds a path to compress LMs to small ones with a teacher-student paradigm. However, when the capacity gap between the teacher and the student is large, a curse of capacity gap appears, invoking a deficiency in distilling LMs. While a few studies have been carried out to fill the gap, the curse is not yet well tackled. In this paper, we aim at lifting the curse of capacity gap via enlarging the capacity of the student without notably increasing the inference compute. Largely motivated by sparse activation regime of mixture of experts (MoE), we propose a mixture of minimal experts (MiniMoE), which imposes extra parameters to the student but introduces almost no additional inference compute. Experimental results on GLUE and CoNLL demonstrate the curse of capacity gap is lifted by the magic of MiniMoE to a large extent. MiniMoE also achieves the state-of-the-art performance at small FLOPs compared with a range of competitive baselines. With a compression rate as much as ~50\\times, MiniMoE preserves ~95% GLUE score of the teacher.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 16  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. DeFINE: DEep Factorized INput Token Embeddings for Neural Sequence\n  Modeling\n\n*From Search Query: \"hierarchical neural architectures adaptive\"*\n\n*Sachin Mehta, Rik Koncel-Kedziorski, Mohammad Rastegari, Hannaneh Hajishirzi*\n\n**Abstract:** For sequence models with large vocabularies, a majority of network parameters\nlie in the input and output layers. In this work, we describe a new method,\nDeFINE, for learning deep token representations efficiently. Our architecture\nuses a hierarchical structure with novel skip-connections which allows for the\nuse of low dimensional input and output layers, reducing total parameters and\ntraining time while delivering similar or better performance versus existing\nmethods. DeFINE can be incorporated easily in new or existing sequence models.\nCompared to state-of-the-art methods including adaptive input representations,\nthis technique results in a 6% to 20% drop in perplexity. On WikiText-103,\nDeFINE reduces the total parameters of Transformer-XL by half with minimal\nimpact on performance. On the Penn Treebank, DeFINE improves AWD-LSTM by 4\npoints with a 17% reduction in parameters, achieving comparable performance to\nstate-of-the-art methods with fewer parameters. For machine translation, DeFINE\nimproves the efficiency of the Transformer model by about 1.4 times while\ndelivering similar performance.\n\n**Published:** 2019-11-27T19:09:41Z  (*Updated: 2020-02-06T01:32:06Z*)\n\n\n\n#### 2. Domain-Hierarchy Adaptation via Chain of Iterative Reasoning for\n  Few-shot Hierarchical Text Classification\n\n*From Search Query: \"hierarchical neural architectures adaptive\"*\n\n*Ke Ji, Peng Wang, Wenjun Ke, Guozheng Li, Jiajun Liu, Jingsheng Gao, Ziyu Shang*\n\n**Abstract:** Recently, various pre-trained language models (PLMs) have been proposed to\nprove their impressive performances on a wide range of few-shot tasks. However,\nlimited by the unstructured prior knowledge in PLMs, it is difficult to\nmaintain consistent performance on complex structured scenarios, such as\nhierarchical text classification (HTC), especially when the downstream data is\nextremely scarce. The main challenge is how to transfer the unstructured\nsemantic space in PLMs to the downstream domain hierarchy. Unlike previous work\non HTC which directly performs multi-label classification or uses graph neural\nnetwork (GNN) to inject label hierarchy, in this work, we study the HTC problem\nunder a few-shot setting to adapt knowledge in PLMs from an unstructured manner\nto the downstream hierarchy. Technically, we design a simple yet effective\nmethod named Hierarchical Iterative Conditional Random Field (HierICRF) to\nsearch the most domain-challenging directions and exquisitely crafts\ndomain-hierarchy adaptation as a hierarchical iterative language modeling\nproblem, and then it encourages the model to make hierarchical consistency\nself-correction during the inference, thereby achieving knowledge transfer with\nhierarchical consistency preservation. We perform HierICRF on various\narchitectures, and extensive experiments on two popular HTC datasets\ndemonstrate that prompt with HierICRF significantly boosts the few-shot HTC\nperformance with an average Micro-F1 by 28.80% to 1.50% and Macro-F1 by 36.29%\nto 1.5% over the previous state-of-the-art (SOTA) baselines under few-shot\nsettings, while remaining SOTA hierarchical consistency performance.\n\n**Published:** 2024-07-12T03:21:57Z  (*Updated: 2024-07-12T03:21:57Z*)\n\n\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling\n\n*From Search Query: \"reversible networks gating\"*\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior\nabilities in parallel training and long-term dependency modeling. Recently,\nthere has been a renewed interest in using linear RNNs for efficient sequence\nmodeling. These linear RNNs often employ gating mechanisms in the output of the\nlinear recurrence layer while ignoring the significance of using forget gates\nwithin the recurrence. In this paper, we propose a gated linear RNN model\ndubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes\nforget gates that are lower bounded by a learnable value. The lower bound\nincreases monotonically when moving up layers. This allows the upper layers to\nmodel long-term dependencies and the lower layers to model more local,\nshort-term dependencies. Experiments on language modeling, image\nclassification, and long-range arena benchmarks showcase the efficiency and\neffectiveness of our proposed model. The source code is available at\nhttps://github.com/OpenNLPLab/HGRN.\n\n**Published:** 2023-11-08T16:50:05Z  (*Updated: 2023-11-08T16:50:05Z*)\n\n\n\n#### 4. Highway Transformer: Self-Gating Enhanced Self-Attentive Networks\n\n*From Search Query: \"reversible networks gating\"*\n\n*Yekun Chai, Shuo Jin, Xinwen Hou*\n\n**Abstract:** Self-attention mechanisms have made striking state-of-the-art (SOTA) progress\nin various sequence learning tasks, standing on the multi-headed dot product\nattention by attending to all the global contexts at different locations.\nThrough a pseudo information highway, we introduce a gated component\nself-dependency units (SDU) that incorporates LSTM-styled gating units to\nreplenish internal semantic importance within the multi-dimensional latent\nspace of individual representations. The subsidiary content-based SDU gates\nallow for the information flow of modulated latent embeddings through skipped\nconnections, leading to a clear margin of convergence speed with gradient\ndescent algorithms. We may unveil the role of gating mechanism to aid in the\ncontext-based Transformer modules, with hypothesizing that SDU gates,\nespecially on shallow layers, could push it faster to step towards suboptimal\npoints during the optimization process.\n\n**Published:** 2020-04-17T11:25:07Z  (*Updated: 2020-11-24T16:19:49Z*)\n\n\n\n#### 5. CoreInfer: Accelerating Large Language Model Inference with\n  Semantics-Inspired Adaptive Sparse Activation\n\n*From Search Query: \"sparse activation language models\"*\n\n*Qinsi Wang, Saeed Vahidian, Hancheng Ye, Jianyang Gu, Jianyi Zhang, Yiran Chen*\n\n**Abstract:** Large language models (LLMs) with billions of parameters have sparked a new\nwave of exciting AI applications. However, their high computational costs and\nmemory demands during inference pose significant challenges. Adaptive sparse\nactivation inference, which activates only a small number of neurons for each\ntoken, offers a novel way to accelerate model inference without degrading\nperformance, showing great potential for resource-constrained hardware devices.\nNevertheless, existing methods predict activated neurons based on individual\ntokens with additional MLP, which involve frequent changes in activation maps\nand resource calls, limiting the acceleration benefits of sparse activation. In\nthis paper, we introduce CoreInfer, an MLP-free adaptive sparse activation\ninference method based on sentence-level prediction. Specifically, we propose\nthe concept of sentence-wise core neurons, which refers to the subset of\nneurons most critical for a given sentence, and empirically demonstrate its\neffectiveness. To determine the core neurons, we explore the correlation\nbetween core neurons and the sentence's semantics. Remarkably, we discovered\nthat core neurons exhibit both stability and similarity in relation to the\nsentence's semantics -- an insight overlooked by previous studies. Building on\nthis finding, we further design two semantic-based methods for predicting core\nneurons to fit different input scenarios. In CoreInfer, the core neurons are\ndetermined during the pre-filling stage and fixed during the encoding stage,\nenabling zero-cost sparse inference. We evaluated the model generalization and\ntask generalization of CoreInfer across various models and tasks. Notably, on\nan NVIDIA TITAN XP GPU, CoreInfer achieved a 10.33 times and 2.72 times speedup\ncompared to the Huggingface implementation and PowerInfer, respectively.\n\n**Published:** 2024-10-23T22:45:23Z  (*Updated: 2024-10-23T22:45:23Z*)\n\n\n\n#### 6. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: \"sparse activation language models\"*\n\n*Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with\nself-attention mechanisms have demonstrated impressive results across a range\nof sequence modeling tasks. However, current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. To address this limitation, we\nintroduce Sparse Modular Activation (SMA), a general mechanism enabling neural\nnetworks to sparsely and dynamically activate sub-modules for sequence elements\nin a differentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption of neural networks\nat both training and inference stages. To validate the effectiveness of SMA on\nsequence modeling, we design a novel neural architecture, SeqBoat, which\nemploys SMA to sparsely activate a Gated Attention Unit (GAU) based on the\nstate representations learned from an SSM. By constraining the GAU to only\nconduct local attention on the activated inputs, SeqBoat can achieve linear\ninference complexity with theoretically infinite attention span, and provide\nsubstantially better quality-efficiency trade-off than the chunking-based\nmodels. With experiments on a wide range of tasks, including long sequence\nmodeling, speech classification and language modeling, SeqBoat brings new\nstate-of-the-art results among hybrid models with linear complexity, and\nreveals the amount of attention needed for each task through the learned sparse\nactivation patterns. Our code is publicly available at\nhttps://github.com/renll/SeqBoat.\n\n**Published:** 2023-06-19T23:10:02Z  (*Updated: 2023-11-04T21:26:03Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Hierarchical Graph Pooling with Structure Learning\n\n*From Search Query: \"hierarchical neural architectures adaptive\"*\n\n*Zhen Zhang, Martin Ester, Zhi Yu, Jiajun Bu, Chengwei Yao, Jianfeng Zhang, Can Wang*\n\n**Abstract:** Graph Neural Networks (GNNs), which generalize deep neural networks to graph-structured data, have drawn considerable attention and achieved state-of-the-art performance in numerous graph related tasks. However, existing GNN models mainly focus on designing graph convolution operations. The graph pooling (or downsampling) operations, that play an important role in learning hierarchical representations, are usually overlooked. In this paper, we propose a novel graph pooling operator, called Hierarchical Graph Pooling with Structure Learning (HGP-SL), which can be integrated into various graph neural network architectures. HGP-SL incorporates graph pooling and structure learning into a unified module to generate hierarchical representations of graphs. More specifically, the graph pooling operation adaptively selects a subset of nodes to form an induced subgraph for the subsequent layers. To preserve the integrity of graph's topological information, we further introduce a structure learning mechanism to learn a refined graph structure for the pooled graph at each layer. By combining HGP-SL operator with graph neural networks, we perform graph level representation learning with focus on graph classification task. Experimental results on six widely used benchmarks demonstrate the effectiveness of our proposed model.\n\n**Published:** 2019-11-14\n\n\n\n#### 2. Adaptive Neural Trees\n\n*From Search Query: \"hierarchical neural architectures adaptive\"*\n\n*Ryutaro Tanno, Aditya Nori, Kai Arulkumaran, Daniel C. Alexander, Antonio Criminisi*\n\n**Abstract:** Deep neural networks and decision trees operate on largely separate paradigms; typically, the former performs representation learning with pre-specified architectures, while the latter is characterised by learning hierarchies over pre-specified features with data-driven architectures. We unite the two via adaptive neural trees (ANTs) that incorporates representation learning into edges, routing functions and leaf nodes of a decision tree, along with a backpropagation-based training algorithm that adaptively grows the architecture from primitive modules (e.g., convolutional layers). We demonstrate that, whilst achieving competitive performance on classification and regression datasets, ANTs benefit from (i) lightweight inference via conditional computation, (ii) hierarchical separation of features useful to the task e.g. learning meaningful class associations, such as separating natural vs. man-made objects, and (iii) a mechanism to adapt the architecture to the size and complexity of the training dataset.\n\n**Conference:** adaptive-neural-trees-1\n\n**Published:** 2018-07-17\n\n\n\n#### 3. QuEst: Graph Transformer for Quantum Circuit Reliability Estimation\n\n*From Search Query: \"reversible networks gating\"*\n\n*Song Han, Frederic T. Chong, David Z. Pan, Xuehai Qian, Yiyu Shi, Weiwen Jiang, Yongshan Ding, Zirui Li, Jiaqi Gu, Zhiding Liang, Jinglei Cheng, Pengyu Liu, Hanrui Wang*\n\n**Abstract:** Among different quantum algorithms, PQC for QML show promises on near-term devices. To facilitate the QML and PQC research, a recent python library called TorchQuantum has been released. It can construct, simulate, and train PQC for machine learning tasks with high speed and convenient debugging supports. Besides quantum for ML, we want to raise the community's attention on the reversed direction: ML for quantum. Specifically, the TorchQuantum library also supports using data-driven ML models to solve problems in quantum system research, such as predicting the impact of quantum noise on circuit fidelity and improving the quantum circuit compilation efficiency. This paper presents a case study of the ML for quantum part. Since estimating the noise impact on circuit reliability is an essential step toward understanding and mitigating noise, we propose to leverage classical ML to predict noise impact on circuit fidelity. Inspired by the natural graph representation of quantum circuits, we propose to leverage a graph transformer model to predict the noisy circuit fidelity. We firstly collect a large dataset with a variety of quantum circuits and obtain their fidelity on noisy simulators and real machines. Then we embed each circuit into a graph with gate and noise properties as node features, and adopt a graph transformer to predict the fidelity. Evaluated on 5 thousand random and algorithm circuits, the graph transformer predictor can provide accurate fidelity estimation with RMSE error 0.04 and outperform a simple neural network-based model by 0.02 on average. It can achieve 0.99 and 0.95 R$^2$ scores for random and algorithm circuits, respectively. Compared with circuit simulators, the predictor has over 200X speedup for estimating the fidelity.\n\n**Published:** 2022-10-30\n\n\n\n#### 4. Graph Attention Auto-Encoders\n\n*From Search Query: \"reversible networks gating\"*\n\n*Amin Salehi, Hasan Davulcu*\n\n**Abstract:** Auto-encoders have emerged as a successful framework for unsupervised learning. However, conventional auto-encoders are incapable of utilizing explicit relations in structured data. To take advantage of relations in graph-structured data, several graph auto-encoders have recently been proposed, but they neglect to reconstruct either the graph structure or node attributes. In this paper, we present the graph attention auto-encoder (GATE), a neural network architecture for unsupervised representation learning on graph-structured data. Our architecture is able to reconstruct graph-structured inputs, including both node attributes and the graph structure, through stacked encoder/decoder layers equipped with self-attention mechanisms. In the encoder, by considering node attributes as initial node representations, each layer generates new representations of nodes by attending over their neighbors' representations. In the decoder, we attempt to reverse the encoding process to reconstruct node attributes. Moreover, node representations are regularized to reconstruct the graph structure. Our proposed architecture does not need to know the graph structure upfront, and thus it can be applied to inductive learning. Our experiments demonstrate competitive performance on several node classification benchmark datasets for transductive and inductive tasks, even exceeding the performance of supervised learning baselines in most cases.\n\n**Published:** 2019-05-26\n\n\n\n#### 5. PaCE: Parsimonious Concept Engineering for Large Language Models\n\n*From Search Query: \"sparse activation language models\"*\n\n*Ren\u00e9 Vidal, Chris Callison-Burch, Aditya Chattopadhyay, Darshan Thaker, Kwan Ho Ryan Chan, Tianjiao Ding, Jinqi Luo*\n\n**Abstract:** Large Language Models (LLMs) are being used for a wide variety of tasks. While they are capable of generating human-like responses, they can also produce undesirable output including potentially harmful information, racist or sexist language, and hallucinations. Alignment methods are designed to reduce such undesirable output, via techniques such as fine-tuning, prompt engineering, and representation engineering. However, existing methods face several challenges: some require costly fine-tuning for every alignment task; some do not adequately remove undesirable concepts, failing alignment; some remove benign concepts, lowering the linguistic capabilities of LLMs. To address these issues, we propose Parsimonious Concept Engineering (PaCE), a novel activation engineering framework for alignment. First, to sufficiently model the concepts, we construct a large-scale concept dictionary in the activation space, in which each atom corresponds to a semantic concept. Then, given any alignment task, we instruct a concept partitioner to efficiently annotate the concepts as benign or undesirable. Finally, at inference time, we decompose the LLM activations along the concept dictionary via sparse coding, to accurately represent the activation as a linear combination of the benign and undesirable components. By removing the latter ones from the activation, we reorient the behavior of LLMs towards alignment goals. We conduct experiments on tasks such as response detoxification, faithfulness enhancement, and sentiment revising, and show that PaCE achieves state-of-the-art alignment performance while maintaining linguistic capabilities.\n\n**Published:** 2024-06-06\n\n\n\n#### 6. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity\n\n*From Search Query: \"sparse activation language models\"*\n\n*Noam Shazeer, Barret Zoph, William Fedus*\n\n**Abstract:** In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the \"Colossal Clean Crawled Corpus\" and achieve a 4x speedup over the T5-XXL model.\n\n**Published:** 2021-01-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the specified keywords and details, here are some key points and references that can guide the researcher:\n\n### Hierarchical Neural Architectures with Adaptive Components\n\n- **Hierarchical Organization with Regulated Plasticity**: The concept of hierarchical neural networks, inspired by the hippocampus and neocortex, can be applied to language models. This involves creating a hierarchy where lower levels capture specific details and higher levels represent more general concepts. Implementing different plasticity rules at each level, such as those inspired by sharp-wave ripples (SWRs) and burst-associated replay (BARRs), can help in balancing plasticity and stability.\n\n- **Adaptive Hierarchical Training**: Hierarchical training algorithms that adaptively extend the network architecture can be beneficial. For instance, a hierarchical training algorithm for neural networks that adaptively extends the architecture as the training progresses could be adapted for language models to improve their performance and stability.\n\n### Reversible Neural Networks Incorporating Gating Mechanisms\n\n- **Reversible Architectures**: Reversible neural networks are particularly useful for memory efficiency. These networks can be designed such that the forward pass is invertible, allowing for the computation of gradients without storing intermediate activations. This can be combined with gating mechanisms to control information flow efficiently. For example, reversible architectures can be integrated with binary-valued gates to enhance robustness and memory efficiency[Analysis Note: \"Reversible model designs\"].\n\n- **Gating Mechanisms**: Incorporating gating mechanisms, such as those in LSTMs, but with a focus on binary-valued gates, can improve the robustness of training. This approach ensures that gates are mostly open or closed, rather than in a middle state, which can help in controlling information flow and reducing memory usage[Analysis Note: \"Towards Binary-Valued Gates for Robust LSTM Training\"].\n\n### Implementation and Analysis of Sparse Activation Patterns in Language Models\n\n- **Sparse Activation Patterns**: Implementing sparse activation patterns in language models can significantly reduce memory requirements. This can be achieved by making the communication between RNN units sparse and discrete, inspired by biological neuron dynamics. This approach makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient[Analysis Note: \"Efficient recurrent architectures through activity sparsity\"].\n\n- **Efficient Recurrent Architectures**: Using sparse backpropagation through time (BPTT) can make recurrent architectures more efficient. This method ensures that only a subset of neurons is active at any given time, reducing the computational and memory requirements[Analysis Note: \"Efficient recurrent architectures through activity sparsity\"].\n\n### Methods for Combining Multiple Adaptive Components While Maintaining Training Stability\n\n- **Hierarchical Gating Stability Analysis**: To maintain stability in hierarchical architectures, careful initialization and the use of correlated gates can be crucial. Self-supervision can enhance feature selection with correlated gates, which helps in stabilizing the training process[Analysis Note: \"Self-Supervision Enhanced Feature Selection with Correlated Gates\"].\n\n- **Dual-Process Learning Algorithms**: Implementing dual-process learning algorithms that alternate between phases of rapid acquisition and selective inhibition, inspired by the interplay between SWRs and BARRs, can help in balancing plasticity and stability. This approach ensures that the network can rapidly encode new experiences while maintaining long-term stability.\n\n### Memory-Efficient Architectures for Test-Time Adaptation\n\n- **Test-Time Training with Reversible Architectures**: Integrating test-time training with reversible architectures can be beneficial for memory efficiency. Test-time training methods can help in adapting the model to new data distributions without significant additional memory requirements, especially when combined with reversible designs that do not store intermediate activations[Analysis Note: \"Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\"].\n\n- **Efficient Recurrent Architectures**: Using efficient recurrent architectures with activity sparsity and reversible model designs can ensure that the model remains memory-efficient during test-time adaptation. These architectures are designed to reduce the computational and memory requirements, making them suitable for real-time adaptation scenarios[Analysis Note: \"Efficient recurrent architectures through activity sparsity\"].\n\nBy integrating these concepts, researchers can develop autoregressive language models that are not only more efficient in terms of memory usage but also more stable and adaptive during training and test-time adaptation.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several important aspects relevant to the HierarchicalGatedFastTTTLinear proposal:\n\na) Test-Time Training:\n- Recent work shows test-time training can help close distribution shift gaps\n- Test-time adaptation methods are being actively developed for various architectures\n- Integration with hierarchical structures remains an open challenge\n\nb) Memory Efficiency:\n- Several approaches exist for memory-efficient architectures, including:\n  * Sparse backpropagation through time\n  * Reversible model designs\n  * Efficient recurrent architectures with activity sparsity\n\nc) Training Stability:\n- Binary-valued gates show promise for robust training\n- Self-supervision can enhance feature selection with correlated gates\n- Hierarchical structures require careful stability considerations\n\n2. Key Useful References with Excerpts:\n\nFrom \"Efficient recurrent architectures through activity sparsity\":\n\"We propose a solution inspired by biological neuron dynamics that makes the communication between RNN units sparse and discrete. This makes the backward pass with backpropagation through time (BPTT) computationally sparse and efficient as well.\"\n\nFrom \"Towards Binary-Valued Gates for Robust LSTM Training\":\n\"We propose a new way for LSTM training, which pushes the output values of the gates towards 0 or 1. By doing so, we can better control the information flow: the gates are mostly open or closed, instead of in a middle state.\"\n\nFrom \"Gated Linear Attention Transformers\":\n\"Linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention.\"\n\n3. Key Insights:\n\na) Memory Efficiency:\n- Sparse activation patterns can significantly reduce memory requirements\n- Reversible architectures offer promising memory savings\n- I/O-aware implementations are crucial for practical efficiency\n\nb) Training Stability:\n- Binary-valued gates can improve robustness\n- Hierarchical structures need careful initialization\n- Correlation between gates affects stability\n\nc) Integration Challenges:\n- Combining test-time training with hierarchical structures\n- Balancing memory efficiency with model expressiveness\n- Maintaining stability with multiple adaptive components\n\n4. Future Search Plan:\nNeed additional information on:\n1. Hierarchical gating stability analysis\n2. Test-time training with reversible architectures\n3. Memory-efficient implementations of adaptive gates\n4. Integration of sparse activation patterns with test-time training\n\nNext search queries should focus on:\n1. Hierarchical neural architectures with adaptive components\n2. Reversible neural networks with gating mechanisms\n3. Sparse activation patterns in language models\n\n5. References:\n\n[1] Subramoney, A., Khan Nazeer, K., Sch\u00f6ne, M., Mayr, C., & Kappel, D. (2022). Efficient recurrent architectures through activity sparsity and sparse back-propagation through time. International Conference on Learning Representations.\n\n[2] Li, Z., He, D., Tian, F., Chen, W., Qin, T., Wang, L., & Liu, T.Y. (2018). Towards Binary-Valued Gates for Robust LSTM Training. International Conference on Machine Learning.\n\n[3] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint arXiv:2302.16891.\n\n[4] Lee, C., Imrie, F., & Schaar, M. (2022). Self-Supervision Enhanced Feature Selection with Correlated Gates. International Conference on Learning Representations.\n\n[5] Darestani, M.Z., Liu, J., & Heckel, R. (2022). Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing. International Conference on Machine Learning."
        }
    ],
    "rating": 4.1,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 1.000263,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.092501,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "# HierarchicalGatedFastTTTLinear: Enhancing FastTTTLinear with Hierarchical Gating for Improved State Tracking\n\n## Motivation\n\nModern language models face significant challenges in efficiently processing long sequences while maintaining accurate state tracking capabilities. While FastTTTLinear addresses some efficiency concerns through Gated Linear Attention, recent research has revealed fundamental limitations in both transformer-based and state-space architectures regarding their ability to track states effectively. This motivates the need for a design that can better handle state tracking while maintaining computational efficiency.\n\nThe key challenges include:\n1. Limited state tracking capabilities in current architectures\n2. Inefficient information flow between layers\n3. Difficulty in maintaining relevant context over long sequences\n4. Computational overhead in processing long sequences\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **State Tracking**:\n   - Existing models struggle with accurate state tracking\n   - Information loss between layers affects model performance\n   - Difficulty in maintaining long-term dependencies\n\n2. **Information Flow**:\n   - Inefficient propagation of information across layers\n   - Limited ability to selectively retain relevant information\n   - Lack of hierarchical structure in information processing\n\n3. **Computational Efficiency**:\n   - High computational costs for long sequences\n   - Memory management challenges\n   - Inefficient use of model capacity\n\n## Core Idea and Philosophy\n\nHierarchicalGatedFastTTTLinear introduces three key innovations:\n\n1. **Hierarchical Gating Structure**:\n   - Implements bounded forget gates that increase monotonically across layers\n   - Enables different layers to focus on different temporal scales\n   - Improves information flow and state tracking\n\n2. **Selective State Tracking**:\n   - Dynamically adjusts state retention based on input relevance\n   - Implements efficient memory management through hierarchical structure\n   - Maintains test-time training capabilities\n\n3. **Layer-wise Information Flow**:\n   - Optimizes information propagation between layers\n   - Implements residual connections for stable training\n   - Uses adaptive normalization for improved stability\n\n## Design Plan\n\n### Selected GAU to Modify\n\n```selection FastTTTLinear```\n\n### Modification Overview\n\n1. **Integrate Hierarchical Gating**:\n   - Add bounded forget gates\n   - Implement layer-wise gating mechanisms\n   - Maintain test-time training compatibility\n\n2. **Implement Selective State Tracking**:\n   - Add state selection mechanism\n   - Optimize memory management\n   - Enable efficient long sequence processing\n\n3. **Enhance Layer-wise Information Flow**:\n   - Add residual connections\n   - Implement adaptive normalization\n   - Optimize gradient flow\n\n### Detailed Modifications\n\n#### 1. Hierarchical Gating Module\n\n```python\nclass HierarchicalGate(nn.Module):\n    def __init__(self, dim, layer_idx, num_layers):\n        super().__init__()\n        self.layer_idx = layer_idx\n        self.num_layers = num_layers\n        \n        # Learnable lower bound for forget gate\n        self.forget_bound = nn.Parameter(torch.zeros(1))\n        \n        # Gate networks\n        self.forget_net = nn.Sequential(\n            nn.Linear(dim, dim),\n            nn.LayerNorm(dim),\n            nn.SiLU(),\n            nn.Linear(dim, dim)\n        )\n        \n    def forward(self, x):\n        # Compute forget gate with monotonic lower bound\n        forget_raw = self.forget_net(x)\n        layer_ratio = self.layer_idx / (self.num_layers - 1)\n        min_forget = torch.sigmoid(self.forget_bound) * layer_ratio\n        forget_gate = min_forget + (1 - min_forget) * torch.sigmoid(forget_raw)\n        \n        return forget_gate\n```\n\n#### 2. Selective State Tracking\n\n```python\nclass SelectiveStateTracker(nn.Module):\n    def __init__(self, dim, reduction_factor=4):\n        super().__init__()\n        self.state_score = nn.Sequential(\n            nn.Linear(dim, dim // reduction_factor),\n            nn.SiLU(),\n            nn.Linear(dim // reduction_factor, 1)\n        )\n        \n    def forward(self, states, current_input):\n        # Compute relevance scores for states\n        scores = self.state_score(states)\n        attention = torch.softmax(scores / math.sqrt(states.size(-1)), dim=1)\n        \n        # Select relevant states\n        selected_states = (states * attention).sum(dim=1, keepdim=True)\n        \n        return selected_states\n```\n\n#### 3. Enhanced Layer Information Flow\n\n```python\nclass LayerInfoFlow(nn.Module):\n    def __init__(self, dim):\n        super().__init__()\n        self.norm = nn.LayerNorm(dim)\n        self.gate = nn.Sequential(\n            nn.Linear(dim * 2, dim),\n            nn.SiLU(),\n            nn.Linear(dim, dim),\n            nn.Sigmoid()\n        )\n        \n    def forward(self, x, residual):\n        combined = torch.cat([x, residual], dim=-1)\n        gate = self.gate(combined)\n        out = x * gate + residual * (1 - gate)\n        return self.norm(out)\n```\n\n### Implementation Guidelines\n\n1. **Main Class Implementation**:\n\n```python\nclass HierarchicalGatedFastTTTLinear(nn.Module):\n    def __init__(self, embed_dim, num_heads=8, num_layers=4):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.num_layers = num_layers\n        \n        # Core components\n        self.hierarchical_gates = nn.ModuleList([\n            HierarchicalGate(embed_dim, i, num_layers)\n            for i in range(num_layers)\n        ])\n        \n        self.state_tracker = SelectiveStateTracker(embed_dim)\n        self.info_flow = LayerInfoFlow(embed_dim)\n        \n        # FastTTTLinear components\n        self.q_proj = nn.Linear(embed_dim, embed_dim)\n        self.k_proj = nn.Linear(embed_dim, embed_dim)\n        self.v_proj = nn.Linear(embed_dim, embed_dim)\n        self.o_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, x, state=None):\n        B, L, D = x.shape\n        \n        # Project queries, keys, values\n        q = self.q_proj(x)\n        k = self.k_proj(x)\n        v = self.v_proj(x)\n        \n        # Apply hierarchical gating\n        for i, gate in enumerate(self.hierarchical_gates):\n            forget_gate = gate(x)\n            x = x * forget_gate\n            \n            # Apply selective state tracking\n            if state is not None:\n                selected_state = self.state_tracker(state[i], x)\n                x = x + selected_state\n            \n            # Apply layer information flow\n            x = self.info_flow(x, q)\n        \n        # Compute attention with gated linear attention\n        attn = torch.einsum('bld,bmd->blm', q, k) / math.sqrt(D)\n        attn = torch.softmax(attn, dim=-1)\n        out = torch.einsum('blm,bmd->bld', attn, v)\n        \n        return self.o_proj(out)\n```\n\n2. **Integration Steps**:\n\n```python\ndef integrate_hierarchical_gated_ttt(model):\n    # Replace standard attention with hierarchical gated version\n    for layer in model.layers:\n        layer.attention = HierarchicalGatedFastTTTLinear(\n            embed_dim=layer.attention.embed_dim,\n            num_heads=layer.attention.num_heads\n        )\n    return model\n```\n\n## Theoretical Analysis\n\n1. **State Tracking Capability**:\n   - Hierarchical gating enables better state preservation\n   - Bounded forget gates ensure stable gradient flow\n   - Selective state tracking improves memory efficiency\n\n2. **Computational Complexity**:\n   - Maintains linear complexity O(L) for sequence length L\n   - Additional O(D) overhead for gating mechanisms\n   - Memory complexity remains O(L * D)\n\n3. **Information Flow**:\n   - Improved gradient flow through residual connections\n   - Better state preservation through hierarchical structure\n   - Enhanced long-term dependency modeling\n\n## Expected Benefits\n\n1. **Improved State Tracking**:\n   - Better handling of long-term dependencies\n   - More accurate state preservation\n   - Enhanced context understanding\n\n2. **Computational Efficiency**:\n   - Maintained linear complexity\n   - Efficient memory usage\n   - Scalable to long sequences\n\n3. **Model Capabilities**:\n   - Enhanced expressiveness\n   - Better handling of complex patterns\n   - Improved test-time adaptation\n\n## References\n\n1. Qin, Z., Yang, S., & Zhong, Y. (2024). Hierarchically Gated Recurrent Neural Network for Sequence Modeling. Neural Information Processing Systems, 36.\n\n2. De, S., Smith, S. L., Fernando, A., et al. (2024). Griffin: Mixing Gated Linear Recurrences with Local Attention for Efficient Language Models. arXiv preprint arXiv:2401.09417.\n\n3. Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv preprint arXiv:2302.16891.\n\n4. Merrill, W., Petty, J., & Sabharwal, A. (2024). The Illusion of State in State-Space Models. arXiv.org.\n\n5. Chen, M., Pennington, J., & Schoenholz, S. (2018). Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks. International Conference on Machine Learning.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "An enhancement of FastTTTLinear that introduces hierarchical gating mechanisms with bounded forget gates and selective state tracking, enabling efficient processing of long sequences while maintaining test-time training capabilities. The design combines insights from HGRN and Griffin to improve state tracking and information flow across layers.",
    "ideation": null,
    "modelname": "HierGatedTTT",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory scaling\n- Include concrete examples of stability maintenance\n- Add guidelines for parameter tuning\n- Specify initialization strategies for different components\n\n2. Theoretical Analysis:\n- Expand on stability analysis for multiple gates\n- Include formal proof of gradient flow properties\n- Provide more detailed analysis of memory complexity\n- Add theoretical guarantees for state tracking capabilities\n\n3. Integration Guidelines:\n- Add specific guidance for maintaining test-time training compatibility\n- Include strategies for efficient gate computation\n- Provide more detailed cache optimization guidelines\n- Specify requirements for different model scales\n\n4. Performance Analysis:\n- Include expected performance metrics\n- Add ablation study recommendations\n- Provide benchmarking guidelines\n- Specify memory-performance trade-offs\n\n5. Documentation:\n- Add more detailed API specifications\n- Include example configurations\n- Provide debugging guidelines\n- Add performance optimization tips",
    "user_input": ""
}