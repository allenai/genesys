{
    "variantname": "MatrixFreeAdaptiveTTTLinear",
    "review": "### Comprehensive Analysis\n\n#### 1. Clarity\nThe proposal is well-articulated with clear objectives. It outlines the motivation for eliminating matrix multiplications, implementing adaptive state compression, and using evolution-guided adaptation. The design plan is detailed, with mathematical formulations and pseudo-code that provide a clear understanding of the proposed modifications.\n\n#### 2. Innovation\nThe proposal introduces innovative concepts such as matrix-free operations and evolution-guided adaptation, which are relatively novel in the context of language models. The integration of adaptive state compression is also a valuable addition, aligning with current research trends in efficient memory usage. Compared to existing research, the proposal stands out by combining these elements into a cohesive framework.\n\n#### 3. Feasibility\nThe design appears feasible, supported by detailed pseudo-code and implementation guidelines. However, the complexity of integrating matrix-free operations and evolution strategies may pose challenges. The proposal would benefit from further exploration of potential implementation difficulties and strategies to mitigate them.\n\n#### 4. Scalability\nThe design is likely to scale efficiently with larger models and more data due to its focus on linear complexity and efficient memory usage. The adaptive compression and matrix-free operations are particularly promising for handling long sequences without significant computational overhead.\n\n#### 5. Accuracy and Robustness\nThe proposal has the potential to improve model performance by enhancing the ability to handle long sequences and diverse inputs. The adaptive compression and evolution strategies could contribute to increased robustness. However, empirical validation is needed to confirm these benefits.\n\n#### 6. Efficiency\nThe design offers potential improvements in computational efficiency and memory usage by eliminating matrix multiplications and using adaptive compression. These changes align with the goal of reducing computational costs while maintaining model expressiveness.\n\n### Strengths and Concerns\n\n**Strengths:**\n- Innovative integration of matrix-free operations and evolution strategies.\n- Clear and detailed design plan with mathematical formulations.\n- Potential for significant improvements in efficiency and scalability.\n\n**Concerns:**\n- Complexity of implementation, particularly for matrix-free operations and evolution strategies.\n- Lack of empirical validation to support theoretical claims.\n- Potential challenges in maintaining stability and performance during adaptation.\n\n### Suggestions for Improvement\n1. Conduct empirical studies to validate the theoretical claims and assess the practical impact on model performance and robustness.\n2. Provide more detailed implementation guidelines, particularly for matrix-free operations and evolution strategies.\n3. Explore potential challenges in the integration process and propose solutions or mitigations.\n4. Consider providing a detailed analysis of the trade-offs involved in the proposed modifications, including any potential impacts on training complexity and parameter tuning.\n\n### Final Rating\nBased on the proposal's clarity, innovation, feasibility, scalability, and potential impact on efficiency and accuracy, I assign a rating of **4.0**. The proposal is promising and well-conceived, but it requires empirical validation and further exploration of implementation challenges to achieve a higher rating.",
    "search_stack": [
        {
            "ready": false,
            "query": "frequency domain neural networks, efficient state compression, hybrid ssm attention",
            "detail": "Provide details on:\n1. Implementations of frequency-domain operations in neural networks, particularly focusing on efficient FFT alternatives and hardware optimization\n2. Methods for compressing and managing state information in neural networks\n3. Architectures that successfully combine state-space models with attention mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide details on:\n1. Implementations of frequency-domain operations in neural networks, particularly focusing on efficient FFT alternatives and hardware optimization\n2. Methods for compressing and managing state information in neural networks\n3. Architectures that successfully combine state-space models with attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.98)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.91)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 4/67 (Score: 0.91)*\n\n```\n0}$ | $\\mathbf{9 . 6}$ | $\\mathbf{9 . 0}$ | Attention-Free, MLP-Free |\n\n## 6 Related Work\n\nLong Convolutions Recent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [26,63, 66-68]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices. Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [79] or orthogonal polynomial bases [32]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases. Optimization of deep learning primitives There is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute. Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory $[44,76]$. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply-accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [62] have added kernel fusion capabilities, although the very best performance usually still arises from\nhandwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [53]. Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [13] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques. Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work $[5,9,12,14,15,17,24,88]$. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 21, 22, 40, 51]. Others use low-rank approximations of the attention matrix [11, 77, 88] or kernel methods instead [7, 39]. A subset use a combination of these techniques, such as $[6,71]$. Finally, a third category of methods $[25,63]$ aim to replace attention entirely, relying on state-space models [31]. ## 7 Discussion and Conclusion\n\nWe explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling. While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.\n```\n\n#### 4. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.90)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 8/46 (Score: 0.90)*\n\n```\nbioRxiv, pages 2022-11, 2022. [2] Ben Athiwaratkun, Sujan Kumar Gonugondla, Sanjay Krishna Gouda, Haifeng Qian, Hantian Ding, Qing Sun, Jun Wang, Liangfu Chen, Jiacheng Guo, Parminder Bhatia, et al. On io-efficient attention mechanisms: Context-aware bifurcated attention and the generalized multi-group attention. In Workshop on Efficient Systems for Foundation Models@ ICML2023, 2023. [3] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. Effective gene expression prediction from sequence by integrating long-range interactions. Nature methods, 18(10):1196-1203, 2021. [4] Manohar Ayinala, Michael Brown, and Keshab K Parhi. Pipelined parallel fft architectures via folding transformation. IEEE Transactions on Very Large Scale Integration (VLSI) Systems, 20(6):1068-1081, 2011. [5] Jun Ho Bahn, Jung Sook Yang, Wen-Hsiang Hu, and Nader Bagherzadeh. Parallel fft algorithms on network-on-chips. Journal of Circuits, Systems, and Computers, 18(02):255-269, 2009. [6] David H Bailey. Ffts in external of hierarchical memory. In Proceedings of the 1989 ACM/IEEE conference on Supercomputing, pages 234-242, 1989. [7] AJAA Bekele. Cooley-tukey fft algorithms. Advanced algorithms, 2016. [8] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer.\n```\n\n#### 5. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.80)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.80)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: frequency domain neural networks, efficient state compression, hybrid ssm attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Rethinking and Improving Robustness of Convolutional Neural Networks: a Shapley Value-based Approach in Frequency Domain\n\n*From Search Query: frequency domain neural networks*\n\n*Yiting Chen, Qibing Ren, Junchi Yan*\n\n**Abstract:** None\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 16  (*Influential: 1*)\n\n#### 2. Frequency-Domain Dynamic Pruning for Convolutional Neural Networks\n\n*From Search Query: frequency domain neural networks*\n\n*Z. Liu, Jizheng Xu, Xiulian Peng, Ruiqin Xiong*\n\n**TL;DR:** Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin and can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.\n\n**Abstract:** Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-110.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 130  (*Influential: 11*)\n\n#### 3. COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\n\n*From Search Query: efficient state compression*\n\n*Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan*\n\n**TL;DR:** This paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.\n\n**Abstract:** Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\\times$ speedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the existing works.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 4. Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression\n\n*From Search Query: efficient state compression*\n\n*Yufeng Zhang, Hang Yu, Jianguo Li, Weiyao Lin*\n\n**TL;DR:** A novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression that incorporates an efficient autoregressive Markov model based entropy coder and a Straight-Through Hardmax Quantization scheme to enhance the optimization of discrete latent space.\n\n**Abstract:** A BSTRACT Learned lossless data compression has garnered significant attention recently due to its superior compression ratios compared to traditional compressors. However, the computational efficiency of these models jeopardizes their practicality. This paper proposes a novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression. Our approach incorporates two essential innovations. First, we propose the Finite-State AutoRe-gressive (FSAR) entropy coder, an efficient autoregressive Markov model based entropy coder that utilizes a lookup table to expedite autoregressive entropy coding. Next, we present a Straight-Through Hardmax Quantization (STHQ) scheme to enhance the optimization of discrete latent space. Our experiments show that the proposed lossless compression method could improve the compression ratio by up to 6% compared to the baseline, with negligible extra computational time. Our work provides valuable insights into enhancing the computational efficiency of learned lossless data compression, which can have practical applications in various fields. Code is available at https://github.com/alipay/Finite_ State_Autoregressive_Entropy_Coding .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. ScaleCom: Scalable Sparsified Gradient Compression for Communication-Efficient Distributed Training\n\n*From Search Query: efficient state compression*\n\n*Chia-Yu Chen, Jiamin Ni, Songtao Lu, Xiaodong Cui, Pin-Yu Chen, Xiao Sun, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Srinivasan, Wei Zhang, K. Gopalakrishnan*\n\n**TL;DR:** This work proposes a new compression technique, Scalable Sparsified Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability and provides favorable convergence guarantees and is compatible with gradient all-reduce techniques.\n\n**Abstract:** Large-scale distributed training of Deep Neural Networks (DNNs) on state-of-the-art platforms is expected to be severely communication constrained. To overcome this limitation, numerous gradient compression techniques have been proposed and have demonstrated high compression ratios. However, most existing methods do not scale well to large scale distributed systems (due to gradient build-up) and/or fail to evaluate model fidelity (test accuracy) on large datasets. To mitigate these issues, we propose a new compression technique, Scalable Sparsified Gradient Compression (ScaleCom), that leverages similarity in the gradient distribution amongst learners to provide significantly improved scalability. Using theoretical analysis, we show that ScaleCom provides favorable convergence guarantees and is compatible with gradient all-reduce techniques. Furthermore, we experimentally demonstrate that ScaleCom has small overheads, directly reduces gradient traffic and provides high compression rates (65-400X) and excellent scalability (up to 64 learners and 8-12X larger batch sizes over standard training) across a wide range of applications (image, language, and speech) without significant accuracy loss.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 60  (*Influential: 3*)\n\n#### 6. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: hybrid ssm attention*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 270  (*Influential: 21*)\n\n#### 7. Block-State Transformers\n\n*From Search Query: hybrid ssm attention*\n\n*Mahan Fathi, Jonathan Pilault, Pierre-Luc Bacon, C. Pal, Orhan Firat, Ross Goroshin*\n\n**TL;DR:** This work proposes a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sub layer for short-term representation of sequences and shows that the model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences.\n\n**Abstract:** State space models (SSMs) have shown impressive results on tasks that require modeling long-range dependencies and efficiently scale to long sequences owing to their subquadratic runtime complexity. Originally designed for continuous signals, SSMs have shown superior performance on a plethora of tasks, in vision and audio; however, SSMs still lag Transformer performance in Language Modeling tasks. In this work, we propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences. We study three different, and completely parallelizable, variants that integrate SSMs and block-wise attention. We show that our model outperforms similar Transformer-based architectures on language modeling perplexity and generalizes to longer sequences. In addition, the Block-State Transformer demonstrates more than tenfold increase in speed at the layer level compared to the Block-Recurrent Transformer when model parallelization is employed.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 8. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: hybrid ssm attention*\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Training behavior of deep neural network in frequency domain\n\n*From Search Query: frequency domain neural networks*\n\n*Yaoyu Zhang, Zhi-Qin John Xu, Yanyang Xiao*\n\n**Abstract:** Why deep neural networks (DNNs) capable of overfitting often generalize well in practice is a mystery [#zhang2016understanding]. To find a potential mechanism, we focus on the study of implicit biases underlying the training process of DNNs. In this work, for both real and synthetic datasets, we empirically find that a DNN with common settings first quickly captures the dominant low-frequency components, and then relatively slowly captures the high-frequency ones. We call this phenomenon Frequency Principle (F-Principle). The F-Principle can be observed over DNNs of various structures, activation functions, and training algorithms in our experiments. We also illustrate how the F-Principle help understand the effect of early-stopping as well as the generalization of DNNs. This F-Principle potentially provides insights into a general principle underlying DNN optimization and generalization.\n\n**Published:** 2018-07-03\n\n\n\n#### 2. Learning Frequency Domain Approximation for Binary Neural Networks\n\n*From Search Query: frequency domain neural networks*\n\n*Yunhe Wang, Chunjing Xu, Yehui Tang, Chang Xu, Kai Han, Yixing Xu*\n\n**Abstract:** Binary neural networks (BNNs) represent original full-precision weights and activations into 1-bit with sign function. Since the gradient of the conventional sign function is almost zero everywhere which cannot be used for back-propagation, several attempts have been proposed to alleviate the optimization difficulty by using approximate gradient. However, those approximations corrupt the main direction of factual gradient. To this end, we propose to estimate the gradient of sign function in the Fourier frequency domain using the combination of sine functions for training BNNs, namely frequency domain approximation (FDA). The proposed approach does not affect the low-frequency information of the original sign function which occupies most of the overall energy, and high-frequency coefficients will be ignored to avoid the huge computational overhead. In addition, we embed a noise adaptation module into the training phase to compensate the approximation error. The experiments on several benchmark datasets and neural architectures illustrate that the binary network learned using our method achieves the state-of-the-art accuracy. Code will be available at \\textit{https://gitee.com/mindspore/models/tree/master/research/cv/FDA-BNN}.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-03-01\n\n\n\n#### 3. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: efficient state compression*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 4. HPTQ: Hardware-Friendly Post Training Quantization\n\n*From Search Query: efficient state compression*\n\n*Arnon Netzer, Roy H. Jennings, Idit Diamant, Oranit Dror, Lior Dikstein, Elad Cohen, Reuven Peretz, Hai Victor Habi*\n\n**Abstract:** Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.\n\n**Published:** 2021-09-19\n\n\n\n#### 5. OTCE: Hybrid SSM and Attention with Cross Domain Mixture of Experts to construct Observer-Thinker-Conceiver-Expresser\n\n*From Search Query: hybrid ssm attention*\n\n*Kai Wang, Chunjun Zheng, Bingheng Wu, Ting Xie, Jingze Shi*\n\n**Abstract:** Recent research has shown that combining Mamba with Transformer architecture, which has selective state space and quadratic self-attention mechanism, outperforms using Mamba or Transformer architecture alone in language modeling tasks. The quadratic self-attention mechanism effectively alleviates the shortcomings of selective state space in handling long-term dependencies of any element in the sequence. We propose a position information injection method that connects the selective state space model with the quadratic attention, and integrates these two architectures with hybrid experts with cross-sharing domains, so that we can enjoy the advantages of both. We design a new architecture with a more biomimetic idea: Observer-Thinker-Conceiver-Expresser (OTCE), which can compete with well-known medium-scale open-source language models on a small scale in language modeling tasks.\n\n**Published:** 2024-06-24\n\n\n\n#### 6. Samba: Simple Hybrid State Space Models for Efficient Unlimited Context Language Modeling\n\n*From Search Query: hybrid ssm attention*\n\n*Weizhu Chen, Chen Liang, Yelong Shen, Yadong Lu, Yang Liu, Liliang Ren*\n\n**Abstract:** Efficiently modeling sequences with infinite context length has been a long-standing problem. Past works suffer from either the quadratic computation complexity or the limited extrapolation ability on length generalization. In this work, we present Samba, a simple hybrid architecture that layer-wise combines Mamba, a selective State Space Model (SSM), with Sliding Window Attention (SWA). Samba selectively compresses a given sequence into recurrent hidden states while still maintaining the ability to precisely recall memories with the attention mechanism. We scale Samba up to 3.8B parameters with 3.2T training tokens and show that Samba substantially outperforms the state-of-the-art models based on pure attention or SSMs on a wide range of benchmarks. When trained on 4K length sequences, Samba can be efficiently extrapolated to 256K context length with perfect memory recall and show improved token predictions up to 1M context length. As a linear-time sequence model, Samba enjoys a 3.73x higher throughput compared to Transformers with grouped-query attention when processing user prompts of 128K length, and 3.64x speedup when generating 64K tokens with unlimited streaming. A sample implementation of Samba is publicly available in https://github.com/microsoft/Samba.\n\n**Published:** 2024-06-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficiency, state management, and the integration of state-space models with attention mechanisms, here are some key insights and methods derived from the provided details and references:\n\n### Implementations of Frequency-Domain Operations in Neural Networks\n\n#### Efficient FFT Alternatives\n- The FlashFFTConv approach is noteworthy for optimizing FFT operations using matrix multiply units, which improves hardware efficiency. This method reduces the computational cost of FFT from \\(O(N \\log N)\\) to a more efficient implementation using tensor cores, enhancing hardware utilization for long-sequence processing.\n\n#### Hardware Optimization\n- The use of Fast Fourier Transform (FFT) in models like LightMed demonstrates how operating in the frequency domain can enhance efficiency. LightMed uses FFT to reduce computational complexity while preserving essential image features, which can be adapted for sequence models to improve efficiency.\n\n### Methods for Compressing and Managing State Information\n\n#### Dynamic Linear Interpolation and Semantic Compression\n- RWKV6's approach to dynamic linear interpolation and semantic compression is beneficial for extending context handling without significant computational costs. This method can be integrated into state-space models to manage state information efficiently, allowing for better context retention and reduced memory usage[Analysis].\n\n#### State-Space Models\n- State-space models with layer-wise nonlinearity have been shown to be universal approximators with exponential decaying memory. This property makes them suitable for managing state information in a compact and efficient manner, which can be particularly useful in autoregressive models[Analysis].\n\n### Architectures Combining State-Space Models with Attention Mechanisms\n\n#### Hybrid SSM-Attention Architectures\n- Combining state-space models (SSMs) with attention mechanisms can leverage the strengths of both approaches. SSMs are effective in tracking state and handling long-term dependencies, while attention mechanisms excel in capturing contextual relationships. This hybrid approach can enhance the expressive capacity of the model, especially in tasks requiring both long-term memory and contextual understanding[Analysis].\n\n#### Example Implementations\n- The idea of stacking state-space models with layer-wise nonlinear activation can be integrated with attention mechanisms to create a hybrid model. For instance, using SSMs to handle the sequential dependencies and attention to focus on relevant parts of the input sequence can provide a balanced and efficient architecture[Analysis].\n\n### Additional Considerations\n\n#### Frequency Domain Analysis\n- The method proposed in the paper on frequency domain normalization for non-stationary time series forecasting can be adapted for sequence models. By analyzing datasets from a frequency perspective and adaptively up-weighting key frequency components, this approach can help in capturing dynamic patterns more effectively, which might be beneficial in language models dealing with long sequences.\n\n#### Efficient Feature Extraction\n- The use of frequency domain techniques, such as those in LightMed and the SCTN-based resonator, demonstrates how feature extraction can be optimized. These methods focus on low-frequency components to reduce computational complexity while preserving essential features, which can be applied to language models to enhance efficiency and robustness.\n\nBy integrating these insights, researchers can design more efficient and robust autoregressive language models that leverage the strengths of frequency-domain operations, state-space models, and attention mechanisms. This hybrid approach can improve the model's ability to handle long sequences, manage state information efficiently, and enhance overall performance.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design FastTTTLinear and provided references reveal several key areas for potential improvement in LM block design:\n\n1. Efficiency in Long Sequence Processing:\n- FastTTTLinear uses Gated Linear Attention for linear complexity\n- FlashFFTConv demonstrates efficient convolution techniques for long sequences\n- State-space models offer alternative approaches to sequence modeling\n\n2. State Management and Compression:\n- RWKV6 shows benefits of dynamic linear interpolation and semantic compression\n- State-space models with layer-wise nonlinearity prove universal approximation capabilities\n- Formal language analysis reveals limitations in current SSM designs\n\n3. Computational Efficiency:\n- FlashFFTConv optimizes FFT operations using matrix multiply units\n- Hardware-efficient implementations are crucial for practical deployment\n- Trade-offs between expressiveness and computational cost need careful consideration\n\nKey Insights and Analysis:\n\n1. Convolution and FFT Optimization:\nFrom FlashFFTConv paper:\n\"A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in O(N logN) time in sequence length N but has poor hardware utilization.\"\n\nThis suggests potential for improving hardware efficiency in frequency-domain operations, particularly relevant for long-sequence processing.\n\n2. State-Space Model Integration:\nFrom the SSM paper:\n\"stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship\"\n\nThis indicates that incorporating SSM principles could enhance model expressiveness while maintaining efficiency.\n\n3. Semantic Compression:\nFrom RWKV6:\n\"Semantic compression can effectively extend context handling without significant computational costs\"\n\nThis points to opportunities for efficient context management through content-aware compression.\n\n4. Formal Language Analysis:\nFrom \"The Expressive Capacity of State Space Models\":\n\"SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly.\"\n\nThis suggests potential benefits of combining SSM strengths with attention mechanisms.\n\nFuture Search Plan:\n1. Investigate specific implementations of frequency-domain operations in neural networks\n2. Research efficient state compression techniques\n3. Explore hybrid architectures combining SSM and attention mechanisms\n\nReferences:\n\n1. Fu, D. Y., Kumbong, H., Nguyen, E. N. D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint arXiv:2311.05908.\n\n2. Wang, S., & Xue, B. (2023). State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Neural Information Processing Systems.\n\n3. Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv preprint arXiv:2401.12244.\n\n4. Yu, R., Yu, W., & Wang, X. (2024). KAN or MLP: A Fairer Comparison. arXiv preprint arXiv:2401.12244."
        },
        {
            "ready": false,
            "query": "hardware efficient linear operations, dynamic state selection compression, efficient architectural integration",
            "detail": "Provide details on:\n1. Hardware-efficient implementations of linear operations, particularly focusing on matrix multiplication and memory access patterns\n2. Methods for dynamic state selection and compression in neural networks\n3. Techniques for efficiently integrating multiple architectural components in deep learning models",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide details on:\n1. Hardware-efficient implementations of linear operations, particularly focusing on matrix multiplication and memory access patterns\n2. Methods for dynamic state selection and compression in neural networks\n3. Techniques for efficiently integrating multiple architectural components in deep learning models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.69)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.69)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.54)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.54)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. Scalable MatMul-free Language Modeling (Avg. Score: 0.50)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.96)*\n\n```\nEshraghian ${ }^{1 *}$<br>${ }^{1}$ University of California, Santa Cruz ${ }^{2}$ Soochow University<br>${ }^{3}$ University of California, Davis ${ }^{4}$ LuxiTech\n\n\n#### Abstract\n\nMatrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to $61 \\%$ over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than $10 \\times$ compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13 W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github. com/ridgerchu/matmulfreellm. ## 1 Introduction\n\nMatrix Multiplication (MatMul) is the dominant operation in most neural networks, where dense layers involve vector-matrix multiplication (VMM), convolutions can be implemented as block-sparse VMMs with shared weights, and self-attention relies on matrix-matrix multiplication (MMM). The prevalence of MatMul is primarily due to Graphics Processing Units (GPUs) being optimized for MatMul operations. By leveraging Compute Unified Device Architecture (CUDA) and highly optimized linear algebra libraries such as cuBLAS, the MatMul operation can be efficiently parallelized and accelerated. This optimization was a key factor in the victory of AlexNet in the ILSVRC2012 competition and a historic marker for the rise of deep learning [1]. AlexNet notably utilized GPUs to boost training speed beyond CPU capabilities, and as such, deep learning won the 'hardware lottery' [2]. It also helped that both training and inference rely on MatMul. Despite its prevalence in deep learning, MatMul operations account for the dominant portion of computational expense, often consuming the majority of the execution time and memory access during\n\n[^0]both training and inference phases. Several works have replaced MatMul with simpler operations through two main strategies. The first strategy involves substituting MatMul with elementary operations, e.g., AdderNet replaces multiplication with signed addition in convolutional neural networks (CNNs) [3]. Given the focus on convolutions, AdderNet is intended for use in computer vision over language modeling. The second approach employs binary or ternary quantization, simplifying MatMul to operations where values are either flipped or zeroed out before accumulation. Quantization can be applied to either activations or weights: spiking neural networks (SNNs) use binarized activations [4, 5, 6], while binary and ternary neural networks (BNNs and TNNs) use quantized weights [7]. Both methods can also be combined $[8,9]$. Recent advances in language modeling, like BitNet [10, 11], demonstrate quantization's scalability, replacing all dense layer weights with binary/ternary values to support up to 3 billion parameters. Despite replacing VMMs with accumulations in all dense layers, BitNet retains the self-attention mechanism which relies on an expensive MMM. Dynamically computed matrices $Q$ (query) and $K$ (key) are multiplied to form the attention map. Since both $Q$ and $K$ matrices are dynamically computed from pre-activation values, achieving optimal hardware efficiency on GPUs requires custom optimizations, such as specialized kernels and advanced memory access patterns. Despite these efforts, such MatMul operations remain resource-intensive on GPUs, as they involve extensive data movement and synchronization which can significantly hinder computational throughput and efficiency [12]. In our experiments, ternary quantization of the attention matrices in BitNet causes a significant drop in performance and failure to reach model convergence (see Fig. 1). This raises the question: is it possible to completely eliminate MatMul from LLMs? In this work, we develop the first scalable MatMul-free language model (Matmul-free LM) by using additive operations in dense layers and element-wise Hadamard products for self-attention-like functions. Specifically, ternary weights eliminate MatMul in dense layers, similar to BNNs. To remove MatMul from self-attention, we optimize the Gated Recurrent Unit (GRU) [13] to rely solely on element-wise products and show that this model competes with state-of-the-art Transformers while eliminating all MatMul operations. To quantify the hardware benefits of lightweight models, we provide an optimized GPU implementation in addition to a custom FPGA accelerator. By using fused kernels in the GPU implementation of the ternary dense layers, training is accelerated by $25.6 \\%$ and memory consumption is reduced by up to $61.0 \\%$ over an unoptimized baseline on GPU. Furthermore, by employing lower-bit optimized CUDA kernels, inference speed is increased by 4.57 times, and memory usage is reduced by a factor of 10 when the model is scaled up to 13B parameters. This work goes beyond software-only implementations of lightweight models and shows how scalable, yet lightweight, language models can both reduce computational demands and energy use in the real-world. ## 2 Related Works\n\nBinary, Ternary, and Low-Precision Quantization for Language Models: The effort to quantize language models began with reducing a ternary BERT into a binarized model [14], achieving $41 \\%$ average accuracy on the GLUE benchmarks with subsequent fine-tuning. Ref. [15] distilled the intermediate outputs from a full precision BERT to a quantized version. Recently, Ref. [16] introduced an incremental quantization approach, progressively quantizing a model from 32-bit to 4-bit, 2-bit, and finally to binary model parameters. Following the quantization of BERT, low-precision language generation models have gained momentum. Ref. [17] used Quantization-Aware Training (QAT) to successfully train a model with 2-bit weights. BitNet pushed this to 3-billion-parameter binary and ternary models while maintaining competitive performance with Llama-like language models [10, 11]. MatMul-free Transformers: The use of MatMul-free Transformers has been largely concentrated in the domain of SNNs. Spikformer led the first integration of the Transformer architecture with SNNs [18, 19], with later work developing alternative Spike-driven Transformers [20, 21]. These techniques demonstrated success in vision tasks. In the language understanding domain, SpikingBERT [22] and SpikeBERT [23] applied SNNs to BERT utilizing knowledge distillation techniques to perform sentiment analysis. In language generation, SpikeGPT trained a 216 M -parameter generative model using a spiking RWKV architecture. However, these models remain constrained in size, with\n\nSpikeGPT being the largest, reflecting the challenges of scaling with binarized activations. In addition to SNNs, BNNs have also made significant progress in this area. BinaryViT [24] and BiViT [25] successfully applied Binary Vision Transformers to visual tasks. Beyond these approaches, Kosson et al. [26] achieve multiplication-free training by replacing multiplications, divisions, and non-linearities with piecewise affine approximations while maintaining performance. ## 3 Method\n\nIn this section, we break down the components of the proposed MatMul-free LM. We first describe the MatMul-free dense layers (BitLinear layers) that use ternary weights. By constraining the weights to the set $\\{-1,0,+1\\}$ and applying additional quantization techniques, MatMul operations are replaced with addition and negation operations. This reduces computational cost and memory utilization, while preserving the expressiveness of the network. We then provide further detail of our MatMul-free LM architecture, which includes a token mixer for capturing sequential dependencies and a channel mixer for integrating information across embedding dimensions. The Method section is structured as follows. First, in Sec. 3.1, we provide a comprehensive description of the MatMul-free dense layers with ternary weights, which form the foundation of our approach. Next, Sec. 3.2 introduces our hardware-efficient fused BitLinear layer, designed to optimize the implementation of BitLinear layers. Building upon these components, Sec. 3.3 delves into the details of our MatMul-free LM architecture. We present the MatMul-free token mixer, where we propose the MatMul-free Linear Gated Recurrent Unit (MLGRU), and the MatMul-free channel mixer, which employs the Gated Linear Unit (GLU) with BitLinear layers. By combining the MLGRU token mixer and the GLU channel mixer with ternary weights, our proposed architecture relies solely on addition and element-wise products.\n```\n\n##### *Relevant Chunk: No. 8/27 (Score: 0.03)*\n\n```\nWe assume a constant clock rate for all implementations. We note that the FPGA implementation is done in RTL from top to bottom, and there are many optimizations that could be added. For example, we are not using any vendor-provided IPs, and we are not bursting DDR transactions, both of which would significantly accelerate operation. This approach is to achieve the most generic and cross-platform evaluation possible. ## 6 Conclusion\n\nWe have demonstrated the feasibility and effectiveness of the first scalable MatMul-free language model. Our work challenges the paradigm that MatMul operations are indispensable for building high-performing language models and paves the way for the development of more efficient and hardware-friendly architectures. We achieve performance on par with state-of-the-art Transformers while eliminating the need for MatMul operations, with an optimized implementation that significantly enhances both training and inference efficiency, reducing both memory usage and latency. As the demand for deploying language models on various platforms grows, MatMul-free LMs present a promising direction for creating models that are both effective and resource-efficient. However, one limitation of our work is that the MatMul-free LM has not been tested on extremely large-scale models (e.g., 100B+ parameters) due to computational constraints. This work serves as a call to action for institutions and organizations that have the resources to build the largest language models to invest in accelerating lightweight models. By prioritizing the development and deployment of MatMul-free architectures such as this one, the future of LLMs will only become more accessible, efficient, and sustainable. ## References\n\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [2] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [3] Hanting Chen, Yunhe Wang, Chunjing Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chao Xu, Chunfeng Xu, and Qi Tian. The addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $1468-1477,2020$. [4] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659-1671, 1997. [5] Jason K Eshraghian, Max Ward, Emre O Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. Proceedings of the IEEE, 2023. [6] Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian. SpikeGPT: Generative pretrained language model with spiking neural networks.\n```\n\n#### 4. Monarch Mixer: A Simple Sub-Quadratic GEMM-Based Architecture (Avg. Score: 0.17)\n\n*Daniel Y. Fu, Simran Arora, Jessica Grogan, Isys Johnson, Sabri Eyuboglu, Armin W. Thomas, Benjamin Spector, Michael Poli, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 2*)\n\n**TL;DR:** A novel theoretical view of Monarch matrices is developed based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic, showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n**Abstract:** Machine learning models are increasingly being scaled in both sequence length and model dimension to reach longer contexts and better performance. However, existing architectures such as Transformers scale quadratically along both these axes. We ask: are there performant architectures that can scale sub-quadratically along sequence length and model dimension? We introduce Monarch Mixer (M2), a new architecture that uses the same sub-quadratic primitive along both sequence length and model dimension: Monarch matrices, a simple class of expressive structured matrices that captures many linear transforms, achieves high hardware efficiency on GPUs, and scales sub-quadratically. As a proof of concept, we explore the performance of M2 in three domains: non-causal BERT-style language modeling, ViT-style image classification, and causal GPT-style language modeling. For non-causal BERT-style modeling, M2 matches BERT-base and BERT-large in downstream GLUE quality with up to 27% fewer parameters, and achieves up to 9.1$\\times$ higher throughput at sequence length 4K. On ImageNet, M2 outperforms ViT-b by 1% in accuracy, with only half the parameters. Causal GPT-style models introduce a technical challenge: enforcing causality via masking introduces a quadratic bottleneck. To alleviate this bottleneck, we develop a novel theoretical view of Monarch matrices based on multivariate polynomial evaluation and interpolation, which lets us parameterize M2 to be causal while remaining sub-quadratic. Using this parameterization, M2 matches GPT-style Transformers at 360M parameters in pretraining perplexity on The PILE--showing for the first time that it may be possible to match Transformer quality without attention or MLPs.\n\n##### *Relevant Chunk: No. 4/67 (Score: 0.17)*\n\n```\n0}$ | $\\mathbf{9 . 6}$ | $\\mathbf{9 . 0}$ | Attention-Free, MLP-Free |\n\n## 6 Related Work\n\nLong Convolutions Recent work proposes to use long convolution layers as a replacement for the Transformer attention layers in sequence modeling [26,63, 66-68]. Many of these models rely on the FFT convolution theorem to compute the long convolutions. We build on the insights in many of these architectures in constructing our M2 architectures, and additionally replaces the FFT operations with Monarch matrices. Our work is also related to a rich literature in convolutions in other bases, such as Chebyshev bases [79] or orthogonal polynomial bases [32]. These approaches have analogues in our multivariate analysis; replacing the basis polynomials of the Monarch matrices in Monarch Mixer may be able to approximate some of these operations. An interesting question for future work would be to study how well our techniques and concerns about causality and hardware utilization translate to these alternative convolution bases. Optimization of deep learning primitives There is a rich history of the optimization of deep learning primitives, as accelerating their performance can yield substantial savings in compute and cost for large models. There are many approaches to speed up these operations, but they usually either reduce data movement or compute. Reducing data movement: In many applications, the major bottleneck is the storage and movement of large amounts of memory. One popular approach to reducing data movement is checkpointing, wherein one stores fewer intermediate results and recomputes the others on-the-fly where they are needed, trading additional compute for memory $[44,76]$. Another approach is kernel fusion, wherein algorithms initially described as sequential steps can often be fused in ways that improve their properties. For example, it is generally faster to implement a dot-product through a multiply-accumulate rather than first multiplying and then accumulating. Recently, libraries such as PyTorch 2.0 [62] have added kernel fusion capabilities, although the very best performance usually still arises from\nhandwritten kernels. Third, in order to better exploit memory locality, it is often fastest to load small blocks of memory, do intensive computation on them, and then write the results a tile at a time [80]. Finally, many algorithms also have hand-optimizations that can remove unnecessary computation or memory accesses [53]. Efficient algorithms usually make use of a combination of these techniques. For example, FlashAttention [13] uses all four to dramatically decrease both the latency and memory consumption of multi-head attention. Though we have made a modest effort to implement Monarch Mixer efficiently, we think it likely that Monarch Mixer could be further optimized by these techniques. Reducing flops: A first target for optimization is the multi-layer perceptron (MLP), owing to its ubiquity. A variety of structured sparse factorizations exist, many of which we draw on in this work $[5,9,12,14,15,17,24,88]$. Attention is also a popular target for optimization. Recently, a plethora of sub-quadratic approximations of attention have emerged, that aim to approximate attention to reduce its quadratic complexity. Some methods rely on sparsification, relying on the fact that the attention matrix is extremely sparse at long sequence lengths [2, 21, 22, 40, 51]. Others use low-rank approximations of the attention matrix [11, 77, 88] or kernel methods instead [7, 39]. A subset use a combination of these techniques, such as $[6,71]$. Finally, a third category of methods $[25,63]$ aim to replace attention entirely, relying on state-space models [31]. ## 7 Discussion and Conclusion\n\nWe explore Monarch Mixer (M2), a new architecture that is sub-quadratic in both sequence length and model dimension and is hardware-efficient on modern accelerators. We motivate M2 from both theoretical and systems performance perspectives and conduct a preliminary proof-of-concept investigation into performance on masked language modeling, image classification, and causal language modeling. While our initial results are promising, our work is only a first step in this direction. The M2 layer can likely be further optimized with systems optimization techniques such as kernel fusion. Our work has also not been optimized for inference like more well-established models such as Transformers, or even more recent models such as state space models. It also remains to be seen whether M2 layers can have as widespread applicability as Transformers. We hope that these can be fruitful directions for future work. ## Acknowledgments\n\nWe gratefully acknowledge the support of DARPA under Nos. FA86501827865 (SDH) and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, Brown Institute for Media Innovation, Department of Defense (DoD) through the National Defense Science and Engineering Graduate Fellowship (NDSEG) Program, Fannie and John Hertz Foundation, National Science Foundation Graduate Research Fellowship Program, Texas Instruments Stanford Graduate Fellowship in Science and Engineering, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: hardware efficient linear operations, dynamic state selection compression, efficient architectural integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions\n\n*From Search Query: hardware efficient linear operations*\n\n*Neo Wei Ming, Zhehui Wang, Cheng Liu, R. Goh, Tao Luo*\n\n**TL;DR:** MA-BERT is proposed, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss and enables hardware-friendly processing on various computing engines, including CPUs and GPUs.\n\n**Abstract:** Due to their superior results, Transformer-based models such as BERT have become de facto standards in many Natural Language Processing (NLP) applications. However, the intensive use of complex non-linear functions within the Transformer architecture impairs its computing efficiency and complicates corresponding accelerator designs, because non-linear functions are generally computation-intensive and require special hardware support. In light of this, we propose MA-BERT, which allows matrix arithmetic-only operations in Transformer-based NLP models and achieves efficient inference with negligible accuracy loss. Specifically, we propose four correlated techniques that include approximating softmax with a two-layer neural network, replacing GELU with ReLU, fusing normalization layers with adjacent linear layers, and leveraging knowledge transfer from baseline models. Through these techniques, we are able to eliminate the major non-linear functions in Transformer-based models and obtain MA-BERT with only matrix arithmetic and trivial ReLU operations without compromising on accuracy. With mainly regular matrix arithmetic operations, MA-BERT enables hardware-friendly processing on various computing engines, including CPUs and GPUs. Our experimental results show that MA-BERT achieves up to 27% and 41% reduction in inference time on CPU and GPU, respectively, with comparable accuracy on many downstream tasks compared to\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics\n\n*From Search Query: hardware efficient linear operations*\n\n*Siqi Miao, Zhiyuan Lu, Mia Liu, Javier Duarte, Pan Li*\n\n**TL;DR:** A novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics, and proposes LSH-based Efficient Point Transformer (HEPT), which combines E$^2$LSH with OR&AND constructions and is built upon regular computations.\n\n**Abstract:** This study introduces a novel transformer model optimized for large-scale point cloud processing in scientific domains such as high-energy physics (HEP) and astrophysics. Addressing the limitations of graph neural networks and standard transformers, our model integrates local inductive bias and achieves near-linear complexity with hardware-friendly regular operations. One contribution of this work is the quantitative analysis of the error-complexity tradeoff of various sparsification techniques for building efficient transformers. Our findings highlight the superiority of using locality-sensitive hashing (LSH), especially OR&AND-construction LSH, in kernel approximation for large-scale point cloud data with local inductive bias. Based on this finding, we propose LSH-based Efficient Point Transformer (HEPT), which combines E$^2$LSH with OR&AND constructions and is built upon regular computations. HEPT demonstrates remarkable performance on two critical yet time-consuming HEP tasks, significantly outperforming existing GNNs and transformers in accuracy and computational speed, marking a significant advancement in geometric deep learning and large-scale scientific data processing. Our code is available at https://github.com/Graph-COM/HEPT.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 3. Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention\n\n*From Search Query: hardware efficient linear operations*\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**TL;DR:** Lightning Attention is presented, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption and TransNormerLLM (TNL) is introduced, a new architecture that is tailored to the authors' lightning attention.\n\n**Abstract:** We present Lightning Attention, the first linear attention implementation that maintains a constant training speed for various sequence lengths under fixed memory consumption. Due to the issue with cumulative summation operations (cumsum), previous linear attention implementations cannot achieve their theoretical advantage in a casual setting. However, this issue can be effectively solved by utilizing different attention calculation strategies to compute the different parts of attention. Specifically, we split the attention calculation into intra-blocks and inter-blocks and use conventional attention computation for intra-blocks and linear attention kernel tricks for inter-blocks. This eliminates the need for cumsum in the linear attention calculation. Furthermore, a tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. To enhance accuracy while preserving efficacy, we introduce TransNormerLLM (TNL), a new architecture that is tailored to our lightning attention. We conduct rigorous testing on standard and self-collected datasets with varying model sizes and sequence lengths. TNL is notably more efficient than other language models. In addition, benchmark results indicate that TNL performs on par with state-of-the-art LLMs utilizing conventional transformer structures. The source code is released at github.com/OpenNLPLab/TransnormerLLM.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming\n\n*From Search Query: dynamic state selection compression*\n\n*Jinuk Kim, Yeonwoo Jeong, Deokjae Lee, Hyun Oh Song*\n\n**TL;DR:** A subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent Convolution operations for efficient end-to-end inference latency is proposed.\n\n**Abstract:** Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by TensorRT for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in MobileNetV2 on the ImageNet dataset. Specifically, we achieve $1.41\\times$ speed-up with $0.11$\\%p accuracy gain in MobileNetV2-1.0 on the ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Response Selection for Multi-Party Conversations with Dynamic Topic Tracking\n\n*From Search Query: dynamic state selection compression*\n\n*Weishi Wang, Shafiq R. Joty, S. Hoi*\n\n**TL;DR:** This work frames response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context and proposes Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning.\n\n**Abstract:** While participants in a multi-party multi-turn conversation simultaneously engage in multiple conversation topics, existing response selection methods are developed mainly focusing on a two-party single-conversation scenario. Hence, the prolongation and transition of conversation topics are ignored by current methods. In this work, we frame response selection as a dynamic topic tracking task to match the topic between the response and relevant conversation context. With this new formulation, we propose a novel multi-task learning framework that supports efficient encoding through large pretrained models with only two utterances at once to perform dynamic topic disentanglement and response selection. We also propose Topic-BERT an essential pretraining step to embed topic information into BERT with self-supervised learning. Experimental results on the DSTC-8 Ubuntu IRC dataset show state-of-the-art results in response selection and topic disentanglement tasks outperforming existing methods by a good margin.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 42  (*Influential: 4*)\n\n#### 6. Compression of Generative Pre-trained Language Models via Quantization\n\n*From Search Query: dynamic state selection compression*\n\n*Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong*\n\n**TL;DR:** This paper compress generative PLMs by quantization with comparable performance with the full-precision models, and proposes a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules.\n\n**Abstract:** The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 88  (*Influential: 6*)\n\n#### 7. Decision Stacks: Flexible Reinforcement Learning via Modular Generative Models\n\n*From Search Query: efficient architectural integration*\n\n*Siyan Zhao, Aditya Grover*\n\n**TL;DR:** Decision Stacks is presented, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules that simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing.\n\n**Abstract:** Reinforcement learning presents an attractive paradigm to reason about several distinct aspects of sequential decision making, such as specifying complex goals, planning future observations and actions, and critiquing their utilities. However, the combined integration of these capabilities poses competing algorithmic challenges in retaining maximal expressivity while allowing for flexibility in modeling choices for efficient learning and inference. We present Decision Stacks, a generative framework that decomposes goal-conditioned policy agents into 3 generative modules. These modules simulate the temporal evolution of observations, rewards, and actions via independent generative models that can be learned in parallel via teacher forcing. Our framework guarantees both expressivity and flexibility in designing individual modules to account for key factors such as architectural bias, optimization objective and dynamics, transferrability across domains, and inference speed. Our empirical results demonstrate the effectiveness of Decision Stacks for offline policy optimization for several MDP and POMDP environments, outperforming existing methods and enabling flexible generative decision making.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 8. Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models\n\n*From Search Query: efficient architectural integration*\n\n*Qiuhao Lu, D. Dou, Thien Huu Nguyen*\n\n**TL;DR:** This work introduces an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way and proposes to encode domain knowledge via adapters, which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs.\n\n**Abstract:** Domain-speci\ufb01c pre-trained language models (PLMs) have achieved great success over various downstream tasks in different domains. However, existing domain-speci\ufb01c PLMs mostly rely on self-supervised learning over large amounts of domain text, without explicitly integrating domain-speci\ufb01c knowledge, which can be essential in many domains. Moreover, in knowledge-sensitive ar-eas such as the biomedical domain, knowledge is stored in multiple sources and formats, and existing biomedical PLMs either neglect them or utilize them in a limited manner. In this work, we introduce an architecture to integrate domain knowledge from diverse sources into PLMs in a parameter-ef\ufb01cient way. More speci\ufb01cally, we propose to encode domain knowledge via adapters , which are small bottleneck feed-forward networks inserted between intermediate transformer layers in PLMs. These knowledge adapters are pre-trained for individual domain knowledge sources and integrated via an attention-based knowledge controller to enrich PLMs. Taking the biomedical domain as a case study, we explore three knowledge-speci\ufb01c adapters for PLMs based on the UMLS Metathesaurus graph, the Wikipedia articles for diseases, and the semantic grouping information for biomedical concepts. Extensive experiments on different biomedical NLP tasks and datasets demonstrate the bene\ufb01ts of the proposed architecture and the knowledge-speci\ufb01c adapters across multiple PLMs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 3*)\n\n#### 9. Learning to Grow Pretrained Models for Efficient Transformer Training\n\n*From Search Query: efficient architectural integration*\n\n*Peihao Wang, Rameswar Panda, Lucas Torroba Hennigen, P. Greengard, Leonid Karlinsky, R. Feris, David D. Cox, Zhangyang Wang, Yoon Kim*\n\n**TL;DR:** This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where it is learned to linearly map the parameters of the smaller model to initialize the larger model.\n\n**Abstract:** Scaling transformers has led to significant breakthroughs in many domains, leading to a paradigm in which larger versions of existing models are trained and released on a periodic basis. New instances of such models are typically trained completely from scratch, despite the fact that they are often just scaled-up versions of their smaller counterparts. How can we use the implicit knowledge in the parameters of smaller, extant models to enable faster training of newer, larger models? This paper describes an approach for accelerating transformer training by learning to grow pretrained transformers, where we learn to linearly map the parameters of the smaller model to initialize the larger model. For tractable learning, we factorize the linear transformation as a composition of (linear) width- and depth-growth operators, and further employ a Kronecker factorization of these growth operators to encode architectural knowledge. Extensive experiments across both language and vision transformers demonstrate that our learned Linear Growth Operator (LiGO) can save up to 50% computational cost of training from scratch, while also consistently outperforming strong baselines that also reuse smaller pretrained models to initialize larger models.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 40  (*Influential: 8*)\n\n### 6 related papers from Papers with Code\n\n#### 1. EfficientViT: Multi-Scale Linear Attention for High-Resolution Dense Prediction\n\n*From Search Query: hardware efficient linear operations*\n\n*Chuang Gan, Junyan Li, Muyan Hu, Song Han, Han Cai*\n\n**Abstract:** High-resolution dense prediction enables many appealing real-world applications, such as computational photography, autonomous driving, etc. However, the vast computational cost makes deploying state-of-the-art high-resolution dense prediction models on hardware devices difficult. This work presents EfficientViT, a new family of high-resolution vision models with novel multi-scale linear attention. Unlike prior high-resolution dense prediction models that rely on heavy softmax attention, hardware-inefficient large-kernel convolution, or complicated topology structure to obtain good performances, our multi-scale linear attention achieves the global receptive field and multi-scale learning (two desirable features for high-resolution dense prediction) with only lightweight and hardware-efficient operations. As such, EfficientViT delivers remarkable performance gains over previous state-of-the-art models with significant speedup on diverse hardware platforms, including mobile CPU, edge GPU, and cloud GPU. Without performance loss on Cityscapes, our EfficientViT provides up to 13.9$\\times$ and 6.2$\\times$ GPU latency reduction over SegFormer and SegNeXt, respectively. For super-resolution, EfficientViT delivers up to 6.4x speedup over Restormer while providing 0.11dB gain in PSNR. For Segment Anything, EfficientViT delivers 48.9x higher throughput on A100 GPU while achieving slightly better zero-shot instance segmentation performance on COCO.\n\n**Published:** 2022-05-29\n\n\n\n#### 2. Ristretto: Hardware-Oriented Approximation of Convolutional Neural Networks\n\n*From Search Query: hardware efficient linear operations*\n\n*Philipp Gysel*\n\n**Abstract:** Convolutional neural networks (CNN) have achieved major breakthroughs in\nrecent years. Their performance in computer vision have matched and in some\nareas even surpassed human capabilities. Deep neural networks can capture\ncomplex non-linear features; however this ability comes at the cost of high\ncomputational and memory requirements. State-of-art networks require billions\nof arithmetic operations and millions of parameters. To enable embedded devices\nsuch as smartphones, Google glasses and monitoring cameras with the astonishing\npower of deep learning, dedicated hardware accelerators can be used to decrease\nboth execution time and power consumption. In applications where fast\nconnection to the cloud is not guaranteed or where privacy is important,\ncomputation needs to be done locally. Many hardware accelerators for deep\nneural networks have been proposed recently. A first important step of\naccelerator design is hardware-oriented approximation of deep networks, which\nenables energy-efficient inference. We present Ristretto, a fast and automated\nframework for CNN approximation. Ristretto simulates the hardware arithmetic of\na custom hardware accelerator. The framework reduces the bit-width of network\nparameters and outputs of resource-intense layers, which reduces the chip area\nfor multiplication units significantly. Alternatively, Ristretto can remove the\nneed for multipliers altogether, resulting in an adder-only arithmetic. The\ntool fine-tunes trimmed networks to achieve high classification accuracy. Since\ntraining of deep neural networks can be time-consuming, Ristretto uses highly\noptimized routines which run on the GPU. This enables fast compression of any\ngiven network. Given a maximum tolerance of 1%, Ristretto can successfully\ncondense CaffeNet and SqueezeNet to 8-bit. The code for Ristretto is available.\n\n**Published:** 2016-05-20\n\n\n\n#### 3. A Winning Hand: Compressing Deep Networks Can Improve Out-Of-Distribution Robustness\n\n*From Search Query: dynamic state selection compression*\n\n*Bhavya Kailkhura, Jize Zhang, Shreya Chaganti, Brian R. Bartoldson, James Diffenderfer*\n\n**Abstract:** Successful adoption of deep learning (DL) in the wild requires models to be: (1) compact, (2) accurate, and (3) robust to distributional shifts. Unfortunately, efforts towards simultaneously meeting these requirements have mostly been unsuccessful. This raises an important question: Is the inability to create Compact, Accurate, and Robust Deep neural networks (CARDs) fundamental? To answer this question, we perform a large-scale analysis of popular model compression techniques which uncovers several intriguing patterns. Notably, in contrast to traditional pruning approaches (e.g., fine tuning and gradual magnitude pruning), we find that \"lottery ticket-style\" approaches can surprisingly be used to produce CARDs, including binary-weight CARDs. Specifically, we are able to create extremely compact CARDs that, compared to their larger counterparts, have similar test accuracy and matching (or better) robustness -- simply by pruning and (optionally) quantizing. Leveraging the compactness of CARDs, we develop a simple domain-adaptive test-time ensembling approach (CARD-Decks) that uses a gating module to dynamically select appropriate CARDs from the CARD-Deck based on their spectral-similarity with test samples. The proposed approach builds a \"winning hand'' of CARDs that establishes a new state-of-the-art (on RobustBench) on CIFAR-10-C accuracies (i.e., 96.8% standard and 92.75% robust) and CIFAR-100-C accuracies (80.6% standard and 71.3% robust) with better memory usage than non-compressed baselines (pretrained CARDs and CARD-Decks available at https://github.com/RobustBench/robustbench). Finally, we provide theoretical support for our empirical findings.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-06-16\n\n\n\n#### 4. SkipNet: Learning Dynamic Routing in Convolutional Networks\n\n*From Search Query: dynamic state selection compression*\n\n*Zi-Yi Dou, Joseph E. Gonzalez, Fisher Yu, Xin Wang, Trevor Darrell*\n\n**Abstract:** While deeper convolutional networks are needed to achieve maximum accuracy in\nvisual perception tasks, for many inputs shallower networks are sufficient. We\nexploit this observation by learning to skip convolutional layers on a\nper-input basis. We introduce SkipNet, a modified residual network, that uses a\ngating network to selectively skip convolutional blocks based on the\nactivations of the previous layer. We formulate the dynamic skipping problem in\nthe context of sequential decision making and propose a hybrid learning\nalgorithm that combines supervised learning and reinforcement learning to\naddress the challenges of non-differentiable skipping decisions. We show\nSkipNet reduces computation by 30-90% while preserving the accuracy of the\noriginal model on four benchmark datasets and outperforms the state-of-the-art\ndynamic networks and static compression methods. We also qualitatively evaluate\nthe gating policy to reveal a relationship between image scale and saliency and\nthe number of layers skipped.\n\n**Conference:** skipnet-learning-dynamic-routing-in-1\n\n**Published:** 2017-11-26\n\n\n\n#### 5. HybridQ: A Hybrid Simulator for Quantum Circuits\n\n*From Search Query: efficient architectural integration*\n\n*Anonymous*\n\n**Abstract:** Developing state-of-the-art classical simulators of quantum circuits is of utmost importance to test and evaluate early quantum technology and understand the true potential of full-blown error-corrected quantum computers. In the past few years, multiple theoretical and numerical advances have continuously pushed the boundary of what is classically simulable, hence the development of a plethora of tools which are often limited to a specific purpose or designed for a particular hardware (e.g. CPUs vs. GPUs). Moreover, such tools are typically developed using tailored languages and syntax, which makes it hard to compare results from, and create hybrid approaches using, different simulation techniques. To support unified and optimized use of these techniques across platforms, we developed HybridQ, a highly extensible platform designed to provide a common framework to integrate multiple state-of-the-art techniques to run on a variety of hardware. The philosophy behind its development has been driven by three main pillars: \"Easy to Use\", \"Easy to Extend\", and \"Use the Best Available Technology\". The powerful tools of HybridQ allow users to manipulate, develop, and extend noiseless and noisy circuits for different hardware architectures. HybridQ supports large-scale high-performance computing (HPC) simulations, automatically balancing workload among different processor nodes and enabling the use of multiple backends to maximize parallel efficiency. Everything is then glued together by a simple and expressive language that allows seamless switching from one technique to another as well as from one hardware to the next, without the need to write lengthy translations, thus greatly simplifying the development of new hybrid algorithms and techniques.\n\n**Published:** 2021-11-12\n\n\n\n#### 6. LeViT: a Vision Transformer in ConvNet's Clothing for Faster Inference\n\n*From Search Query: efficient architectural integration*\n\n*Matthijs Douze, Herv\u00e9 J\u00e9gou, Armand Joulin, Pierre Stock, Hugo Touvron, Alaaeldin El-Nouby, Ben Graham*\n\n**Abstract:** We design a family of image classification architectures that optimize the trade-off between accuracy and efficiency in a high-speed regime. Our work exploits recent findings in attention-based architectures, which are competitive on highly parallel processing hardware. We revisit principles from the extensive literature on convolutional neural networks to apply them to transformers, in particular activation maps with decreasing resolutions. We also introduce the attention bias, a new way to integrate positional information in vision transformers. As a result, we propose LeVIT: a hybrid neural network for fast inference image classification. We consider different measures of efficiency on different hardware platforms, so as to best reflect a wide range of application scenarios. Our extensive experiments empirically validate our technical choices and show they are suitable to most architectures. Overall, LeViT significantly outperforms existing convnets and vision transformers with respect to the speed/accuracy tradeoff. For example, at 80% ImageNet top-1 accuracy, LeViT is 5 times faster than EfficientNet on CPU. We release the code at https://github.com/facebookresearch/LeViT\n\n**Proceeding:** iccv-2021-1\n\n**Published:** 2021-04-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, focusing on hardware-efficient linear operations, dynamic state selection and compression, and efficient architectural integration, here are some key insights and techniques from the provided sources and analysis:\n\n## Hardware-Efficient Implementations of Linear Operations\n\n### Matrix Multiplication\n- **Analog In-Memory Computing (AIMC):** The use of AIMC, as demonstrated in the IBM HERMES Project Chip, allows for efficient matrix-vector multiplications (MVMs) by leveraging resistive memory devices. This approach enables single-phase MVMs, reducing latency and increasing throughput and energy efficiency compared to traditional four-phase schemes.\n- **Memristor-Based Accelerators:** Memristor-based in-memory computing can significantly accelerate matrix multiplication operations, particularly in transformer networks. This method stores weights and inputs in memristor crossbars, reducing the computational latency and power consumption. For example, a memristor-based design achieved a 10\u00d7 acceleration in transformer self-attention compared to digital counterparts.\n\n### Memory Access Patterns\n- **Cache Tiling and Register Utilization:** Optimizing matrix multiplication through cache tiling and efficient use of vector registers can minimize memory latency and maximize computational throughput. Techniques like adjusting register tile sizes to fit data into vector registers and ensuring data locality in the cache hierarchy are crucial. For instance, using an outer product approach with RVV (RISC-V Vector) instructions and optimizing register tile sizes can achieve 100% utilization of vector registers, significantly improving performance.\n\n## Methods for Dynamic State Selection and Compression\n\n### Selective State Updates\n- **DenseSSM and Mamba:** These models demonstrate the benefits of selectively integrating shallow-layer hidden states into deeper layers. DenseSSM retains fine-grained information by dense connections between layers, while Mamba improves content-based reasoning by making SSM parameters functions of the input addresses. These approaches enhance efficiency and maintain training parallelizability and inference efficiency.\n\n### Dynamic Module Activation\n- **Sparse Modular Activation (SMA):** SMA allows neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. This mechanism can improve efficiency by only activating necessary modules, reducing computational overhead.\n\n## Techniques for Efficient Integration of Multiple Architectural Components\n\n### Hybrid Architectures\n- **Block-State Transformers (BST):** BST combines an SSM sublayer for long-range contextualization with a Block Transformer sublayer for short-term representation of sequences. This hybrid approach balances the strengths of both mechanisms, enhancing model expressiveness and efficiency.\n\n### Layer-Wise Integration\n- **Layer-Wise Combination:** Integrating different mechanisms layer-wise, rather than fully integrating them, can be more effective. For example, combining SSM and sliding window attention in a layer-wise manner, as seen in Samba, can optimize performance and efficiency.\n\n### Efficient Architectural Design\n- **IO-Aware Design:** Designing models with IO-aware principles is crucial for real-world performance. This involves optimizing memory access patterns and leveraging specialized hardware acceleration for key operations, such as matrix multiplication.\n\nIn summary:\n\n- **Hardware Efficiency:** Utilize AIMC and memristor-based accelerators for efficient matrix multiplication, and optimize memory access patterns through cache tiling and efficient register utilization.\n- **Dynamic State Management:** Implement selective state updates and dynamic module activation mechanisms like DenseSSM, Mamba, and SMA to enhance efficiency and maintain model expressiveness.\n- **Architectural Integration:** Employ hybrid architectures like BST and layer-wise integration strategies to balance computational efficiency with model expressiveness.\n\nThese techniques can significantly improve the performance, efficiency, and scalability of autoregressive language models.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis:\nThe initial search reveals several promising directions for enhancing FastTTTLinear:\n\na) Hardware-Efficient Operations:\n- FlashFFTConv demonstrates efficient matrix decomposition techniques for hardware optimization\n- Importance of IO-aware algorithms for memory hierarchy optimization\n- Potential for specialized hardware acceleration of key operations\n\nb) State Management:\n- DenseSSM shows benefits of selective integration of shallow-layer states\n- Mamba's selective state mechanism provides efficient content-based filtering\n- Hybrid approaches combining SSM and attention show promise\n\nc) Architectural Innovations:\n- Block-State Transformers demonstrate effective hybridization of SSM and attention\n- Sparse Modular Activation offers dynamic module activation\n- Samba shows benefits of layer-wise combination of SSM and sliding window attention\n\n2. Useful References with Excerpts:\n\nFrom DenseMamba:\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom Mamba:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\n\nFrom Block-State Transformers:\n\"We propose a hybrid layer named Block-State Transformer (BST), that internally combines an SSM sublayer for long-range contextualization, and a Block Transformer sublayer for short-term representation of sequences.\"\n\nFrom Sparse Modular Activation:\n\"We introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner.\"\n\n3. Key Insights and Analysis:\n\na) Hardware Optimization:\n- Matrix decomposition techniques from FlashFFTConv could be adapted for more efficient linear operations\n- IO-aware design principles are crucial for real-world performance\n- Potential for specialized hardware acceleration of key operations\n\nb) State Management:\n- Dense connections between layers can improve information flow\n- Selective state updates based on content can enhance efficiency\n- Hybrid approaches combining different state mechanisms show promise\n\nc) Architectural Considerations:\n- Layer-wise combination of different mechanisms may be more effective than full integration\n- Dynamic activation of modules can improve efficiency\n- Need to balance computational efficiency with model expressiveness\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementations of hardware-efficient linear operations\n2. Methods for dynamic state selection and compression\n3. Techniques for efficient integration of multiple architectural components\n\n5. References:\n\n1. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint arXiv:2403.00818.\n\n2. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n3. Fathi, M., Pilault, J., Bacon, P.L., Pal, C., Firat, O., & Goroshin, R. (2023). Block-State Transformers. Neural Information Processing Systems.\n\n4. Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., & Zhai, C. (2023). Sparse Modular Activation for Efficient Sequence Modeling. Neural Information Processing Systems.\n\n5. Fu, D.Y., Kumbong, H., Nguyen, E.N.D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint arXiv:2311.05908."
        },
        {
            "ready": false,
            "query": "efficient state compression mechanisms, matrix-free test-time training, hardware efficient adaptive computation",
            "detail": "Provide details on:\n1. Methods for efficient state compression and update mechanisms in neural networks, particularly focusing on content-aware and adaptive approaches\n2. Techniques for implementing test-time training without matrix multiplications\n3. Hardware-efficient implementations of adaptive computation mechanisms",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide details on:\n1. Methods for efficient state compression and update mechanisms in neural networks, particularly focusing on content-aware and adaptive approaches\n2. Techniques for implementing test-time training without matrix multiplications\n3. Hardware-efficient implementations of adaptive computation mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Scalable MatMul-free Language Modeling (Avg. Score: 0.22)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.22)*\n\n```\nEshraghian ${ }^{1 *}$<br>${ }^{1}$ University of California, Santa Cruz ${ }^{2}$ Soochow University<br>${ }^{3}$ University of California, Davis ${ }^{4}$ LuxiTech\n\n\n#### Abstract\n\nMatrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to $61 \\%$ over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than $10 \\times$ compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13 W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github. com/ridgerchu/matmulfreellm. ## 1 Introduction\n\nMatrix Multiplication (MatMul) is the dominant operation in most neural networks, where dense layers involve vector-matrix multiplication (VMM), convolutions can be implemented as block-sparse VMMs with shared weights, and self-attention relies on matrix-matrix multiplication (MMM). The prevalence of MatMul is primarily due to Graphics Processing Units (GPUs) being optimized for MatMul operations. By leveraging Compute Unified Device Architecture (CUDA) and highly optimized linear algebra libraries such as cuBLAS, the MatMul operation can be efficiently parallelized and accelerated. This optimization was a key factor in the victory of AlexNet in the ILSVRC2012 competition and a historic marker for the rise of deep learning [1]. AlexNet notably utilized GPUs to boost training speed beyond CPU capabilities, and as such, deep learning won the 'hardware lottery' [2]. It also helped that both training and inference rely on MatMul. Despite its prevalence in deep learning, MatMul operations account for the dominant portion of computational expense, often consuming the majority of the execution time and memory access during\n\n[^0]both training and inference phases. Several works have replaced MatMul with simpler operations through two main strategies. The first strategy involves substituting MatMul with elementary operations, e.g., AdderNet replaces multiplication with signed addition in convolutional neural networks (CNNs) [3]. Given the focus on convolutions, AdderNet is intended for use in computer vision over language modeling. The second approach employs binary or ternary quantization, simplifying MatMul to operations where values are either flipped or zeroed out before accumulation. Quantization can be applied to either activations or weights: spiking neural networks (SNNs) use binarized activations [4, 5, 6], while binary and ternary neural networks (BNNs and TNNs) use quantized weights [7]. Both methods can also be combined $[8,9]$. Recent advances in language modeling, like BitNet [10, 11], demonstrate quantization's scalability, replacing all dense layer weights with binary/ternary values to support up to 3 billion parameters. Despite replacing VMMs with accumulations in all dense layers, BitNet retains the self-attention mechanism which relies on an expensive MMM. Dynamically computed matrices $Q$ (query) and $K$ (key) are multiplied to form the attention map. Since both $Q$ and $K$ matrices are dynamically computed from pre-activation values, achieving optimal hardware efficiency on GPUs requires custom optimizations, such as specialized kernels and advanced memory access patterns. Despite these efforts, such MatMul operations remain resource-intensive on GPUs, as they involve extensive data movement and synchronization which can significantly hinder computational throughput and efficiency [12]. In our experiments, ternary quantization of the attention matrices in BitNet causes a significant drop in performance and failure to reach model convergence (see Fig. 1). This raises the question: is it possible to completely eliminate MatMul from LLMs? In this work, we develop the first scalable MatMul-free language model (Matmul-free LM) by using additive operations in dense layers and element-wise Hadamard products for self-attention-like functions. Specifically, ternary weights eliminate MatMul in dense layers, similar to BNNs. To remove MatMul from self-attention, we optimize the Gated Recurrent Unit (GRU) [13] to rely solely on element-wise products and show that this model competes with state-of-the-art Transformers while eliminating all MatMul operations. To quantify the hardware benefits of lightweight models, we provide an optimized GPU implementation in addition to a custom FPGA accelerator. By using fused kernels in the GPU implementation of the ternary dense layers, training is accelerated by $25.6 \\%$ and memory consumption is reduced by up to $61.0 \\%$ over an unoptimized baseline on GPU. Furthermore, by employing lower-bit optimized CUDA kernels, inference speed is increased by 4.57 times, and memory usage is reduced by a factor of 10 when the model is scaled up to 13B parameters. This work goes beyond software-only implementations of lightweight models and shows how scalable, yet lightweight, language models can both reduce computational demands and energy use in the real-world. ## 2 Related Works\n\nBinary, Ternary, and Low-Precision Quantization for Language Models: The effort to quantize language models began with reducing a ternary BERT into a binarized model [14], achieving $41 \\%$ average accuracy on the GLUE benchmarks with subsequent fine-tuning. Ref. [15] distilled the intermediate outputs from a full precision BERT to a quantized version. Recently, Ref. [16] introduced an incremental quantization approach, progressively quantizing a model from 32-bit to 4-bit, 2-bit, and finally to binary model parameters. Following the quantization of BERT, low-precision language generation models have gained momentum. Ref. [17] used Quantization-Aware Training (QAT) to successfully train a model with 2-bit weights. BitNet pushed this to 3-billion-parameter binary and ternary models while maintaining competitive performance with Llama-like language models [10, 11]. MatMul-free Transformers: The use of MatMul-free Transformers has been largely concentrated in the domain of SNNs. Spikformer led the first integration of the Transformer architecture with SNNs [18, 19], with later work developing alternative Spike-driven Transformers [20, 21]. These techniques demonstrated success in vision tasks. In the language understanding domain, SpikingBERT [22] and SpikeBERT [23] applied SNNs to BERT utilizing knowledge distillation techniques to perform sentiment analysis. In language generation, SpikeGPT trained a 216 M -parameter generative model using a spiking RWKV architecture. However, these models remain constrained in size, with\n\nSpikeGPT being the largest, reflecting the challenges of scaling with binarized activations. In addition to SNNs, BNNs have also made significant progress in this area. BinaryViT [24] and BiViT [25] successfully applied Binary Vision Transformers to visual tasks. Beyond these approaches, Kosson et al. [26] achieve multiplication-free training by replacing multiplications, divisions, and non-linearities with piecewise affine approximations while maintaining performance. ## 3 Method\n\nIn this section, we break down the components of the proposed MatMul-free LM. We first describe the MatMul-free dense layers (BitLinear layers) that use ternary weights. By constraining the weights to the set $\\{-1,0,+1\\}$ and applying additional quantization techniques, MatMul operations are replaced with addition and negation operations. This reduces computational cost and memory utilization, while preserving the expressiveness of the network. We then provide further detail of our MatMul-free LM architecture, which includes a token mixer for capturing sequential dependencies and a channel mixer for integrating information across embedding dimensions. The Method section is structured as follows. First, in Sec. 3.1, we provide a comprehensive description of the MatMul-free dense layers with ternary weights, which form the foundation of our approach. Next, Sec. 3.2 introduces our hardware-efficient fused BitLinear layer, designed to optimize the implementation of BitLinear layers. Building upon these components, Sec. 3.3 delves into the details of our MatMul-free LM architecture. We present the MatMul-free token mixer, where we propose the MatMul-free Linear Gated Recurrent Unit (MLGRU), and the MatMul-free channel mixer, which employs the Gated Linear Unit (GLU) with BitLinear layers. By combining the MLGRU token mixer and the GLU channel mixer with ternary weights, our proposed architecture relies solely on addition and element-wise products.\n```\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.19)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.19)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 3. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.05)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 27/30 (Score: 0.07)*\n\n```\nAdvances in Neural Information Processing Systems, 36, 2024. [48] L. Zancato, A. Achille, G. Paolini, A. Chiuso, and S. Soatto. Stacked residuals of dynamic layers for time series anomaly detection, 2022. [49] Luca Zancato, Alessandro Achille, Tian Yu Liu, Matthew Trager, Pramuditha Perera, and Stefano Soatto. Train/test-time adaptation with retrieval. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15911-15921, 2023. [50] Luca Zancato and Alessandro Chiuso. A novel deep neural network architecture for non-linear system identification. IFAC-PapersOnLine, 54(7):186-191, 2021. 19th IFAC Symposium on System Identification SYSID 2021. [51] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. [52] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning (still) requires rethinking generalization. Communications of the ACM, 64(3):107$115,2021$. [53] Jacob Ziv and Abraham Lempel. Compression of individual sequences via variable-rate coding. IEEE transactions on Information Theory, 24(5):530-536, 1978. ## A Induction and Transduction\n\nExample A. 1 (Biology). We note that biological agents have no option but to operate inductively, due to (a) hard memory bounds, and (b) evolutionary pressure towards minimizing inference latency: When faced with a threat, a biological agent is better served by a quick suboptimal decision than by reasoning over all past experience. AI built on silicon has no such limitations: Memory can grow unbounded and test-time computation can be distributed and improved by hardware design. Nonetheless, any practical realization involves some kind of constraint on inference time or compute resources. Therefore, resource-constrained optimal inference hinges on how to best use the available resources against a growing memory. Example A. 2 (CNN Classifiers, VAEs and GANs). A trained representation can be co-opted to generate data. For example, a CNN can be used to classify random data until one is labeled with the desired class, and the resulting sample considered as being \"generated\" by the CNN. Similarly, one could generate random data indirectly by feeding noise to an encoder, as done in Generative Adversarial Networks (GANs), again co-opting a representation for generating data. In a Variational Autoencoder (VAE), data is generated by perturbing the latent representation of a map trained to re-construct the dataset. Example A. 3 (Diffusion Models). Diffusion Models are representations, trained to re-construct the original data, but the mechanics used to reconstruct the data during training are sequential, using an artificial \"time\" variable, akin to a realization. This makes their use as \"generative representation\" natural since the reconstruction process is already a stochastic realization $8^{8}$\nExample A.\n```\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.03)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 4. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.05)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.05)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient state compression mechanisms, matrix-free test-time training, hardware efficient adaptive computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Efficient Transformer Knowledge Distillation: A Performance Review\n\n*From Search Query: efficient state compression mechanisms*\n\n*Nathan Brown, Ashton Williamson, Tahj Anderson, Logan Lawrence*\n\n**TL;DR:** It is found that distilled efficient attention transformers can preserve a significant amount of original model performance, and performing knowledge distillation is an effective method to yield high-performing efficient attention models with low costs.\n\n**Abstract:** As pretrained transformer language models continue to achieve state-of-the-art performance, the Natural Language Processing community has pushed for advances in model compression and efficient attention mechanisms to address high computational requirements and limited input sequence length. Despite these separate efforts, no investigation has been done into the intersection of these two fields. In this work, we provide an evaluation of model compression via knowledge distillation on efficient attention transformers. We provide cost-performance trade-offs for the compression of state-of-the-art efficient attention architectures and the gains made in performance in comparison to their full attention counterparts. Furthermore, we introduce a new long-context Named Entity Recognition dataset, GONERD, to train and test the performance of NER models on long sequences. We find that distilled efficient attention transformers can preserve a significant amount of original model performance, preserving up to 98.6% across short-context tasks (GLUE, SQUAD, CoNLL-2003), up to 94.6% across long-context Question-and-Answering tasks (HotpotQA, TriviaQA), and up to 98.8% on long-context Named Entity Recognition (GONERD), while decreasing inference times by up to 57.8%. We find that, for most models on most tasks, performing knowledge distillation is an effective method to yield high-performing efficient attention models with low costs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 2. 2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression\n\n*From Search Query: efficient state compression mechanisms*\n\n*A. Tyurin, Peter Richt'arik*\n\n**TL;DR:** This work develops a new and provably accelerated method based on fast bidirectional compressed communication and a new bespoke error-feedback mechanism which may be of independent interest, which improves upon the communication complexity of the vanilla accelerated gradient descent (AGD) method.\n\n**Abstract:** We consider distributed convex optimization problems in the regime when the communication between the server and the workers is expensive in both uplink and downlink directions. We develop a new and provably accelerated method, which we call 2Direction, based on fast bidirectional compressed communication and a new bespoke error-feedback mechanism which may be of independent interest. Indeed, we find that the EF and EF21-P mechanisms (Seide et al., 2014; Gruntkowska et al., 2023) that have considerable success in the design of efficient non-accelerated methods are not appropriate for accelerated methods. In particular, we prove that 2Direction improves the previous state-of-the-art communication complexity $\\widetilde{\\Theta}\\left(K \\times \\left(\\frac{L}{\\alpha \\mu} + \\frac{L_{\\max} \\omega}{n \\mu} + \\omega\\right)\\right)$ (Gruntkowska et al., 2023) to $\\widetilde{\\Theta}(K \\times (\\sqrt{\\frac{L (\\omega + 1)}{\\alpha \\mu}} + \\sqrt{\\frac{L_{\\max} \\omega^2}{n \\mu}} + \\frac{1}{\\alpha} + \\omega))$ in the $\\mu$-strongly-convex setting, where $L$ and $L_{\\max}$ are smoothness constants, $n$ is # of workers, $\\omega$ and $\\alpha$ are compression errors of the Rand$K$ and Top$K$ sparsifiers (as examples), $K$ is # of coordinates/bits that the server and workers send to each other. Moreover, our method is the first that improves upon the communication complexity of the vanilla accelerated gradient descent (AGD) method (Nesterov, 2018). We obtain similar improvements in the general convex regime as well. Finally, our theoretical findings are corroborated by experimental evidence.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression\n\n*From Search Query: efficient state compression mechanisms*\n\n*Yufeng Zhang, Hang Yu, Jianguo Li, Weiyao Lin*\n\n**TL;DR:** A novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression that incorporates an efficient autoregressive Markov model based entropy coder and a Straight-Through Hardmax Quantization scheme to enhance the optimization of discrete latent space.\n\n**Abstract:** A BSTRACT Learned lossless data compression has garnered significant attention recently due to its superior compression ratios compared to traditional compressors. However, the computational efficiency of these models jeopardizes their practicality. This paper proposes a novel system for improving the compression ratio while maintaining computational efficiency for learned lossless data compression. Our approach incorporates two essential innovations. First, we propose the Finite-State AutoRe-gressive (FSAR) entropy coder, an efficient autoregressive Markov model based entropy coder that utilizes a lookup table to expedite autoregressive entropy coding. Next, we present a Straight-Through Hardmax Quantization (STHQ) scheme to enhance the optimization of discrete latent space. Our experiments show that the proposed lossless compression method could improve the compression ratio by up to 6% compared to the baseline, with negligible extra computational time. Our work provides valuable insights into enhancing the computational efficiency of learned lossless data compression, which can have practical applications in various fields. Code is available at https://github.com/alipay/Finite_ State_Autoregressive_Entropy_Coding .\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 4. Extrapolative Continuous-time Bayesian Neural Network for Fast Training-free Test-time Adaptation\n\n*From Search Query: matrix-free test-time training*\n\n*Hengguan Huang, Xiangming Gu, Hongya Wang, Chang Xiao, Hongfu Liu, Ye Wang*\n\n**TL;DR:** This paper proposes to formulate internal predictive modeling as a continuous-time Bayesian filtering problem within a stochastic dynamical system context and develops extrap-olative continuous-time Bayesian neural networks (ECBNN 2 ), which generalize existing Bayesian neural networks to represent temporal dynamics and allow us to extrapolate the distribution of model parameters before observing the incoming data, therefore effectively reducing the latency.\n\n**Abstract:** Human intelligence has shown remarkably lower latency and higher precision than most AI systems when processing non-stationary streaming data in real-time. Numerous neuroscience studies suggest that such abilities may be driven by internal predictive modeling. In this paper, we explore the possibility of introducing such a mechanism in unsupervised domain adaptation (UDA) for handling non-stationary streaming data for real-time streaming applications. We propose to formulate internal predictive modeling as a continuous-time Bayesian filtering problem within a stochastic dynamical system context. Such a dynamical system describes the dynamics of model parameters of a UDA model evolving with non-stationary streaming data. Building on such a dynamical system, we then develop extrap-olative continuous-time Bayesian neural networks (ECBNN 2 ), which generalize existing Bayesian neural networks to represent temporal dynamics and allow us to extrapolate the distribution of model parameters before observing the incoming data, therefore effectively reducing the latency. Remarkably, our empirical results show that ECBNN is capable of continuously generating better distributions of model parameters along the time axis given historical data only, thereby achieving (1) training-free test-time adaptation with low latency, (2) gradually improved alignment between the source and target features and (3) gradually improved model performance over time during the real-time testing stage.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 5. DELTA: degradation-free fully test-time adaptation\n\n*From Search Query: matrix-free test-time training*\n\n*Bowen Zhao, Chen Chen, Shutao Xia*\n\n**TL;DR:** A plug-in solution called DELTA for Degradation-freE fuLly Test-time Adaptation, which consists of two components: test-time Batch Renormalization (TBR), introduced to improve the estimated normalization statistics, and Dynamic Online re-weighTing (DOT), designed to address the class bias within optimization.\n\n**Abstract:** Fully test-time adaptation aims at adapting a pre-trained model to the test stream during real-time inference, which is urgently required when the test distribution differs from the training distribution. Several efforts have been devoted to improving adaptation performance. However, we find that two unfavorable defects are concealed in the prevalent adaptation methodologies like test-time batch normalization (BN) and self-learning. First, we reveal that the normalization statistics in test-time BN are completely affected by the currently received test samples, resulting in inaccurate estimates. Second, we show that during test-time adaptation, the parameter update is biased towards some dominant classes. In addition to the extensively studied test stream with independent and class-balanced samples, we further observe that the defects can be exacerbated in more complicated test environments, such as (time) dependent or class-imbalanced data. We observe that previous approaches work well in certain scenarios while show performance degradation in others due to their faults. In this paper, we provide a plug-in solution called DELTA for Degradation-freE fuLly Test-time Adaptation, which consists of two components: (i) Test-time Batch Renormalization (TBR), introduced to improve the estimated normalization statistics. (ii) Dynamic Online re-weighTing (DOT), designed to address the class bias within optimization. We investigate various test-time adaptation methods on three commonly used datasets with four scenarios, and a newly introduced real-world dataset. DELTA can help them deal with all scenarios simultaneously, leading to SOTA performance.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 39  (*Influential: 3*)\n\n#### 6. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation\n\n*From Search Query: hardware efficient adaptive computation*\n\n*Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, L. Lakshmanan, A. Awadallah, S\u00e9bastien Bubeck, Jianfeng Gao*\n\n**TL;DR:** AutoMoE is developed -- a framework for designing heterogeneous MoE's under computational constraints and leverages Neural Architecture Search to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, on aggregate over benchmark datasets for NMT.\n\n**Abstract:** Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 7. HELP: Hardware-Adaptive Efficient Latency Predictor for NAS via Meta-Learning\n\n*From Search Query: hardware efficient adaptive computation*\n\n*Hayeon Lee, Sewoong Lee, S. Chong, S. Hwang*\n\n**TL;DR:** Hardware-adaptive Efficient Latency Predictor (HELP) is proposed, which formulates the device-specific latency estimation problem as a meta-learning problem, such that it can estimate the latency of a model's performance for a given task on an unseen device with a few samples.\n\n**Abstract:** For deployment, neural architecture search should be hardware-aware, in order to satisfy the device-specific constraints (e.g., memory usage, latency and energy consumption) and enhance the model efficiency. Existing methods on hardware-aware NAS collect a large number of samples (e.g., accuracy and latency) from a target device, either builds a lookup table or a latency estimator. However, such approach is impractical in real-world scenarios as there exist numerous devices with different hardware specifications, and collecting samples from such a large number of devices will require prohibitive computational and monetary cost. To overcome such limitations, we propose Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the device-specific latency estimation problem as a meta-learning problem, such that we can estimate the latency of a model's performance for a given task on an unseen device with a few samples. To this end, we introduce novel hardware embeddings to embed any devices considering them as black-box functions that output latencies, and meta-learn the hardware-adaptive latency predictor in a device-dependent manner, using the hardware embeddings. We validate the proposed HELP for its latency estimation performance on unseen platforms, on which it achieves high estimation performance with as few as 10 measurement samples, outperforming all relevant baselines. We also validate end-to-end NAS frameworks using HELP against ones without it, and show that it largely reduces the total time cost of the base NAS method, in latency-constrained settings. Code is available at https://github.com/HayeonLee/HELP.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 24  (*Influential: 4*)\n\n#### 8. Adaptive Window Pruning for Efficient Local Motion Deblurring\n\n*From Search Query: hardware efficient adaptive computation*\n\n*Haoying Li, Jixin Zhao, Shangchen Zhou, H. Feng, Chongyi Li, Chen Change Loy*\n\n**TL;DR:** This paper proposes a local motion deblurring vision Transformer built on adaptive window pruning Transformer blocks (AdaWPT), and removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements compared to state-of-the-art methods.\n\n**Abstract:** Local motion blur commonly occurs in real-world photography due to the mixing between moving objects and stationary backgrounds during exposure. Existing image deblurring methods predominantly focus on global deblurring, inadvertently affecting the sharpness of backgrounds in locally blurred images and wasting unnecessary computation on sharp pixels, especially for high-resolution images. This paper aims to adaptively and efficiently restore high-resolution locally blurred images. We propose a local motion deblurring vision Transformer (LMD-ViT) built on adaptive window pruning Transformer blocks (AdaWPT). To focus deblurring on local regions and reduce computation, AdaWPT prunes unnecessary windows, only allowing the active windows to be involved in the deblurring processes. The pruning operation relies on the blurriness confidence predicted by a confidence predictor that is trained end-to-end using a reconstruction loss with Gumbel-Softmax re-parameterization and a pruning loss guided by annotated blur masks. Our method removes local motion blur effectively without distorting sharp regions, demonstrated by its exceptional perceptual and quantitative improvements compared to state-of-the-art methods. In addition, our approach substantially reduces FLOPs by 66% and achieves more than a twofold increase in inference speed compared to Transformer-based deblurring methods. We will make our code and annotated blur masks publicly available.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. High Fidelity Neural Audio Compression\n\n*From Search Query: efficient state compression mechanisms*\n\n*Yossi Adi, Gabriel Synnaeve, Jade Copet, Alexandre D\u00e9fossez*\n\n**Abstract:** We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.\n\n**Published:** 2022-10-24\n\n\n\n#### 2. Mini-Monkey: Multi-Scale Adaptive Cropping for Multimodal Large Language Models\n\n*From Search Query: efficient state compression mechanisms*\n\n*Xiang Bai, Lianwen Jin, Dingkang Liang, Yuliang Liu, Mingxin Huang*\n\n**Abstract:** Recently, there has been significant interest in enhancing the capability of multimodal large language models (MLLMs) to process high-resolution images. Most existing methods focus on adopting a cropping strategy to improve the ability of multimodal large language models to understand image details. However, this cropping operation inevitably causes the segmentation of objects and connected areas, which impairs the MLLM's ability to recognize small or irregularly shaped objects or text. This issue is particularly evident in lightweight MLLMs. Addressing this issue, we propose Mini-Monkey, a lightweight MLLM that incorporates a plug-and-play method called multi-scale adaptive crop strategy (MSAC). Mini-Monkey adaptively generates multi-scale representations, allowing it to select non-segmented objects from various scales. To mitigate the computational overhead introduced by MSAC, we propose a Scale Compression Mechanism (SCM), which effectively compresses image tokens. Mini-Monkey achieves state-of-the-art performance among 2B-parameter MLLMs. It not only demonstrates leading performance on a variety of general multimodal understanding tasks but also shows consistent improvements in document understanding capabilities. On the OCRBench, Mini-Monkey achieves a score of 802, outperforming 8B-parameter state-of-the-art model InternVL2-8B. Besides, our model and training strategy are very efficient, which can be trained with only eight RTX 3090. The code is available at https://github.com/Yuliang-Liu/Monkey.\n\n**Published:** 2024-08-04\n\n\n\n#### 3. Unsupervised Music Source Separation Using Differentiable Parametric Source Models\n\n*From Search Query: matrix-free test-time training*\n\n*Liam Kelley, Ga\u00ebl Richard, Roland Badeau, Clement S. J. Doire, Kilian Schulze-Forster*\n\n**Abstract:** Supervised deep learning approaches to underdetermined audio source separation achieve state-of-the-art performance but require a dataset of mixtures along with their corresponding isolated source signals. Such datasets can be extremely costly to obtain for musical mixtures. This raises a need for unsupervised methods. We propose a novel unsupervised model-based deep learning approach to musical source separation. Each source is modelled with a differentiable parametric source-filter model. A neural network is trained to reconstruct the observed mixture as a sum of the sources by estimating the source models' parameters given their fundamental frequencies. At test time, soft masks are obtained from the synthesized source signals. The experimental evaluation on a vocal ensemble separation task shows that the proposed method outperforms learning-free methods based on nonnegative matrix factorization and a supervised deep learning baseline. Integrating domain knowledge in the form of source models into a data-driven method leads to high data efficiency: the proposed approach achieves good separation quality even when trained on less than three minutes of audio. This work makes powerful deep learning based separation usable in scenarios where training data with ground truth is expensive or nonexistent.\n\n**Published:** 2022-01-24\n\n\n\n#### 4. Test-Time Model Adaptation with Only Forward Passes\n\n*From Search Query: matrix-free test-time training*\n\n*Peilin Zhao, Pengcheng Wu, Guohao Chen, Chunyan Miao, Shuaicheng Niu*\n\n**Abstract:** Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Optimization Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C.\n\n**Published:** 2024-04-02\n\n\n\n#### 5. Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer\n\n*From Search Query: hardware efficient adaptive computation*\n\n*Dhabaleswar K. Panda, Hari Subramoni, Aamir Shafi, Olatunji Ruwase, Masahiro Tanaka, Sam Ade Jacobs, Jinghan Yao*\n\n**Abstract:** Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.\n\n**Published:** 2024-08-30\n\n\n\n#### 6. Tune: A Research Platform for Distributed Model Selection and Training\n\n*From Search Query: hardware efficient adaptive computation*\n\n*Ion Stoica, Philipp Moritz, Robert Nishihara, Joseph E. Gonzalez, Eric Liang, Richard Liaw*\n\n**Abstract:** Modern machine learning algorithms are increasingly computationally\ndemanding, requiring specialized hardware and distributed computation to\nachieve high performance in a reasonable time frame. Many hyperparameter search\nalgorithms have been proposed for improving the efficiency of model selection,\nhowever their adaptation to the distributed compute environment is often\nad-hoc. We propose Tune, a unified framework for model selection and training\nthat provides a narrow-waist interface between training scripts and search\nalgorithms. We show that this interface meets the requirements for a broad\nrange of hyperparameter search algorithms, allows straightforward scaling of\nsearch to large clusters, and simplifies algorithm implementation. We\ndemonstrate the implementation of several state-of-the-art hyperparameter\nsearch algorithms in Tune. Tune is available at\nhttp://ray.readthedocs.io/en/latest/tune.html.\n\n**Published:** 2018-07-13\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on efficient state compression, matrix-free test-time training, and hardware-efficient adaptive computation, here are some key insights and methods derived from the provided sources and analysis:\n\n## Methods for Efficient State Compression and Update Mechanisms\n\n### Efficient State Space Models\nThe work on Efficient State Space Models (eSSM) introduces a novel approach to modeling sequential data efficiently. eSSM uses a convolutional representation of multi-input multi-output (MIMO) state space models, which allows for fewer parameters and faster training speeds. This model employs techniques such as diagonalization and fast tensor convolution to reduce parameter size and improve computational efficiency. The block diagonalization of the SSM further reduces the model parameters, making it a promising approach for efficient state compression and update mechanisms.\n\n### Compression of Optimizer States and Activations\nThe COAT framework proposes compressing optimizer states and activations into lower precision formats (FP8) to reduce memory footprint. This method uses Dynamic Range Expansion to align optimizer state distributions with the FP8 representation range, minimizing quantization error. Additionally, Mixed-Granularity Activation Quantization optimizes activation memory using per-tensor and per-group quantization strategies. These techniques can be adapted to compress and update states efficiently in autoregressive models, reducing memory consumption without significant performance degradation.\n\n## Techniques for Implementing Test-Time Training Without Matrix Multiplications\n\n### Matrix-Free Operations\nResearch on scalable MatMul-free language modeling suggests that matrix multiplication can be eliminated from large language models while maintaining strong performance. This can be achieved by replacing matrix multiplications with simpler operations like additions and element-wise operations. Such hardware-friendly alternatives can significantly reduce computational costs and are crucial for efficient test-time training [Analysis Note].\n\n### Lightning Attention\nThe Lightning Attention mechanism maintains constant training speed for various sequence lengths under fixed memory consumption by utilizing different attention calculation strategies. This approach can be integrated into test-time training to avoid matrix multiplications, thereby enhancing efficiency[Analysis Note].\n\n## Hardware-Efficient Implementations of Adaptive Computation Mechanisms\n\n### Hardware-Efficient State Space Models\nThe eSSM architecture is designed to be computationally efficient and can be trained efficiently using convolutional representations. This model's use of diagonalization, fast tensor convolution, and block diagonalization makes it highly suitable for hardware-efficient implementations. The ability to parallelize the training process using these techniques can significantly enhance the model's performance on various hardware platforms.\n\n### IO-Aware Design and Memory Access Patterns\nThe analysis highlights the importance of IO-aware design principles and memory access patterns for real-world performance. Efficient state compression and update mechanisms should be designed with these considerations in mind to optimize memory usage and reduce computational overhead. Techniques like selective state updates and content-based filtering mechanisms can be integrated to make the model more adaptive and efficient[Analysis Note].\n\n### Low-Precision Training\nThe COAT framework demonstrates that low-precision training (FP8) can significantly reduce memory footprint and speed up training without sacrificing performance. This approach can be combined with other efficient computation mechanisms to create a highly efficient and adaptive autoregressive language model.\n\nBy integrating these methods, researchers can develop an autoregressive language model that is not only efficient in terms of state compression and update mechanisms but also optimized for hardware efficiency and adaptive computation, leading to improved overall performance.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis:\n\nThe search results reveal several promising directions for enhancing FastTTTLinear:\n\na) Hardware-Efficient Operations:\n- Matrix-free architectures show potential for significant efficiency gains\n- IO-aware design principles are crucial for real-world performance\n- Memory access patterns and cache utilization significantly impact efficiency\n\nb) Dynamic State Management:\n- Selective state updates can improve efficiency while maintaining expressiveness\n- Content-based filtering mechanisms show promise for adaptive computation\n- Compression techniques can reduce memory requirements without sacrificing performance\n\nc) Architectural Integration:\n- Hybrid approaches combining different mechanisms show strong results\n- Layer-wise integration may be more effective than full integration\n- Hardware-aware design principles are crucial for practical deployment\n\n2. Useful References with Excerpts:\n\nFrom \"Scalable MatMul-free Language Modeling\":\n\"Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs)... we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales.\"\n\nFrom \"Various Lengths, Constant Speed\":\n\"Lightning Attention... maintains a constant training speed for various sequence lengths under fixed memory consumption... by utilizing different attention calculation strategies to compute the different parts of attention.\"\n\nFrom \"MA-BERT\":\n\"...eliminating complex non-linear functions within the Transformer architecture... enables hardware-friendly processing on various computing engines, including CPUs and GPUs.\"\n\n3. Key Insights and Analysis:\n\na) Matrix-Free Operations:\n- Replacing matrix multiplications with simpler operations can significantly reduce computational costs\n- Hardware-friendly alternatives like additions and element-wise operations show promise\n- Memory access patterns are crucial for efficiency\n\nb) Dynamic Computation:\n- Selective processing based on input content can improve efficiency\n- Adaptive mechanisms can reduce unnecessary computation\n- State compression can be effective when content-aware\n\nc) Integration Strategies:\n- Layer-wise combination of different mechanisms may be more effective than full integration\n- Hardware considerations should guide architectural choices\n- IO-aware design principles are crucial for practical performance\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific techniques for efficient state compression and update mechanisms\n2. Methods for combining matrix-free operations with test-time training\n3. Hardware-efficient implementations of adaptive computation\n\n5. References:\n\n1. Zhu, R., Zhang, Y., Sifferman, E., Sheaves, T., Wang, Y., Richmond, D., Zhou, P., & Eshraghian, J. (2024). Scalable MatMul-free Language Modeling. arXiv preprint arXiv:2401.04678.\n\n2. Qin, Z., Sun, W., Li, D., Shen, X., Sun, W., & Zhong, Y. (2024). Various Lengths, Constant Speed: Efficient Language Modeling with Lightning Attention. International Conference on Machine Learning.\n\n3. Wei Ming, N., Wang, Z., Liu, C., Goh, R., & Luo, T. (2023). MA-BERT: Towards Matrix Arithmetic-only BERT Inference by Eliminating Complex Non-Linear Functions. International Conference on Learning Representations.\n\n4. Miao, S., Lu, Z., Liu, M., Duarte, J., & Li, P. (2024). Locality-Sensitive Hashing-Based Efficient Point Transformer with Applications in High-Energy Physics. International Conference on Machine Learning.\n\n5. Lu, Q., Dou, D., & Nguyen, T. H. (2021). Parameter-Efficient Domain Knowledge Integration from Multiple Sources for Biomedical Pre-trained Language Models. Conference on Empirical Methods in Natural Language Processing."
        },
        {
            "ready": false,
            "query": "evolution strategies test-time adaptation, matrix-free state compression, selective computation hardware",
            "detail": "Provide details on:\n1. Implementations of evolution strategies for test-time adaptation, particularly focusing on forward-pass-only approaches\n2. Methods for combining matrix-free operations with state compression in neural networks\n3. Hardware-efficient implementations of selective computation mechanisms",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nProvide details on:\n1. Implementations of evolution strategies for test-time adaptation, particularly focusing on forward-pass-only approaches\n2. Methods for combining matrix-free operations with state compression in neural networks\n3. Hardware-efficient implementations of selective computation mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.59)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.70)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.48)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Scalable MatMul-free Language Modeling (Avg. Score: 0.18)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 2/27 (Score: 0.18)*\n\n```\nEshraghian ${ }^{1 *}$<br>${ }^{1}$ University of California, Santa Cruz ${ }^{2}$ Soochow University<br>${ }^{3}$ University of California, Davis ${ }^{4}$ LuxiTech\n\n\n#### Abstract\n\nMatrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to $61 \\%$ over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than $10 \\times$ compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13 W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github. com/ridgerchu/matmulfreellm. ## 1 Introduction\n\nMatrix Multiplication (MatMul) is the dominant operation in most neural networks, where dense layers involve vector-matrix multiplication (VMM), convolutions can be implemented as block-sparse VMMs with shared weights, and self-attention relies on matrix-matrix multiplication (MMM). The prevalence of MatMul is primarily due to Graphics Processing Units (GPUs) being optimized for MatMul operations. By leveraging Compute Unified Device Architecture (CUDA) and highly optimized linear algebra libraries such as cuBLAS, the MatMul operation can be efficiently parallelized and accelerated. This optimization was a key factor in the victory of AlexNet in the ILSVRC2012 competition and a historic marker for the rise of deep learning [1]. AlexNet notably utilized GPUs to boost training speed beyond CPU capabilities, and as such, deep learning won the 'hardware lottery' [2]. It also helped that both training and inference rely on MatMul. Despite its prevalence in deep learning, MatMul operations account for the dominant portion of computational expense, often consuming the majority of the execution time and memory access during\n\n[^0]both training and inference phases. Several works have replaced MatMul with simpler operations through two main strategies. The first strategy involves substituting MatMul with elementary operations, e.g., AdderNet replaces multiplication with signed addition in convolutional neural networks (CNNs) [3]. Given the focus on convolutions, AdderNet is intended for use in computer vision over language modeling. The second approach employs binary or ternary quantization, simplifying MatMul to operations where values are either flipped or zeroed out before accumulation. Quantization can be applied to either activations or weights: spiking neural networks (SNNs) use binarized activations [4, 5, 6], while binary and ternary neural networks (BNNs and TNNs) use quantized weights [7]. Both methods can also be combined $[8,9]$. Recent advances in language modeling, like BitNet [10, 11], demonstrate quantization's scalability, replacing all dense layer weights with binary/ternary values to support up to 3 billion parameters. Despite replacing VMMs with accumulations in all dense layers, BitNet retains the self-attention mechanism which relies on an expensive MMM. Dynamically computed matrices $Q$ (query) and $K$ (key) are multiplied to form the attention map. Since both $Q$ and $K$ matrices are dynamically computed from pre-activation values, achieving optimal hardware efficiency on GPUs requires custom optimizations, such as specialized kernels and advanced memory access patterns. Despite these efforts, such MatMul operations remain resource-intensive on GPUs, as they involve extensive data movement and synchronization which can significantly hinder computational throughput and efficiency [12]. In our experiments, ternary quantization of the attention matrices in BitNet causes a significant drop in performance and failure to reach model convergence (see Fig. 1). This raises the question: is it possible to completely eliminate MatMul from LLMs? In this work, we develop the first scalable MatMul-free language model (Matmul-free LM) by using additive operations in dense layers and element-wise Hadamard products for self-attention-like functions. Specifically, ternary weights eliminate MatMul in dense layers, similar to BNNs. To remove MatMul from self-attention, we optimize the Gated Recurrent Unit (GRU) [13] to rely solely on element-wise products and show that this model competes with state-of-the-art Transformers while eliminating all MatMul operations. To quantify the hardware benefits of lightweight models, we provide an optimized GPU implementation in addition to a custom FPGA accelerator. By using fused kernels in the GPU implementation of the ternary dense layers, training is accelerated by $25.6 \\%$ and memory consumption is reduced by up to $61.0 \\%$ over an unoptimized baseline on GPU. Furthermore, by employing lower-bit optimized CUDA kernels, inference speed is increased by 4.57 times, and memory usage is reduced by a factor of 10 when the model is scaled up to 13B parameters. This work goes beyond software-only implementations of lightweight models and shows how scalable, yet lightweight, language models can both reduce computational demands and energy use in the real-world. ## 2 Related Works\n\nBinary, Ternary, and Low-Precision Quantization for Language Models: The effort to quantize language models began with reducing a ternary BERT into a binarized model [14], achieving $41 \\%$ average accuracy on the GLUE benchmarks with subsequent fine-tuning. Ref. [15] distilled the intermediate outputs from a full precision BERT to a quantized version. Recently, Ref. [16] introduced an incremental quantization approach, progressively quantizing a model from 32-bit to 4-bit, 2-bit, and finally to binary model parameters. Following the quantization of BERT, low-precision language generation models have gained momentum. Ref. [17] used Quantization-Aware Training (QAT) to successfully train a model with 2-bit weights. BitNet pushed this to 3-billion-parameter binary and ternary models while maintaining competitive performance with Llama-like language models [10, 11]. MatMul-free Transformers: The use of MatMul-free Transformers has been largely concentrated in the domain of SNNs. Spikformer led the first integration of the Transformer architecture with SNNs [18, 19], with later work developing alternative Spike-driven Transformers [20, 21]. These techniques demonstrated success in vision tasks. In the language understanding domain, SpikingBERT [22] and SpikeBERT [23] applied SNNs to BERT utilizing knowledge distillation techniques to perform sentiment analysis. In language generation, SpikeGPT trained a 216 M -parameter generative model using a spiking RWKV architecture. However, these models remain constrained in size, with\n\nSpikeGPT being the largest, reflecting the challenges of scaling with binarized activations. In addition to SNNs, BNNs have also made significant progress in this area. BinaryViT [24] and BiViT [25] successfully applied Binary Vision Transformers to visual tasks. Beyond these approaches, Kosson et al. [26] achieve multiplication-free training by replacing multiplications, divisions, and non-linearities with piecewise affine approximations while maintaining performance. ## 3 Method\n\nIn this section, we break down the components of the proposed MatMul-free LM. We first describe the MatMul-free dense layers (BitLinear layers) that use ternary weights. By constraining the weights to the set $\\{-1,0,+1\\}$ and applying additional quantization techniques, MatMul operations are replaced with addition and negation operations. This reduces computational cost and memory utilization, while preserving the expressiveness of the network. We then provide further detail of our MatMul-free LM architecture, which includes a token mixer for capturing sequential dependencies and a channel mixer for integrating information across embedding dimensions. The Method section is structured as follows. First, in Sec. 3.1, we provide a comprehensive description of the MatMul-free dense layers with ternary weights, which form the foundation of our approach. Next, Sec. 3.2 introduces our hardware-efficient fused BitLinear layer, designed to optimize the implementation of BitLinear layers. Building upon these components, Sec. 3.3 delves into the details of our MatMul-free LM architecture. We present the MatMul-free token mixer, where we propose the MatMul-free Linear Gated Recurrent Unit (MLGRU), and the MatMul-free channel mixer, which employs the Gated Linear Unit (GLU) with BitLinear layers. By combining the MLGRU token mixer and the GLU channel mixer with ternary weights, our proposed architecture relies solely on addition and element-wise products.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.01)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality (Avg. Score: 0.01)\n\n*Tri Dao, Albert Gu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 25  (*Influential: 5*)\n\n**TL;DR:** The state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n##### *Relevant Chunk: No. 3/86 (Score: 0.01)*\n\n```\nBeyond its intrinsic theoretical value, our framework opens up a broad set of directions for understanding and improving sequence models. Efficient Algorithms. First and most importantly, our framework exposes new efficient and easily-implementable algorithms for computing SSMs (Section 6). We introduce a new SSD algorithm, based on block decompositions of semiseparable matrices, that takes advantage of both the linear SSM recurrence and quadratic dual form, obtaining optimal tradeoffs on all main efficiency axes (e.g. training and inference compute, memory usage, and ability to leverage matrix multiplication units on modern hardware). A dedicated implementation of SSD is $2-8 \\times$ faster than the optimized selective scan implementation of Mamba, while simultaneously allowing for much larger recurrent state sizes ( $8 \\times$ the size of Mamba or even higher, with minimal slowdown). SSD is highly competitive with optimized implementations of softmax attention (FlashAttention-2 (Dao 2024)), crossing over at sequence length 2 K and $6 \\times$ faster at sequence length 16 K . Architecture Design. One major obstacle to adopting new architectures such as SSMs is the ecosystem tailored to Transformers, such as hardware-efficient optimization and parallelism techniques for large-scale training. Our framework allows using established conventions and techniques for attention to build a vocabulary of architecture design choices for SSMs, and further improve them (Section 7). For example, we introduce the analog of heads from multi-head attention (MHA) to SSMs. We show that the Mamba architecture is a multi-input SSM (MIS) that turns out to be analogous to multi-value attention (MVA), and compare other variants of Mamba with different head structures. We also use these ideas to make slight modifications to the Mamba block, which allows tensor parallelism to be implemented (e.g.\n```\n\n\n\n---\n## Found 14 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: evolution strategies test-time adaptation, matrix-free state compression, selective computation hardware\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. TTN: A Domain-Shift Aware Batch Normalization in Test-Time Adaptation\n\n*From Search Query: evolution strategies test-time adaptation*\n\n*Hyesu Lim, Byeonggeun Kim, J. Choo, Sungha Choi*\n\n**TL;DR:** A new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer is presented and improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios.\n\n**Abstract:** This paper proposes a novel batch normalization strategy for test-time adaptation. Recent test-time adaptation methods heavily rely on the modified batch normalization, i.e., transductive batch normalization (TBN), which calculates the mean and the variance from the current test batch rather than using the running mean and variance obtained from the source data, i.e., conventional batch normalization (CBN). Adopting TBN that employs test batch statistics mitigates the performance degradation caused by the domain shift. However, re-estimating normalization statistics using test data depends on impractical assumptions that a test batch should be large enough and be drawn from i.i.d. stream, and we observed that the previous methods with TBN show critical performance drop without the assumptions. In this paper, we identify that CBN and TBN are in a trade-off relationship and present a new test-time normalization (TTN) method that interpolates the statistics by adjusting the importance between CBN and TBN according to the domain-shift sensitivity of each BN layer. Our proposed TTN improves model robustness to shifted domains across a wide range of batch sizes and in various realistic evaluation scenarios. TTN is widely applicable to other test-time adaptation methods that rely on updating model parameters via backpropagation. We demonstrate that adopting TTN further improves their performance and achieves state-of-the-art performance in various standard benchmarks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 68  (*Influential: 6*)\n\n#### 2. ViDA: Homeostatic Visual Domain Adapter for Continual Test Time Adaptation\n\n*From Search Query: evolution strategies test-time adaptation*\n\n*Jiaming Liu, Senqiao Yang, Peidong Jia, Ming Lu, Yandong Guo, Wei Xue, Shanghang Zhang*\n\n**TL;DR:** A Visual Domain Adapter (ViDA) for CTTA is designed, explicitly handling both domain-specific and domain-shared knowledge, and a Homeostatic Knowledge Allotment (HKA) strategy is proposed, which adaptively combines different knowledge from each ViDA.\n\n**Abstract:** Since real-world machine systems are running in non-stationary environments, Continual Test-Time Adaptation (CTTA) task is proposed to adapt the pre-trained model to continually changing target domains. Recently, existing methods mainly focus on model-based adaptation, which aims to leverage a self-training manner to extract the target domain knowledge. However, pseudo labels can be noisy and the updated model parameters are unreliable under dynamic data distributions, leading to error accumulation and catastrophic forgetting in the continual adaptation process. To tackle these challenges and maintain the model plasticity, we design a Visual Domain Adapter (ViDA) for CTTA, explicitly handling both domain-specific and domain-shared knowledge. Specifically, we first comprehensively explore the different domain representations of the adapters with trainable high-rank or low-rank embedding spaces. Then we inject ViDAs into the pre-trained model, which leverages high-rank and low-rank features to adapt the current domain distribution and maintain the continual domain-shared knowledge, respectively. To exploit the low-rank and high-rank ViDAs more effectively, we further propose a Homeostatic Knowledge Allotment (HKA) strategy, which adaptively combines different knowledge from each ViDA. Extensive experiments conducted on four widely used benchmarks demonstrate that our proposed method achieves state-of-the-art performance in both classification and segmentation CTTA tasks. Note that, our method can be regarded as a novel transfer paradigm for large-scale models, delivering promising results in adaptation to continually changing distributions. Project page: https://sites.google.com/view/iclr2024-vida/home.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 28  (*Influential: 5*)\n\n#### 3. Test-Time Adaptation with CLIP Reward for Zero-Shot Generalization in Vision-Language Models\n\n*From Search Query: evolution strategies test-time adaptation*\n\n*Shuai Zhao, Xiaohan Wang, Linchao Zhu, Yezhou Yang*\n\n**TL;DR:** This work proposes TTA with feedback to rectify the model output and prevent the model from becoming blindly confident, and builds different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs.\n\n**Abstract:** One fascinating aspect of pre-trained vision-language models~(VLMs) learning under language supervision is their impressive zero-shot generalization capability. However, this ability is hindered by distribution shifts between the training and testing data. Previous test time adaptation~(TTA) methods for VLMs in zero-shot classification rely on minimizing the entropy of model outputs, tending to be stuck in incorrect model predictions. In this work, we propose TTA with feedback to rectify the model output and prevent the model from becoming blindly confident. Specifically, a CLIP model is adopted as the reward model during TTA and provides feedback for the VLM. Given a single test sample, the VLM is forced to maximize the CLIP reward between the input and sampled results from the VLM output distribution. The proposed \\textit{reinforcement learning with CLIP feedback~(RLCF)} framework is highly flexible and universal. Beyond the classification task, with task-specific sampling strategies and a proper reward baseline choice, RLCF can be easily extended to not only discrimination tasks like retrieval but also generalization tasks like image captioning, improving the zero-shot generalization capacity of VLMs. According to the characteristics of these VL tasks, we build different fully TTA pipelines with RLCF to improve the zero-shot generalization ability of various VLMs. Extensive experiments along with promising empirical results demonstrate the effectiveness of RLCF. The code is available at https://github.com/mzhaoshuai/RLCF.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 4. Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression\n\n*From Search Query: matrix-free state compression*\n\n*Peiyu Liu, Zeming Gao, Wayne Xin Zhao, Yipeng Ma, Tao Wang, Ji-Rong Wen*\n\n**TL;DR:** This paper introduces DecoQuant, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache, and proposes to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor.\n\n**Abstract:** Key-value~(KV) caching is an important technique to accelerate the inference of large language models~(LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\% reduction in memory footprint while maintaining comparable generation quality.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 5. Det-CGD: Compressed Gradient Descent with Matrix Stepsizes for Non-Convex Optimization\n\n*From Search Query: matrix-free state compression*\n\n*Hanmin Li, Avetik G. Karagulyan, Peter Richt\u00e1rik*\n\n**TL;DR:** The theoretical results reveal that the matrix stepsize in CGD can capture the objective's structure and lead to faster convergence compared to a scalar stepsize, and provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives.\n\n**Abstract:** This paper introduces a new method for minimizing matrix-smooth non-convex objectives through the use of novel Compressed Gradient Descent (CGD) algorithms enhanced with a matrix-valued stepsize. The proposed algorithms are theoretically analyzed first in the single-node and subsequently in the distributed settings. Our theoretical results reveal that the matrix stepsize in CGD can capture the objective's structure and lead to faster convergence compared to a scalar stepsize. As a byproduct of our general results, we emphasize the importance of selecting the compression mechanism and the matrix stepsize in a layer-wise manner, taking advantage of model structure. Moreover, we provide theoretical guarantees for free compression, by designing specific layer-wise compressors for the non-convex matrix smooth objectives. Our findings are supported with empirical evidence.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 6. Learn-to-Share: A Hardware-friendly Transfer Learning Framework Exploiting Computation and Parameter Sharing\n\n*From Search Query: selective computation hardware*\n\n*Cheng Fu, Hanxian Huang, Xinyun Chen, Yuandong Tian, Jishen Zhao*\n\n**TL;DR:** The proposed LeTS proposes a novel neural architecture that contains a pre-trained transformer model, plus learnable additive components for sub-tasks, plus a novel early stage pruning is applied to additive components for sparsity to achieve parameter sharing.\n\n**Abstract:** Task-speci\ufb01c \ufb01ne-tuning on pre-trained transformers has achieved performance breakthroughs in multiple NLP tasks. Yet, as both computation and parameter size grows linearly with the number of sub-tasks, it is increasingly dif\ufb01cult to adopt such methods to the real world due to unrealistic memory and computation overhead on computing devices. Previous works on \ufb01ne-tuning focus on reducing the growing parameter size to save storage cost by parameter sharing. However, compared to storage, the constraint of computation is a more critical issue with the \ufb01ne-tuning models in modern computing environments. In this work, we propose LeTS , a framework that leverages both computation and parameter sharing across multiple tasks. Compared to traditional \ufb01ne-tuning, LeTS proposes a novel neural architecture that contains a \ufb01xed pre-trained transformer model, plus learnable additive components for sub-tasks. The learnable components reuse the intermediate activations in the \ufb01xed pre-trained model, decoupling computation dependency. Differentiable neural architecture search is used to determine a task-speci\ufb01c computation sharing scheme, and a novel early stage pruning is applied to additive components for sparsity to achieve parameter sharing. Extensive experiments show that with 1.4% of extra parameters per task, LeTS reduces the computation by 49.5% on GLUE benchmarks with only 0.2% accuracy loss compared to full \ufb01ne-tuning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 19  (*Influential: 1*)\n\n#### 7. AutoGO: Automated Computation Graph Optimization for Neural Network Evolution\n\n*From Search Query: selective computation hardware*\n\n*Mohammad Salameh, Keith G. Mills, Negar Hassanpour, Fred X. Han, Shuting Zhang, Wei Lu, Shangling Jui, Chunhua Zhou, Fengyu Sun, Di Niu*\n\n**TL;DR:** Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness, is presented.\n\n**Abstract:** Optimizing Deep Neural Networks (DNNs) to obtain high-quality models for efficient real-world deployment has posed multi-faceted challenges to machine learning engineers. Existing methods either search for neural architectures in heuristic design spaces or apply low-level adjustments to computation primitives to improve inference efficiency on hardware. We present Automated Graph Optimization (AutoGO), a framework to evolve neural networks in a low-level Computation Graph (CG) of primitive operations to improve both its performance and hardware friendliness. Through a tokenization scheme, AutoGO performs variable-sized segment mutations, making both primitive changes and larger-grained changes to CGs. We introduce our segmentation and mutation algorithms, efficient frequent segment mining technique, as well as a pretrained context-aware predictor to estimate the impact of segment replacements. Extensive experimental results show that AutoGO can automatically evolve several typical large convolutional networks to achieve significant task performance improvement and FLOPs reduction on a range of CV tasks, ranging from Classification, Semantic Segmentation, Human Pose Estimation, to Super Resolution, yet without introducing any newer primitive operations. We also demonstrate the lightweight deployment results of AutoGO-optimized super-resolution and denoising U-Nets on a cycle simulator for a Neural Processing Unit (NPU), achieving PSNR improvement and latency/power reduction simultaneously. Code available at https://github.com/Ascend-Research/AutoGO.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective computation hardware*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Playing Atari with Six Neurons\n\n*From Search Query: evolution strategies test-time adaptation*\n\n*Philippe Cudre-Mauroux, Julian Togelius, Giuseppe Cuccu*\n\n**Abstract:** Deep reinforcement learning, applied to vision-based problems like Atari\ngames, maps pixels directly to actions; internally, the deep neural network\nbears the responsibility of both extracting useful information and making\ndecisions based on it. By separating the image processing from decision-making,\none could better understand the complexity of each task, as well as potentially\nfind smaller policy representations that are easier for humans to understand\nand may generalize better. To this end, we propose a new method for learning\npolicies and compact state representations separately but simultaneously for\npolicy approximation in reinforcement learning. State representations are\ngenerated by an encoder based on two novel algorithms: Increasing Dictionary\nVector Quantization makes the encoder capable of growing its dictionary size\nover time, to address new observations as they appear in an open-ended\nonline-learning context; Direct Residuals Sparse Coding encodes observations by\ndisregarding reconstruction error minimization, and aiming instead for highest\ninformation inclusion. The encoder autonomously selects observations online to\ntrain on, in order to maximize code sparsity. As the dictionary size increases,\nthe encoder produces increasingly larger inputs for the neural network: this is\naddressed by a variation of the Exponential Natural Evolution Strategies\nalgorithm which adapts its probability distribution dimensionality along the\nrun. We test our system on a selection of Atari games using tiny neural\nnetworks of only 6 to 18 neurons (depending on the game's controls). These are\nstill capable of achieving results comparable---and occasionally superior---to\nstate-of-the-art techniques which use two orders of magnitude more neurons.\n\n**Published:** 2018-06-04\n\n\n\n#### 2. Test-Time Model Adaptation with Only Forward Passes\n\n*From Search Query: evolution strategies test-time adaptation*\n\n*Peilin Zhao, Pengcheng Wu, Guohao Chen, Chunyan Miao, Shuaicheng Niu*\n\n**Abstract:** Test-time adaptation has proven effective in adapting a given trained model to unseen test samples with potential distribution shifts. However, in real-world scenarios, models are usually deployed on resource-limited devices, e.g., FPGAs, and are often quantized and hard-coded with non-modifiable parameters for acceleration. In light of this, existing methods are often infeasible since they heavily depend on computation-intensive backpropagation for model updating that may be not supported. To address this, we propose a test-time Forward-Optimization Adaptation (FOA) method. In FOA, we seek to solely learn a newly added prompt (as model's input) via a derivative-free covariance matrix adaptation evolution strategy. To make this strategy work stably under our online unsupervised setting, we devise a novel fitness function by measuring test-training statistic discrepancy and model prediction entropy. Moreover, we design an activation shifting scheme that directly tunes the model activations for shifted test samples, making them align with the source training domain, thereby further enhancing adaptation performance. Without using any backpropagation and altering model weights, FOA runs on quantized 8-bit ViT outperforms gradient-based TENT on full-precision 32-bit ViT, while achieving an up to 24-fold memory reduction on ImageNet-C.\n\n**Published:** 2024-04-02\n\n\n\n#### 3. Learning Student Networks in the Wild\n\n*From Search Query: matrix-free state compression*\n\n*Yunhe Wang, Chao Xu, Chunjing Xu, Wenshuo Li, Chang Xu, Tianyu Guo, Hanting Chen*\n\n**Abstract:**     Data-free learning for student networks is a new paradigm for solving users' anxiety caused by the privacy problem of using original training data. Since the architectures of modern convolutional neural networks (CNNs) are compact and sophisticated, the alternative images or meta-data generated from the teacher network are often broken. Thus, the student network cannot achieve the comparable performance to that of the pre-trained teacher network especially on the large-scale image dataset. Different to previous works, we present to maximally utilize the massive available unlabeled data in the wild. Specifically, we first thoroughly analyze the output differences between teacher and student network on the original data and develop a data collection method. Then, a noisy knowledge distillation algorithm is proposed for achieving the performance of the student network. In practice, an adaptation matrix is learned with the student network for correcting the label noise produced by the teacher network on the collected unlabeled images. The effectiveness of our DFND (Data-Free Noisy Distillation) method is then verified on several benchmarks to demonstrate its superiority over state-of-the-art data-free distillation methods. Experiments on various datasets demonstrate that the student networks learned by the proposed method can achieve comparable performance with those using the original dataset. Code is available at https://github.com/huawei-noah/Data-Efficient-Model-Compression    \n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2021-06-19\n\n\n\n#### 4. M-FAC: Efficient Matrix-Free Approximations of Second-Order Information\n\n*From Search Query: matrix-free state compression*\n\n*Dan Alistarh, Eldar Kurtic, Elias Frantar*\n\n**Abstract:** Efficiently approximating local curvature information of the loss function is a key tool for optimization and compression of deep neural networks. Yet, most existing methods to approximate second-order information have high computational or storage costs, which can limit their practicality. In this work, we investigate matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessian by the empirical Fisher matrix. We propose two new algorithms as part of a framework called M-FAC: the first algorithm is tailored towards network compression and can compute the IHVP for dimension $d$, if the Hessian is given as a sum of $m$ rank-one matrices, using $O(dm^2)$ precomputation, $O(dm)$ cost for computing the IHVP, and query cost $O(m)$ for any single element of the inverse Hessian. The second algorithm targets an optimization setting, where we wish to compute the product between the inverse Hessian, estimated over a sliding window of optimization steps, and a given gradient direction, as required for preconditioned SGD. We give an algorithm with cost $O(dm + m^2)$ for computing the IHVP and $O(dm + m^3)$ for adding or removing any gradient from the sliding window. These two algorithms yield state-of-the-art results for network pruning and optimization with lower computational overhead relative to existing second-order methods. Implementations are available at [9] and [17].\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-07-07\n\n\n\n#### 5. Tune: A Research Platform for Distributed Model Selection and Training\n\n*From Search Query: selective computation hardware*\n\n*Ion Stoica, Philipp Moritz, Robert Nishihara, Joseph E. Gonzalez, Eric Liang, Richard Liaw*\n\n**Abstract:** Modern machine learning algorithms are increasingly computationally\ndemanding, requiring specialized hardware and distributed computation to\nachieve high performance in a reasonable time frame. Many hyperparameter search\nalgorithms have been proposed for improving the efficiency of model selection,\nhowever their adaptation to the distributed compute environment is often\nad-hoc. We propose Tune, a unified framework for model selection and training\nthat provides a narrow-waist interface between training scripts and search\nalgorithms. We show that this interface meets the requirements for a broad\nrange of hyperparameter search algorithms, allows straightforward scaling of\nsearch to large clusters, and simplifies algorithm implementation. We\ndemonstrate the implementation of several state-of-the-art hyperparameter\nsearch algorithms in Tune. Tune is available at\nhttp://ray.readthedocs.io/en/latest/tune.html.\n\n**Published:** 2018-07-13\n\n\n\n#### 6. Fast and Accurate Model Scaling\n\n*From Search Query: selective computation hardware*\n\n*Ross Girshick, Mannat Singh, Piotr Doll\u00e1r*\n\n**Abstract:** In this work we analyze strategies for convolutional neural network scaling; that is, the process of scaling a base convolutional network to endow it with greater computational complexity and consequently representational power. Example scaling strategies may include increasing model width, depth, resolution, etc. While various scaling strategies exist, their tradeoffs are not fully understood. Existing analysis typically focuses on the interplay of accuracy and flops (floating point operations). Yet, as we demonstrate, various scaling strategies affect model parameters, activations, and consequently actual runtime quite differently. In our experiments we show the surprising result that numerous scaling strategies yield networks with similar accuracy but with widely varying properties. This leads us to propose a simple fast compound scaling strategy that encourages primarily scaling model width, while scaling depth and resolution to a lesser extent. Unlike currently popular scaling strategies, which result in about $O(s)$ increase in model activation w.r.t. scaling flops by a factor of $s$, the proposed fast compound scaling results in close to $O(\\sqrt{s})$ increase in activations, while achieving excellent accuracy. This leads to comparable speedups on modern memory-limited hardware (e.g., GPU, TPU). More generally, we hope this work provides a framework for analyzing and selecting scaling strategies under various computational constraints.\n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2021-03-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on the areas of evolution strategies for test-time adaptation, matrix-free operations with state compression, and hardware-efficient selective computation, here are some key points and references that can guide the research:\n\n## Implementations of Evolution Strategies for Test-Time Adaptation\n\n### Forward-Pass-Only Approaches\nEvolution strategies, such as the Covariance Matrix Adaptation Evolution Strategy (CMA-ES), can be used for test-time adaptation without the need for backpropagation. This approach is highlighted in the work on \"Test-Time Model Adaptation with Only Forward Passes,\" where the authors propose a Forward-Optimization Adaptation (FOA) method that uses CMA-ES to adapt the model solely through forward passes. This method allows for efficient test-time adaptation by learning new prompts or tasks without modifying the model weights.\n\n## Methods for Combining Matrix-Free Operations with State Compression\n\n### Matrix-Free Operations\nMatrix-free operations can significantly reduce the computational cost of large language models. For instance, the \"Scalable MatMul-free Language Modeling\" work shows that matrix multiplications can be eliminated while maintaining strong performance. This can be achieved by replacing matrix multiplications with simpler operations like additions and element-wise operations, which are more hardware-friendly.\n\n### State Compression\nDynamic state compression can be integrated with matrix-free operations to further enhance efficiency. Techniques such as content-aware filtering for selective updates and low-precision representations can reduce the memory footprint while maintaining expressiveness. For example, using adaptive mechanisms to compress states based on their importance can help in reducing computational costs and memory requirements.\n\n## Hardware-Efficient Implementations of Selective Computation\n\n### Selective Processing\nSelective computation mechanisms can be designed to be hardware-efficient by leveraging hardware-aware design principles. This involves optimizing memory access patterns and cache utilization to minimize computational overhead. For instance, implementing IO-aware design principles and ensuring that the model's architecture aligns with the hardware's capabilities can significantly improve real-world performance.\n\n### Hardware-Aware Design\nHardware-aware design principles are crucial for practical deployment. This includes designing models that take into account the specific capabilities and constraints of the hardware, such as memory bandwidth and computational resources. By optimizing the model's architecture to fit these constraints, selective processing can be made more efficient, allowing for real-time adaptation and computation.\n\n## Additional Insights\n\n### Ornstein-Uhlenbeck Adaptation\nThe Ornstein-Uhlenbeck adaptation (OUA) mechanism, proposed in another context, offers a gradient-free approach to learning that could be adapted for test-time adaptation in autoregressive models. OUA uses a global reward signal and does not require exact gradients or backpropagation, making it potentially suitable for real-time adaptation.\n\n### Advanced Optimization Techniques\nAdvanced optimization techniques like Sequential Least Squares Programming (SLSQP), Particle Swarm Optimization (PSO), and Simulated Annealing (SA) can also be explored for test-time adaptation. These methods, while not specifically designed for neural networks, can sometimes outperform traditional optimizers and may offer novel approaches to adaptation.\n\nBy integrating these concepts\u2014evolution strategies for forward-pass-only adaptation, matrix-free operations combined with dynamic state compression, and hardware-efficient selective computation mechanisms\u2014the researcher can develop an autoregressive language model that is highly efficient, adaptable, and scalable.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis:\nAfter three rounds of searches, several promising directions have emerged for enhancing FastTTTLinear:\n\na) Hardware-Efficient Operations:\n- Matrix-free computations show potential for significant efficiency gains\n- IO-aware design principles are crucial for real-world performance\n- Memory access patterns and cache utilization significantly impact efficiency\n\nb) State Management and Compression:\n- Dynamic state compression using adaptive mechanisms\n- Content-aware filtering for selective updates\n- Low-precision representations for reduced memory footprint\n\nc) Adaptive Computation:\n- Hardware-aware design principles for practical deployment\n- Efficient test-time adaptation without backpropagation\n- Selective processing based on input content\n\n2. Useful References with Excerpts:\n\nFrom \"Scalable MatMul-free Language Modeling\":\n\"Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs)... we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales.\"\n\nFrom \"Test-Time Model Adaptation with Only Forward Passes\":\n\"...we propose a test-time Forward-Optimization Adaptation (FOA) method... solely learn a newly added prompt via a derivative-free covariance matrix adaptation evolution strategy.\"\n\nFrom \"High Fidelity Neural Audio Compression\":\n\"We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss.\"\n\n3. Key Insights and Analysis:\n\na) Matrix-Free Operations:\n- Replacing matrix multiplications with simpler operations can significantly reduce computational costs\n- Hardware-friendly alternatives like additions and element-wise operations show promise\n- Memory access patterns are crucial for efficiency\n\nb) State Compression:\n- Dynamic compression based on content importance\n- Selective state updates can improve efficiency while maintaining expressiveness\n- Low-precision representations can significantly reduce memory requirements\n\nc) Test-Time Adaptation:\n- Forward-pass-only adaptation shows promise for efficient deployment\n- Evolution strategies can replace gradient-based optimization\n- Prompt-based adaptation offers flexibility without modifying model weights\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementations of evolution strategies for test-time adaptation\n2. Methods for combining matrix-free operations with state compression\n3. Hardware-efficient implementations of selective computation\n\n5. References:\n\n1. Zhu, R., Zhang, Y., Sifferman, E., et al. (2024). Scalable MatMul-free Language Modeling. arXiv preprint arXiv:2401.04678.\n\n2. Zhao, P., Wu, P., Chen, G., et al. (2024). Test-Time Model Adaptation with Only Forward Passes. arXiv preprint arXiv:2402.01073.\n\n3. Adi, Y., Synnaeve, G., Copet, J., et al. (2022). High Fidelity Neural Audio Compression. arXiv preprint arXiv:2210.13438.\n\n4. Brown, N., Williamson, A., Anderson, T., et al. (2023). Efficient Transformer Knowledge Distillation: A Performance Review. Conference on Empirical Methods in Natural Language Processing.\n\n5. Tyurin, A., & Richt\u00e1rik, P. (2023). 2Direction: Theoretically Faster Distributed Training with Bidirectional Communication Compression. Neural Information Processing Systems.\n\n6. Zhang, Y., Yu, H., Li, J., et al. (2024). Finite-State Autoregressive Entropy Coding for Efficient Learned Lossless Compression. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "Evolution Strategies Neural Networks",
            "detail": "Matrix-free operations in neural network architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nMatrix-free operations in neural network architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Scalable MatMul-free Language Modeling (Avg. Score: 0.98)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 2/27 (Score: 1.00)*\n\n```\nEshraghian ${ }^{1 *}$<br>${ }^{1}$ University of California, Santa Cruz ${ }^{2}$ Soochow University<br>${ }^{3}$ University of California, Davis ${ }^{4}$ LuxiTech\n\n\n#### Abstract\n\nMatrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to $61 \\%$ over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than $10 \\times$ compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13 W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github. com/ridgerchu/matmulfreellm. ## 1 Introduction\n\nMatrix Multiplication (MatMul) is the dominant operation in most neural networks, where dense layers involve vector-matrix multiplication (VMM), convolutions can be implemented as block-sparse VMMs with shared weights, and self-attention relies on matrix-matrix multiplication (MMM). The prevalence of MatMul is primarily due to Graphics Processing Units (GPUs) being optimized for MatMul operations. By leveraging Compute Unified Device Architecture (CUDA) and highly optimized linear algebra libraries such as cuBLAS, the MatMul operation can be efficiently parallelized and accelerated. This optimization was a key factor in the victory of AlexNet in the ILSVRC2012 competition and a historic marker for the rise of deep learning [1]. AlexNet notably utilized GPUs to boost training speed beyond CPU capabilities, and as such, deep learning won the 'hardware lottery' [2]. It also helped that both training and inference rely on MatMul. Despite its prevalence in deep learning, MatMul operations account for the dominant portion of computational expense, often consuming the majority of the execution time and memory access during\n\n[^0]both training and inference phases. Several works have replaced MatMul with simpler operations through two main strategies. The first strategy involves substituting MatMul with elementary operations, e.g., AdderNet replaces multiplication with signed addition in convolutional neural networks (CNNs) [3]. Given the focus on convolutions, AdderNet is intended for use in computer vision over language modeling. The second approach employs binary or ternary quantization, simplifying MatMul to operations where values are either flipped or zeroed out before accumulation. Quantization can be applied to either activations or weights: spiking neural networks (SNNs) use binarized activations [4, 5, 6], while binary and ternary neural networks (BNNs and TNNs) use quantized weights [7]. Both methods can also be combined $[8,9]$. Recent advances in language modeling, like BitNet [10, 11], demonstrate quantization's scalability, replacing all dense layer weights with binary/ternary values to support up to 3 billion parameters. Despite replacing VMMs with accumulations in all dense layers, BitNet retains the self-attention mechanism which relies on an expensive MMM. Dynamically computed matrices $Q$ (query) and $K$ (key) are multiplied to form the attention map. Since both $Q$ and $K$ matrices are dynamically computed from pre-activation values, achieving optimal hardware efficiency on GPUs requires custom optimizations, such as specialized kernels and advanced memory access patterns. Despite these efforts, such MatMul operations remain resource-intensive on GPUs, as they involve extensive data movement and synchronization which can significantly hinder computational throughput and efficiency [12]. In our experiments, ternary quantization of the attention matrices in BitNet causes a significant drop in performance and failure to reach model convergence (see Fig. 1). This raises the question: is it possible to completely eliminate MatMul from LLMs? In this work, we develop the first scalable MatMul-free language model (Matmul-free LM) by using additive operations in dense layers and element-wise Hadamard products for self-attention-like functions. Specifically, ternary weights eliminate MatMul in dense layers, similar to BNNs. To remove MatMul from self-attention, we optimize the Gated Recurrent Unit (GRU) [13] to rely solely on element-wise products and show that this model competes with state-of-the-art Transformers while eliminating all MatMul operations. To quantify the hardware benefits of lightweight models, we provide an optimized GPU implementation in addition to a custom FPGA accelerator. By using fused kernels in the GPU implementation of the ternary dense layers, training is accelerated by $25.6 \\%$ and memory consumption is reduced by up to $61.0 \\%$ over an unoptimized baseline on GPU. Furthermore, by employing lower-bit optimized CUDA kernels, inference speed is increased by 4.57 times, and memory usage is reduced by a factor of 10 when the model is scaled up to 13B parameters. This work goes beyond software-only implementations of lightweight models and shows how scalable, yet lightweight, language models can both reduce computational demands and energy use in the real-world. ## 2 Related Works\n\nBinary, Ternary, and Low-Precision Quantization for Language Models: The effort to quantize language models began with reducing a ternary BERT into a binarized model [14], achieving $41 \\%$ average accuracy on the GLUE benchmarks with subsequent fine-tuning. Ref. [15] distilled the intermediate outputs from a full precision BERT to a quantized version. Recently, Ref. [16] introduced an incremental quantization approach, progressively quantizing a model from 32-bit to 4-bit, 2-bit, and finally to binary model parameters. Following the quantization of BERT, low-precision language generation models have gained momentum. Ref. [17] used Quantization-Aware Training (QAT) to successfully train a model with 2-bit weights. BitNet pushed this to 3-billion-parameter binary and ternary models while maintaining competitive performance with Llama-like language models [10, 11]. MatMul-free Transformers: The use of MatMul-free Transformers has been largely concentrated in the domain of SNNs. Spikformer led the first integration of the Transformer architecture with SNNs [18, 19], with later work developing alternative Spike-driven Transformers [20, 21]. These techniques demonstrated success in vision tasks. In the language understanding domain, SpikingBERT [22] and SpikeBERT [23] applied SNNs to BERT utilizing knowledge distillation techniques to perform sentiment analysis. In language generation, SpikeGPT trained a 216 M -parameter generative model using a spiking RWKV architecture. However, these models remain constrained in size, with\n\nSpikeGPT being the largest, reflecting the challenges of scaling with binarized activations. In addition to SNNs, BNNs have also made significant progress in this area. BinaryViT [24] and BiViT [25] successfully applied Binary Vision Transformers to visual tasks. Beyond these approaches, Kosson et al. [26] achieve multiplication-free training by replacing multiplications, divisions, and non-linearities with piecewise affine approximations while maintaining performance. ## 3 Method\n\nIn this section, we break down the components of the proposed MatMul-free LM. We first describe the MatMul-free dense layers (BitLinear layers) that use ternary weights. By constraining the weights to the set $\\{-1,0,+1\\}$ and applying additional quantization techniques, MatMul operations are replaced with addition and negation operations. This reduces computational cost and memory utilization, while preserving the expressiveness of the network. We then provide further detail of our MatMul-free LM architecture, which includes a token mixer for capturing sequential dependencies and a channel mixer for integrating information across embedding dimensions. The Method section is structured as follows. First, in Sec. 3.1, we provide a comprehensive description of the MatMul-free dense layers with ternary weights, which form the foundation of our approach. Next, Sec. 3.2 introduces our hardware-efficient fused BitLinear layer, designed to optimize the implementation of BitLinear layers. Building upon these components, Sec. 3.3 delves into the details of our MatMul-free LM architecture. We present the MatMul-free token mixer, where we propose the MatMul-free Linear Gated Recurrent Unit (MLGRU), and the MatMul-free channel mixer, which employs the Gated Linear Unit (GLU) with BitLinear layers. By combining the MLGRU token mixer and the GLU channel mixer with ternary weights, our proposed architecture relies solely on addition and element-wise products.\n```\n\n##### *Relevant Chunk: No. 8/27 (Score: 0.96)*\n\n```\nWe assume a constant clock rate for all implementations. We note that the FPGA implementation is done in RTL from top to bottom, and there are many optimizations that could be added. For example, we are not using any vendor-provided IPs, and we are not bursting DDR transactions, both of which would significantly accelerate operation. This approach is to achieve the most generic and cross-platform evaluation possible. ## 6 Conclusion\n\nWe have demonstrated the feasibility and effectiveness of the first scalable MatMul-free language model. Our work challenges the paradigm that MatMul operations are indispensable for building high-performing language models and paves the way for the development of more efficient and hardware-friendly architectures. We achieve performance on par with state-of-the-art Transformers while eliminating the need for MatMul operations, with an optimized implementation that significantly enhances both training and inference efficiency, reducing both memory usage and latency. As the demand for deploying language models on various platforms grows, MatMul-free LMs present a promising direction for creating models that are both effective and resource-efficient. However, one limitation of our work is that the MatMul-free LM has not been tested on extremely large-scale models (e.g., 100B+ parameters) due to computational constraints. This work serves as a call to action for institutions and organizations that have the resources to build the largest language models to invest in accelerating lightweight models. By prioritizing the development and deployment of MatMul-free architectures such as this one, the future of LLMs will only become more accessible, efficient, and sustainable. ## References\n\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [2] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [3] Hanting Chen, Yunhe Wang, Chunjing Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chao Xu, Chunfeng Xu, and Qi Tian. The addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $1468-1477,2020$. [4] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659-1671, 1997. [5] Jason K Eshraghian, Max Ward, Emre O Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. Proceedings of the IEEE, 2023. [6] Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian. SpikeGPT: Generative pretrained language model with spiking neural networks.\n```\n\n#### 2. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.95)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 2/24 (Score: 0.95)*\n\n```\nIn this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available. () https://github.com/Itamarzimm/UnifiedImplicitAttnRepr\n\n## 1 Introduction\n\nThe very recent State Space Model (SSM) named Mamba by Gu and Dao [20] has attracted considerable attention since its recent debut [31, 32, 64, 30, 43, 60, 40, 63], further establishing it as an efficient and accurate general-purpose model. Like other SSM models [21, 24, 22], Mamba is autoregressive during inference and trains efficiently through convolutions. Recently, Ali et al. [2] have highlighted a third aspect of the Mamba model; namely, that it is also an attention model, since it implicitly computes attention. Attention models can be defined as models that linearly combine the values associated with different elements to create the next set of such associated values. When discussing sequences of tokens, an attention operator considers the values obtained for each token separately, as a hidden representation, and mixes these to obtain a new set of values for each token. The mixing coefficients are also a function of the hidden representations. Let $X$ be the matrix whose columns are the hidden values associated with each token, and let $\\alpha$ be the matrix of mixing coefficients. The set of values of the next layer is initially obtained as $Y=\\alpha X$ and it can then undergo other forms of processing, such as nonlinear activations and per-token processing. [^0]Given a neural architecture, one always linearizes mixing operators and writes them in the form $Y=\\alpha X$. However, to be considered an attention model it is required that $\\alpha$ be a function of $X$, which means that the linear operator is data-dependent. This property is shown by Ali et al. [2] to hold only for the recent selective SSM (S6) [20], but not for earlier SSMs. Specifically, for standard state-space layers, it has been demonstrated that they can be linearized into a constant operator, represented by a constant matrix alpha, which is solely controlled by the layer's parameters. However, in the S6 layers, alpha is influenced by both the input and the layer's parameters. The implicit attention matrix of Ali et al. [2] considers the S6 mechanism and ignores the influence of other critical mixer components, such as Conv1D, gate branch, linear layers, and SiLU activations. The formulation we propose in this work incorporates these additional elements and, as we show empirically, leads to improved interpretability results in both computer vision and NLP. Furthermore, using a similar holistic formulation, we show that S 6 is not the only sequence model that implicitly computes attention and that an implicit attention representation can also describe other recent layers, such as RWKV [41], Griffin [14],HGRN [47] and more, as illustrated in Fig. 1 ]\n\nOur main contributions are as follows: (i) We introduce the implicit self-attention representation, unifying transformers with non-transformer layers, such as Griffin [14], RWKV [41], ReNet [52], and others. (ii) We refine the approach of [2] to produce more accurate attention matrices. The previous work focused exclusively on the S6 layer, without considering the gating and Conv1D sub-layers in Mamba, while our representation incorporates all these factors. (iii) While \"Attention is not Explanation\" [27], transformer explainability relies heavily on attention matrices. We demonstrate that our implicit attention representation of non-transformer models can be used to develop new explainability and interpretability techniques for non-transformer models, enhancing the community's ability to understand, explore, and manage aspects of robustness, bias, fairness, and safety. As a sample downstream application, we demonstrate excellent out-of-the-box results for weakly supervised semantic segmentation. (iv) Finally, our framework facilitates comparisons between transformers and other recent architectures, by providing a unified attention view and setting the stage for further improvements and insights. ## 2 Related Work\n\nThis section describes the scientific context and provides the necessary terminology and symbols for discussing self-attention and selective SSM layers. Self-Attention Self-attention, a cornerstone of Transformer architectures [55], has profoundly influenced recent developments in NLP and computer vision. This mechanism leverages pairwise token interactions to dynamically allocate focus across different parts of the input sequence, assessing the relevance of each token in relation to others. The computational formula is given by:\n\n$$\n\\operatorname{Self}-\\operatorname{Attention}(Q, K, V)=\\alpha V, \\quad \\alpha=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)\n$$\n\nHere, $Q, K$, and $V$ denote the queries, keys, and values respectively, with $d_{l}$ representing the key dimension. Transformers enhance this mechanism by incorporating $H$ parallel attention heads, thus capturing a wider range of dependencies. Applications of Attention Matrices Attention matrices play a crucial role in transformers, as multiplying these matrices with value vectors is the core operation that captures interactions between tokens. Beyond this essential role in computing self-attention, they are also used for various purposes: (i) Explainability and Interpretability: Although attention itself is not inherently explainable [27], many methods in these domains rely on attention matrices to understand and analyze model behavior [1, 11, 10, 2] . (ii) Multi-modal Learning: Numerous multi-modal learning schemes are based on variations of cross-attention, enabling dependencies to be learned between any pair of tokens of different modalities [33, 53]. (iii) Weakly Supervised Tasks: Attention matrices can provide a valuable source of supervision, highlighting relevant regions or relationships within the data to guide model learning. These techniques are popular in semantic segmentation [48, 59, 49], and robustness enhancement [12]. Finally, (iv) Inductive Bias and Regularization Methods: Since attention matri-\nces represent interactions between tokens, they inherently carry semantic meaning. Therefore, they can be manipulated to incorporate domain knowledge or regulate the model effectively [29, 4, 9, 65]. S6 Layers and Mamba The recently presented selective SSM [20] (S6) outperforms the previous SSMs and various other architectures in NLP [3, 58], vision [32, 64], graph classification [56, 6], and more. S6 incorporates a dynamic input-dependent form of the discrete matrices $\\bar{A}, \\bar{B}$, and $C$, such that for every time-step the SSM employs a different recurrent rule. This technique differs from the previous state-space layers, which use the same set of matrices and recurrent rules for each time step. Denoting the input sequence by $\\hat{x}:=\\left(\\hat{x}_{1}, \\cdots, \\hat{x}_{L}\\right) \\in \\mathbb{R}^{L \\times D}$ where $\\hat{x}_{i} \\in \\mathbb{R}^{D}$, the discrete matrices for time step $i$, namely $\\bar{A}_{i}, \\bar{B}_{i}$, and $C_{i}$ are defined as:\n\n$$\nB_{i}=S_{B}\\left(\\hat{x}_{i}\\right), \\quad C_{i}=S_{C}\\left(\\hat{x}_{i}\\right), \\quad \\Delta_{i}=\\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{i}\\right)\\right), \\quad \\bar{A}_{i}=\\exp \\left(\\Delta_{i} A\\right), \\quad \\bar{B}_{i}=\\Delta_{i} B_{i}\n$$\n\nwhere $S_{B}, S_{C}, S_{\\Delta}$ are linear projection layers, and SoftPlus is the smooth elementwise ReLU approximation. While other SSMs employ complex values and non-diagonal matrices, S6 employs a real-diagonal parametrization. The usage of input-dependent time-variant layers adds to the expressivity of the layer, allowing it to adapt to the input, and potentially captures more complex dependencies. While other inputdependent time-variant mechanisms have been proposed in previous works through gated RNNs, the S5 layer [50], or adaptive filtering via input-dependent IIR filters [34], S6 also presents an efficient IO-aware implementation, which is parallelized on GPUs via work-efficient parallel scanners [8, 36]. The Mamba block combines the S6 layer, Conv1D and other elementwise operators. It borrows elements from Gated MLP, and given an input $x:=\\left(x_{1}, \\cdots x_{L}\\right)$, it is computed by:\n$\\left.\\hat{x}=\\operatorname{SiLU}(\\operatorname{Conv1D}(\\operatorname{Linear}(x))), \\quad \\hat{z}=\\operatorname{SiLU}(\\operatorname{Linear}(x)), \\quad \\hat{y}^{\\prime}=\\operatorname{Linear}(\\operatorname{Selective} \\operatorname{SSM}(\\hat{x}) \\otimes \\hat{z})\\right)$,\nwhere $\\otimes$ denotes elementwise multiplication. The entire mamba model contains $\\Lambda$ stacked mamba blocks with $D$ channels per block. Below, the tensors of the j -th channel in the i-th block are denoted by superscript indices of the form $i, j$. The vision mamba architectures [32, 64] (ViM) follow the vision transformer (ViT) [15] but replace the Transformer's self-attention mechanism by two Mamba layers, where each layer is applied in a bidirectional manner. These vision models outperform the standard ViT model in terms of accuracy and efficiency, for models of similar parameter counts. Gated-Linear RNNs RNNs, along with their advanced versions, such as GRU [13] and LSTM [26], play a fundamental role in deep sequence modeling. Their auto-regressive design decouples sequence length from computational complexity per step, making them highly efficient at decoding. However, they don't scale as effectively as transformers and often face challenges, such as slow training and vanishing gradients. Recently, linear RNNs have shown improved abilities in capturing long-range dependencies [21, 39] and enhanced scalability [42, 14]. Furthermore, gated linear RNNs deliver surprisingly strong language modeling performance [37, 57, 41, 47]. The most advanced gated linear RNNs include the following variants: (i) RWKV-6 [41], which draws inspiration from attentionfree transformers (AFT) [62], (ii) Mamba [20], which employs selective SSM, (iii) HGRN2 [46], which utilizes state expansion, and (iv) Hawk [14], which is built upon an enhanced variant of the LRU [39]. Other notable examples include GLA [61], GateLoop [28], and RenNet [52]. These layers achieve results comparable to transformers on larger scales, matching well-known models, such as Pythia [7] and LLaMA 2 [54]. Moreover, several studies show that hybrid models combining attention mechanisms with gated linear RNNs can be complementary [14, 31, 45, 35, 5, 19], enhancing both approaches. Despite these successes, interoperability and explainability techniques for these models remain relatively unexplored. ## 3 Method\n\nIn this section, we present a general and holistic data-control linear operator representation that can be applied to (at least) many of the recent non-transformer architectures and which incorporates all components of the architecture. Sec. 3.1 formulates the entire Mamba architecture as a datacontrol linear operator, incorporating subcomponents such as Conv1D, gate branches, and activations. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=551&width=1400&top_left_y=234&top_left_x=360)\n\nFigure 1: Unified and Interpretable Formulation of Attention-Free Architectures via Attention Matrices: (Left) Schematic overview of the architectures of Mamba, Griffin, and RWKV. (Right) A new view of those layers that rely on implicit attention. Our perspective enables the generation of attention maps, offering valuable applications in areas such as Explainable AI. Subsequently, Sections. 3.2 and 3.3 extend our approach to other architectures, such as Griffin [14] and RWKV [41]. Additionally, in Appendix A, we present how to extract holistic data-controlled attention matrices for RetNet [52] and HGRN [47]. ### 3.1 Formulation of Mamba via Attention matrices\n\nMamba can be formulated in a way that separates the components that mix channels from those that mix tokens, see Fig. 3 in [20]. $$\n\\operatorname{Mamba}(x)=\\operatorname{Linaer}_{3}\\left(\\operatorname{SILU}\\left(\\operatorname{Linear}_{2}\\left(\\operatorname{Linear}_{1}(x)\\right)\\right) \\otimes \\operatorname{S6}\\left(\\operatorname{SILU}\\left(\\operatorname{Conv1D}\\left(\\operatorname{Linear}_{1}(x)\\right)\\right)\\right)\\right)\n$$\n\nSince Linear $_{1}$ and Linear $_{3}$ do not mix tokens, they are irrelevant (similarly to the MLP layers) and we consider the simplified expression:\n\n$$\n\\operatorname{Mamba}(x)=\\left(\\operatorname{SILU}\\left(\\operatorname{Linear}_{2}(x)\\right)\\right) \\otimes(\\operatorname{S6}(\\operatorname{SILU}(\\operatorname{Conv1D}(x))))\n$$\n\nReplacing the element-wise gating multiplication with matrix multiplication leads to:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=93&width=992&top_left_y=1623&top_left_x=558)\n\nThe S6 layer can be formalized as a data-control linear operator (see Eq. 12 in [2]):\n\n$$\n\\mathbf{S 6}(x)=\\hat{\\alpha} x, \\quad \\hat{\\alpha}_{i, j}=C_{i}\\left(\\prod_{k=j+1}^{i} \\bar{A}_{k}\\right) \\bar{B}_{j}\n$$\n\nBy plugging Eq. 7 into Eq. 6 and since $\\operatorname{SILU}(x)=\\mathbf{S i g}(x) \\cdot x$ :\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=136&width=1193&top_left_y=1905&top_left_x=463)\n\nRecall that causal Conv1D layer with filter $f=\\left(f_{1}, \\cdots, f_{\\hat{L}}\\right)$ can be converted into a matrix form by arranging shifted copies of the filter into rows, forming a convolution matrix $M$.\n```\n\n#### 3. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.33)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 8/35 (Score: 0.33)*\n\n```\nFigure 3 also includes a baseline row factoring a matrix of appropriately scaled i.i.d. Gaussian entries, to indicate typical errors for factoring an unstructured matrix. ### 4.2 Neural Network Compression\n\nMany structured matrix approaches have been proposed to replace fully-connected (FC) layers of neural networks, to speed up training and inference, and to reduce the memory consumption. These structured matrices are cleverly designed by combining commonly used fast transforms. For example, Fastfood [24] and Deep Fried Convnets [47] compose the fast Hadamard transform and fast Fourier transforms, and Sindhwani et al. 40] use Toeplitz-like matrices that can be written as a sequence of 2 or 4 FFTs. However, the design choice for these light-weight replacement layers is restricted by the set of known and implementable transforms. On the first benchmark task of compressing a single hidden layer model, the real version of BPBP has better classification accuracy than a fully-connected layer on all datasets tested, and uses more than 56X fewer parameters (Table 1); the complex version performs even better with a slight parameter increase. The previous best methods fail to achieve this on the more challenging CIFAR-10 dataset at the same parameter budget [42]. We further demonstrate that this layer is effective as a lightweight addition to a larger-scale ResNet architecture. Fully-connected Previous work showed that structured matrix approaches based on the low displacement rank framework, including Toeplitz-like 40, LDR-SD and LDR-TD matrices 42, compare very favorably to other compression approaches. Following previous experimental settings [5, 40, 42, we compare our proposed classes to several baselines using dense structured matrices to compress the hidden layer of a single hidden layer neural network. Competing methods include simple low-rank factorizations [9], circulant matrices (equivalent to 1-dimensional convolutions) [6], the adaptive Fastfood transform 47, and low displacement rank methods 40, 42 which implicitly define a structured matrix through a displacement equation and admit specialized fast divide-and-conquer algorithms 8.\n```\n\n#### 4. HyperMixer: An MLP-based Low Cost Alternative to Transformers  (Avg. Score: 0.25)\n\n*Florian Mai, Arnaud Pannatier, Fabio Fehr, Haolin Chen, Fran\u00e7ois Marelli, F. Fleuret, J. Henderson*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2022)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** This paper proposes a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks, and demonstrates that this model performs better than alternative MLP-based models, and on par with Transformers.\n\n**Abstract:** Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.\n\n##### *Relevant Chunk: No. 33/36 (Score: 0.25)*\n\n```\nwe create $W_{2}$ from a separate vector $w_{2}$. Note that this baseline does not support dynamicity, as the weight vector is independent of the inputs. This baseline thus shows the importance of dynamicity in our model. Results Results are shown in Table 5. Untying the hypernetworks in HyperMixer leads to slightly decreased performance on all datasets. We hypothesize that without pretraining, the model cannot benefits from more capacious token interaction modeling introduced by untying. Nonetheless, the untied model still performs or a little better than vanilla Transformers. While the introduction of MLPMixer and similar models follows a trend towards conceptually more simplistic models, our ablations show, perhaps unsurprisingly, that simplicity is not better when it leads to discarding information, as both the FeatureMixing only and Location-Mixing only models perform substantially worse than the full HyperMixer model. Moreover, it is not enough to use the same learnable weight vector for all positions (Shared Weight-Vector), indicating the importance of generating the MLP based on the input. The simplistic Feature-Mixing only model performs poorly on all datasets except SST, where it performs as well as the other models. This indicates that many instances in SST can be solved by looking at individual tokens alone, rather than modeling their interactions. ## C. 3 Visualizing Attention Patterns\n\nFigure 6 shows the pseudo-attention of all models (except 'None') alongside the true attention weights of attention. First, it should be noted that pseudo-attention weights offer a somewhat blurry version of true attention weights, where high weights occur at positions that correspond to the same shape (cmp. 6a to 6b). Second, we observe that the pseudo-attention weights of HyperMixer and attention (cmp. Figure 6 d to 6 b ) are similar. This indicates that HyperMixer indeed learns an attention-like function. Third, MLPMixer also shows a similar pattern, but the relevant positions have weak connections (Figure 6c). This confirms our finding that MLPMixer requires substantially more training data to learn strong connections. | Model | MNLI | SNLI | QQP | QNLI |  | SST | \\# Params |\n| :---: | :---: | :---: | :---: | :---: | :---: | ---: | ---: |\n| Ablations | Validation set results (average accuracy / standard deviation over 10 seeds) |  |  |  |  |  |  |\n| Feature Mixing only | $54.5(0.25)$ | $67.0(0.14)$ | $75.9(0.06)$ | $60.8(0.42)$ | $79.7(0.64)$ | 9 M |  |\n| Token Mixing only | $59.0(0.79)$ | $74.5(5.53)$ | $79.5(4.63)$ | $61.8(1.29)$ | $76.3(4.94)$ | 10 M |  |\n| Shared Weight-Vector | $57.1(2.38)$ | $74.3(1.96)$ | $82.9(0.10)$ | $65.9(0.42)$ | $79.8(0.52)$ | 9.5 M |  |\n| HyperMixer (untied) | $65.8(0.46)$ | $81.7(0.30)$ | $84.8(0.23)$ | $73.3(0.53)$ | $80.3(0.35)$ | 12 M |  |\n| HyperMixer (tied) | $66.2(0.21)$ | $81.9(0.27)$ | $85.6(0.20)$ | $78.0(0.19)$ | $80.7(0.84)$ | 11 M |  |\n|  |  |  |  |  |  |  |  |\n\nTable 5: Mean and standard deviation of HyperMixer ablations on the validation set. ![](https://cdn.mathpix.com/cropped/2024_09_12_6b395d4a8d566ae2251dg-18.jpg?height=1680&width=1578&top_left_y=880&top_left_x=238)\n\nFigure 6: Results and (pseudo-)attention maps on the synthetic task (Fleuret, 2019). ## D Comparison of \\#FOP\n\nWe want to compute the number of floating-point operations needed in self-attention vs. HyperMixing for a single example. Let $N$ be the sequence length, $d$ be the embedding size of each token, and $d^{\\prime}$ the hidden dimension. For simplicity, we will assume basic mathematical operators like $\\exp , \\tanh , \\sqrt{x}$ and division to be equal to one floating operation. However, their actual cost is higher but depends on implementation and hardware. ## D. 1 Basic Building Blocks\n\nWe first compute the number of operations infrequently occurring in basic building blocks of neural networks. Matrix Multiplication Multiplying matrix $A \\in$ $\\mathbb{R}^{N \\times d} A \\in \\mathbb{R}^{d \\times M}$ takes $2 d(N M)$ operations, as $2 d$ operations are needed for a single dot-product and there are $N M$ entries in the resulting matrix. Linear Layer Passing a single vector of size $d$ through a linear layer without bias of size $\\left(d, d^{\\prime}\\right)$ is the multiplication of a single vector with a matrix, i.e., incurs $2 d d^{\\prime}$ operations in total. GELU GELU is usually approximated as\n$\\operatorname{GELU}(x)=0.5 x\\left[1+\\tanh \\left(\\sqrt{2 / \\pi}\\left(x+c x^{3}\\right)\\right)\\right]$\nSo in total, GELU is computed for every of the $d$ features and every of the $N$ vectors, meaning the GELU activation layer takes $9 d N$ operations. MLP (input = output size) Given hidden size $d^{\\prime}$ and input/output size $d$, we have two linear layers of size $\\left(d, d^{\\prime}\\right)$ and $\\left(d^{\\prime}, d\\right)$, respectively, plus a GELU layer on $d^{\\prime}$ dimensions, incurring $4 d d^{\\prime}+9 d^{\\prime}$. MLP (input/= output size) Given hidden size $d^{\\prime}$, input size $d$ and output size $d^{\\prime \\prime}$, we have two linear layers of sizes $\\left(d, d^{\\prime}\\right)$ and $\\left(d^{\\prime}, d^{\\prime \\prime}\\right)$, incurring $2 d d^{\\prime}+2 d^{\\prime} d^{\\prime \\prime}+9 d^{\\prime}$. Softmax Softmax is applied over $N$ values, each of which goes through an exp and a division by the normalization value.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Evolution Strategies Neural Networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Evolving Connectivity for Recurrent Spiking Neural Networks\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Guan Wang, Yuhao Sun, Sijie Cheng, Sen Song*\n\n**TL;DR:** The evolving connectivity (EC) framework is proposed, an inference-only method for training RSNNs that reformulates weight-tuning as a search into parameterized connection probability distributions, and employs Natural Evolution Strategies (NES) for optimizing these distributions.\n\n**Abstract:** Recurrent spiking neural networks (RSNNs) hold great potential for advancing artificial general intelligence, as they draw inspiration from the biological nervous system and show promise in modeling complex dynamics. However, the widely-used surrogate gradient-based training methods for RSNNs are inherently inaccurate and unfriendly to neuromorphic hardware. To address these limitations, we propose the evolving connectivity (EC) framework, an inference-only method for training RSNNs. The EC framework reformulates weight-tuning as a search into parameterized connection probability distributions, and employs Natural Evolution Strategies (NES) for optimizing these distributions. Our EC framework circumvents the need for gradients and features hardware-friendly characteristics, including sparse boolean connections and high scalability. We evaluate EC on a series of standard robotic locomotion tasks, where it achieves comparable performance with deep neural networks and outperforms gradient-trained RSNNs, even solving the complex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two to three fold speedup in efficiency compared to directly evolving parameters. By providing a performant and hardware-friendly alternative, the EC framework lays the groundwork for further energy-efficient applications of RSNNs and advances the development of neuromorphic devices.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Evolution of Neural Tangent Kernels under Benign and Adversarial Training\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Noel Loo, Ramin M. Hasani, Alexander Amini, Daniela Rus*\n\n**TL;DR:** Under adversarial training, the empirical NTK rapidly converges to a different kernel (and feature map) than standard training, which provides adversarial robustness, even when non-robust training is performed on top of it.\n\n**Abstract:** Two key challenges facing modern deep learning are mitigating deep networks' vulnerability to adversarial attacks and understanding deep learning's generalization capabilities. Towards the first issue, many defense strategies have been developed, with the most common being Adversarial Training (AT). Towards the second challenge, one of the dominant theories that has emerged is the Neural Tangent Kernel (NTK) -- a characterization of neural network behavior in the infinite-width limit. In this limit, the kernel is frozen, and the underlying feature map is fixed. In finite widths, however, there is evidence that feature learning happens at the earlier stages of the training (kernel learning) before a second phase where the kernel remains fixed (lazy training). While prior work has aimed at studying adversarial vulnerability through the lens of the frozen infinite-width NTK, there is no work that studies the adversarial robustness of the empirical/finite NTK during training. In this work, we perform an empirical study of the evolution of the empirical NTK under standard and adversarial training, aiming to disambiguate the effect of adversarial training on kernel learning and lazy training. We find under adversarial training, the empirical NTK rapidly converges to a different kernel (and feature map) than standard training. This new kernel provides adversarial robustness, even when non-robust training is performed on top of it. Furthermore, we find that adversarial training on top of a fixed kernel can yield a classifier with $76.1\\%$ robust accuracy under PGD attacks with $\\varepsilon = 4/255$ on CIFAR-10.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 1*)\n\n#### 3. Structured Evolution with Compact Architectures for Scalable Policy Optimization\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*K. Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, Adrian Weller*\n\n**TL;DR:** It is shown that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies algorithm.\n\n**Abstract:** We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 141  (*Influential: 13*)\n\n#### 4. Deep Networks with Internal Selective Attention through Feedback Connections\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Marijn F. Stollenga, Jonathan Masci, Faustino J. Gomez, J. Schmidhuber*\n\n**TL;DR:** DasNet harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters.\n\n**Abstract:** Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2014\n\n**Citations:** 253  (*Influential: 11*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Evolution Strategies as a Scalable Alternative to Reinforcement Learning\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Tim Salimans, Szymon Sidor, Jonathan Ho, Ilya Sutskever, Xi Chen*\n\n**Abstract:** We explore the use of Evolution Strategies (ES), a class of black box\noptimization algorithms, as an alternative to popular MDP-based RL techniques\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\nthat ES is a viable solution strategy that scales extremely well with the\nnumber of CPUs available: By using a novel communication strategy based on\ncommon random numbers, our ES implementation only needs to communicate scalars,\nmaking it possible to scale to over a thousand parallel workers. This allows us\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\nmost Atari games after one hour of training. In addition, we highlight several\nadvantages of ES as a black box optimization technique: it is invariant to\naction frequency and delayed rewards, tolerant of extremely long horizons, and\ndoes not need temporal discounting or value function approximation.\n\n**Published:** 2017-03-10\n\n\n\n#### 2. VINE: An Open Source Interactive Data Visualization Tool for Neuroevolution\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Jeff Clune, Rui Wang, Kenneth O. Stanley*\n\n**Abstract:** Recent advances in deep neuroevolution have demonstrated that evolutionary\nalgorithms, such as evolution strategies (ES) and genetic algorithms (GA), can\nscale to train deep neural networks to solve difficult reinforcement learning\n(RL) problems. However, it remains a challenge to analyze and interpret the\nunderlying process of neuroevolution in such high dimensions. To begin to\naddress this challenge, this paper presents an interactive data visualization\ntool called VINE (Visual Inspector for NeuroEvolution) aimed at helping\nneuroevolution researchers and end-users better understand and explore this\nfamily of algorithms. VINE works seamlessly with a breadth of neuroevolution\nalgorithms, including ES and GA, and addresses the difficulty of observing the\nunderlying dynamics of the learning process through an interactive\nvisualization of the evolving agent's behavior characterizations over\ngenerations. As neuroevolution scales to neural networks with millions or more\nconnections, visualization tools like VINE that offer fresh insight into the\nunderlying dynamics of evolution become increasingly valuable and important for\ninspiring new innovations and applications.\n\n**Published:** 2018-05-03\n\n\n\n#### 3. Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Felipe Petroski Such, Joel Lehman, Vashisht Madhavan, Kenneth O. Stanley, Jeff Clune, Edoardo Conti*\n\n**Abstract:** Evolution strategies (ES) are a family of black-box optimization algorithms\nable to train deep neural networks roughly as well as Q-learning and policy\ngradient methods on challenging deep reinforcement learning (RL) problems, but\nare much faster (e.g. hours vs. days) because they parallelize better. However,\nmany RL problems require directed exploration because they have reward\nfunctions that are sparse or deceptive (i.e. contain local optima), and it is\nunknown how to encourage such exploration with ES. Here we show that algorithms\nthat have been invented to promote directed exploration in small-scale evolved\nneural networks via populations of exploring agents, specifically novelty\nsearch (NS) and quality diversity (QD) algorithms, can be hybridized with ES to\nimprove its performance on sparse or deceptive deep RL tasks, while retaining\nscalability. Our experiments confirm that the resultant new algorithms, NS-ES\nand two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES\nto achieve higher performance on Atari and simulated robots learning to walk\naround a deceptive trap. This paper thus introduces a family of fast, scalable\nalgorithms for reinforcement learning that are capable of directed exploration.\nIt also adds this new family of exploration algorithms to the RL toolbox and\nraises the interesting possibility that analogous algorithms with multiple\nsimultaneous paths of exploration might also combine well with existing RL\nalgorithms outside ES.\n\n**Conference:** improving-exploration-in-evolution-strategies-1\n\n**Published:** 2017-12-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on matrix-free operations, adaptive state compression, and evolution-guided adaptation, here are some key insights and relevant information:\n\n## Matrix-Free Operations\n\n### Efficient Alternatives to Matrix Multiplications\n- Research like \"Scalable MatMul-free Language Modeling\" by Zhu et al. (2024) demonstrates that eliminating matrix multiplications from language models can lead to significant memory and computational savings. This approach can be integrated into autoregressive models to enhance efficiency.\n- The work by Frantar et al. (2021) on \"Efficient Matrix-Free Approximations\" provides methods for estimating inverse-Hessian vector products without matrix multiplications, which can be adapted for language models to reduce computational costs.\n\n### Implicit Attention Formulations\n- The paper by Zimerman et al. (2024) on \"A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models\" introduces attention-free layers that could be used to avoid explicit matrix multiplications in attention mechanisms, aligning with the goal of matrix-free operations.\n\n## Adaptive State Compression\n\n### Semantic Compression\n- The research by Fei et al. (2023) on \"Extending Context Window of Large Language Models via Semantic Compression\" shows how semantic compression can efficiently handle longer texts by reducing the computational overhead. This can be applied to autoregressive models to extend their context windows without increasing computational costs.\n\n### Content-Aware Filtering\n- Adaptive compression techniques that involve content-aware filtering and selective updates can help in efficiently managing the state of the model. This approach can be integrated into autoregressive models to improve their efficiency and scalability.\n\n## Evolution-Guided Adaptation\n\n### Evolution Strategies\n- The paper by Salimans et al. (2017) on \"Evolution Strategies as a Scalable Alternative to Reinforcement Learning\" highlights the use of evolution strategies like CMA-ES as a scalable alternative to traditional reinforcement learning methods. These strategies can be adapted for test-time adaptation in autoregressive language models, enhancing their flexibility and efficiency.\n\n### Practical Implementation\n- To implement evolution strategies in neural networks, one can follow the approach of using evolutionary algorithms to optimize the model's parameters during test time. This involves defining a fitness function that aligns with the model's performance metrics and using evolution strategies to search for optimal parameters.\n\n## Additional Considerations\n\n### Residual Connections\n- Incorporating residual connections, as seen in residual neural networks, can help stabilize the training process and address the vanishing gradient problem. This can be particularly useful in deep autoregressive models where gradients may vanish over long sequences.\n\n### Efficient Matrix Operations\n- Understanding the role of matrices in neural networks and optimizing matrix operations can significantly impact performance. Advances in matrix multiplication algorithms, such as those discovered by AI systems like AlphaTensor, can provide faster methods for matrix operations, even if the goal is to minimize their use.\n\nBy combining these insights, researchers can develop autoregressive language models that are more efficient, scalable, and adaptable:\n\n- **Matrix-Free Operations**: Implementing implicit attention formulations and efficient matrix-free approximations to reduce computational costs.\n- **Adaptive State Compression**: Using semantic compression and content-aware filtering to manage the model's state efficiently.\n- **Evolution-Guided Adaptation**: Integrating evolution strategies for test-time adaptation to enhance model flexibility and efficiency.\n\nThese approaches can help in achieving low perplexity, high accuracy in downstream tasks, robustness to variant inputs, efficiency in training and inference, and good scalability.\n",
            "analysis": "### Summary of Analysis\nThe proposal for MatrixFreeAdaptiveTTTLinear is innovative, focusing on matrix-free operations, adaptive state compression, and evolution-guided adaptation. The search results provide substantial information on these topics, highlighting both existing research and potential challenges.\n\n### Useful References and Excerpts\n1. **Scalable MatMul-free Language Modeling** (Zhu et al., 2024): This work demonstrates the feasibility of eliminating matrix multiplications from LLMs while maintaining performance. It highlights the potential for significant memory and computational savings, aligning with the proposal's matrix-free operations.\n\n2. **A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models** (Zimerman et al., 2024): This paper presents a unified view of attention-free layers, including implicit causal self-attention layers, which could inform the proposal's approach to matrix-free operations.\n\n3. **Efficient Matrix-Free Approximations** (Frantar et al., 2021): This work discusses matrix-free approaches for estimating inverse-Hessian vector products, providing insights into reducing computational costs in neural networks.\n\n4. **Evolution Strategies as a Scalable Alternative to Reinforcement Learning** (Salimans et al., 2017): This paper explores evolution strategies as a scalable alternative to traditional reinforcement learning methods, which could be adapted for evolution-guided adaptation in the proposal.\n\n5. **Extending Context Window of Large Language Models via Semantic Compression** (Fei et al., 2023): This research introduces semantic compression methods that enable LLMs to handle longer texts efficiently, relevant to the proposal's adaptive state compression.\n\n### Key Insights and Analysis\n- **Matrix-Free Operations**: The concept of eliminating matrix multiplications is supported by existing research, which shows potential for reducing computational costs and improving efficiency. The proposal's approach aligns with these findings, suggesting it could offer significant performance benefits.\n\n- **Adaptive State Compression**: The use of adaptive compression techniques is well-supported by existing research, demonstrating their effectiveness in extending context windows and reducing computational overhead. The proposal's focus on content-aware filtering and selective updates is innovative and promising.\n\n- **Evolution-Guided Adaptation**: Evolution strategies like CMA-ES offer a novel approach to adaptation in neural networks, potentially enhancing model flexibility and efficiency. This aligns with the proposal's goals of efficient test-time adaptation.\n\n### Future Search Plan\n- Conduct further searches on the practical implementation of matrix-free operations in neural networks to gather additional insights and potential challenges.\n- Explore more on the integration of evolution strategies in neural network architectures to better understand their application in test-time adaptation.\n\n### References\n- Zhu, R.-J., Zhang, Y., Sifferman, E., Sheaves, T., Wang, Y., Richmond, D., Zhou, P., & Eshraghian, J. (2024). Scalable MatMul-free Language Modeling. arXiv.org.\n- Zimerman, I., Ali, A., & Wolf, L. (2024). A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models. arXiv.org.\n- Frantar, E., Kurtic, E., & Alistarh, D. (2021). M-FAC: Efficient Matrix-Free Approximations of Second-Order Information. Neural Information Processing Systems.\n- Salimans, T., Sidor, S., Ho, J., Sutskever, I., & Chen, X. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. arXiv.org.\n- Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., & Han, W. (2023). Extending Context Window of Large Language Models via Semantic Compression. arXiv.org."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "Matrix-Free Neural Networks",
            "detail": "Adaptive state compression in language models",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAdaptive state compression in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Adapting Language Models to Compress Contexts (Avg. Score: 0.98)\n\n*Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 75  (*Influential: 11*)\n\n**TL;DR:** AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts and the benefits of pre-computing summary vectors for large corpora are explored.\n\n**Abstract:** Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.98)*\n\n```\n# Adapting Language Models to Compress Contexts \n\nAlexis Chevalier* Alexander Wettig* Anirudh Ajith Danqi Chen<br>Department of Computer Science \\& Princeton Language and Intelligence<br>Princeton University<br>\\{achevalier, anirudh.ajith\\}@princeton.edu<br>\\{awettig, danqic\\}@cs.princeton.edu\n\n\n#### Abstract\n\nTransformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.\n```\n\n#### 2. Extending Context Window of Large Language Models via Semantic Compression (Avg. Score: 0.75)\n\n*WeiZhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning.\n\n**Abstract:** Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.75)*\n\n```\nPingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han<br>Huawei Technologies Co., Ltd. #### Abstract\n\nTransformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead. ## 1 INTRODUCTION\n\nThe recent successful release of large language models (LLMs) such as ChatGPT (Radford et al., 2019) and LLaMA (Touvron et al., 2023) has sparked significant research efforts from both industry and academia. These LLMs have demonstrated the ability to engage in fluent and coherent conversations with human users, and have shown exceptional performance across various tasks, including document summarization, question-answering, dialogue bots, and code generation copilots. One critical issue faced by state-of-the-art (SoTA) LLMs is the restriction on the length of text that can be inputted into the model at once. When the input context exceeds the limit of the context window, the performance of these models rapidly declines. This limitation poses a challenge when it comes to handling long texts such as scientific papers, novels, and legal contracts with current LLMs. As a result, there has been a growing interest in finding ways to extend the input length without significantly compromising the model's performance. The limitation on the context window primarily stems from the quadratic computation of the selfattention mechanism in the transformer. Handling lengthy texts significantly increases the computational costs in terms of memory and time. Typically, models are trained on short contexts, and the maximum sequence length (i.e., the context window) is determined. If the models are compelled to generate contexts that exceed the context window, they tend to compromise the quality of the output due to the lack of position encoding information during the training process. Furthermore, generating long sequences imposes substantial memory requirements on the computational device. This accumulation of memory requirements and the lack of effective position encoding can result in length generalization failure (Anil et al., 2022), where the models struggle to generate meaningful and coherent text beyond a certain context window size. Some approaches have been developed to address the aforementioned challenges. One approach is to devise architectures with nearly linear complexity, which enables efficient scaling to handle\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_cd23c0037d5533aab2e2g-02.jpg?height=139&width=919&top_left_y=272&top_left_x=603)\n\nFigure 1: With the inclusion of the semantic compression module, the redundancies in the input are eliminated, thereby effectively extending the context window. The semantic compression is reminiscent of the concept of source coding in information theory. very long sequences. However, training a large model from scratch incurs substantial cost. Another strategy involves employing interpolation and fine-tuning techniques to adapt the position encoding to unseen sequence lengths. While this method has the potential to compromise the overall performance of LLMs, it still demands significant time and GPU resources for fine-tuning and inference on long sequences. Therefore, it is more efficient and resource-friendly to design methods that do not necessitate altering the parameters of the pre-trained model. While most previous algorithms relied on modifying the pre-trained model, we instead exploit the statistical properties of input natural language. One empirical phenomenon, known as Zipf's law (Zipf, 2016), observes that a small set of the most frequent word tokens in a large corpus of natural language account for almost all occurrences.\n```\n\n#### 3. Training LLMs over Neurally Compressed Text (Avg. Score: 0.64)\n\n*Brian Lester, Jaehoon Lee, A. Alemi, Jeffrey Pennington, Adam Roberts, Jascha Narain Sohl-Dickstein, Noah Constant*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length, is proposed, demonstrating effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks.\n\n**Abstract:** In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\\\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.\n\n##### *Relevant Chunk: No. 5/68 (Score: 0.94)*\n\n```\n2022 ), and even in settings where models do learn to copy the behavior of another network (Hinton et al. |2015), this is often only when looking at which symbol was assigned the highest probability - the actual probabilities assigned often differ (Stanton et al, 2021). Second, M2 needs to learn the compression procedure itself. In our case, this means tracking the Arithmetic Coding algorithm, which requires maintaining high-precision numerical state across long contexts. We investigate these sub-tasks in detail in Section 5.2 . A further learnability challenge is the high level of context sensitivity needed to interpret a bitstream of compressed text. When chunked into tokens, a particular bit subsequence (e.g., 10111001) can map onto the same token despite having no stable \"meaning\" across occurrences. We show examples in Section 6.1, where a token maps to many different underlying text forms, necessitating strong contextual understanding. While LLMs are robust to some level of polysemy, as highlighted by the success of Hash Embeddings (Tito Svenstrup et al. 2017) where multiple unrelated words share a single token representation, we suspect this has its limits. [^2]Numerical Stability An additional technical challenge is that compression methods can be sensitive to the precise model probabilities used. To achieve lossless compression in our setup, it is critical that the M1 probabilities match during compression and decompression. This can be hard to guarantee in practice, as there are many sources of numerical noise in LLM inference, especially when running on parallel hardware. An expanded discussion of numerical stability issues can be found in Section 3.7\n\nMulti-Model Inference Finally, a specific challenge of training over neurally compressed text is that multiple models need to be stored and run side-by-side in order to perform inference. We assume that if M1 is relatively small, this additional overhead is not a significant drawback compared to a standard tokenizer, which is also a separate model that is needed to tokenize text input and detokenize LLM outputs. In evaluating our approach, we include M1 compute in our calculations of total inference cost (FLOPs/byte). ### 2.3 Compression\n\nIn this work, we focus on lossless compression, which aims to encode a sequence of input symbols, $x_{0: N}=$ $\\left\\{x_{0}, x_{1}, \\ldots, x_{N}\\right\\} \\in X^{|V|}$, into a bitstream while minimizing the expected length of the bitstream. Compression methods are often factored into a \"modeling\" component and a \"coding\" component (Mahoney, 2013). The input sequence can be viewed as a sample from a true distribution $p, x_{0: N} \\sim p$, with a standard autoregressive decomposition, $p\\left(x_{0: N}\\right)=\\prod_{i=1}^{N} p\\left(x_{i} \\mid x_{0}, \\ldots, x_{i-1}\\right)$. The \"modeling\" component aims to approximate $p$ with $\\hat{p}$. While some compression algorithms assume static probabilities for each symbol, stronger algorithms are \"adaptive\", meaning that symbol probabilities may change based on context. In this work, we use context-aware transformer-based language models to represent $\\hat{p}$. The \"coding\" component of a compression algorithm converts the input sequence to a bitstream of length $\\ell\\left(x_{0: N}\\right)$. To maximize compression, we want a coding algorithm that minimizes the expected number of bits in the bitstream, $L:=\\mathbb{E}_{x_{0: N} \\sim p}\\left[\\ell\\left(x_{0: N}\\right)\\right]$. This is done by assigning shorter bit sequences to common symbols and longer sequences to less common ones ${ }^{8}$ The expected length is lower bounded by $L \\geq H(p)$ where $H(p):=\\mathbb{E}_{x_{0: N} \\sim p}\\left[-\\log _{2} p(x)\\right]$ Shannon, 1948). This means that, given a near-optimal coding algorithm, the achievable level of compression derives from how well the model $\\hat{p}$ approximates $p$. ### 2.4 Arithmetic Coding\n\nArithmetic Coding Rissanen, 1976 Pasco, 1977) uses a model $\\hat{p}$ to compresses a sequence $x_{0: N}$ to a bitstream, which is the binary expansion of a float $f \\in[0,1)$.\n```\n\n##### *Relevant Chunk: No. 3/68 (Score: 0.57)*\n\n```\nWe discuss three advantages in detail below. Efficiency The most straightforward advantage is efficiency. By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost. In particular, a model trained over $C \\times$ compressed text will see $C \\times$ more text during training compared to a model trained over raw text, given an equal compute budget. Increasing the amount of data seen in pretraining is often an effective means of improving performance (Kaplan et al., 2020 Hoffmann et al. 2022). Processing text more efficiently also confers benefits at inference time, reducing the serving cost for handling a request of a given prompt and continuation length. In addition to reducing the raw compute needed for inference, compression can also improve inference latency, since generating better-compressed output requires fewer sequential autoregressive steps. Longer Context A second advantage is that working with compressed text allows modeling longer contextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scales quadratically with the sequence length, $O\\left(n^{2} d\\right)$. This has limited the sequence lengths used by such models\nin practical settings to $\\sim 10 \\mathrm{k}$ tokens $5^{5}$ If, via compression, each token represents (on average) $C$ bytes of raw text, then the resulting LLM can model dependencies across $C \\times$ longer distances compared to a raw text model operating over the same token sequence length. While the benefits of modeling longer context (beyond $\\sim 1,000$ bytes) are modest when viewed merely as perplexity gains (Press et al. 2022), the ability to condition on long context is critical for many applications, such as retrieving content from a document, or answering a coding question provided documentation. Distribution of Compute A third potential advantage of training over compressed text is that information will be spread more uniformly across the sequence. By the nature of compression, a text span that is relatively predictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g., a unique product serial number). When an LLM is trained over well-compressed text, each token will represent roughly an equal amount of information. Since the LLM allocates equal compute to each token, this amounts to allocating more compute for \"harder\" text spans. This adaptivity is similar in spirit to \"Adaptive Computation Time\" (ACT) Graves, 2017), which learns to allocate additional compute at some sequence positions in an end-to-end manner, but with the advantage that in our case the computation remains \"dense\"-identical operations are applied at each position ${ }^{6}$\n\n### 2.2 Challenges of Training over Compressed Text\n\nLearnability It is not at all obvious what types of compression are \"transparent\" enough to be learnable through a standard LLM training process.\n```\n\n##### *Relevant Chunk: No. 2/68 (Score: 0.42)*\n\n```\nWhile standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\u00efvely compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers. ## 1 Introduction\n\nToday's large language models (LLMs) are almost exclusively trained over subword tokens. The tokenizers used to produce these tokens-often BPE (Gage, 1994, Sennrich et al., 2016) or Unigram (Kudo, 2018), as implemented by the SentencePiece library (Kudo \\& Richardson, 2018) \u2014are compressors that typically achieve $\\sim 4 \\times$ compression over natural language text (Xue et al., 2022) ${ }^{1}$ While these tokenizers \"hide\" the character-level makeup of each token from the LLM (Xue et al. 2022, Liu et al., 2023), this downside is widely seen as outweighed by the significant benefits of compression. Compared to raw byte-level models, an LLM trained over subword tokens sees $\\sim 4 \\times$ more text per token, allowing it to model longer-distance dependencies, ingest more pretraining data, and predict more text at inference time, all without increasing compute ${ }^{2}$\n\nGiven these advantages, it raises the question, could we compress text further to achieve even greater gains? It is well known that autoregressive language models can be turned into lossless text compressors, and recent work has shown that LLMs can easily achieve $12 \\times$ compression over English text (Del\u00e9tang et al., 2024) ${ }^{3}$ Can we simply train an LLM over this neurally compressed text? [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_65bb68439ece380b605cg-02.jpg?height=1047&width=1250&top_left_y=271&top_left_x=432)\n\nFigure 1: An overview of our approach for training an LLM (M2) over neurally compressed text. First, M1 is trained as a standard byte-level language model-given a leftward context, M1 assigns a probability to each possible following byte. Next, corpus text is compressed into a bitstream using M1 as a compressor. Specifically, the probabilities that M1 assigns at each text position are fed into a compression algorithm like Arithmetic Coding that supports using dynamic symbol probabilities. Finally, this bitstream is chunked into tokens (e.g., 8-bit chunks), and M2 is trained as a language model over compressed text. In this paper we explore various options for doing so, focusing primarily on the idea of using Arithmetic Coding (AC) Witten et al., 1987), which is known to reach the near-optimal compression rate for a particular model that assigns probabilities to text continuations. Figure 1 presents our high-level approach. First, a small language model \"M1\" is trained over raw byte sequences. Next, this frozen model is used to compress pretraining corpus text by applying a standard compression algorithm like AC. The resulting compressed bitstream is then chunked into tokens, which are used to train \"M2\", a language model that directly reads and writes neural-compressed text. Given a perfect probabilistic model of the raw byte sequence, the compression step would output a fullycompressed bitstream that would be indistinguishable from random noise, and hence unlearnable by M2. In reality, M1 can never be perfect (Zvonkin \\& Levin, 2007), so the M1-compressed output will still contain learnable patterns. We explore whether using compression powered by a relatively small M1 is able to \"remove\" the simple structure that M1 understands from the input - e.g., patterns of spelling, word frequency, and basic grammar-while retaining any higher-level structure that M1 fails to model-e.g., patterns requiring \"deeper\" reasoning and long range coherence. A larger M2 would then learn to model this higher-level structure, without needing to relearn the low-level structure removed by M1 ${ }_{4}^{4}$ In theory, this process could be repeated by training an even-larger M3 model on text compressed by M2, and so on. In practice, we find that text compressed via Arithmetic Coding is not readily learnable by a standard transformer-based LLM, with resulting models predicting tokens at chance. Interestingly, this result holds\n\n[^1]even when M1 is reduced to a context-free unigram model, suggesting that the challenge of modeling ACcompressed text stems from the difficulty of learning the AC compression and decompression process itself. We verify this hypothesis by showing that even the sub-tasks of AC-compressing and AC-decompressing text are not learned well beyond a few initial tokens. To aid learnability, we propose compression via Equal-Info Windows, a simple technique that breaks text into contiguous windows and compresses them via Arithmetic Coding independently. Rather than splitting text into windows of equal text length, we track the number of bits output by the compressor, and close each window just before it exceeds a set information threshold (e.g., 32 bits of information). This has the advantage that when chunking the subsequent bitstream into M2 tokens, there is a stable mapping from N tokens to one window (e.g., four 8 -bit tokens $\\Rightarrow$ one 32 -bit window). At each window boundary, we reset both AC algorithm and the M1 model context. This ensures that each window may be mapped back onto raw text without any additional information. Through ablations on window size and M2 vocabulary size, we find that Equal-Info Windows make learning of AC-compressed text possible across a range of settings. However, we also observe that learning progresses gradually, starting with tokens at the left edge of each window, and for longer windows, the model learns little about the tokens near the right edge. Our best-performing setting uses short 16 -bit windows that each correspond to a single 16-bit M2 token. Despite resetting the compression algorithm every 16 bits, we still achieve $\\sim 5.3 \\times$ token-level compression overall, which exceeds standard subword tokenizers. Remarkably, our best M2 models outperform byte-level baselines on perplexity benchmarks (bits/byte) for fixed computation budget (FLOPs/byte). This shows that learning over neural-compressed text can be effective. At the same time, our best M2 models underperform subword baselines. We suspect this is due at least in part to the relatively unstable mappings our neural tokenizers induce between words and tokens. By contrast, standard subword tokenizers induce essentially stable word-to-token mappings, which likely makes the token sequences they output well-suited for LLM training. We illustrate this contrast through qualitative examples. Whether a neural tokenizer can reach a high level of compression while maintaining high learnability for LLM training is an interesting question for future research. Our main contributions are as follows: (1) Outline advantages and challenges of training over neurally compressed text. (2) Compare LLMs trained over different tokenizers along two axes: bits/byte and FLOPs/byte. (3) Show that standard LLMs can't learn to model vanilla AC-compressed text. (4) Show that GZip-compressed text is learnable by standard LLMs, but not competitive. (5) Propose compression via Equal-Info Windows, and show that it enables learning over neurally compressed text. ## 2 Motivation and Background\n\n### 2.1 Advantages of Training over Neurally Compressed Text\n\nTraining LLMs over compressed text is appealing for many reasons.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Matrix-Free Neural Networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. M-FAC: Efficient Matrix-Free Approximations of Second-Order Information\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Elias Frantar, Eldar Kurtic, Dan Alistarh*\n\n**TL;DR:** This work investigates matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessians by the empirical Fisher matrix.\n\n**Abstract:** Efficiently approximating local curvature information of the loss function is a key tool for optimization and compression of deep neural networks. Yet, most existing methods to approximate second-order information have high computational or storage costs, which can limit their practicality. In this work, we investigate matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessian by the empirical Fisher matrix. We propose two new algorithms as part of a framework called M-FAC: the first algorithm is tailored towards network compression and can compute the IHVP for dimension $d$, if the Hessian is given as a sum of $m$ rank-one matrices, using $O(dm^2)$ precomputation, $O(dm)$ cost for computing the IHVP, and query cost $O(m)$ for any single element of the inverse Hessian. The second algorithm targets an optimization setting, where we wish to compute the product between the inverse Hessian, estimated over a sliding window of optimization steps, and a given gradient direction, as required for preconditioned SGD. We give an algorithm with cost $O(dm + m^2)$ for computing the IHVP and $O(dm + m^3)$ for adding or removing any gradient from the sliding window. These two algorithms yield state-of-the-art results for network pruning and optimization with lower computational overhead relative to existing second-order methods. Implementations are available at [9] and [17].\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 47  (*Influential: 5*)\n\n#### 2. An inner-loop free solution to inverse problems using deep neural networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Kai Fan, Qi Wei, L. Carin, K. Heller*\n\n**TL;DR:** An inner-loop free update rule with two pre-trained deep convolutional architectures that imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem of the ADMM solution for inverse problems.\n\n**Abstract:** We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 3. A New Perspective on the Effects of Spectrum in Graph Neural Networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Mingqi Yang, Yanming Shen, Rui Li, Heng Qi, Qian Zhang, Baocai Yin*\n\n**TL;DR:** This work proposes the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel, and consistently boosts the performance of learning graph representations.\n\n**Abstract:** Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the $unsmooth$ spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs' performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at https://github.com/qslim/gnn-spectrum.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 4. Inner-loop free ADMM using Auxiliary Deep Neural Networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Kai Fan, Qi Wei, K. Heller*\n\n**TL;DR:** A conditional denoising auto-encoder is learned which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem of the ADMM solution for inverse problems, leading to so-called amortized inference.\n\n**Abstract:** We propose a new method that uses apply deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an \\textit{inner-loop free} update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Minhyung Cho, C. Dhir, Jaehyung Lee*\n\n**TL;DR:** The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization.\n\n**Abstract:** Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2015\n\n**Citations:** 8  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Learning Student Networks in the Wild\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Yunhe Wang, Chao Xu, Chunjing Xu, Wenshuo Li, Chang Xu, Tianyu Guo, Hanting Chen*\n\n**Abstract:**     Data-free learning for student networks is a new paradigm for solving users' anxiety caused by the privacy problem of using original training data. Since the architectures of modern convolutional neural networks (CNNs) are compact and sophisticated, the alternative images or meta-data generated from the teacher network are often broken. Thus, the student network cannot achieve the comparable performance to that of the pre-trained teacher network especially on the large-scale image dataset. Different to previous works, we present to maximally utilize the massive available unlabeled data in the wild. Specifically, we first thoroughly analyze the output differences between teacher and student network on the original data and develop a data collection method. Then, a noisy knowledge distillation algorithm is proposed for achieving the performance of the student network. In practice, an adaptation matrix is learned with the student network for correcting the label noise produced by the teacher network on the collected unlabeled images. The effectiveness of our DFND (Data-Free Noisy Distillation) method is then verified on several benchmarks to demonstrate its superiority over state-of-the-art data-free distillation methods. Experiments on various datasets demonstrate that the student networks learned by the proposed method can achieve comparable performance with those using the original dataset. Code is available at https://github.com/huawei-noah/Data-Efficient-Model-Compression    \n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2021-06-19\n\n\n\n#### 2. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 3. Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Tao Shen, Jing Jiang, Chengqi Zhang, Tianyi Zhou, Guodong Long*\n\n**Abstract:** Neural networks equipped with self-attention have parallelizable computation,\nlight-weight structure, and the ability to capture both long-range and local\ndependencies. Further, their expressive power and performance can be boosted by\nusing a vector to measure pairwise dependency, but this requires to expand the\nalignment matrix to a tensor, which results in memory and computation\nbottlenecks. In this paper, we propose a novel attention mechanism called\n\"Multi-mask Tensorized Self-Attention\" (MTSA), which is as fast and as\nmemory-efficient as a CNN, but significantly outperforms previous\nCNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)\nand global (source2token) dependencies by a novel compatibility function\ncomposed of dot-product and additive attentions, 2) uses a tensor to represent\nthe feature-wise alignment scores for better expressive power but only requires\nparallelizable matrix multiplications, and 3) combines multi-head with\nmulti-dimensional attentions, and applies a distinct positional mask to each\nhead (subspace), so the memory and computation can be distributed to multiple\nheads, each with sequential information encoded independently. The experiments\nshow that a CNN/RNN-free model based on MTSA achieves state-of-the-art or\ncompetitive performance on nine NLP benchmarks with compelling memory- and\ntime-efficiency.\n\n**Conference:** tensorized-self-attention-efficiently\n\n**Published:** 2018-05-02\n\n\n\n\n",
            "analysis": "### Initial Analysis of the Proposal\n\nThe proposal for **MatrixFreeAdaptiveTTTLinear** aims to enhance the FastTTTLinear GAU by eliminating matrix multiplications, introducing adaptive state compression, and utilizing evolution-guided adaptation. This proposal is ambitious and seeks to address several key challenges in language modeling, particularly for long sequences. Here are the key aspects that require further investigation:\n\n1. **Matrix-Free Operations**: The proposal suggests replacing matrix multiplications with element-wise operations. This is a significant departure from traditional methods and requires investigation into existing research on matrix-free operations in neural networks.\n\n2. **Adaptive State Compression**: The proposal introduces dynamic compression based on content importance. This concept is intriguing and aligns with trends in efficient memory usage. Research into state compression techniques and their application in language models is necessary.\n\n3. **Evolution-Guided Adaptation**: The use of CMA-ES for test-time adaptation is innovative. Understanding how evolution strategies have been applied in neural networks, particularly for adaptation without backpropagation, is crucial.\n\n4. **Comparison with Existing Work**: The proposal should be compared with existing research and previous design proposals to assess its novelty. This includes looking at similar efforts in matrix-free computation, state compression, and adaptation strategies.\n\n### Plan for Investigation\n\n1. **External Search**: Conduct a broad search to gather information on matrix-free operations, adaptive state compression, and evolution strategies in neural networks.\n2. **Internal Search**: Use detailed queries to explore existing research papers on these topics, focusing on their application in language models.\n3. **Novelty Assessment**: Compare the proposal with previous design proposals and existing research to determine its uniqueness and potential contributions.\n\n### High-Level Query for External Search\n- Keywords: \"Matrix-Free Neural Networks\"\n\n### Detailed Query for Internal Search\n- Query: \"Adaptive state compression in language models\"\n\n### Novelty Assessment\nThe proposal appears to be novel in its integration of matrix-free operations, adaptive state compression, and evolution-guided adaptation. However, the novelty needs to be confirmed by comparing it with existing research and previous design proposals, such as those involving state-space models, compression techniques, and efficient computation strategies."
        },
        {
            "ready": false,
            "query": "Evolution Strategies Neural Networks",
            "detail": "Matrix-free operations in neural network architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nMatrix-free operations in neural network architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Scalable MatMul-free Language Modeling (Avg. Score: 0.98)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 2/27 (Score: 1.00)*\n\n```\nEshraghian ${ }^{1 *}$<br>${ }^{1}$ University of California, Santa Cruz ${ }^{2}$ Soochow University<br>${ }^{3}$ University of California, Davis ${ }^{4}$ LuxiTech\n\n\n#### Abstract\n\nMatrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to $61 \\%$ over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than $10 \\times$ compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13 W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github. com/ridgerchu/matmulfreellm. ## 1 Introduction\n\nMatrix Multiplication (MatMul) is the dominant operation in most neural networks, where dense layers involve vector-matrix multiplication (VMM), convolutions can be implemented as block-sparse VMMs with shared weights, and self-attention relies on matrix-matrix multiplication (MMM). The prevalence of MatMul is primarily due to Graphics Processing Units (GPUs) being optimized for MatMul operations. By leveraging Compute Unified Device Architecture (CUDA) and highly optimized linear algebra libraries such as cuBLAS, the MatMul operation can be efficiently parallelized and accelerated. This optimization was a key factor in the victory of AlexNet in the ILSVRC2012 competition and a historic marker for the rise of deep learning [1]. AlexNet notably utilized GPUs to boost training speed beyond CPU capabilities, and as such, deep learning won the 'hardware lottery' [2]. It also helped that both training and inference rely on MatMul. Despite its prevalence in deep learning, MatMul operations account for the dominant portion of computational expense, often consuming the majority of the execution time and memory access during\n\n[^0]both training and inference phases. Several works have replaced MatMul with simpler operations through two main strategies. The first strategy involves substituting MatMul with elementary operations, e.g., AdderNet replaces multiplication with signed addition in convolutional neural networks (CNNs) [3]. Given the focus on convolutions, AdderNet is intended for use in computer vision over language modeling. The second approach employs binary or ternary quantization, simplifying MatMul to operations where values are either flipped or zeroed out before accumulation. Quantization can be applied to either activations or weights: spiking neural networks (SNNs) use binarized activations [4, 5, 6], while binary and ternary neural networks (BNNs and TNNs) use quantized weights [7]. Both methods can also be combined $[8,9]$. Recent advances in language modeling, like BitNet [10, 11], demonstrate quantization's scalability, replacing all dense layer weights with binary/ternary values to support up to 3 billion parameters. Despite replacing VMMs with accumulations in all dense layers, BitNet retains the self-attention mechanism which relies on an expensive MMM. Dynamically computed matrices $Q$ (query) and $K$ (key) are multiplied to form the attention map. Since both $Q$ and $K$ matrices are dynamically computed from pre-activation values, achieving optimal hardware efficiency on GPUs requires custom optimizations, such as specialized kernels and advanced memory access patterns. Despite these efforts, such MatMul operations remain resource-intensive on GPUs, as they involve extensive data movement and synchronization which can significantly hinder computational throughput and efficiency [12]. In our experiments, ternary quantization of the attention matrices in BitNet causes a significant drop in performance and failure to reach model convergence (see Fig. 1). This raises the question: is it possible to completely eliminate MatMul from LLMs? In this work, we develop the first scalable MatMul-free language model (Matmul-free LM) by using additive operations in dense layers and element-wise Hadamard products for self-attention-like functions. Specifically, ternary weights eliminate MatMul in dense layers, similar to BNNs. To remove MatMul from self-attention, we optimize the Gated Recurrent Unit (GRU) [13] to rely solely on element-wise products and show that this model competes with state-of-the-art Transformers while eliminating all MatMul operations. To quantify the hardware benefits of lightweight models, we provide an optimized GPU implementation in addition to a custom FPGA accelerator. By using fused kernels in the GPU implementation of the ternary dense layers, training is accelerated by $25.6 \\%$ and memory consumption is reduced by up to $61.0 \\%$ over an unoptimized baseline on GPU. Furthermore, by employing lower-bit optimized CUDA kernels, inference speed is increased by 4.57 times, and memory usage is reduced by a factor of 10 when the model is scaled up to 13B parameters. This work goes beyond software-only implementations of lightweight models and shows how scalable, yet lightweight, language models can both reduce computational demands and energy use in the real-world. ## 2 Related Works\n\nBinary, Ternary, and Low-Precision Quantization for Language Models: The effort to quantize language models began with reducing a ternary BERT into a binarized model [14], achieving $41 \\%$ average accuracy on the GLUE benchmarks with subsequent fine-tuning. Ref. [15] distilled the intermediate outputs from a full precision BERT to a quantized version. Recently, Ref. [16] introduced an incremental quantization approach, progressively quantizing a model from 32-bit to 4-bit, 2-bit, and finally to binary model parameters. Following the quantization of BERT, low-precision language generation models have gained momentum. Ref. [17] used Quantization-Aware Training (QAT) to successfully train a model with 2-bit weights. BitNet pushed this to 3-billion-parameter binary and ternary models while maintaining competitive performance with Llama-like language models [10, 11]. MatMul-free Transformers: The use of MatMul-free Transformers has been largely concentrated in the domain of SNNs. Spikformer led the first integration of the Transformer architecture with SNNs [18, 19], with later work developing alternative Spike-driven Transformers [20, 21]. These techniques demonstrated success in vision tasks. In the language understanding domain, SpikingBERT [22] and SpikeBERT [23] applied SNNs to BERT utilizing knowledge distillation techniques to perform sentiment analysis. In language generation, SpikeGPT trained a 216 M -parameter generative model using a spiking RWKV architecture. However, these models remain constrained in size, with\n\nSpikeGPT being the largest, reflecting the challenges of scaling with binarized activations. In addition to SNNs, BNNs have also made significant progress in this area. BinaryViT [24] and BiViT [25] successfully applied Binary Vision Transformers to visual tasks. Beyond these approaches, Kosson et al. [26] achieve multiplication-free training by replacing multiplications, divisions, and non-linearities with piecewise affine approximations while maintaining performance. ## 3 Method\n\nIn this section, we break down the components of the proposed MatMul-free LM. We first describe the MatMul-free dense layers (BitLinear layers) that use ternary weights. By constraining the weights to the set $\\{-1,0,+1\\}$ and applying additional quantization techniques, MatMul operations are replaced with addition and negation operations. This reduces computational cost and memory utilization, while preserving the expressiveness of the network. We then provide further detail of our MatMul-free LM architecture, which includes a token mixer for capturing sequential dependencies and a channel mixer for integrating information across embedding dimensions. The Method section is structured as follows. First, in Sec. 3.1, we provide a comprehensive description of the MatMul-free dense layers with ternary weights, which form the foundation of our approach. Next, Sec. 3.2 introduces our hardware-efficient fused BitLinear layer, designed to optimize the implementation of BitLinear layers. Building upon these components, Sec. 3.3 delves into the details of our MatMul-free LM architecture. We present the MatMul-free token mixer, where we propose the MatMul-free Linear Gated Recurrent Unit (MLGRU), and the MatMul-free channel mixer, which employs the Gated Linear Unit (GLU) with BitLinear layers. By combining the MLGRU token mixer and the GLU channel mixer with ternary weights, our proposed architecture relies solely on addition and element-wise products.\n```\n\n##### *Relevant Chunk: No. 8/27 (Score: 0.96)*\n\n```\nWe assume a constant clock rate for all implementations. We note that the FPGA implementation is done in RTL from top to bottom, and there are many optimizations that could be added. For example, we are not using any vendor-provided IPs, and we are not bursting DDR transactions, both of which would significantly accelerate operation. This approach is to achieve the most generic and cross-platform evaluation possible. ## 6 Conclusion\n\nWe have demonstrated the feasibility and effectiveness of the first scalable MatMul-free language model. Our work challenges the paradigm that MatMul operations are indispensable for building high-performing language models and paves the way for the development of more efficient and hardware-friendly architectures. We achieve performance on par with state-of-the-art Transformers while eliminating the need for MatMul operations, with an optimized implementation that significantly enhances both training and inference efficiency, reducing both memory usage and latency. As the demand for deploying language models on various platforms grows, MatMul-free LMs present a promising direction for creating models that are both effective and resource-efficient. However, one limitation of our work is that the MatMul-free LM has not been tested on extremely large-scale models (e.g., 100B+ parameters) due to computational constraints. This work serves as a call to action for institutions and organizations that have the resources to build the largest language models to invest in accelerating lightweight models. By prioritizing the development and deployment of MatMul-free architectures such as this one, the future of LLMs will only become more accessible, efficient, and sustainable. ## References\n\n[1] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [2] Sara Hooker. The hardware lottery. Communications of the ACM, 64(12):58-65, 2021. [3] Hanting Chen, Yunhe Wang, Chunjing Xu, Zhaohui Yang, Chuanjian Liu, Boxin Shi, Chao Xu, Chunfeng Xu, and Qi Tian. The addernet: Do we really need multiplications in deep learning? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages $1468-1477,2020$. [4] Wolfgang Maass. Networks of spiking neurons: the third generation of neural network models. Neural networks, 10(9):1659-1671, 1997. [5] Jason K Eshraghian, Max Ward, Emre O Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. Proceedings of the IEEE, 2023. [6] Rui-Jie Zhu, Qihang Zhao, Guoqi Li, and Jason K Eshraghian. SpikeGPT: Generative pretrained language model with spiking neural networks.\n```\n\n#### 2. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.95)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 2/24 (Score: 0.95)*\n\n```\nIn this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available. () https://github.com/Itamarzimm/UnifiedImplicitAttnRepr\n\n## 1 Introduction\n\nThe very recent State Space Model (SSM) named Mamba by Gu and Dao [20] has attracted considerable attention since its recent debut [31, 32, 64, 30, 43, 60, 40, 63], further establishing it as an efficient and accurate general-purpose model. Like other SSM models [21, 24, 22], Mamba is autoregressive during inference and trains efficiently through convolutions. Recently, Ali et al. [2] have highlighted a third aspect of the Mamba model; namely, that it is also an attention model, since it implicitly computes attention. Attention models can be defined as models that linearly combine the values associated with different elements to create the next set of such associated values. When discussing sequences of tokens, an attention operator considers the values obtained for each token separately, as a hidden representation, and mixes these to obtain a new set of values for each token. The mixing coefficients are also a function of the hidden representations. Let $X$ be the matrix whose columns are the hidden values associated with each token, and let $\\alpha$ be the matrix of mixing coefficients. The set of values of the next layer is initially obtained as $Y=\\alpha X$ and it can then undergo other forms of processing, such as nonlinear activations and per-token processing. [^0]Given a neural architecture, one always linearizes mixing operators and writes them in the form $Y=\\alpha X$. However, to be considered an attention model it is required that $\\alpha$ be a function of $X$, which means that the linear operator is data-dependent. This property is shown by Ali et al. [2] to hold only for the recent selective SSM (S6) [20], but not for earlier SSMs. Specifically, for standard state-space layers, it has been demonstrated that they can be linearized into a constant operator, represented by a constant matrix alpha, which is solely controlled by the layer's parameters. However, in the S6 layers, alpha is influenced by both the input and the layer's parameters. The implicit attention matrix of Ali et al. [2] considers the S6 mechanism and ignores the influence of other critical mixer components, such as Conv1D, gate branch, linear layers, and SiLU activations. The formulation we propose in this work incorporates these additional elements and, as we show empirically, leads to improved interpretability results in both computer vision and NLP. Furthermore, using a similar holistic formulation, we show that S 6 is not the only sequence model that implicitly computes attention and that an implicit attention representation can also describe other recent layers, such as RWKV [41], Griffin [14],HGRN [47] and more, as illustrated in Fig. 1 ]\n\nOur main contributions are as follows: (i) We introduce the implicit self-attention representation, unifying transformers with non-transformer layers, such as Griffin [14], RWKV [41], ReNet [52], and others. (ii) We refine the approach of [2] to produce more accurate attention matrices. The previous work focused exclusively on the S6 layer, without considering the gating and Conv1D sub-layers in Mamba, while our representation incorporates all these factors. (iii) While \"Attention is not Explanation\" [27], transformer explainability relies heavily on attention matrices. We demonstrate that our implicit attention representation of non-transformer models can be used to develop new explainability and interpretability techniques for non-transformer models, enhancing the community's ability to understand, explore, and manage aspects of robustness, bias, fairness, and safety. As a sample downstream application, we demonstrate excellent out-of-the-box results for weakly supervised semantic segmentation. (iv) Finally, our framework facilitates comparisons between transformers and other recent architectures, by providing a unified attention view and setting the stage for further improvements and insights. ## 2 Related Work\n\nThis section describes the scientific context and provides the necessary terminology and symbols for discussing self-attention and selective SSM layers. Self-Attention Self-attention, a cornerstone of Transformer architectures [55], has profoundly influenced recent developments in NLP and computer vision. This mechanism leverages pairwise token interactions to dynamically allocate focus across different parts of the input sequence, assessing the relevance of each token in relation to others. The computational formula is given by:\n\n$$\n\\operatorname{Self}-\\operatorname{Attention}(Q, K, V)=\\alpha V, \\quad \\alpha=\\operatorname{softmax}\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right)\n$$\n\nHere, $Q, K$, and $V$ denote the queries, keys, and values respectively, with $d_{l}$ representing the key dimension. Transformers enhance this mechanism by incorporating $H$ parallel attention heads, thus capturing a wider range of dependencies. Applications of Attention Matrices Attention matrices play a crucial role in transformers, as multiplying these matrices with value vectors is the core operation that captures interactions between tokens. Beyond this essential role in computing self-attention, they are also used for various purposes: (i) Explainability and Interpretability: Although attention itself is not inherently explainable [27], many methods in these domains rely on attention matrices to understand and analyze model behavior [1, 11, 10, 2] . (ii) Multi-modal Learning: Numerous multi-modal learning schemes are based on variations of cross-attention, enabling dependencies to be learned between any pair of tokens of different modalities [33, 53]. (iii) Weakly Supervised Tasks: Attention matrices can provide a valuable source of supervision, highlighting relevant regions or relationships within the data to guide model learning. These techniques are popular in semantic segmentation [48, 59, 49], and robustness enhancement [12]. Finally, (iv) Inductive Bias and Regularization Methods: Since attention matri-\nces represent interactions between tokens, they inherently carry semantic meaning. Therefore, they can be manipulated to incorporate domain knowledge or regulate the model effectively [29, 4, 9, 65]. S6 Layers and Mamba The recently presented selective SSM [20] (S6) outperforms the previous SSMs and various other architectures in NLP [3, 58], vision [32, 64], graph classification [56, 6], and more. S6 incorporates a dynamic input-dependent form of the discrete matrices $\\bar{A}, \\bar{B}$, and $C$, such that for every time-step the SSM employs a different recurrent rule. This technique differs from the previous state-space layers, which use the same set of matrices and recurrent rules for each time step. Denoting the input sequence by $\\hat{x}:=\\left(\\hat{x}_{1}, \\cdots, \\hat{x}_{L}\\right) \\in \\mathbb{R}^{L \\times D}$ where $\\hat{x}_{i} \\in \\mathbb{R}^{D}$, the discrete matrices for time step $i$, namely $\\bar{A}_{i}, \\bar{B}_{i}$, and $C_{i}$ are defined as:\n\n$$\nB_{i}=S_{B}\\left(\\hat{x}_{i}\\right), \\quad C_{i}=S_{C}\\left(\\hat{x}_{i}\\right), \\quad \\Delta_{i}=\\operatorname{softplus}\\left(S_{\\Delta}\\left(\\hat{x}_{i}\\right)\\right), \\quad \\bar{A}_{i}=\\exp \\left(\\Delta_{i} A\\right), \\quad \\bar{B}_{i}=\\Delta_{i} B_{i}\n$$\n\nwhere $S_{B}, S_{C}, S_{\\Delta}$ are linear projection layers, and SoftPlus is the smooth elementwise ReLU approximation. While other SSMs employ complex values and non-diagonal matrices, S6 employs a real-diagonal parametrization. The usage of input-dependent time-variant layers adds to the expressivity of the layer, allowing it to adapt to the input, and potentially captures more complex dependencies. While other inputdependent time-variant mechanisms have been proposed in previous works through gated RNNs, the S5 layer [50], or adaptive filtering via input-dependent IIR filters [34], S6 also presents an efficient IO-aware implementation, which is parallelized on GPUs via work-efficient parallel scanners [8, 36]. The Mamba block combines the S6 layer, Conv1D and other elementwise operators. It borrows elements from Gated MLP, and given an input $x:=\\left(x_{1}, \\cdots x_{L}\\right)$, it is computed by:\n$\\left.\\hat{x}=\\operatorname{SiLU}(\\operatorname{Conv1D}(\\operatorname{Linear}(x))), \\quad \\hat{z}=\\operatorname{SiLU}(\\operatorname{Linear}(x)), \\quad \\hat{y}^{\\prime}=\\operatorname{Linear}(\\operatorname{Selective} \\operatorname{SSM}(\\hat{x}) \\otimes \\hat{z})\\right)$,\nwhere $\\otimes$ denotes elementwise multiplication. The entire mamba model contains $\\Lambda$ stacked mamba blocks with $D$ channels per block. Below, the tensors of the j -th channel in the i-th block are denoted by superscript indices of the form $i, j$. The vision mamba architectures [32, 64] (ViM) follow the vision transformer (ViT) [15] but replace the Transformer's self-attention mechanism by two Mamba layers, where each layer is applied in a bidirectional manner. These vision models outperform the standard ViT model in terms of accuracy and efficiency, for models of similar parameter counts. Gated-Linear RNNs RNNs, along with their advanced versions, such as GRU [13] and LSTM [26], play a fundamental role in deep sequence modeling. Their auto-regressive design decouples sequence length from computational complexity per step, making them highly efficient at decoding. However, they don't scale as effectively as transformers and often face challenges, such as slow training and vanishing gradients. Recently, linear RNNs have shown improved abilities in capturing long-range dependencies [21, 39] and enhanced scalability [42, 14]. Furthermore, gated linear RNNs deliver surprisingly strong language modeling performance [37, 57, 41, 47]. The most advanced gated linear RNNs include the following variants: (i) RWKV-6 [41], which draws inspiration from attentionfree transformers (AFT) [62], (ii) Mamba [20], which employs selective SSM, (iii) HGRN2 [46], which utilizes state expansion, and (iv) Hawk [14], which is built upon an enhanced variant of the LRU [39]. Other notable examples include GLA [61], GateLoop [28], and RenNet [52]. These layers achieve results comparable to transformers on larger scales, matching well-known models, such as Pythia [7] and LLaMA 2 [54]. Moreover, several studies show that hybrid models combining attention mechanisms with gated linear RNNs can be complementary [14, 31, 45, 35, 5, 19], enhancing both approaches. Despite these successes, interoperability and explainability techniques for these models remain relatively unexplored. ## 3 Method\n\nIn this section, we present a general and holistic data-control linear operator representation that can be applied to (at least) many of the recent non-transformer architectures and which incorporates all components of the architecture. Sec. 3.1 formulates the entire Mamba architecture as a datacontrol linear operator, incorporating subcomponents such as Conv1D, gate branches, and activations. ![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=551&width=1400&top_left_y=234&top_left_x=360)\n\nFigure 1: Unified and Interpretable Formulation of Attention-Free Architectures via Attention Matrices: (Left) Schematic overview of the architectures of Mamba, Griffin, and RWKV. (Right) A new view of those layers that rely on implicit attention. Our perspective enables the generation of attention maps, offering valuable applications in areas such as Explainable AI. Subsequently, Sections. 3.2 and 3.3 extend our approach to other architectures, such as Griffin [14] and RWKV [41]. Additionally, in Appendix A, we present how to extract holistic data-controlled attention matrices for RetNet [52] and HGRN [47]. ### 3.1 Formulation of Mamba via Attention matrices\n\nMamba can be formulated in a way that separates the components that mix channels from those that mix tokens, see Fig. 3 in [20]. $$\n\\operatorname{Mamba}(x)=\\operatorname{Linaer}_{3}\\left(\\operatorname{SILU}\\left(\\operatorname{Linear}_{2}\\left(\\operatorname{Linear}_{1}(x)\\right)\\right) \\otimes \\operatorname{S6}\\left(\\operatorname{SILU}\\left(\\operatorname{Conv1D}\\left(\\operatorname{Linear}_{1}(x)\\right)\\right)\\right)\\right)\n$$\n\nSince Linear $_{1}$ and Linear $_{3}$ do not mix tokens, they are irrelevant (similarly to the MLP layers) and we consider the simplified expression:\n\n$$\n\\operatorname{Mamba}(x)=\\left(\\operatorname{SILU}\\left(\\operatorname{Linear}_{2}(x)\\right)\\right) \\otimes(\\operatorname{S6}(\\operatorname{SILU}(\\operatorname{Conv1D}(x))))\n$$\n\nReplacing the element-wise gating multiplication with matrix multiplication leads to:\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=93&width=992&top_left_y=1623&top_left_x=558)\n\nThe S6 layer can be formalized as a data-control linear operator (see Eq. 12 in [2]):\n\n$$\n\\mathbf{S 6}(x)=\\hat{\\alpha} x, \\quad \\hat{\\alpha}_{i, j}=C_{i}\\left(\\prod_{k=j+1}^{i} \\bar{A}_{k}\\right) \\bar{B}_{j}\n$$\n\nBy plugging Eq. 7 into Eq. 6 and since $\\operatorname{SILU}(x)=\\mathbf{S i g}(x) \\cdot x$ :\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_4cce2468fce7f22e1a9ag-04.jpg?height=136&width=1193&top_left_y=1905&top_left_x=463)\n\nRecall that causal Conv1D layer with filter $f=\\left(f_{1}, \\cdots, f_{\\hat{L}}\\right)$ can be converted into a matrix form by arranging shifted copies of the filter into rows, forming a convolution matrix $M$.\n```\n\n#### 3. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.33)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 8/35 (Score: 0.33)*\n\n```\nFigure 3 also includes a baseline row factoring a matrix of appropriately scaled i.i.d. Gaussian entries, to indicate typical errors for factoring an unstructured matrix. ### 4.2 Neural Network Compression\n\nMany structured matrix approaches have been proposed to replace fully-connected (FC) layers of neural networks, to speed up training and inference, and to reduce the memory consumption. These structured matrices are cleverly designed by combining commonly used fast transforms. For example, Fastfood [24] and Deep Fried Convnets [47] compose the fast Hadamard transform and fast Fourier transforms, and Sindhwani et al. 40] use Toeplitz-like matrices that can be written as a sequence of 2 or 4 FFTs. However, the design choice for these light-weight replacement layers is restricted by the set of known and implementable transforms. On the first benchmark task of compressing a single hidden layer model, the real version of BPBP has better classification accuracy than a fully-connected layer on all datasets tested, and uses more than 56X fewer parameters (Table 1); the complex version performs even better with a slight parameter increase. The previous best methods fail to achieve this on the more challenging CIFAR-10 dataset at the same parameter budget [42]. We further demonstrate that this layer is effective as a lightweight addition to a larger-scale ResNet architecture. Fully-connected Previous work showed that structured matrix approaches based on the low displacement rank framework, including Toeplitz-like 40, LDR-SD and LDR-TD matrices 42, compare very favorably to other compression approaches. Following previous experimental settings [5, 40, 42, we compare our proposed classes to several baselines using dense structured matrices to compress the hidden layer of a single hidden layer neural network. Competing methods include simple low-rank factorizations [9], circulant matrices (equivalent to 1-dimensional convolutions) [6], the adaptive Fastfood transform 47, and low displacement rank methods 40, 42 which implicitly define a structured matrix through a displacement equation and admit specialized fast divide-and-conquer algorithms 8.\n```\n\n#### 4. HyperMixer: An MLP-based Low Cost Alternative to Transformers  (Avg. Score: 0.25)\n\n*Florian Mai, Arnaud Pannatier, Fabio Fehr, Haolin Chen, Fran\u00e7ois Marelli, F. Fleuret, J. Henderson*\n\n**Published in:** Annual Meeting of the Association for Computational Linguistics (2022)\t**Cited by** 7  (*Influential: 2*)\n\n**TL;DR:** This paper proposes a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks, and demonstrates that this model performs better than alternative MLP-based models, and on par with Transformers.\n\n**Abstract:** Transformer-based architectures are the model of choice for natural language understanding, but they come at a significant cost, as they have quadratic complexity in the input length, require a lot of training data, and can be difficult to tune. In the pursuit of lower costs, we investigate simple MLP-based architectures. We find that existing architectures such as MLPMixer, which achieves token mixing through a static MLP applied to each feature independently, are too detached from the inductive biases required for natural language understanding. In this paper, we propose a simple variant, HyperMixer, which forms the token mixing MLP dynamically using hypernetworks. Empirically, we demonstrate that our model performs better than alternative MLP-based models, and on par with Transformers. In contrast to Transformers, HyperMixer achieves these results at substantially lower costs in terms of processing time, training data, and hyperparameter tuning.\n\n##### *Relevant Chunk: No. 33/36 (Score: 0.25)*\n\n```\nwe create $W_{2}$ from a separate vector $w_{2}$. Note that this baseline does not support dynamicity, as the weight vector is independent of the inputs. This baseline thus shows the importance of dynamicity in our model. Results Results are shown in Table 5. Untying the hypernetworks in HyperMixer leads to slightly decreased performance on all datasets. We hypothesize that without pretraining, the model cannot benefits from more capacious token interaction modeling introduced by untying. Nonetheless, the untied model still performs or a little better than vanilla Transformers. While the introduction of MLPMixer and similar models follows a trend towards conceptually more simplistic models, our ablations show, perhaps unsurprisingly, that simplicity is not better when it leads to discarding information, as both the FeatureMixing only and Location-Mixing only models perform substantially worse than the full HyperMixer model. Moreover, it is not enough to use the same learnable weight vector for all positions (Shared Weight-Vector), indicating the importance of generating the MLP based on the input. The simplistic Feature-Mixing only model performs poorly on all datasets except SST, where it performs as well as the other models. This indicates that many instances in SST can be solved by looking at individual tokens alone, rather than modeling their interactions. ## C. 3 Visualizing Attention Patterns\n\nFigure 6 shows the pseudo-attention of all models (except 'None') alongside the true attention weights of attention. First, it should be noted that pseudo-attention weights offer a somewhat blurry version of true attention weights, where high weights occur at positions that correspond to the same shape (cmp. 6a to 6b). Second, we observe that the pseudo-attention weights of HyperMixer and attention (cmp. Figure 6 d to 6 b ) are similar. This indicates that HyperMixer indeed learns an attention-like function. Third, MLPMixer also shows a similar pattern, but the relevant positions have weak connections (Figure 6c). This confirms our finding that MLPMixer requires substantially more training data to learn strong connections. | Model | MNLI | SNLI | QQP | QNLI |  | SST | \\# Params |\n| :---: | :---: | :---: | :---: | :---: | :---: | ---: | ---: |\n| Ablations | Validation set results (average accuracy / standard deviation over 10 seeds) |  |  |  |  |  |  |\n| Feature Mixing only | $54.5(0.25)$ | $67.0(0.14)$ | $75.9(0.06)$ | $60.8(0.42)$ | $79.7(0.64)$ | 9 M |  |\n| Token Mixing only | $59.0(0.79)$ | $74.5(5.53)$ | $79.5(4.63)$ | $61.8(1.29)$ | $76.3(4.94)$ | 10 M |  |\n| Shared Weight-Vector | $57.1(2.38)$ | $74.3(1.96)$ | $82.9(0.10)$ | $65.9(0.42)$ | $79.8(0.52)$ | 9.5 M |  |\n| HyperMixer (untied) | $65.8(0.46)$ | $81.7(0.30)$ | $84.8(0.23)$ | $73.3(0.53)$ | $80.3(0.35)$ | 12 M |  |\n| HyperMixer (tied) | $66.2(0.21)$ | $81.9(0.27)$ | $85.6(0.20)$ | $78.0(0.19)$ | $80.7(0.84)$ | 11 M |  |\n|  |  |  |  |  |  |  |  |\n\nTable 5: Mean and standard deviation of HyperMixer ablations on the validation set. ![](https://cdn.mathpix.com/cropped/2024_09_12_6b395d4a8d566ae2251dg-18.jpg?height=1680&width=1578&top_left_y=880&top_left_x=238)\n\nFigure 6: Results and (pseudo-)attention maps on the synthetic task (Fleuret, 2019). ## D Comparison of \\#FOP\n\nWe want to compute the number of floating-point operations needed in self-attention vs. HyperMixing for a single example. Let $N$ be the sequence length, $d$ be the embedding size of each token, and $d^{\\prime}$ the hidden dimension. For simplicity, we will assume basic mathematical operators like $\\exp , \\tanh , \\sqrt{x}$ and division to be equal to one floating operation. However, their actual cost is higher but depends on implementation and hardware. ## D. 1 Basic Building Blocks\n\nWe first compute the number of operations infrequently occurring in basic building blocks of neural networks. Matrix Multiplication Multiplying matrix $A \\in$ $\\mathbb{R}^{N \\times d} A \\in \\mathbb{R}^{d \\times M}$ takes $2 d(N M)$ operations, as $2 d$ operations are needed for a single dot-product and there are $N M$ entries in the resulting matrix. Linear Layer Passing a single vector of size $d$ through a linear layer without bias of size $\\left(d, d^{\\prime}\\right)$ is the multiplication of a single vector with a matrix, i.e., incurs $2 d d^{\\prime}$ operations in total. GELU GELU is usually approximated as\n$\\operatorname{GELU}(x)=0.5 x\\left[1+\\tanh \\left(\\sqrt{2 / \\pi}\\left(x+c x^{3}\\right)\\right)\\right]$\nSo in total, GELU is computed for every of the $d$ features and every of the $N$ vectors, meaning the GELU activation layer takes $9 d N$ operations. MLP (input = output size) Given hidden size $d^{\\prime}$ and input/output size $d$, we have two linear layers of size $\\left(d, d^{\\prime}\\right)$ and $\\left(d^{\\prime}, d\\right)$, respectively, plus a GELU layer on $d^{\\prime}$ dimensions, incurring $4 d d^{\\prime}+9 d^{\\prime}$. MLP (input/= output size) Given hidden size $d^{\\prime}$, input size $d$ and output size $d^{\\prime \\prime}$, we have two linear layers of sizes $\\left(d, d^{\\prime}\\right)$ and $\\left(d^{\\prime}, d^{\\prime \\prime}\\right)$, incurring $2 d d^{\\prime}+2 d^{\\prime} d^{\\prime \\prime}+9 d^{\\prime}$. Softmax Softmax is applied over $N$ values, each of which goes through an exp and a division by the normalization value.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Evolution Strategies Neural Networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Evolving Connectivity for Recurrent Spiking Neural Networks\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Guan Wang, Yuhao Sun, Sijie Cheng, Sen Song*\n\n**TL;DR:** The evolving connectivity (EC) framework is proposed, an inference-only method for training RSNNs that reformulates weight-tuning as a search into parameterized connection probability distributions, and employs Natural Evolution Strategies (NES) for optimizing these distributions.\n\n**Abstract:** Recurrent spiking neural networks (RSNNs) hold great potential for advancing artificial general intelligence, as they draw inspiration from the biological nervous system and show promise in modeling complex dynamics. However, the widely-used surrogate gradient-based training methods for RSNNs are inherently inaccurate and unfriendly to neuromorphic hardware. To address these limitations, we propose the evolving connectivity (EC) framework, an inference-only method for training RSNNs. The EC framework reformulates weight-tuning as a search into parameterized connection probability distributions, and employs Natural Evolution Strategies (NES) for optimizing these distributions. Our EC framework circumvents the need for gradients and features hardware-friendly characteristics, including sparse boolean connections and high scalability. We evaluate EC on a series of standard robotic locomotion tasks, where it achieves comparable performance with deep neural networks and outperforms gradient-trained RSNNs, even solving the complex 17-DoF humanoid task. Additionally, the EC framework demonstrates a two to three fold speedup in efficiency compared to directly evolving parameters. By providing a performant and hardware-friendly alternative, the EC framework lays the groundwork for further energy-efficient applications of RSNNs and advances the development of neuromorphic devices.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 2. Evolution of Neural Tangent Kernels under Benign and Adversarial Training\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Noel Loo, Ramin M. Hasani, Alexander Amini, Daniela Rus*\n\n**TL;DR:** Under adversarial training, the empirical NTK rapidly converges to a different kernel (and feature map) than standard training, which provides adversarial robustness, even when non-robust training is performed on top of it.\n\n**Abstract:** Two key challenges facing modern deep learning are mitigating deep networks' vulnerability to adversarial attacks and understanding deep learning's generalization capabilities. Towards the first issue, many defense strategies have been developed, with the most common being Adversarial Training (AT). Towards the second challenge, one of the dominant theories that has emerged is the Neural Tangent Kernel (NTK) -- a characterization of neural network behavior in the infinite-width limit. In this limit, the kernel is frozen, and the underlying feature map is fixed. In finite widths, however, there is evidence that feature learning happens at the earlier stages of the training (kernel learning) before a second phase where the kernel remains fixed (lazy training). While prior work has aimed at studying adversarial vulnerability through the lens of the frozen infinite-width NTK, there is no work that studies the adversarial robustness of the empirical/finite NTK during training. In this work, we perform an empirical study of the evolution of the empirical NTK under standard and adversarial training, aiming to disambiguate the effect of adversarial training on kernel learning and lazy training. We find under adversarial training, the empirical NTK rapidly converges to a different kernel (and feature map) than standard training. This new kernel provides adversarial robustness, even when non-robust training is performed on top of it. Furthermore, we find that adversarial training on top of a fixed kernel can yield a classifier with $76.1\\%$ robust accuracy under PGD attacks with $\\varepsilon = 4/255$ on CIFAR-10.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 11  (*Influential: 1*)\n\n#### 3. Structured Evolution with Compact Architectures for Scalable Policy Optimization\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*K. Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, Adrian Weller*\n\n**TL;DR:** It is shown that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies algorithm.\n\n**Abstract:** We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 141  (*Influential: 13*)\n\n#### 4. Deep Networks with Internal Selective Attention through Feedback Connections\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Marijn F. Stollenga, Jonathan Masci, Faustino J. Gomez, J. Schmidhuber*\n\n**TL;DR:** DasNet harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters.\n\n**Abstract:** Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNets feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2014\n\n**Citations:** 253  (*Influential: 11*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Evolution Strategies as a Scalable Alternative to Reinforcement Learning\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Tim Salimans, Szymon Sidor, Jonathan Ho, Ilya Sutskever, Xi Chen*\n\n**Abstract:** We explore the use of Evolution Strategies (ES), a class of black box\noptimization algorithms, as an alternative to popular MDP-based RL techniques\nsuch as Q-learning and Policy Gradients. Experiments on MuJoCo and Atari show\nthat ES is a viable solution strategy that scales extremely well with the\nnumber of CPUs available: By using a novel communication strategy based on\ncommon random numbers, our ES implementation only needs to communicate scalars,\nmaking it possible to scale to over a thousand parallel workers. This allows us\nto solve 3D humanoid walking in 10 minutes and obtain competitive results on\nmost Atari games after one hour of training. In addition, we highlight several\nadvantages of ES as a black box optimization technique: it is invariant to\naction frequency and delayed rewards, tolerant of extremely long horizons, and\ndoes not need temporal discounting or value function approximation.\n\n**Published:** 2017-03-10\n\n\n\n#### 2. VINE: An Open Source Interactive Data Visualization Tool for Neuroevolution\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Jeff Clune, Rui Wang, Kenneth O. Stanley*\n\n**Abstract:** Recent advances in deep neuroevolution have demonstrated that evolutionary\nalgorithms, such as evolution strategies (ES) and genetic algorithms (GA), can\nscale to train deep neural networks to solve difficult reinforcement learning\n(RL) problems. However, it remains a challenge to analyze and interpret the\nunderlying process of neuroevolution in such high dimensions. To begin to\naddress this challenge, this paper presents an interactive data visualization\ntool called VINE (Visual Inspector for NeuroEvolution) aimed at helping\nneuroevolution researchers and end-users better understand and explore this\nfamily of algorithms. VINE works seamlessly with a breadth of neuroevolution\nalgorithms, including ES and GA, and addresses the difficulty of observing the\nunderlying dynamics of the learning process through an interactive\nvisualization of the evolving agent's behavior characterizations over\ngenerations. As neuroevolution scales to neural networks with millions or more\nconnections, visualization tools like VINE that offer fresh insight into the\nunderlying dynamics of evolution become increasingly valuable and important for\ninspiring new innovations and applications.\n\n**Published:** 2018-05-03\n\n\n\n#### 3. Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents\n\n*From Search Query: Evolution Strategies Neural Networks*\n\n*Felipe Petroski Such, Joel Lehman, Vashisht Madhavan, Kenneth O. Stanley, Jeff Clune, Edoardo Conti*\n\n**Abstract:** Evolution strategies (ES) are a family of black-box optimization algorithms\nable to train deep neural networks roughly as well as Q-learning and policy\ngradient methods on challenging deep reinforcement learning (RL) problems, but\nare much faster (e.g. hours vs. days) because they parallelize better. However,\nmany RL problems require directed exploration because they have reward\nfunctions that are sparse or deceptive (i.e. contain local optima), and it is\nunknown how to encourage such exploration with ES. Here we show that algorithms\nthat have been invented to promote directed exploration in small-scale evolved\nneural networks via populations of exploring agents, specifically novelty\nsearch (NS) and quality diversity (QD) algorithms, can be hybridized with ES to\nimprove its performance on sparse or deceptive deep RL tasks, while retaining\nscalability. Our experiments confirm that the resultant new algorithms, NS-ES\nand two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES\nto achieve higher performance on Atari and simulated robots learning to walk\naround a deceptive trap. This paper thus introduces a family of fast, scalable\nalgorithms for reinforcement learning that are capable of directed exploration.\nIt also adds this new family of exploration algorithms to the RL toolbox and\nraises the interesting possibility that analogous algorithms with multiple\nsimultaneous paths of exploration might also combine well with existing RL\nalgorithms outside ES.\n\n**Conference:** improving-exploration-in-evolution-strategies-1\n\n**Published:** 2017-12-18\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on matrix-free operations, adaptive state compression, and evolution-guided adaptation, here are some key points and references that can be useful:\n\n### Matrix-Free Operations\n\n- **Efficient Matrix-Free Approximations**: The work by Frantar et al. (2021) on \"M-FAC: Efficient Matrix-Free Approximations of Second-Order Information\" provides insights into matrix-free approaches for estimating inverse-Hessian vector products. This can be adapted to reduce computational costs in neural network architectures, especially in the context of autoregressive language models where efficient processing of long sequences is crucial.\n\n- **Inner-Loop Free Updates**: The paper by Fan et al. (2017) on \"Inner-loop free ADMM using Auxiliary Deep Neural Networks\" discusses strategies for matrix-free computations using deep learning. This approach can be integrated into the architecture of autoregressive models to avoid explicit matrix operations, enhancing efficiency.\n\n### Adaptive State Compression\n\n- **Semantic Compression and AutoCompressors**: The research by Fei et al. (2023) on \"Extending Context Window of Large Language Models via Semantic Compression\" and Chevalier et al. (2023) on \"Adapting Language Models to Compress Contexts\" highlights the benefits of compressing long contexts into summary vectors. These methods can be incorporated into autoregressive models to efficiently handle longer sequences without significant computational overhead.\n\n- **Training on Compressed Text**: Lester et al. (2024) discuss training LLMs on compressed text, which emphasizes the efficiency and learnability challenges. The concept of Equal-Info Windows for compression could inform the adaptive compression strategy in autoregressive models, ensuring that relevant information is retained while reducing computational costs.\n\n### Evolution-Guided Adaptation\n\n- **Evolution Strategies in Neural Networks**: Evolution strategies like the Covariance Matrix Adaptation Evolution Strategy (CMA-ES) can be used for test-time adaptation in neural networks. This approach allows the model to adapt to new data or environments without retraining from scratch. While not directly covered in the provided references, exploring evolution strategies can enhance the flexibility and efficiency of autoregressive models. For example, using CMA-ES to adapt the model's parameters in response to changing input distributions or tasks can improve its robustness and performance.\n\n### Residual Connections and Efficient Architectures\n\n- **Residual Neural Networks**: Incorporating residual connections, as described in residual neural networks, can help in training deeper models more efficiently. This architecture allows the model to learn residual functions, which can simplify the learning process and improve the model's ability to handle long-range dependencies, a common challenge in autoregressive models.\n\n### Integration and Future Directions\n\n- **Combining Matrix-Free Operations and Adaptive Compression**: Integrating matrix-free operations with adaptive state compression techniques can significantly enhance the efficiency of autoregressive language models. This combination can reduce the computational overhead associated with processing long sequences while maintaining or improving the model's performance.\n\n- **Evolution-Guided Adaptation for Dynamic Environments**: Using evolution strategies to guide the adaptation of the model in dynamic environments can ensure that the model remains effective even when the input data or tasks change over time. This can be particularly useful in real-world applications where the model needs to adapt to new data without extensive retraining.\n\nBy focusing on these areas, researchers can develop more efficient, adaptive, and robust autoregressive language models that can handle long sequences effectively and adapt to changing environments.\n",
            "analysis": "### Summary of Analysis\nThe proposal for MatrixFreeAdaptiveTTTLinear introduces innovative concepts such as matrix-free operations, adaptive state compression, and evolution-guided adaptation. The search results provide a wealth of information on these topics, offering insights into existing research and potential challenges.\n\n### Useful References and Excerpts\n1. **Adapting Language Models to Compress Contexts** (Chevalier et al., 2023): This work discusses the use of AutoCompressors to extend context windows in LMs, highlighting the benefits of compressing long contexts into summary vectors. This aligns with the proposal's focus on adaptive state compression.\n\n2. **Extending Context Window of Large Language Models via Semantic Compression** (Fei et al., 2023): This paper introduces a semantic compression method that enables LLMs to handle longer texts without significant computational costs, which is relevant to the proposal's goal of efficient long-sequence processing.\n\n3. **Training LLMs over Neurally Compressed Text** (Lester et al., 2024): This research explores training LLMs on compressed text, emphasizing the efficiency and learnability challenges. The concept of Equal-Info Windows for compression could inform the proposal's adaptive compression strategy.\n\n4. **M-FAC: Efficient Matrix-Free Approximations of Second-Order Information** (Frantar et al., 2021): This work investigates matrix-free approaches for estimating inverse-Hessian vector products, providing insights into matrix-free operations that could be applicable to the proposal.\n\n5. **Inner-loop free ADMM using Auxiliary Deep Neural Networks** (Fan et al., 2017): This paper discusses inner-loop free update rules using deep learning, which may offer strategies for matrix-free computations in the proposal.\n\n### Key Insights and Analysis\n- **Matrix-Free Operations**: The concept of matrix-free operations is explored in several papers, highlighting the potential for reducing computational costs and improving efficiency. The proposal's approach aligns with these findings, suggesting it could offer significant performance benefits.\n\n- **Adaptive State Compression**: The use of adaptive compression techniques is well-supported by existing research, which demonstrates their effectiveness in extending context windows and reducing computational overhead. The proposal's focus on content-aware filtering and selective updates is innovative and promising.\n\n- **Evolution-Guided Adaptation**: While not directly covered in the search results, the use of evolution strategies like CMA-ES for adaptation is a novel approach that could enhance model flexibility and efficiency.\n\n### Future Search Plan\n- Conduct further searches on evolution strategies in neural networks to better understand their application in test-time adaptation.\n- Explore more on the integration of matrix-free operations in neural network architectures to gather additional insights and potential challenges.\n\n### References\n- Chevalier, A., Wettig, A., Ajith, A., & Chen, D. (2023). Adapting Language Models to Compress Contexts. Conference on Empirical Methods in Natural Language Processing.\n- Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., & Han, W. (2023). Extending Context Window of Large Language Models via Semantic Compression. arXiv.org.\n- Lester, B., Lee, J., Alemi, A., Pennington, J., Roberts, A., Sohl-Dickstein, J. N., & Constant, N. (2024). Training LLMs over Neurally Compressed Text. arXiv.org.\n- Frantar, E., Kurtic, E., & Alistarh, D. (2021). M-FAC: Efficient Matrix-Free Approximations of Second-Order Information. Neural Information Processing Systems.\n- Fan, K., Wei, Q., & Heller, K. (2017). Inner-loop free ADMM using Auxiliary Deep Neural Networks. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "Matrix-Free Neural Networks",
            "detail": "Evolution strategies in neural network architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nEvolution strategies in neural network architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Structured state-space models are deep Wiener models (Avg. Score: 0.01)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 16/22 (Score: 0.01)*\n\n```\nForgione, M. and Piga, D. (2021). dynoNet: A neural network architecture for learning dynamical systems. International Journal of Adaptive Control and Signal Processing, 35(4), 612-626. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. (2020). Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 1474-1487. Gu, A., Goel, K., Gupta, A., and R\u00e9, C. (2022). On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 35971-35983. Gu, A., Goel, K., and R\u00e9, C. (2021). Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Gupta, A., Gu, A., and Berant, J. (2022). Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35, $22982-22994$. Kumar, S.K. (2017). On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863. Lanzetti, N. et al. (2019). Recurrent neural network based MPC for process industries. In 2019 18th European Control Conference (ECC), 1005-1010. IEEE. Ljung, L., Zhang, Q., Lindskog, P., and Juditski, A. (2004). Estimation of grey box and black box models for nonlinear circuit data. IFAC Proceedings Volumes, 37(13), $399-404$. Marconato, A., Sj\u00f6berg, J., Suykens, J.A., and Schoukens, J. (2013). Improved initialization for nonlinear statespace modeling. IEEE Transactions on instrumentation and Measurement, 63(4), 972-980.\n```\n\n#### 2. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.01)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 45/64 (Score: 0.01)*\n\n```\nPlease refer to Tay et al. (2022) and Wan et al. (2023) for a comprehensive and in-depth survey of efficient transformers. Recurrent architectures Before the advent of transformers, recurrent neural networks, especially Long Short-Term Memory (LSTM) (Hochreiter \\& Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014), were the dominant architectures in NLP for sequence processing. However, traditional RNNs are hard, if not impossible, to parallelize across the time dimension, susceptible to gradient vanishing and explosion, and ineffective in capturing long-range dependencies, which are ubiquitous in natural language. These shortcomings contributed to the rapid decline of traditional RNNs in NLP. There has been a revival of RNNs in NLP research (Tiezzi et al., 2024) in recent years. Compared to transformers with quadratic complexity, RNNs are highly efficient in autoregressive inference with $\\mathrm{O}(1)$ time complexity per step, making them an attractive architecture for large language models. Many efforts have been devoted to parallelized recurrent models and improving their capability to capture long-range dependency, while maintaining the low inference complexity. The Legendre Memory Unit (LMU) (Voelker et al., 2019) was designed to efficiently handle longrange dependencies with a new type of memory cell for recurrent neural networks. Unlike LSTM units, which struggle with remembering information over very long sequences, LMU use Legendre polynomials to create a memory system that can maintain and process information over extended time periods more effectively. High-order polynomial projection operators (HiPPO) (Gu et al., 2020) generalizes LMU by providing a flexible framework for online compression of signals through polynomial projections, accommodating various polynomial bases beyond Legendre polynomials. It optimizes function approximation over time, adapting to different data timescales without needing predefined hyperparameters. SSMs have inspired a range of follow-up research to incorporate SSMs, or modified SSMs into end-to-end architectures for language modeling, including MEGA (Ma et al., 2022), DSS (Gupta et al., 2022), H3 (Fu et al., 2022), and Linear Recurrent Unit (LRU) (Orvieto et al., 2023). Mamba (Gu \\& Dao, 2023) is a selective SSM that introduces time-dependent selective mechanism to enhance the long-range modeling ability of SSMs. The selectivity removes the linear time-variance property of the SSM, making it no longer possible to parallelize Mamba as a long convolution kernel. Yet Mamba can still be effectively parallelized using parallel associative scan\n(Blelloch, 1990; Martin \\& Cundy, 2018; Smith et al., 2023) with a hardware-aware implementation. Recently proposed GateLoop (Katsch, 2023) also adopts a similar data-dependent state transitions. The data-dependent states, also concurrently proposed in GLA (Yang et al., 2023), are similar to the Weighted Key-Value State in Finch. A contemporary but independent work also proposes recurrent models named as Hawk and Griffin (De et al., 2024). Hawk is a recurrent model with the Real-Gated Linear Recurrent Unit (RG-LRU), whereas Griffin mixes the RG-LRU with local multi-query attention, thereby achieving long-context extrapolation efficiently. Please see Tiezzi et al.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.01)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.01)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n##### *Relevant Chunk: No. 7/74 (Score: 0.00)*\n\n```\nHowever, their effectiveness is limited by how well this state has compressed the context. To understand this principle, we focus on two running examples of synthetic tasks (Figure 2). - The Selective Copying task modifies the popular Copying task (Arjovsky, Shah, and Bengio 2016) by varying the position of the tokens to memorize. It requires content-aware reasoning to be able to memorize the relevant tokens (colored) and filter out the irrelevant ones (white). - The Induction Heads task is a well-known mechanism hypothesized to explain the majority of in-context learning abilities of LLMs (Olsson et al. 2022). It requires context-aware reasoning to know when to produce the correct output in the appropriate context (black). These tasks reveal the failure mode of LTI models. From the recurrent view, their constant dynamics (e.g. the $(\\bar{A}, \\bar{B})$ transitions in (2)) cannot let them select the correct information from their context, or affect the hidden state passed along the sequence in an input-dependent way. From the convolutional view, it is known that global convolutions can solve the vanilla Copying task (Romero et al. 2021) because it only requires time-awareness, but that they have difficulty with the Selective Copying task because of lack of content-awareness (Figure 2). More concretely, the spacing between inputs-to-outputs is varying and cannot be modeled by static convolution kernels. In summary, the efficiency vs. effectiveness tradeoff of sequence models is characterized by how well they compress their state: efficient models must have a small state, while effective models must have a state that contains all necessary information from the context. In turn, we propose that a fundamental principle for building sequence models is selectivity: or the context-aware ability to focus on or filter out inputs into a sequential state. In particular, a selection mechanism controls how information propagates or interacts along the sequence dimension (see Section 3.5 for more discussion). ### 3.2 Improving SSMs with Selection\n\nOne method of incorporating a selection mechanism into models is by letting their parameters that affect interactions along the sequence (e.g. the recurrent dynamics of an RNN or the convolution kernel of a CNN ) be input-dependent. Algorithms 1 and 2 illustrates the main selection mechanism that we use. The main difference is simply making several parameters $\\Delta, B, C$ functions of the input, along with the associated changes to tensor shapes throughout. In particular, we highlight that these parameters now have a length dimension $L$, meaning that the model has changed from time-invariant to time-varying. (Note that shape annotations were described in Section 2.) This loses the equivalence to convolutions (3) with implications for its efficiency, discussed next. We specifically choose $s_{B}(x)=\\operatorname{Linear}_{N}(x), s_{C}(x)=\\operatorname{Linear}_{N}(x), s_{\\Delta}(x)=\\operatorname{Broadcast}_{D}\\left(\\operatorname{Linear}_{1}(x)\\right)$, and $\\tau_{\\Delta}=$ softplus, where Linear $_{d}$ is a parameterized projection to dimension $d$. The choice of $s_{\\Delta}$ and $\\tau_{\\Delta}$ is due to a connection to RNN gating mechanisms explained in Section 3.5. ![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-06.jpg?height=421&width=1722&top_left_y=256&top_left_x=234)\n\nFigure 2: (Left) The standard version of the Copying task involves constant spacing between input and output elements and is easily solved by time-invariant models such as linear recurrences and global convolutions. (Right Top) The Selective Copying task has random spacing in between inputs and requires time-varying models that can selectively remember or ignore inputs depending on their content. (Right Bottom) The Induction Heads task is an example of associative recall that requires retrieving an answer based on context, a key ability for LLMs. ```\nAlgorithm 1 SSM (S4)\nAlgorithm 2 SSM + Selection (S6)\nInput: \\(x:(B, L, D)\\)\nInput: \\(x:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\nOutput: \\(y:(B, L, D)\\)\n    1: \\(A:(D, N) \\leftarrow\\) Parameter\n    1: \\(\\boldsymbol{A}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n\\(\\triangleright\\) Represents structured \\(N \\times N\\) matrix\n                            \\(>\\) Represents structured \\(N \\times N\\) matrix\n        B \\(:(\\mathrm{D}, \\mathrm{N}) \\leftarrow\\) Parameter\n    2: \\(\\boldsymbol{B}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{N}) \\leftarrow s_{B}(x)\\)\n        \\(C:(D, N) \\leftarrow\\) Parameter\n        \\(\\Delta:(\\mathrm{D}) \\leftarrow \\tau_{\\Delta}\\) (Parameter)\n        \\(\\bar{A}, \\bar{B}:(\\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, A, B)\\)\n        \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n            \\(\\Delta\\) Time-invariant: recurrence or convolution\n    return \\(y\\)\n    3: \\(C:(B, L, N) \\leftarrow s_{C}(x)\\)\n    4: \\(\\Delta:(B, L, D) \\leftarrow \\tau_{\\Delta}\\left(\\right.\\) Parameter \\(\\left.+s_{\\Delta}(x)\\right)\\)\n    5: \\(\\bar{A}, \\overline{\\boldsymbol{B}}:(\\mathrm{B}, \\mathrm{L}, \\mathrm{D}, \\mathrm{N}) \\leftarrow \\operatorname{discretize}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})\\)\n    6: \\(y \\leftarrow \\operatorname{SSM}(\\bar{A}, \\bar{B}, C)(x)\\)\n        \\(\\Delta\\) Time-varying: recurrence (scan) only\n    7: return \\(y\\)\n```\n\n\n### 3.3 Efficient Implementation of Selective SSMs\n\nHardware-friendly primitives such as convolutions (Krizhevsky, Sutskever, and Hinton 2012) and attention (Bahdanau, Cho, and Bengio 2015; Vaswani et al. 2017) enjoy widespread application. Here we aim to make selective SSMs efficient on modern hardware (GPUs) as well. The selection mechanism is quite natural, and earlier works attempted to incorporate special cases of selection, such as letting $\\Delta$ vary over time in recurrent $\\operatorname{SSMs}$ (Gu, Dao, et al. 2020). However, as previously mentioned a core limitation in the usage of SSMs is their computational efficiency, which was why S4 and all derivatives used LTI (non-selective) models, most commonly in the form of global convolutions. ### 3.3.1 Motivation of Prior Models\n\nWe first revisit this motivation and overview our approach to overcome limitations of prior methods. - At a high level, recurrent models such as SSMs always balance a tradeoff between expressivity and speed: as discussed in Section 3.1, models with larger hidden state dimension should be more effective but slower. Thus we want to maximize hidden state dimension without paying speed and memory costs. - Note that the recurrent mode is more flexible than the convolution mode, since the latter (3) is derived from expanding the former (2) $(\\mathrm{Gu}$, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021). However, this would require computing and materializing the latent state $h$ with shape (B,L,D,N), which is much larger (by a factor of $N$, the SSM state dimension) than the input $x$ and output $y$ of shape ( $B, L, D)$. Thus the more efficient convolution mode was introduced which could bypass the state computation and materializes a convolution kernel (3a) of size only (B, L, D). - Prior LTI state space models leverage the dual recurrent-convolutional forms to increase the effective state dimension by a factor of $N(\\approx 10-100)$, much larger than traditional RNNs, without efficiency penalties. ### 3.3.2 Overview of Selective Scan: Hardware-Aware State Expansion\n\nThe selection mechanism is designed to overcome the limitations of LTI models; at the same time, we therefore need to revisit the computation problem of SSMs. We address this with three classical techniques: kernel fusion, parallel scan, and recomputation. We make two main observations:\n\n- The naive recurrent computation uses $O(B L D N)$ FLOPs while the convolutional computation uses $O(B L D \\log (L))$ FLOPs, and the former has a lower constant factor. Thus for long sequences and not-too-large state dimension $N$, the recurrent mode can actually use fewer FLOPs. - The two challenges are the sequential nature of recurrence, and the large memory usage. To address the latter, just like the convolutional mode, we can attempt to not actually materialize the full state $h$. The main idea is to leverage properties of modern accelerators (GPUs) to materialize the state $h$ only in more efficient levels of the memory hierarchy. In particular, most operations (except matrix multiplication) are bounded by memory bandwidth (Dao, Fu, Ermon, et al. 2022; Ivanov et al. 2021; Williams, Waterman, and Patterson 2009). This includes our scan operation, and we use kernel fusion to reduce the amount of memory IOs, leading to a significant speedup compared to a standard implementation. Concretely, instead of preparing the scan input $(\\bar{A}, \\bar{B})$ of size (B, L, D, N) in GPU HBM (high-bandwidth memory), we load the SSM parameters ( $\\triangle, A, B, C)$ directly from slow HBM to fast SRAM, perform the discretization and recurrence in SRAM, and then write the final outputs of size (B, L, D) back to HBM. To avoid the sequential recurrence, we observe that despite not being linear it can still be parallelized with a work-efficient parallel scan algorithm (Blelloch 1990; Martin and Cundy 2018; Smith, Warrington, and Linderman 2023). Finally, we must also avoid saving the intermediate states, which are necessary for backpropagation. We carefully apply the classic technique of recomputation to reduce the memory requirements: the intermediate states are not stored but recomputed in the backward pass when the inputs are loaded from HBM to SRAM. As a result, the fused selective scan layer has the same memory requirements as an optimized transformer implementation with FlashAttention. Details of the fused kernel and recomputation are in Appendix D. The full Selective SSM layer and algorithm is illustrated in Figure 1. ### 3.4 A Simplified SSM Architecture\n\nAs with structured SSMs, selective SSMs are standalone sequence transformations that can be flexibly incorporated into neural networks. The H3 architecture is the basis for the most well-known SSM architectures (Section 2), which are generally comprised of a block inspired by linear attention interleaved with an MLP (multi-layer perceptron) block. We simplify this architecture by combining these two components into one, which is stacked homogenously (Figure 3). This is inspired by the gated attention unit (GAU) (Hua et al. 2022), which did something similar for attention. This architecture involves expanding the model dimension $D$ by a controllable expansion factor $E$. For each block, most of the parameters $\\left(3 E D^{2}\\right)$ are in the linear projections ( $2 E D^{2}$ for input projections, $E D^{2}$ for output projection) while the inner SSM contributes less. The number of SSM parameters (projections for $\\Delta, B, C$, and the matrix $A$ ) are much smaller in comparison. We repeat this block, interleaved with standard normalization and residual connections, to form the Mamba architecture. We always fix to $E=2$ in our experiments and use two stacks of the block to match the $12 D^{2}$ parameters of a Transformer's interleaved MHA (multi-head attention) and MLP blocks. We use the SiLU / Swish activation function (Hendrycks and Gimpel 2016; Ramachandran, Zoph, and Quoc V Le 2017), motivated so that the Gated MLP becomes the popular \"SwiGLU\" variant (Chowdhery et al.\n```\n\n#### 4. Liquid Structural State-Space Models (Avg. Score: 0.01)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 28/54 (Score: 0.01)*\n\n```\nJ. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982. J. Jia and A. R. Benson. Neural jump stochastic differential equations. In Advances in Neural Information Processing Systems, pages 9843-9854, 2019. L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation, 31(4):765-783, 2019. A. Kag, Z. Zhang, and V. Saligrama. Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients? In International Conference on Learning Representations, 2019. R. KALMAN. A new approach to linear filtering and prediction problems.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: Matrix-Free Neural Networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. M-FAC: Efficient Matrix-Free Approximations of Second-Order Information\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Elias Frantar, Eldar Kurtic, Dan Alistarh*\n\n**TL;DR:** This work investigates matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessians by the empirical Fisher matrix.\n\n**Abstract:** Efficiently approximating local curvature information of the loss function is a key tool for optimization and compression of deep neural networks. Yet, most existing methods to approximate second-order information have high computational or storage costs, which can limit their practicality. In this work, we investigate matrix-free, linear-time approaches for estimating Inverse-Hessian Vector Products (IHVPs) for the case when the Hessian can be approximated as a sum of rank-one matrices, as in the classic approximation of the Hessian by the empirical Fisher matrix. We propose two new algorithms as part of a framework called M-FAC: the first algorithm is tailored towards network compression and can compute the IHVP for dimension $d$, if the Hessian is given as a sum of $m$ rank-one matrices, using $O(dm^2)$ precomputation, $O(dm)$ cost for computing the IHVP, and query cost $O(m)$ for any single element of the inverse Hessian. The second algorithm targets an optimization setting, where we wish to compute the product between the inverse Hessian, estimated over a sliding window of optimization steps, and a given gradient direction, as required for preconditioned SGD. We give an algorithm with cost $O(dm + m^2)$ for computing the IHVP and $O(dm + m^3)$ for adding or removing any gradient from the sliding window. These two algorithms yield state-of-the-art results for network pruning and optimization with lower computational overhead relative to existing second-order methods. Implementations are available at [9] and [17].\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 47  (*Influential: 5*)\n\n#### 2. An inner-loop free solution to inverse problems using deep neural networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Kai Fan, Qi Wei, L. Carin, K. Heller*\n\n**TL;DR:** An inner-loop free update rule with two pre-trained deep convolutional architectures that imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem of the ADMM solution for inverse problems.\n\n**Abstract:** We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 21  (*Influential: 1*)\n\n#### 3. A New Perspective on the Effects of Spectrum in Graph Neural Networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Mingqi Yang, Yanming Shen, Rui Li, Heng Qi, Qian Zhang, Baocai Yin*\n\n**TL;DR:** This work proposes the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel, and consistently boosts the performance of learning graph representations.\n\n**Abstract:** Many improvements on GNNs can be deemed as operations on the spectrum of the underlying graph matrix, which motivates us to directly study the characteristics of the spectrum and their effects on GNN performance. By generalizing most existing GNN architectures, we show that the correlation issue caused by the $unsmooth$ spectrum becomes the obstacle to leveraging more powerful graph filters as well as developing deep architectures, which therefore restricts GNNs' performance. Inspired by this, we propose the correlation-free architecture which naturally removes the correlation issue among different channels, making it possible to utilize more sophisticated filters within each channel. The final correlation-free architecture with more powerful filters consistently boosts the performance of learning graph representations. Code is available at https://github.com/qslim/gnn-spectrum.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2021\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 4. Inner-loop free ADMM using Auxiliary Deep Neural Networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Kai Fan, Qi Wei, K. Heller*\n\n**TL;DR:** A conditional denoising auto-encoder is learned which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem of the ADMM solution for inverse problems, leading to so-called amortized inference.\n\n**Abstract:** We propose a new method that uses apply deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an \\textit{inner-loop free} update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., it is data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2017\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Minhyung Cho, C. Dhir, Jaehyung Lee*\n\n**TL;DR:** The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization.\n\n**Abstract:** Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2015\n\n**Citations:** 8  (*Influential: 1*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Learning Student Networks in the Wild\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Yunhe Wang, Chao Xu, Chunjing Xu, Wenshuo Li, Chang Xu, Tianyu Guo, Hanting Chen*\n\n**Abstract:**     Data-free learning for student networks is a new paradigm for solving users' anxiety caused by the privacy problem of using original training data. Since the architectures of modern convolutional neural networks (CNNs) are compact and sophisticated, the alternative images or meta-data generated from the teacher network are often broken. Thus, the student network cannot achieve the comparable performance to that of the pre-trained teacher network especially on the large-scale image dataset. Different to previous works, we present to maximally utilize the massive available unlabeled data in the wild. Specifically, we first thoroughly analyze the output differences between teacher and student network on the original data and develop a data collection method. Then, a noisy knowledge distillation algorithm is proposed for achieving the performance of the student network. In practice, an adaptation matrix is learned with the student network for correcting the label noise produced by the teacher network on the collected unlabeled images. The effectiveness of our DFND (Data-Free Noisy Distillation) method is then verified on several benchmarks to demonstrate its superiority over state-of-the-art data-free distillation methods. Experiments on various datasets demonstrate that the student networks learned by the proposed method can achieve comparable performance with those using the original dataset. Code is available at https://github.com/huawei-noah/Data-Efficient-Model-Compression    \n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2021-06-19\n\n\n\n#### 2. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 3. Tensorized Self-Attention: Efficiently Modeling Pairwise and Global Dependencies Together\n\n*From Search Query: Matrix-Free Neural Networks*\n\n*Tao Shen, Jing Jiang, Chengqi Zhang, Tianyi Zhou, Guodong Long*\n\n**Abstract:** Neural networks equipped with self-attention have parallelizable computation,\nlight-weight structure, and the ability to capture both long-range and local\ndependencies. Further, their expressive power and performance can be boosted by\nusing a vector to measure pairwise dependency, but this requires to expand the\nalignment matrix to a tensor, which results in memory and computation\nbottlenecks. In this paper, we propose a novel attention mechanism called\n\"Multi-mask Tensorized Self-Attention\" (MTSA), which is as fast and as\nmemory-efficient as a CNN, but significantly outperforms previous\nCNN-/RNN-/attention-based models. MTSA 1) captures both pairwise (token2token)\nand global (source2token) dependencies by a novel compatibility function\ncomposed of dot-product and additive attentions, 2) uses a tensor to represent\nthe feature-wise alignment scores for better expressive power but only requires\nparallelizable matrix multiplications, and 3) combines multi-head with\nmulti-dimensional attentions, and applies a distinct positional mask to each\nhead (subspace), so the memory and computation can be distributed to multiple\nheads, each with sequential information encoded independently. The experiments\nshow that a CNN/RNN-free model based on MTSA achieves state-of-the-art or\ncompetitive performance on nine NLP benchmarks with compelling memory- and\ntime-efficiency.\n\n**Conference:** tensorized-self-attention-efficiently\n\n**Published:** 2018-05-02\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on matrix-free operations, adaptive state compression, and evolution-guided adaptation, here are some key points and references that can guide the researchers:\n\n### Matrix-Free Operations\n\n- **Scalable MatMul-free Language Modeling**: Research by Zhu et al. (2024) demonstrates the feasibility of eliminating matrix multiplications from large language models (LLMs) while maintaining performance. This approach can lead to significant memory and computational savings, which is crucial for efficient autoregressive models[6|.\n- **Implicit Attention Formulation**: The work by Zimerman et al. (2024) on a unified implicit attention formulation for gated-linear recurrent sequence models provides insights into attention-free layers, which can be adapted for matrix-free operations. This could help in designing more efficient autoregressive models without the need for explicit matrix multiplications.\n\n### Adaptive State Compression\n\n- **Semantic Compression**: The research by Fei et al. (2023) on extending the context window of LLMs via semantic compression is relevant. This method enables models to handle longer texts efficiently by compressing the state, which can be integrated into autoregressive models to reduce computational overhead and improve performance[6|.\n- **Content-Aware Filtering**: The proposal's focus on content-aware filtering and selective updates aligns with the idea of adaptive state compression. This approach can help in selectively updating the model's state based on the relevance of the input, thereby improving efficiency and reducing the need for extensive computational resources.\n\n### Evolution-Guided Adaptation\n\n- **Evolution Strategies**: The paper by Salimans et al. (2017) on evolution strategies as a scalable alternative to reinforcement learning provides a framework for using evolution strategies in neural network adaptation. This can be applied to autoregressive models for test-time adaptation, enhancing the model's flexibility and efficiency[6|.\n- **CMA-ES**: The Covariance Matrix Adaptation Evolution Strategy (CMA-ES) is a powerful evolution strategy that can be used for adapting neural networks. Integrating CMA-ES into the autoregressive model design can help in optimizing the model's parameters during test time, leading to better performance and adaptability.\n\n### Practical Implementation and Challenges\n\n- **Efficient Matrix-Free Approximations**: The work by Frantar et al. (2021) on efficient matrix-free approximations of second-order information can provide practical insights into implementing matrix-free operations. This includes methods for estimating inverse-Hessian vector products without explicit matrix multiplications, which can be challenging but crucial for efficient autoregressive models.\n\n### Future Directions\n\n- **Integration of Evolution Strategies**: Further research is needed on how to integrate evolution strategies seamlessly into the architecture of autoregressive language models. This includes exploring how to balance the trade-offs between computational efficiency, model performance, and the complexity of evolution strategies.\n- **Practical Implementation Challenges**: Conducting further searches on the practical implementation of matrix-free operations and adaptive state compression will help in identifying potential challenges and optimizing the design for real-world applications.\n\nBy leveraging these insights and references, researchers can develop innovative autoregressive language models that are efficient, scalable, and adaptable, aligning with the goals of low perplexity, high accuracy, robustness, and efficiency.\n",
            "analysis": "### Summary of Analysis\nThe proposal for MatrixFreeAdaptiveTTTLinear is innovative, focusing on matrix-free operations, adaptive state compression, and evolution-guided adaptation. The search results provide substantial information on these topics, highlighting both existing research and potential challenges.\n\n### Useful References and Excerpts\n1. **Scalable MatMul-free Language Modeling** (Zhu et al., 2024): This work demonstrates the feasibility of eliminating matrix multiplications from LLMs while maintaining performance. It highlights the potential for significant memory and computational savings, aligning with the proposal's matrix-free operations.\n\n2. **A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models** (Zimerman et al., 2024): This paper presents a unified view of attention-free layers, including implicit causal self-attention layers, which could inform the proposal's approach to matrix-free operations.\n\n3. **Efficient Matrix-Free Approximations** (Frantar et al., 2021): This work discusses matrix-free approaches for estimating inverse-Hessian vector products, providing insights into reducing computational costs in neural networks.\n\n4. **Evolution Strategies as a Scalable Alternative to Reinforcement Learning** (Salimans et al., 2017): This paper explores evolution strategies as a scalable alternative to traditional reinforcement learning methods, which could be adapted for evolution-guided adaptation in the proposal.\n\n5. **Extending Context Window of Large Language Models via Semantic Compression** (Fei et al., 2023): This research introduces semantic compression methods that enable LLMs to handle longer texts efficiently, relevant to the proposal's adaptive state compression.\n\n### Key Insights and Analysis\n- **Matrix-Free Operations**: The concept of eliminating matrix multiplications is supported by existing research, which shows potential for reducing computational costs and improving efficiency. The proposal's approach aligns with these findings, suggesting it could offer significant performance benefits.\n\n- **Adaptive State Compression**: The use of adaptive compression techniques is well-supported by existing research, demonstrating their effectiveness in extending context windows and reducing computational overhead. The proposal's focus on content-aware filtering and selective updates is innovative and promising.\n\n- **Evolution-Guided Adaptation**: Evolution strategies like CMA-ES offer a novel approach to adaptation in neural networks, potentially enhancing model flexibility and efficiency. This aligns with the proposal's goals of efficient test-time adaptation.\n\n### Future Search Plan\n- Conduct further searches on the practical implementation of matrix-free operations in neural networks to gather additional insights and potential challenges.\n- Explore more on the integration of evolution strategies in neural network architectures to better understand their application in test-time adaptation.\n\n### References\n- Zhu, R.-J., Zhang, Y., Sifferman, E., Sheaves, T., Wang, Y., Richmond, D., Zhou, P., & Eshraghian, J. (2024). Scalable MatMul-free Language Modeling. arXiv.org.\n- Zimerman, I., Ali, A., & Wolf, L. (2024). A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models. arXiv.org.\n- Frantar, E., Kurtic, E., & Alistarh, D. (2021). M-FAC: Efficient Matrix-Free Approximations of Second-Order Information. Neural Information Processing Systems.\n- Salimans, T., Sidor, S., Ho, J., Sutskever, I., & Chen, X. (2017). Evolution Strategies as a Scalable Alternative to Reinforcement Learning. arXiv.org.\n- Fei, W., Niu, X., Zhou, P., Hou, L., Bai, B., Deng, L., & Han, W. (2023). Extending Context Window of Large Language Models via Semantic Compression. arXiv.org."
        }
    ],
    "rating": 4.0,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.731136,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.50148,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "\n# MatrixFreeAdaptiveTTTLinear: Efficient Matrix-Free Test-Time Training with Adaptive State Compression\n\n## Motivation\n\nModern language models face significant challenges in efficiently processing long sequences while maintaining adaptability and computational efficiency. The FastTTTLinear GAU addresses some of these challenges through Gated Linear Attention, but still relies heavily on matrix multiplications and has limitations in state management and hardware utilization. Recent advances in matrix-free operations, evolution strategies for test-time adaptation, and selective computation suggest opportunities for significant improvements.\n\n## Problem Analysis\n\n### Current Limitations\n\n1. **Computational Inefficiency**:\n   - Heavy reliance on matrix multiplications\n   - Quadratic memory requirements for attention mechanisms\n   - Inefficient hardware utilization\n\n2. **State Management**:\n   - Fixed state representations regardless of content importance\n   - Limited compression capabilities\n   - Inefficient memory usage for long sequences\n\n3. **Adaptation Mechanisms**:\n   - Dependency on backpropagation for test-time training\n   - High computational cost for adaptation\n   - Limited hardware compatibility\n\n## Core Idea and Philosophy\n\nWe propose enhancing FastTTTLinear by:\n\n1. **Eliminating Matrix Multiplications**:\n   - Replace matrix operations with efficient element-wise operations\n   - Utilize hardware-friendly alternatives\n   - Maintain model expressiveness through careful design\n\n2. **Adaptive State Compression**:\n   - Dynamically compress states based on content importance\n   - Implement selective update mechanisms\n   - Optimize memory usage through content-aware filtering\n\n3. **Evolution-Guided Adaptation**:\n   - Enable forward-pass-only test-time adaptation\n   - Utilize CMA-ES for efficient parameter updates\n   - Maintain adaptability without backpropagation\n\n## Design Plan\n\n### Selected GAU to Modify\n```selection FastTTTLinear```\n\n### Modification Overview\n\n1. **Matrix-Free Operations**:\n   - Replace matrix multiplications with element-wise operations\n   - Implement efficient linear transformations\n   - Optimize memory access patterns\n\n2. **Adaptive State Management**:\n   - Introduce dynamic compression mechanisms\n   - Implement selective state updates\n   - Optimize state representation\n\n3. **Evolution-Based Adaptation**:\n   - Integrate CMA-ES for test-time adaptation\n   - Implement forward-pass-only updates\n   - Optimize adaptation efficiency\n\n### Mathematical Formulation\n\n1. **Matrix-Free Transformation**:\nInstead of traditional matrix multiplication QK^T, we use:\n\n```python\ndef matrix_free_attention(Q, K, V):\n    # Element-wise operations\n    Q_transformed = element_wise_transform(Q)\n    K_transformed = element_wise_transform(K)\n    \n    # Compute attention scores without matrix multiplication\n    scores = cumulative_sum(Q_transformed * K_transformed)\n    \n    # Apply to values\n    output = selective_combine(scores, V)\n    return output\n```\n\n2. **Adaptive State Compression**:\nFor state s and input x:\n\n```python\ndef compress_state(s, x):\n    # Compute importance scores\n    importance = sigmoid(element_wise_product(s, x))\n    \n    # Selective compression\n    compressed = s * importance\n    \n    # Update state\n    new_state = update_compressed_state(compressed, x)\n    return new_state\n```\n\n3. **Evolution-Guided Adaptation**:\nUsing CMA-ES for parameter updates:\n\n```python\ndef evolution_update(model, x, fitness_fn):\n    # Initialize CMA-ES\n    cma = CMAES(model.parameters())\n    \n    # Update parameters using only forward passes\n    for _ in range(n_steps):\n        candidates = cma.ask()\n        fitness = [fitness_fn(c, x) for c in candidates]\n        cma.tell(candidates, fitness)\n    \n    return cma.result\n```\n\n### Implementation Guidelines\n\n```python\nclass MatrixFreeAdaptiveTTTLinear(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        \n        # Element-wise transformations\n        self.q_transform = ElementWiseTransform(embed_dim)\n        self.k_transform = ElementWiseTransform(embed_dim)\n        self.v_transform = ElementWiseTransform(embed_dim)\n        \n        # State compression\n        self.compressor = AdaptiveCompressor(embed_dim)\n        \n        # Evolution strategy\n        self.evolution = CMAESAdapter(embed_dim)\n        \n    def forward(self, x, state=None):\n        # Matrix-free attention\n        q = self.q_transform(x)\n        k = self.k_transform(x)\n        v = self.v_transform(x)\n        \n        # Compress state if exists\n        if state is not None:\n            state = self.compressor(state, x)\n        \n        # Compute attention without matrix multiplication\n        attn = self.matrix_free_attention(q, k, v)\n        \n        # Apply evolution-guided adaptation\n        if self.training:\n            attn = self.evolution.adapt(attn, x)\n        \n        return attn, state\n```\n\n## Research Summary\n\nKey findings from research:\n\n1. **Matrix-Free Operations**:\n- Elimination of matrix multiplications can significantly reduce computational costs\n- Hardware-friendly alternatives maintain model expressiveness\n- Efficient memory access patterns crucial for performance\n\n2. **State Compression**:\n- Dynamic compression based on content importance improves efficiency\n- Selective updates reduce memory requirements\n- Content-aware filtering enhances performance\n\n3. **Evolution-Based Adaptation**:\n- Forward-pass-only adaptation enables efficient deployment\n- CMA-ES provides effective parameter updates without backpropagation\n- Hardware-aware design principles crucial for practical implementation\n\n## Evolution of Design\n\nThe design evolved through several iterations:\n\n1. Initial focus on matrix-free operations\n2. Integration of adaptive state compression\n3. Addition of evolution-guided adaptation\n4. Optimization for hardware efficiency\n\n## Theoretical Analysis\n\n### Advantages\n\n1. **Computational Efficiency**:\n- Linear complexity through elimination of matrix multiplications\n- Reduced memory requirements via state compression\n- Efficient hardware utilization\n\n2. **Adaptability**:\n- Forward-pass-only adaptation enables efficient deployment\n- Evolution-guided updates maintain model flexibility\n- Dynamic state management improves efficiency\n\n3. **Scalability**:\n- Linear scaling with sequence length\n- Efficient memory usage through compression\n- Hardware-friendly design\n\n### Trade-offs\n\n1. **Implementation Complexity**:\n- More complex implementation than traditional approaches\n- Requires careful tuning of compression parameters\n- Need for specialized optimization strategies\n\n2. **Training Requirements**:\n- Initial training may require more iterations\n- Parameter tuning for evolution strategy\n- Balancing compression and expressiveness\n\n## Conclusion\n\nMatrixFreeAdaptiveTTTLinear represents a significant advancement in efficient language model design. By eliminating matrix multiplications, incorporating adaptive state compression, and utilizing evolution-guided adaptation, it achieves improved computational efficiency while maintaining model expressiveness. The design is particularly well-suited for deployment on modern hardware and shows promise for scaling to longer sequences and larger models.\n\n## References\n\n1. Zhu, R., Zhang, Y., Sifferman, E., et al. (2024). Scalable MatMul-free Language Modeling. arXiv preprint arXiv:2401.04678.\n\n2. Zhao, P., Wu, P., Chen, G., et al. (2024). Test-Time Model Adaptation with Only Forward Passes. arXiv preprint arXiv:2402.01073.\n\n3. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint arXiv:2312.00752.\n\n4. Fu, D.Y., Kumbong, H., Nguyen, E.N.D., & R\u00e9, C. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint arXiv:2311.05908.\n\n5. Wang, S., & Xue, B. (2023). State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Neural Information Processing Systems.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "gpt4o_0806",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel enhancement of FastTTTLinear that eliminates matrix multiplications through adaptive state compression and evolution-guided test-time adaptation, achieving efficient processing of long sequences while maintaining model expressiveness. The design incorporates hardware-aware selective computation and forward-pass-only adaptation mechanisms to reduce computational complexity and memory requirements.",
    "ideation": null,
    "modelname": "matrixfreeadaptivelm",
    "suggestions": "1. Conduct empirical studies to validate theoretical claims, focusing on robustness and adaptability.\n2. Provide detailed implementation guidelines for integrating matrix-free operations and evolution strategies.\n3. Explore potential challenges in integration and propose solutions or mitigations.\n4. Consider providing a detailed analysis of the trade-offs involved in the proposed modifications, including any potential impacts on training complexity and parameter tuning.",
    "user_input": ""
}