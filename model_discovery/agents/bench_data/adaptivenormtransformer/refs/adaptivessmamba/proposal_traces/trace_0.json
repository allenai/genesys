{
    "variantname": null,
    "review": "The AdaptiveSSMamba proposal presents a novel approach to combining state space models with test-time training capabilities through hardware-efficient implementation. After comprehensive analysis of existing research and related work, here is the detailed review:\n\nCLARITY (4.5/5):\nThe proposal is exceptionally well-structured and clearly articulated, with:\n- Well-defined objectives and motivation\n- Detailed mathematical formulations\n- Clear implementation guidelines\n- Comprehensive theoretical analysis\nThe only minor weakness is in the explanation of some parameter initialization strategies.\n\nINNOVATION (4.3/5):\nThe proposal demonstrates significant innovation in several aspects:\n+ Novel integration of adaptive state compression with SSMs\n+ Unique approach to hardware-efficient test-time training\n+ Original unified memory management strategy\nHowever:\n- Some concepts build heavily on existing Mamba2 architecture\n- The adaptive mechanism shares similarities with existing approaches\n\nFEASIBILITY (4.2/5):\nThe implementation appears feasible within the given framework:\n+ Detailed implementation guidelines provided\n+ Clear mathematical formulations\n+ Hardware considerations well-addressed\nConcerns:\n- Complex integration of multiple components may present challenges\n- Stability of adaptive updates needs more detailed analysis\n\nSCALABILITY (4.4/5):\nThe design shows strong potential for scalability:\n+ Linear complexity maintained through careful design\n+ Efficient memory management strategy\n+ Hardware-aware implementation\nMinor concerns:\n- Memory requirements for storing adaptive parameters\n- Potential bottlenecks in very long sequences\n\nACCURACY AND ROBUSTNESS (4.3/5):\nExpected improvements in performance:\n+ Enhanced expressiveness through adaptive state compression\n+ Better handling of varying input patterns\n+ Improved long-range dependency capture\nConcerns:\n- Stability of adaptive updates needs more validation\n- Impact of compression on model accuracy requires testing\n\nEFFICIENCY (4.5/5):\nStrong focus on computational efficiency:\n+ Hardware-optimized implementation\n+ Efficient memory management\n+ Linear complexity maintenance\n+ Careful consideration of memory hierarchy\n\nCOMPARATIVE ANALYSIS:\nThe proposal differs significantly from existing approaches:\n- More hardware-efficient than FastTTTLinear\n- More adaptive than original Mamba2\n- Distinct from sibling proposals (densessmamba, hierarchical_gated_graph_attention)\n- Novel combination of adaptive compression and test-time training\n\nTECHNICAL DEPTH:\nThe proposal demonstrates strong technical understanding:\n+ Comprehensive mathematical formulations\n+ Detailed implementation guidelines\n+ Thorough consideration of hardware aspects\n+ Clear analysis of trade-offs",
    "search_stack": [
        {
            "ready": false,
            "query": "mamba state space model selective attention",
            "detail": "Find technical details about Mamba's selective state space model architecture, focusing on its core mechanisms and performance characteristics.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about Mamba's selective state space model architecture, focusing on its core mechanisms and performance characteristics.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 1.00)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n##### *Relevant Chunk: No. 25/74 (Score: 1.00)*\n\n```\nThis validates our core motivation in Sections 3.1 and 3.3. ## 5 Discussion\n\nWe discuss related work, limitations, and some future directions. Related Work. Appendix A discusses how the selection mechanism relates to similar concepts. Appendix B has an extended related work of SSMs and other related models. Table 9: (Ablations: Expressivity of $\\Delta$. ) The selection mechanism of $\\Delta$ constructs it with a projection of the input. Projecting it even to dim. 1 provides a large increase in performance; increasing it further provides further improvements at the cost of a modest increase in parameters. State size fixed to $N=16$. | Size of $\\Delta$ ProJ. | Params (M) | Perplexity |\n| :--- | :--- | :--- |\n| - | 358.9 | 9.12 |\n| 1 | 359.1 | 8.97 |\n| 2 | 359.3 | 8.97 |\n| 4 | 359.7 | 8.91 |\n| 8 | 360.5 | 8.83 |\n| 16 | 362.1 | 8.84 |\n| 32 | 365.2 | 8.80 |\n| 64 | 371.5 | 8.71 |\n\nTable 10: (Ablations: SSM state dimension.) (Top) Constant $\\boldsymbol{B}$ and $\\boldsymbol{C}$ (Bottom) Selective $B$ and $C$. Increasing the SSM state dimension $N$, which can be viewed as an expansion factor on the dimension of the recurrent state, can significantly improve performance for a negligible cost in parameters/FLOPs, but only when $\\boldsymbol{B}$ and $\\boldsymbol{C}$ are also selective. Size of $\\Delta$ projection fixed to 64 . No Free Lunch: Continuous-Discrete Spectrum. Structured SSMs were originally defined as discretizations of continuous systems (1), and have had a strong inductive bias toward continuous-time data modalities such as perceptual signals (e.g. audio, video). As discussed in Sections 3.1 and 3.5, the selection mechanism overcomes their weaknesses on discrete modalities such as text and DNA; but this conversely can impede their performance on data that LTI SSMs excel on. Our ablations on audio waveforms examine this tradeoff in more detail. Downstream Affordances. Transformer-based foundation models (particularly LLMs) have a rich ecosystem of properties and modes of interaction with pretrained models, such as fine-tuning, adaptation, prompting, in-context learning, instruction tuning, RLHF, quantization, and so on. We are particularly interested in whether Transformer alternatives such as SSMs have similar properties and affordances. Scaling. Our empirical evaluation is limited to small model sizes, below the threshold of most strong open source LLMs (e.g. Llama (Touvron et al. 2023)) as well as other recurrent models such as RWKV (B. Peng et al. 2023) and RetNet (Y. Sun et al. 2023), which have been evaluated at the 7B parameter scale and beyond. It remains to assess whether Mamba still compares favorably at these larger sizes. We also note that scaling SSMs may involve further engineering challenges and adjustments to the model that are not discussed in this paper. ## 6 Conclusion\n\nWe introduce a selection mechanism to structured state space models, allowing them to perform context-dependent reasoning while scaling linearly in sequence length. When incorporated into a simple attention-free architecture, Mamba achieves state-of-the-art results on a diverse set of domains, where it matches or exceeds the performance of strong Transformer models. We are excited about the broad applications of selective state space models to build foundation models for different domains, especially in emerging modalities requiring long context such as genomics, audio, and video. Our results suggest that Mamba is a strong candidate to be a general sequence model backbone. ## Acknowledgments\n\nWe thank Karan Goel, Arjun Desai, and Kush Bhatia for helpful feedback on the draft. ## References\n\n[1] Martin Arjovsky, Amar Shah, and Yoshua Bengio. \"Unitary Evolution Recurrent Neural Networks\". In: The International Conference on Machine Learning (ICML). 2016, pp. 1120-1128. [2] \u017diga Avsec, Vikram Agarwal, Daniel Visentin, Joseph R Ledsam, Agnieszka Grabska-Barwinska, Kyle R Taylor, Yannis Assael, John Jumper, Pushmeet Kohli, and David R Kelley. \"Effective Gene Expression Prediction from Sequence by Integrating Long-range Interactions\".\n```\n\n##### *Relevant Chunk: No. 1/74 (Score: 1.00)*\n\n```\n# Mamba: Linear-Time Sequence Modeling with Selective State Spaces \n\nAlbert $\\mathrm{Gu}^{*^{1}}$ and Tri Dao ${ }^{*^{2}}$<br>${ }^{1}$ Machine Learning Department, Carnegie Mellon University<br>${ }^{2}$ Department of Computer Science, Princeton University<br>agu@cs.cmu.edu, tri@tridao.me\n\n\n#### Abstract\n\nFoundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module.\n```\n\n#### 2. MambaLRP: Explaining Selective State Space Sequence Models (Avg. Score: 1.00)\n\n*F. Jafari, G. Montavon, Klaus-Robert M\u00fcller, Oliver Eberle*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work identifies specific components in the Mamba architecture, which cause unfaithful explanations and proposes MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components.\n\n**Abstract:** Recent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest. These models allow efficient processing of long sequences in linear time and are rapidly being adopted in a wide range of applications such as language modeling, demonstrating promising performance. To foster their reliable use in real-world scenarios, it is crucial to augment their transparency. Our work bridges this critical gap by bringing explainability, particularly Layer-wise Relevance Propagation (LRP), to the Mamba architecture. Guided by the axiom of relevance conservation, we identify specific components in the Mamba architecture, which cause unfaithful explanations. To remedy this issue, we propose MambaLRP, a novel algorithm within the LRP framework, which ensures a more stable and reliable relevance propagation through these components. Our proposed method is theoretically sound and excels in achieving state-of-the-art explanation performance across a diverse range of models and datasets. Moreover, MambaLRP facilitates a deeper inspection of Mamba architectures, uncovering various biases and evaluating their significance. It also enables the analysis of previous speculations regarding the long-range capabilities of Mamba models.\n\n##### *Relevant Chunk: No. 1/59 (Score: 1.00)*\n\n```\n# MambaLRP: Explaining Selective State Space Sequence Models \n\nFarnoush Rezaei Jafari ${ }^{1,2}$ Gr\u00e9goire Montavon ${ }^{3,2,1}$ Klaus-Robert M\u00fcller ${ }^{1,2,4,5,6} \\quad$ Oliver Eberle $^{1,2}$<br>${ }^{1}$ Machine Learning Group, Technische Universit\u00e4t Berlin, 10587 Berlin, Germany<br>${ }^{2}$ BIFOLD - Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany<br>${ }^{3}$ Department of Mathematics and Computer Science, Freie Universit\u00e4t Berlin,<br>Arnimallee 14, 14195 Berlin, Germany<br>${ }^{4}$ Department of Artificial Intelligence, Korea University, Seoul 136-713, South Korea<br>${ }^{5}$ Max Planck Institute for Informatics, Stuhlsatzenhausweg 4, 66123 Saarbr\u00fccken, Germany<br>${ }^{6}$ Google DeepMind, Berlin, Germany<br>Correspondence to: rezaeijafari@campus.tu-berlin.de, oliver.eberle@tu-berlin.de\n\n\n#### Abstract\n\nRecent sequence modeling approaches using Selective State Space Sequence Models, referred to as Mamba models, have seen a surge of interest.\n```\n\n#### 3. The Hidden Attention of Mamba Models  (Avg. Score: 1.00)\n\n*Ameen Ali, Itamar Zimerman, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 19  (*Influential: 3*)\n\n**TL;DR:** A third view is added and it is shown that selective SSMs can be viewed as attention-driven models and empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers.\n\n**Abstract:** The Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision. Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner. We add a third view and show that such models can be viewed as attention-driven models. This new perspective enables us to empirically and theoretically compare the underlying mechanisms to that of the self-attention layers in transformers and allows us to peer inside the inner workings of the Mamba model with explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 1/33 (Score: 1.00)*\n\n```\n# The Hidden Attention of Mamba Models \n\nAmeen Ali*, Itamar Zimerman*, and Lior Wolf<br>School of Computer Science, Tel Aviv University\n\n\n#### Abstract\n\nThe Mamba layer offers an efficient selective state space model (SSM) that is highly effective in modeling multiple domains, including NLP, long-range sequence processing, and computer vision.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: mamba state space model selective attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: mamba state space model selective attention*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 352  (*Influential: 57*)\n\n#### 2. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: mamba state space model selective attention*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: mamba state space model selective attention*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 270  (*Influential: 21*)\n\n#### 4. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: mamba state space model selective attention*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality\n\n*From Search Query: mamba state space model selective attention*\n\n*Albert Gu, Tri Dao*\n\n**Abstract:** While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\n\n**Published:** 2024-05-31\n\n\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: mamba state space model selective attention*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 3. An Empirical Study of Mamba-based Language Models\n\n*From Search Query: mamba state space model selective attention*\n\n*Bryan Catanzaro, Mohammad Shoeybi, Jan Kautz, Jared Casper, Vartika Singh, Garvit Kulshreshtha, Deepak Narayanan, Sudhakar Singh, Ali Hatamizadeh, Albert Gu, Tri Dao, Vijay Korthikanti, Brandon Norick, Duncan Riach, Wonmin Byeon, Roger Waleffe*\n\n**Abstract:** Selective state-space models (SSMs) like Mamba overcome some of the shortcomings of Transformers, such as quadratic computational complexity with sequence length and large inference-time memory requirements from the key-value cache. Moreover, recent studies have shown that SSMs can match or exceed the language modeling capabilities of Transformers, making them an attractive alternative. In a controlled setting (e.g., same data), however, studies so far have only presented small scale experiments comparing SSMs to Transformers. To understand the strengths and weaknesses of these architectures at larger scales, we present a direct comparison between 8B-parameter Mamba, Mamba-2, and Transformer models trained on the same datasets of up to 3.5T tokens. We also compare these models to a hybrid architecture consisting of 43% Mamba-2, 7% attention, and 50% MLP layers (Mamba-2-Hybrid). Using a diverse set of tasks, we answer the question of whether Mamba models can match Transformers at larger training budgets. Our results show that while pure SSMs match or exceed Transformers on many tasks, they lag behind Transformers on tasks which require strong copying or in-context learning abilities (e.g., 5-shot MMLU, Phonebook) or long-context reasoning. In contrast, we find that the 8B Mamba-2-Hybrid exceeds the 8B Transformer on all 12 standard tasks we evaluated (+2.65 points on average) and is predicted to be up to 8x faster when generating tokens at inference time. To validate long-context capabilities, we provide additional experiments evaluating variants of the Mamba-2-Hybrid and Transformer extended to support 16K, 32K, and 128K sequences. On an additional 23 long-context tasks, the hybrid model continues to closely match or exceed the Transformer on average. To enable further study, we release the checkpoints as well as the code used to train our models as part of NVIDIA's Megatron-LM project.\n\n**Published:** 2024-06-12\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model using the Mamba state space model architecture, here are the key technical details and performance characteristics that are relevant:\n\n## Selective State Space Models (SSMs) in Mamba\n\n### Core Mechanisms\n- Mamba integrates selective state space models (SSMs) that allow for the selective propagation or forgetting of information based on the current token. This is achieved by making the SSM parameters functions of the input, enabling content-based reasoning.\n- The model uses a simplified neural network architecture without attention or MLP blocks, relying on the SSMs to capture sequence dependencies. This involves state transition equations, where the state variables are updated based on the current input and previous states.\n\n### State Space Duality (SSD)\n- Mamba2 introduces the concept of State Space Duality (SSD), which simplifies the matrix \\(A\\) in the SSM formulation into a scalar. This simplification allows for both linear and quadratic forms of the SSM, enhancing efficiency and performance.\n\n### Efficient Processing and Scaling\n- Mamba achieves fast inference with up to 5 times higher throughput than Transformers and linear scaling in sequence length. This is due to a hardware-aware parallel algorithm designed for recurrent mode, which maintains efficiency even for long sequences.\n\n### Bidirectional Modeling\n- For certain applications, such as in SC-MAMBA2, a bidirectional modeling approach is implemented within the state-space modules. This involves processing the input sequence in both forward and backward directions and combining the outputs to capture bidirectional contextual relationships.\n\n## Performance Characteristics\n\n### Computational Efficiency\n- Mamba's architecture is designed to overcome the computational inefficiency of Transformer-based models on long sequences. It achieves linear complexity and faster inference times compared to Transformers.\n\n### Adaptability and Performance\n- The selective propagation and forgetting mechanism in Mamba allows the model to adapt well to different contexts and modalities, including language, audio, and genomics. Mamba models have been shown to outperform similarly sized Transformers and match the performance of larger Transformers in both pretraining and downstream evaluation tasks.\n\n### Scalability\n- Mamba's design enables efficient and scalable modeling of large sequences. For example, SC-MAMBA2 can handle full gene sequences encompassing over 60,000 genes, demonstrating its capability to model extensive sequence lengths efficiently.\n\n## Integration and Optimization Opportunities\n\n### Combining with Other Mechanisms\n- Integrating Mamba's SSD framework with adaptive gates from models like FastTTTLinear could enhance the model's ability to adapt to varying contexts. Additionally, leveraging test-time training for state updates could further improve the model's performance and adaptability.\n\n### Hardware Efficiency\n- Maintaining hardware efficiency is crucial. Mamba's design, which includes hardware-aware parallel algorithms, can be combined with other efficient architectures to ensure that the combined model remains computationally efficient.\n\nBy focusing on these aspects, researchers can build upon the strengths of Mamba's selective state space model architecture to create an improved autoregressive language model that is efficient, scalable, and highly performant.\n",
            "analysis": "Initial Analysis of Parent Architectures\n\n1. Summary of Analysis:\n\nThe two parent architectures - Mamba2 and FastTTTLinear - represent different approaches to efficient sequence modeling:\n\nMamba2:\n- Core innovation: Uses state space duality (SSD) framework\n- Key components: SSDMinimalDiscrete for efficient state processing\n- Advantages: \n  * 2-8X faster than original Mamba\n  * Linear complexity\n  * Competitive with Transformers\n- Limitations:\n  * Fixed state representation\n  * May not adapt well to varying contexts\n\nFastTTTLinear:\n- Core innovation: Combines Gated Linear Attention with test-time training\n- Key components: Linear attention with data-dependent gates\n- Advantages:\n  * Linear complexity\n  * Adaptive computation\n  * Test-time training capability\n- Limitations:\n  * Memory I/O challenges\n  * Complex integration requirements\n\n2. Key References:\n\nFrom \"Transformers are SSMs\":\n\"Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\"\n\nFrom FastTTTLinear proposal:\n\"The key idea is to make the hidden state a machine learning model itself, where the update rule is a step of self-supervised learning. This allows for continuous training of the hidden state even on test sequences.\"\n\n3. Key Insights:\n\na) Complementary Strengths:\n- Mamba2 excels in efficient state processing\n- FastTTTLinear provides adaptive computation\n- Both achieve linear complexity through different mechanisms\n\nb) Integration Opportunities:\n- Combine SSD framework with adaptive gates\n- Leverage test-time training for state updates\n- Maintain hardware efficiency of both approaches\n\n4. Future Search Plan:\n- Need to investigate:\n  1. Linear attention mechanisms and their variants\n  2. Test-time training in state space models\n  3. Hardware-efficient implementations of combined architectures\n\n5. References:\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv:2402.18391.\n\n[2] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. arXiv:2302.16891.\n\nNeed to continue search to gather more specific technical details about implementation approaches and potential optimizations."
        },
        {
            "ready": false,
            "query": "test time training state space models",
            "detail": "Find technical details about test-time training mechanisms in state space models and their implementation approaches, focusing on stability and efficiency considerations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about test-time training mechanisms in state space models and their implementation approaches, focusing on stability and efficiency considerations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.79)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.79)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.68)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 4/27 (Score: 0.68)*\n\n```\nThis allows to remove a mixing layer present in the postprocessing of the S4 and S4D output. ## D. Linear Recurrent Unit (LRU)\n\nThe LRU model attempts to simplify previous SSM proposals by unveiling their essential components. One of the main contributions of LRU is to explicitly encode long-range memory through eigenvalues. This allows to move away from the HiPPO theory and directly use a discrete-time model together with concepts of marginal stability from control theory. a) Parameterization: The LRU model directly parameterizes the discrete-time dynamics (4), i.e.,\n\n$$\n\\bar{A}=e^{-e^{\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)}+i \\operatorname{idiag}\\left(\\theta_{1}, \\ldots, \\theta_{p}\\right)}, \\quad \\bar{B}=e^{\\gamma} \\Gamma\n$$\n\nwith $i$ the complex unit, $\\lambda_{j}, \\theta_{j} \\in \\mathbb{R} \\forall j=1, \\ldots, p, \\Gamma \\in \\mathbb{C}^{p \\times q}$ a dense complex-valued matrix, and $\\gamma \\in \\mathbb{R}$. Notice that this parameterization directly represents the diagonal entries of $\\bar{A}$, and therefore the eigenvalues in polar coordinates, i.e. $a_{j}=$ $r_{j}+i \\theta_{j}$ where $r_{j}=e^{-e^{\\lambda_{j}}}$, is constrained to the interval $[0,1]$ by construction. This is also the first parameterization that does not have shared parameters between $\\bar{A}$ and $\\bar{B}$. b) Discretization: The LRU model is the first of the SSMs that is not seen as a discretization of a continuous-time model. Instead, a discrete parametrization of $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ is directly used. c) Structure and Initialization: The structure of the model is identical to S5, where a MIMO system -as opposed to $q$ SISO subsystems- is considered. Given the parametrization 10p, Lemma 2.2 is automatically enforced by constraining the eigenvalues of $\\bar{A}$ to lie in the unitdisk. Hence, the initialization is directly performed in polar coordinates by defining a range for $r$ and $\\theta$ in which $r$ and $\\theta$ are uniformly sampled, resulting in the eigenvalues shown in Figure 2\nd) Implementation: Similar to LRU, the model is implemented using a parallel scan algorithm [16] for both training and inference. e) Scaffolding: The scaffolding used in LRU is identical to the one used in S 5 . ## E. Scan Selective Structured State Space Sequence Model (S6)\n\nThe S6 parametrization [10] introduces for the first time a linear time-varying representation of the dynamics (3). The time-varying nature of the system stems from the matrices $\\bar{A}_{k}, \\bar{B}_{k}$ and $\\bar{C}_{k}$ being functions of the input $u(k)$ at every timestep $k$, which the authors refer to as selectivity. Although more expressive, the time-varying representation presents computational challenges. The main contribution of this paper is to address those so the more expressive time-varying nature of the system can be exploited in practice. a) Parametrization: Similar to S4D, the S6 parametrization relies on a time-invariant diagonal $A$ matrix (8) as per Assumption 2.1. The novelty of the S6 parametrization is that $B$ and $C$ are parameterized to be time-varying given their input-dependent nature:\n\n$$\nB_{k}=W_{B} u(k) \\quad C_{k}=W_{C} u(k)\n$$\n\nwhere $W_{B}$ and $W_{C}$ are linear projection matrices of appropriate dimensions. ![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-6.jpg?height=467&width=1781&top_left_y=162&top_left_x=172)\n\nFig. 2: Complex plane representation of the unit disk and the eigenvalues of discrete-time dynamics matrix $\\bar{A}$ (4) resulting from the initialization method in each of the models S4, S4D, S5, LRU, S6, and RG-LRU. Since the initialization of S6 and RG-LRU are input dependent, we plot the initialization for two sample inputs (blue and orange). b) Discretization: Similar to S4D, the S6 model also uses exact discretization to compute the discrete-time dynamics (4). However, in this case the time-step $\\Delta_{k}$ is itself time-varying since it is a function of the input\n$\\Delta_{k}=\\sigma\\left(W_{\\Delta} u(k)\\right), \\quad \\bar{A}_{k}=e^{\\Delta_{k} A}, \\quad \\bar{B}_{k}=\\left(\\Delta_{k} A\\right)^{-1}\\left(\\bar{A}_{k}-I\\right) \\Delta_{k} B_{k}$,\n$\\bar{C}_{k}=C_{k}$ and $\\bar{D}_{k}=D_{k}$, with $W_{\\Delta} \\in \\mathbb{R}^{1 \\times q}$ and $\\sigma(\\cdot)$ the softplus function. c) Structure and Initialization: Similar to S5, the model is structured in a MIMO manner. In order to initialize the dynamic matrix $A$, its diagonal parametrization is exploited: $\\lambda_{i}=-i \\forall i=1, \\ldots, p$, ensuring that the eigenvalues lie in the negative halfplane. Due to the time-varying nature of the discretization step $\\Delta_{k}$, the eigenvalues of the discrete-time matrices $\\bar{A}_{k}$ have an initialization that is input-dependent as depicted in Figure 2 However, in order to enforce Lemma 2.2 the resulting eigenvalues are guaranteed to lie in the unit disk since $\\Delta_{k}$ and $A$ in (12) are positive and negative, respectively. d) Implementation: One of the main contributions of the work in [10] is to provide an efficient implementation of the time-varying dynamics (4) with matrices (11) and 12 both at inference and training time. In general, the timevarying nature of the S 6 model renders the convolutional representation too computationally expensive for practical use. To overcome these limitations, the S6 paper presents a highly customized variation of the parallel scan algorithm [16] for both training and inference. e) Scaffolding: Another innovation of the work in [10] is the introduction of a new scaffolding: the Mamba scaffolding. Here, the pre-processing relies on both linear and nonlinear maps. The map of the upper signal (linear map) is a linear projection followed by a causal convolution, while the map of the lower signal (nonlinear map) is a linear projection followed by a SiLU nonlinearity. The post-processing is once again a gating function similar to previous scaffolding proposals. ## F. Real-Gated Linear Recurrent Unit (RG-LRU)\n\nThe RG-LRU model is a derivative of the well-known long short-term memory (LSTM) model [27] and therefore offers a different perspective on SSM models. The RG-LRU model fuses ideas from LSTMs, LRU, and S6. a) Parametrization: Following S6, RG-LRU also relies on a time-varying parametrization of the linear dynamics. However, while all previous SSM proposals rely on output feedback dynamics, the RG-LRU model introduces for the first time a state feedback model where $C$ and $D$ are not present. The $A$ and $B$ matrices are then parameterized as\n\n$$\n\\bar{A}_{k}=e^{-c \\phi\\left(W_{A}\\right) \\sigma\\left(W_{\\Delta} u(k)\\right)}, \\quad \\bar{B}_{k}=\\sqrt{1-A_{k}^{2}} \\sigma\\left(W_{B} u(k)\\right)\n$$\n\nwhere $W_{\\Delta}, W_{A}, W_{B}$ are linear projection matrices of appropriate dimensions, $c \\in \\mathbb{R}$ is a scalar constant ${ }_{4}^{4} \\phi(\\cdot)$ is the softplus function, and $\\sigma(\\cdot)$ is the sigmoid function. The operation $\\sqrt{1-A_{k}^{2}}$ is computed element-wise for each entry of $A_{k}$. b) Discretization: Similar to the LRU model, the RGLRU model does not rely on a continuous-time representation and instead directly parametrizes the discrete matrices $\\bar{A}_{k}, \\bar{B}_{k}$. c) Structure and Initialization: Similar to LRU, the RG-LRU model is structured as a MIMO system. Taking inspiration from LSTMs, this models assumes the state dimension to be equal to the input dimension, i.e., $p=q$. The linear projection matrices $W_{\\Delta}, W_{A}, W_{B}$ are initialized with standard initialization methods, e.g. Glorot [14], resulting in the eigenvalues shown in Figure 2 Given the parameterization of $\\bar{A}_{k}$ in 13), its eigenvalues are restricted to the unit disk by construction. d) Implementation: Due to the time-varying nature of the RG-LRU model, it faces the same challenges as the S6 model. Therefore, it also uses a customized variation of the parallel scan algorithm [16] to compute the outputs at both training and inference time. e) Scaffolding: The RG-LRU model uses the same scaffolding as the S6 model, Mamba. However, this work also introduces two additional task-specific scaffoldings around the basic Mamba scaffolding that are tailored to language modelling: Hawk and Griffin [21, Section 2]. [^3]| Model | LRA Task [\\%] |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | ListOps | Text | Retrieval | Image | Pathfinder | Path-X | avg. |\n| Random | 10.00 | 50.00 | 50.00 | 10.00 | 50.00 | 50.00 | 36.67 |\n| Transformer [2] (paper results) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | FAIL | 53.66 |\n| S4 [12] (paper results) | 59.60 | 86.82 | 90.90 | 88.65 | 94.20 | 96.35 | 86.09 |\n| S4D [18] (paper results) | 60.52 | 87.34 | 91.09 | 88.19 | 93.96 | 92.80 | 85.65 |\n| S5 [19] (paper results) | $\\mathbf{6 2 . 1 5}$ | 89.31 | $\\mathbf{9 1 . 4 0}$ | 88.00 | $\\mathbf{9 5 . 3 3}$ | $\\mathbf{9 8 . 5 8}$ | $\\mathbf{8 7 . 4 6}$ |\n| LRU [20] (paper results) | 60.20 | $\\mathbf{8 9 . 4 0}$ | 89.90 | $\\mathbf{8 9 . 0 0}$ | 95.10 | 94.20 | 86.30 |\n| S6 [10] | 38.02 | 82.98 | 72.14 | 69.82 | 69.26 | 67.32 | 66.59 |\n| RG-LRU [21] | 32.34 | 71.75 | 66.58 | 61.15 | 73.38 | 69.53 | 62.45 |\n\nTABLE II: Model performance in terms of test accuracy on the LRA benchmark. The first entry (Random) represents the performance of random guessing on the task, i.e., indicating the baseline above which a model is considered to have learned a meaningful representation. Models failing to exceed this baseline on a task are marked as FAIL. The best model on each task is highlighted in bold. ## IV. Performance in Practice\n\nIn this section, we evaluate the SSM proposals reviewed in Section III on the long-range arena (LRA) benchmark [2]. This benchmark evaluates the models in terms of their reasoning ability and their handling of diverse data types, which is one of the strengths of SSMs. We first introduce the benchmark, before presenting the empirical evaluation. ## A. Long-Range Arena (LRA) Benchmark\n\nThe goal of the LRA benchmark is to evaluate the reasoning capabilities of sequence models in diverse ways. The benchmark consists of 5 different tasks, which we summarize in the following. For more details on the benchmark and the individual tasks, we refer to [2]. a) List Operations (ListOps): This task evaluates a model's ability to capture hierarchical dependencies over long contexts. The goal is to predict the result of a mathematical operation consisting of nested mean, median, max, and $\\min$ operations ${ }^{5}$ The task is a ten-way classification task with maximal input lengths of 2 k . b) Text Classification (Text): This task evaluates a model's ability to capture the tone of long tokenized texts. The dataset consists of IMDb movie reviews, which need to be classified as negative or positive in tone. The task is a binary classification task with maximal input lengths of 4 k . c) Document Retrieval (Retrieval): This task evaluates a model's ability to compress long sequences into representations that are suitable for similarity matching. The dataset consists of tokenized papers published by the American Academy of Neurology (AAN), which need to be classified in having a citation link or not. The task is a binary classification task with maximal input lengths of 8 k . d) Image Classification (Image): This task evaluates a model's ability to learn 2D spatial relations from a 1D vector. The dataset consists of vectorized images, which depict one of ten possible classes, e.g. a horse or a car. The task is a ten-way classification task with maximal input lengths of 1 k . [^4]e) Long-Range Spacial Dependency: This task evaluates a model's ability to learn spacial dependencies in a vectorized image. The dataset consists of images, which depict two circles and multiple dashed paths. The goal is to evaluate whether the two circles are connected by any of the present paths or not. The task is therefore a binary classification task and is divided into two subtasks, which only differ in the size of the image. The first subtask has inputs of length 2 k and we will refer to it as Pathfinder; the second subtask has a maximal input length of 16 k and we will refer to it as Path-X. ## B. Empirical Evaluation of SSM Proposals\n\nThe empirical performance of the reviewed SSM proposals, the Transformer [1], and random guessing are reported in Table II We include the performance of the Transformer as a baseline, since they are the dominant architecture in large language models and sequence modelling. For S4, S4D, S5, and LRU we report the performance of the best variant from the original papers in order to present the most competitive results. Other variants of these models might perform better on tasks not included in the LRA benchmark; for more details on these variants we refer to the original papers. Since performance on the LRA benchmark of S6 and RGLRU have not been reported in the literature, we provide the results of our own implementation of these architectures, which we make available here ${ }^{6}$ The hyperparameters of the models and training details of our implementation are stated in the public code repository. On the LRA benchmark, the LTI-based models S4, S4D, S5, LRU outperform the LTV-based models S6, RG-LRU and the Transformer. From a control theoretic perspective this is surprising, since a general LTV definition encompasses LTI systems as a special case, i.e., a LTV system should perform at least as well as a LTI system. However, this is not the case for the particular time-varying parametrization of S6 or RG-LRU, since e.g. $\\bar{A}=\\bar{A}_{k} \\forall k$ cannot be achieved. We attempted to improve the performance of the LTV-based\n\n[^5]models by changing the initialization of S6 and RG-LRU and forcing the input-dependent eigenvalues of $\\bar{A}_{k} \\forall k$ closer to marginal stability according to Lemma 2.2 However, this resulted in both models to perform considerably worse or fail to learn anything meaningful at all. While marginally stable eigenvalues appear to be important for the LTI-based models, the same is not true for LTV-based models. To date, this behavior is not well understood. Lastly, even though the LTVbased models are closely related to the Transformer [28], they generally perform better on the LRA benchmark. ## V. CONCLUSION AND Future Opportunities\n\nIn this paper, we have provided an overview of stateof-the-art state space models (SSM) and explored their features from a control theoretic perspective. In doing this, we highlighted the many connections to standard control theoretic concepts such as the connection between memory and marginal-stability. Additionally, we compared the reviewed SSMs on the long-range arena (LRA) benchmark, finding that the more recent LTV-based SSMs perform worse than their LTI-based counterparts. From a control theoretic perspective, this raises many interesting research questions concerning a LTV parametrization that attains the same performance as the LTI models, as well as a deeper understanding on the role of the eigenvalues in the LTVbased models. SSMs, particularly the LTV versions, rely on dynamics where the dynamic matrices depend on the input (excitation) to the system. However, in the SSM literature the theoretical properties arising from these dynamics remains poorly understood. The evident connections between SSMs and linearsystem theory give rise to ample opportunities to provide explainability to large foundational models. Moreover, as seen with the LRU model, control theoretic insights have the potential to inform better designs for SSMs.\n```\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.45)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 4/74 (Score: 0.45)*\n\n```\n2023), which may be easier to reason about. Computation. After the parameters have been transformed from $(\\Delta, A, B, C) \\mapsto(\\bar{A}, \\bar{B}, C)$, the model can be computed in two ways, either as a linear recurrence (2) or a global convolution (3). Commonly, the model uses the convolutional mode (3) for efficient parallelizable training (where the whole input sequence is seen ahead of time), and switched into recurrent mode (2) for efficient autoregressive inference (where the inputs are seen one timestep at a time). Linear Time Invariance (LTI). An important property of equations (1) to (3) is that the model's dynamics are constant through time. In other words $(\\Delta, A, B, C)$, and consequently $(\\bar{A}, \\bar{B})$ as well, are fixed for all time-steps. This property is\ncalled linear time invariance ( $L T I$ ), which is deeply connected to recurrence and convolutions. Informally, we think of LTI SSMs as being equivalent to any linear recurrence (2a) or convolution (3b), and use LTI as an umbrella term for these classes of models. Thus far, all structured SSMs have been LTI (e.g. computed as convolutions) because of fundamental efficiency constraints, discussed in Section 3.3. However, a core insight of this work is that LTI models have fundamental limitations in modeling certain types of data, and our technical contributions involve removing the LTI constraint while overcoming the efficiency bottlenecks. Structure and Dimensions. Finally, we note that structured SSMs are so named because computing them efficiently also requires imposing structure on the $A$ matrix. The most popular form of structure is diagonal (Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Smith, Warrington, and Linderman 2023), which we also use. In this case, the $\\boldsymbol{A} \\in \\mathbb{R}^{N \\times N}, \\boldsymbol{B} \\in \\mathbb{R}^{N \\times 1}, \\boldsymbol{C} \\in \\mathbb{R}^{1 \\times N}$ matrices can all be represented by $N$ numbers. To operate over an input sequence $x$ of batch size $B$ and length $L$ with $D$ channels, the SSM is applied independently to each channel. Note that in this case, the total hidden state has dimension $D N$ per input, and computing it over the sequence length requires $O(B L D N)$ time and memory; this is the root of the fundamental efficiency bottleneck addressed in Section 3.3. General State Space Models. We note that the term state space model has a very broad meaning which simply represents the notion of any recurrent process with a latent state. It has been used to refer to many disparate concepts in different disciplines, including Markov decision processes (MDP) (reinforcement learning (Hafner et al. 2020)), dynamic causal modeling (DCM) (computational neuroscience (Friston, Harrison, and Penny 2003)), Kalman filters (controls (Kalman 1960)), hidden Markov models (HMM) and linear dynamical systems (LDS) (machine learning), and recurrent (and sometimes convolutional) models at large (deep learning). Throughout this entire paper we use the term \"SSM\" to refer exclusively to the class of structured SSMs or S4 models (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al.\n```\n\n#### 4. Structured state-space models are deep Wiener models (Avg. Score: 0.40)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 9/22 (Score: 0.40)*\n\n```\nThe training and validation data consist of ten experiments in which a multisine input with random frequency components is applied to the apparatus. Each experiment features 8192 input-output datapoints, collected with a sampling rate of 610.35 Hz . The validation dataset has been constructed by extracting 76 subsequences (of length $T=512$ steps) from a random experiment. From the remaining nine experiments 684 training subsequences have been extracted. The Silverbox dataset also contains an independent test dataset that can be used to assess the accuracy of the identified models. This test dataset consists of 40500 samples collected by exciting the system with a filtered Gaussian noise having linearly increasing mean value. As noted by Tiels (2015), the test dataset is characterized by two regions. The first 25000 time-steps allow quantification of the model's accuracy in interpolation, i.e., within operating regions well explored by the training and validation data. The successive time-steps allow, instead, enable the assessment of the model's extrapolation performances in operating regions not explored in the training and validation datasets, particularly in terms of the input signal amplitude. For this reason, the modeling performances scored by the identified SSMs - measured by RMSE [mV] and FIT index [\\%]-are reported both on the overall test dataset and limitedly to the interpolatory regime. ### 6.1 Identification results\n\nThe training procedure was implemented in PyTorch 2.1 and is described in more detail in the accompanying code ${ }^{4}$\n\n[^3]Table 1. Performance of the identified SSMs\n\n| Model | First 25000 steps |  |  | Full |  |\n| :--- | :---: | :---: | :--- | :---: | :---: |\n|  | RMSE | FIT |  | RMSE | FIT |\n| S4 $\\left(L=4, n_{\\lambda}=10\\right)^{5}$ | 0.81 | 97.60 |  | 4.73 | 95.49 |\n| S5 $\\left(L=4, n_{\\lambda}=10\\right)^{5}$ | 0.73 | 97.78 |  | 3.56 | 96.48 |\n| S5R $\\left(L=4, n_{\\lambda}=10\\right)$ | 0.37 | 97.62 |  | 4.58 | 95.92 |\n| LRU $\\left(L=4, n_{\\lambda}=10\\right)$ | 0.73 | 97.71 |  | 4.18 | 96.37 |\n| TCNN (Andersson et al., 2019) | 0.75 | - |  | 4.9 | - |\n| LSTM (Andersson et al., 2019) | 0.31 | - |  | 4.0 | - |\n| BLA (Tiels, 2015) | - | - |  | 13.7 | - |\n| Wiener (Tiels, 2015) | 1.9 | - |  | 9.2 | - |\n| Grey-box NARX(Ljung et al., 2004) | - | - |  | 0.3 | - |\n\nor in Appendix C. Our hope is that the code can help speed up continued research on these architectures. The following SSM configurations have been considered for identification. Additional details about their structures, initialization, and training hyperparameters are reported in Appendix D. S4 (Gu et al., 2021) \u2014 SSM whose layers are parametrized in the continuous-time domain (7)-(8) by a DLPRstructured state matrix (11) initialized via HiPPO. S5 (Smith et al., 2022) - SSM whose layers are parametrized by continuous-time diagonal systems (7)-(9) initialized via HiPPO. S5R - SSM whose layers are parametrized by continuoustime diagonal systems (7)-(9). Similar to S5, but initialized by random sampling of the eigenvalues, (14). LRU (Orvieto et al., 2023) - SSM whose layers are parametrized by discrete-time diagonal systems (2)-(4). In Table 1 the performance metrics scored by the best identified SSM are reported, and they are compared to some of those reported in the literature ${ }^{6}$, while in Appendix D the trajectory of the simulation error is reported. Note that, although these SSMs are more accurate than traditional Wiener models, they are still in line with those achieved by other deep learning models like TCNNs and LSTMs, while featuring significantly less learnable parameters ${ }^{7}$ and high parallelizability of the training procedure. At last, let us point out that training competitive SSMs requires, in general, a careful selection of the architecture hyperparameters. These not only include the number of layers and the state and output size of each layer, but also a suitable initialization strategy for the selected parametrization. The computational efficiency of these models and their reduced parameter footprint come at the cost of a slightly increased architecture design effort. Further testing of SSMs for nonlinear system identification is thus advisable to establish empirical design criteria. ## 7. CONCLUSIONS AND RESEARCH DIRECTIONS\n\nStructured State-space Models represent an interesting approach to identifying deep Wiener models. In this paper we\n5 At least one eigenvalue falls beyond the Nyquist frequency.\n```\n\n#### 5. Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers (Avg. Score: 0.31)\n\n*Albert Gu, Isys Johnson, Karan Goel, Khaled Kamal Saab, Tri Dao, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2021)\t**Cited by** 244  (*Influential: 14*)\n\n**TL;DR:** A simple sequence model inspired by control systems that generalizes RNN heuristics, temporal convolutions, and neural differential equations while addressing their shortcomings, and introduces a trainable subset of structured matrices that endow LSSLs with long-range memory.\n\n**Abstract:** Recurrent neural networks (RNNs), temporal convolutions, and neural differential equations (NDEs) are popular families of deep learning models for time-series data, each with unique strengths and tradeoffs in modeling power and computational efficiency. We introduce a simple sequence model inspired by control systems that generalizes these approaches while addressing their shortcomings. The Linear State-Space Layer (LSSL) maps a sequence $u \\mapsto y$ by simply simulating a linear continuous-time state-space representation $\\dot{x} = Ax + Bu, y = Cx + Du$. Theoretically, we show that LSSL models are closely related to the three aforementioned families of models and inherit their strengths. For example, they generalize convolutions to continuous-time, explain common RNN heuristics, and share features of NDEs such as time-scale adaptation. We then incorporate and generalize recent theory on continuous-time memorization to introduce a trainable subset of structured matrices $A$ that endow LSSLs with long-range memory. Empirically, stacking LSSL layers into a simple deep neural network obtains state-of-the-art results across time series benchmarks for long dependencies in sequential image classification, real-world healthcare regression tasks, and speech. On a difficult speech classification task with length-16000 sequences, LSSL outperforms prior approaches by 24 accuracy points, and even outperforms baselines that use hand-crafted features on 100x shorter sequences.\n\n##### *Relevant Chunk: No. 31/59 (Score: 0.31)*\n\n```\nData Mining and Knowledge Discovery, pages 1-29, 2021. doi: https://doi.org/10.1007/ s10618-021-00745-9. [56] Trieu H Trinh, Andrew M Dai, Minh-Thang Luong, and Quoc V Le. Learning longer-term dependencies in RNNs with auxiliary losses. In The International Conference on Machine Learning (ICML), 2018. [57] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), 2017. [58] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. In Advances in Neural Information Processing Systems, pages 15544-15553, 2019 . [59] Robert L Williams, Douglas A Lawrence, et al. Linear state-space control systems. Wiley Online Library, 2007 . [60] Max A Woodbury. Inverting modified matrices. Memorandum report, 42:106, 1950. [61] Guofeng Zhang, Tongwen Chen, and Xiang Chen. Performance recovery in digital implementation of analogue systems. SIAM journal on control and optimization, 45(6):2207-2223, 2007. [62] Huaguang Zhang, Zhanshan Wang, and Derong Liu. A comprehensive review of stability analysis of continuous-time recurrent neural networks. IEEE Transactions on Neural Networks and Learning Systems, 25(7):1229-1262, 2014. ## A Related Work\n\nWe provide an extended related work comparing the LSSL to previous recurrent, convolutional, and continuoustime models. HiPPO The LSSL is most closely related to the HiPPO framework for continuous-time memory 24] and its predecessor, the Legendre Memory Unit (LMU) 58. The HiPPO-RNN and the LMU define dynamics of the form of equation (1), and incorporate it into an RNN architecture. A successor to the LMU, the LMU-FFT [12] keeps the original linear dynamics, allowing the LMU to be computed with a cached convolution kernel. These methods all suffer from two main limitations. First, the state matrix $A$ and discretization timescale $\\Delta t$ cannot be trained due to both limitations in theoretical understanding of which $A$ matrices are effective, as well as computational limitations. Second, (1) is a 1-D to $N$-D map, requiring states to be projected back down to 1-D. This creates an overall 1-D bottleneck in the state, limiting the expressivity of the model. Compared to these, the LSSL does not use a conventional RNN architecture, instead keeping the linear recurrence (4) and downprojecting it with the second part of the state space representation (5).\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: test time training state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Semantic Scholar\n\n#### 1. Prompt Consistency for Zero-Shot Task Generalization\n\n*From Search Query: test time training state space models*\n\n*Chunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig*\n\n**TL;DR:** This work takes advantage of the fact that multiple prompts can be used to specify a single task, and proposes to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts, to improve zero-shot performance.\n\n**Abstract:** One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2022\n\n**Citations:** 63  (*Influential: 5*)\n\n#### 2. Robustifying Generalizable Implicit Shape Networks with a Tunable Non-Parametric Model\n\n*From Search Query: test time training state space models*\n\n*Amine Ouasfi, A. Boukhayma*\n\n**TL;DR:** A efficient mechanism to remedy generalization issues in forward generalizable models for implicit shape reconstruction from unoriented point cloud by combining the inter-shape data prior of the network with an intra-shape regularization prior of a Nystr\\\"om Kernel Ridge Regression.\n\n**Abstract:** Feedforward generalizable models for implicit shape reconstruction from unoriented point cloud present multiple advantages, including high performance and inference speed. However, they still suffer from generalization issues, ranging from underfitting the input point cloud, to misrepresenting samples outside of the training data distribution, or with toplogies unseen at training. We propose here an efficient mechanism to remedy some of these limitations at test time. We combine the inter-shape data prior of the network with an intra-shape regularization prior of a Nystr\\\"om Kernel Ridge Regression, that we further adapt by fitting its hyperprameters to the current shape. The resulting shape function defined in a shape specific Reproducing Kernel Hilbert Space benefits from desirable stability and efficiency properties and grants a shape adaptive expressiveness-robustness trade-off. We demonstrate the improvement obtained through our method with respect to baselines and the state-of-the-art using synthetic and real data.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. Hexatagging: Projective Dependency Parsing as Tagging\n\n*From Search Query: test time training state space models*\n\n*Afra Amini, Tianyu Liu, Ryan Cotterell*\n\n**TL;DR:** A novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags is introduced, and its linear time complexity and parallelism significantly improve computational efficiency.\n\n**Abstract:** We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser\u2019s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Neural Continuous-Discrete State Space Models for Irregularly-Sampled Time Series\n\n*From Search Query: test time training state space models*\n\n*Abdul Fatir Ansari, Alvin Heng, Andre Lim, Harold Soh*\n\n**TL;DR:** This work proposes the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations and proposes three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference.\n\n**Abstract:** Learning accurate predictive models of real-world dynamic phenomena (e.g., climate, biological) remains a challenging task. One key issue is that the data generated by both natural and artificial processes often comprise time series that are irregularly sampled and/or contain missing observations. In this work, we propose the Neural Continuous-Discrete State Space Model (NCDSSM) for continuous-time modeling of time series through discrete-time observations. NCDSSM employs auxiliary variables to disentangle recognition from dynamics, thus requiring amortized inference only for the auxiliary variables. Leveraging techniques from continuous-discrete filtering theory, we demonstrate how to perform accurate Bayesian inference for the dynamic states. We propose three flexible parameterizations of the latent dynamics and an efficient training objective that marginalizes the dynamic states during inference. Empirical results on multiple benchmark datasets across various domains show improved imputation and forecasting performance of NCDSSM over existing models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. SSD: Single Shot MultiBox Detector\n\n*From Search Query: test time training state space models*\n\n*Cheng-Yang Fu, Christian Szegedy, Wei Liu, Scott Reed, Dumitru Erhan, Dragomir Anguelov, Alexander C. Berg*\n\n**Abstract:** We present a method for detecting objects in images using a single deep\nneural network. Our approach, named SSD, discretizes the output space of\nbounding boxes into a set of default boxes over different aspect ratios and\nscales per feature map location. At prediction time, the network generates\nscores for the presence of each object category in each default box and\nproduces adjustments to the box to better match the object shape. Additionally,\nthe network combines predictions from multiple feature maps with different\nresolutions to naturally handle objects of various sizes. Our SSD model is\nsimple relative to methods that require object proposals because it completely\neliminates proposal generation and subsequent pixel or feature resampling stage\nand encapsulates all computation in a single network. This makes SSD easy to\ntrain and straightforward to integrate into systems that require a detection\ncomponent. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets\nconfirm that SSD has comparable accuracy to methods that utilize an additional\nobject proposal step and is much faster, while providing a unified framework\nfor both training and inference. Compared to other single stage methods, SSD\nhas much better accuracy, even with a smaller input image size. For $300\\times\n300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan\nX and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a\ncomparable state of the art Faster R-CNN model. Code is available at\nhttps://github.com/weiliu89/caffe/tree/ssd .\n\n**Published:** 2015-12-08\n\n\n\n#### 2. Learning Mesh-Based Simulation with Graph Networks\n\n*From Search Query: test time training state space models*\n\n*Peter W. Battaglia, Alvaro Sanchez-Gonzalez, Meire Fortunato, Tobias Pfaff*\n\n**Abstract:** Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.\n\n**Conference:** learning-mesh-based-simulation-with-graph\n\n**Published:** 2020-10-07\n\n\n\n#### 3. BinaryConnect: Training Deep Neural Networks with binary weights during propagations\n\n*From Search Query: test time training state space models*\n\n*Jean-Pierre David, Yoshua Bengio, Matthieu Courbariaux*\n\n**Abstract:** Deep Neural Networks (DNN) have achieved state-of-the-art results in a wide\nrange of tasks, with the best results obtained with large training sets and\nlarge models. In the past, GPUs enabled these breakthroughs because of their\ngreater computational speed. In the future, faster computation at both training\nand test time is likely to be crucial for further progress and for consumer\napplications on low-power devices. As a result, there is much interest in\nresearch and development of dedicated hardware for Deep Learning (DL). Binary\nweights, i.e., weights which are constrained to only two possible values (e.g.\n-1 or 1), would bring great benefits to specialized DL hardware by replacing\nmany multiply-accumulate operations by simple accumulations, as multipliers are\nthe most space and power-hungry components of the digital implementation of\nneural networks. We introduce BinaryConnect, a method which consists in\ntraining a DNN with binary weights during the forward and backward\npropagations, while retaining precision of the stored weights in which\ngradients are accumulated. Like other dropout schemes, we show that\nBinaryConnect acts as regularizer and we obtain near state-of-the-art results\nwith BinaryConnect on the permutation-invariant MNIST, CIFAR-10 and SVHN.\n\n**Conference:** binaryconnect-training-deep-neural-networks-1\n\n**Published:** 2015-11-02\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on test-time training mechanisms in state space models and their implementation approaches, here are some key technical details and considerations:\n\n## Test-Time Training in State Space Models\n\n### Adaptive State Updates\nTest-time training in state space models involves dynamically updating the model's parameters based on the input at test time. This can be achieved through mechanisms like those described in the Mamba models, where the state space parameters are functions of the input, allowing for content-based reasoning and selective propagation or forgetting of information along the sequence length dimension.\n\n### Implementation Approaches\n- **Selective State Space Models (SSMs):** Mamba models use SSMs that enable input-dependent parameters, which is crucial for test-time training. This approach allows the model to adapt to the current input context, enhancing its ability to handle new or unseen data.\n- **State Space Duality (SSD) Framework:** The SSD framework in Mamba-2 models refines the selective SSM, making it 2-8 times faster while maintaining performance. This framework can be integrated with test-time training mechanisms to ensure efficient and adaptive processing.\n\n### Efficiency and Stability Considerations\n- **Linear Complexity:** To maintain efficiency, it is crucial to preserve linear complexity in sequence length. Mamba models achieve this through their selective SSMs and SSD framework, which ensures that the processing time does not increase significantly with longer input sequences.\n- **Hardware-Aware Implementation:** Ensuring hardware efficiency is vital. The design should be optimized for specific hardware platforms, similar to how Liquid Foundation Models (LFMs) are optimized for different hardware requirements, to minimize memory usage and inference time.\n- **Stability in Adaptive Updates:** To maintain stability, the state space representation must support both efficient processing and adaptability. This can be achieved by carefully designing the adaptive gates and ensuring that the model's updates do not lead to instability or divergence. Techniques such as regularization or constraints on the updates can help in maintaining stability.\n\n## Specific Implementation Details\n\n### Dynamic State Updates\n- The model can be fine-tuned at test time using a single gradient step on each selected sequence, similar to the approach described in test-time fine-tuning for language models. This method adapts the model parameters to the specific prompt or input, improving performance on unseen data.\n\n### Hardware Optimization\n- Optimizing the architecture for specific hardware platforms is essential. This involves designing the model such that it can be automatically optimized for different platforms (e.g., Apple, Qualcomm, Cerebras, and AMD), matching given parameter requirements and inference cache size.\n\n### Methods for Maintaining Stability\n- **Regularization Techniques:** Implementing regularization techniques during test-time training can help in maintaining stability. For example, using L1 or L2 regularization on the updates can prevent large changes in the model parameters.\n- **Constraint-Based Updates:** Ensuring that the updates are constrained within certain bounds can also help in maintaining stability. This can be achieved by using techniques such as clipping the gradients or updates.\n\nBy integrating these approaches, the researcher can develop an autoregressive language model that leverages the strengths of state space models, particularly their ability to perform content-based reasoning and adapt at test time, while ensuring efficiency and stability.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis:\nThe search results reveal key insights about Mamba's selective state space models (SSMs) and their potential for integration with FastTTTLinear's adaptive mechanisms. Key findings include:\n\na) Core Mechanisms:\n- Mamba uses selective SSMs that allow content-based reasoning through input-dependent parameters\n- State Space Duality (SSD) framework in Mamba2 enables 2-8x faster processing while maintaining performance\n- Linear complexity in sequence length with efficient hardware utilization\n\nb) Complementary Features:\n- Mamba: Efficient state processing and hardware-aware implementation\n- FastTTTLinear: Test-time training and adaptive computation\n- Both achieve linear complexity through different approaches\n\n2. Useful References with Excerpts:\n\nFrom \"Mamba: Linear-Time Sequence Modeling\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nFrom \"Transformers are SSMs\":\n\"Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.\"\n\nFrom \"The Hidden Attention of Mamba Models\":\n\"Selective SSMs are viewed as dual models, in which one trains in parallel on the entire sequence via an IO-aware parallel scan, and deploys in an autoregressive manner.\"\n\n3. Key Insights for Design:\n\na) Integration Opportunities:\n- Combine SSD framework with adaptive gates for enhanced selectivity\n- Leverage test-time training for dynamic state updates\n- Maintain hardware efficiency through careful implementation\n\nb) Technical Considerations:\n- Need to preserve linear complexity when combining mechanisms\n- State space representation must support both efficient processing and adaptability\n- Hardware-aware implementation crucial for performance\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific implementation details of test-time training in state space models\n2. Hardware optimization techniques for combined architectures\n3. Methods for maintaining stability in adaptive state updates\n\n5. References:\n\n[1] Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality. arXiv:2402.18391.\n\n[2] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752.\n\n[3] Ali, A., Zimerman, I., & Wolf, L. (2024). The Hidden Attention of Mamba Models. arXiv:2401.08925.\n\n[4] Smith, J.T.H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems.\n\nNeed to continue search to gather more specific technical details about implementation approaches and optimization techniques."
        },
        {
            "ready": false,
            "query": "selective state space model stability optimization",
            "detail": "Find technical details about methods for ensuring stability and optimization in selective state space models, particularly focusing on initialization and regularization techniques.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about methods for ensuring stability and optimization in selective state space models, particularly focusing on initialization and regularization techniques.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. From generalization analysis to optimization designs for state space models (Avg. Score: 0.74)\n\n*Fusheng Liu, Qianxiao Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n##### *Relevant Chunk: No. 4/32 (Score: 0.74)*\n\n```\n(2018); Chen et al. (2019); Tu et al. (2019) proved norm-based generalization bounds, improving the VC dimension-based bounds by the Rademacher complexity technique (Bartlett and Mendelson, 2002) under the uniform-convergence framework. In the overparameterization settings, it was\nshown in Allen-Zhu and Li (2019) that RNNs can learn some concept class in polynomial time given that the model size is large enough. These generalization bounds, however, do not take into account the temporal dependencies and their effects on generalization. In this work, we provide a new generalization bound by combining the memory structure of the model and the temporal structure of the data. Temporal structure analysis on RNNs. Sequence data has long-range temporal dependencies across the time domain, which notably set it apart from non-sequence data. Recent studies have studied the effects of such temporal dependencies on the approximation and optimization of RNNs. For example, in the two works of Li et al. (2021; 2022), a \"curse of memory\" phenomenon was discovered when using linear RNNs to model the temporal input-output relationships. Particularly, when the target relationship between the input and output has a long-term memory, then both approximation and optimization become extremely challenging. In Wang et al. (2023), the \"curse of memory\" phenomenon on approximation and optimization was extended to non-linear RNNs based on the temporal relationships. In this paper, we conduct a finegrained analysis on the effects of the temporal structure analysis on the generalization of RNNs. Optimization of SSMs. RNN optimization is known for two issues: training stability and computational cost (Bengio et al., 1994; Pascanu et al., 2013). To address these issues and capture the long dependencies efficiently in sequence modeling, the S4 model was proposed by new paraemterization, initialization and discretization (Gu et al., 2022a). Recent variants for the S4 model simplified the hidden state matrix by a diagonal matrix to enhance computational efficiency (Gu et al., 2022b; Gupta et al., 2022; Smith et al., 2023; Orvieto et al., 2023). Regularization methods are also applied for SSMs to prevent overfitting, such as dropout, weight decay and the data continuity regularizer ( Qu et al., 2023). However, the principled way to regularize and initialize the parameters still remains to be explored. In this study, we design a new regularization and initialization scheme to improve both optimization and generalization. ## 3 Preliminaries\n\nIn this section, we briefly introduce the SSM in Section 3.1 and the motivation for optimization designs based on the generalization analysis in Section 3.2. ### 3.1 Introduction to SSMs\n\nIn this paper, we consider the following single-input single-output SSM,\n\n$$\nh^{\\prime}(t)=A h(t)+B x(t), \\quad y(t)=C h(t), \\quad t \\geq 0\n$$\n\nwhere $x$ is the input from an input space ${ }^{1} \\mathcal{X}:=C_{0}\\left(\\mathbb{R}_{\\geq 0}, \\mathbb{R}\\right) ; y(t) \\in \\mathbb{R}$ is the output at time $t ; h(t) \\in \\mathbb{R}^{m}$ is the hidden state with $h(0)=0 ; A \\in \\mathbb{R}^{m \\times m}, B \\in \\mathbb{R}^{m \\times 1}, C \\in \\mathbb{R}^{1 \\times m}$ are trainable parameters. Then (1) has an explicit solution $y(t)=\\int_{0}^{t} \\rho_{\\theta}(s) x(t-s) d s$, where $\\rho_{\\theta}(s):=C e^{A s} B$ with $\\theta=(C, A, B)$. The function $\\rho_{\\theta}(s)$ captures the memory structure of the model and the temporal input-output relationship (Li et al., 2022). For the S4 model and its variants (Gu et al., 2022a;b; Gupta et al., 2022; Gu et al., 2023), (1) is usually discretized by the Zero-Order Hold method, i.e., given a timescale $\\Delta \\in \\mathbb{R}, h_{k+1}=$ $\\bar{A} h_{k}+\\bar{B} x_{k}, \\quad y_{k}=\\bar{C} h_{k}, \\quad k=0,1, \\ldots$, where $\\bar{A}=e^{\\Delta \\cdot A}, \\bar{B}=\\left(\\bar{A}-\\mathbb{I}_{m}\\right) A^{-1} B, \\bar{C}=C$. Then, $y_{k}=\\bar{C} \\bar{A}^{k} \\bar{B} x_{0}+\\bar{C} \\bar{A}^{k-1} \\bar{B} x_{1}+\\ldots+\\bar{C} \\bar{B} x_{k}=[\\bar{K} * x]_{k}$ where $\\bar{K}=\\left(\\bar{C} \\bar{B}, \\bar{C} \\bar{A} \\bar{B}, \\ldots, \\bar{C} \\bar{A}{ }^{k} \\bar{B}\\right)$ and $*$ represents to convolution. [^0]![](https://cdn.mathpix.com/cropped/2024_09_12_ba1e391f64f9910c4329g-04.jpg?height=421&width=1435&top_left_y=242&top_left_x=402)\n\nFigure 1: The logic diagram goes from generalization analysis to optimization designs. ### 3.2 Motivation: a linear regression model\n\nIn this subsection, we use a linear regression model on non-sequential data as an example to illustrate the connection between the generalization analysis and the optimization designs. This example then motivates us to extend the connection to SSMs on sequential data. Linear regression. We consider a simple linear model $y=\\theta^{\\top} x$ with input $x \\in \\mathbb{R}^{d}$, output $y \\in \\mathbb{R}$ and parameter $\\theta \\in \\mathbb{R}^{d}$. Let the training data $\\left\\{\\left(x_{i}, y_{i}\\right)\\right\\}_{i=1}^{n}$ be i.i.d. sampled from a distribution $\\mathcal{D}$ such that $\\left\\|x_{i}\\right\\|_{2}=r,\\left|y_{i}\\right| \\leq 1(\\forall i \\in[1: n])$. Define the empirical risk $\\mathcal{L}_{n}(\\theta):=\\frac{1}{n} \\sum_{i=1}^{n}\\left(\\theta^{\\top} x_{i}-y_{i}\\right)^{2}$ and the population risk $\\mathcal{L}_{\\mathcal{D}}(\\theta):=\\mathbb{E}_{x, y}\\left[\\left(\\theta^{\\top} x-y\\right)^{2}\\right]$. Then given a norm-constrained space $\\Theta:=\\left\\{\\theta \\in \\mathbb{R}^{d}:\\|\\theta\\|_{2} \\leq\\right.$ $R\\}$, with probability at least $1-\\delta$ over $\\mathcal{D}$,\n\n$$\n\\sup _{\\theta \\in \\Theta}\\left|\\mathcal{L}_{n}(\\theta)-\\mathcal{L}_{\\mathcal{D}}(\\theta)\\right| \\leq(r R+1)^{2} \\cdot \\mathcal{O}(\\sqrt{\\log (1 / \\delta) / n})\n$$\n\nThis is a well-known norm-based generalization bound based on the Rademacher theory (Mohri et al., 2012), and we provide a proof in Appendix B for completeness. Notice that the key term $r^{2} R^{2}$ in the generalization bound (2) is also an upper bound for the magnitude of the linear model output, i.e., $\\sup _{\\theta \\in \\Theta}\\left(\\theta^{\\top} x_{i}\\right)^{2} \\leq r^{2} R^{2}$. Thus, we connect the model stability with the generalization bound stability, and this connection induces an initialization scheme for the initialization $\\theta^{(0)}$ by setting $\\left\\|\\theta^{(0)}\\right\\|_{2} \\sim \\mathcal{O}(1 / r)$. In particular, if we normalize each input $x_{i}$ such that $r$ is also $\\mathcal{O}(1)$, then $\\left\\|\\theta^{(0)}\\right\\|_{2} \\sim \\mathcal{O}(1)$. Since $\\theta^{(0)} \\in \\mathbb{R}^{d}$, one possible initialization scheme is that $\\theta^{(0)}$ follows a Uniform distribution $U[-1 / \\sqrt{d}, 1 / \\sqrt{d}]$, which corresponds to the Kaiming initialization (up to some constant) (He et al., 2015). When treating the term $r^{2} R^{2}$ as a regularizer to improve the generalization, we get the weight decay method, i.e., the $\\ell_{2}$ regularization w.r.t.\n```\n\n#### 2. On the Parameterization and Initialization of Diagonal State Space Models (Avg. Score: 0.60)\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 166  (*Influential: 33*)\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n##### *Relevant Chunk: No. 1/29 (Score: 0.60)*\n\n```\n# On the Parameterization and Initialization of Diagonal State Space Models \n\nAlbert $\\mathrm{Gu}^{\\dagger}$, Ankit Gupta ${ }^{\\ddagger}$, Karan Goel ${ }^{\\dagger}$, and Christopher R\u00e9 ${ }^{\\dagger}$<br>${ }^{\\dagger}$ Department of Computer Science, Stanford University<br>${ }^{\\ddagger}$ IBM Research<br>${ }^{1}\\{$ albertgu,krng\\}@stanford.edu, chrismre@cs.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.60)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 21/27 (Score: 0.60)*\n\n```\n[17] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" 2014. [Online]. Available: https://arxiv.org/abs/1412. 6980\n[18] A. Gu, A. Gupta, K. Goel, and C. R\u00e9, \"On the Parameterization and Initialization of Diagonal State Space Models,\" 2022. [Online]. Available: https://arxiv.org/abs/2206.11893\n[19] J. T. Smith, A. Warrington, and S. Linderman, \"Simplified State Space Layers for Sequence Modeling,\" in The Eleventh International Conference on Learning Representations, 2023.\n```\n\n#### 4. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.55)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 1/37 (Score: 0.55)*\n\n```\n# Robustifying State-space Models for Long Sequences via Approximate Diagonalization \n\nAnnan Yu, ${ }^{1 *}$ Arnur Nigmetov, ${ }^{2} \\quad$ Dmitriy Morozov, ${ }^{2}$ Michael W. Mahoney, ${ }^{2,3,4}$<br>N. Benjamin Erichson ${ }^{2,3}$<br>${ }^{1}$ Center for Applied Mathematics, Cornell University<br>${ }^{2}$ Lawrence Berkeley National Laboratory<br>${ }^{3}$ International Computer Science Institute<br>${ }^{4}$ Department of Statistics, University of California at Berkeley\n\n\n#### Abstract\n\nState-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable \"perturb-thendiagonalize\" (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the nonnormal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the $\\mathrm{S} 4 \\mathrm{D} / \\mathrm{S} 5$ models. In addition to improved robustness, our S5PTD model averages $87.6 \\%$ accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models. ## 1 Introduction\n\nSequential data are pervasive across a wide range of fields, including natural language processing, speech recognition, robotics and autonomous systems, as well as scientific machine learning and financial time-series analysis, among others. Given that many of these applications produce exceedingly long sequences, sequential models need to capture long-range temporal dependencies in order to yield accurate predictions. To this end, many specialized deep learning methods have been developed to deal with long sequences, including recurrent neural networks (RNNs) [2, 7, 13, 30, 14, 28], convolutional neural networks (CNNs) [4, 29], continuous-time models (CTMs) $[18,37]$, and transformers $[21,8,23,39,26]$. Over the past few years, the new class of state-space models (SSMs) gained vast popularity for sequential modeling due to their outstanding performance on the Long-Range Arena (LRA)\n\n[^0]dataset [33]. An SSM is built upon a continuous-time linear time-invariant (LTI) dynamical system $\\Sigma=(\\mathbf{A}, \\mathbf{B}, \\mathbf{C}, \\mathbf{D})$, which is a system of linear ODEs given by\n\\[\n\n$$\n\\begin{aligned}\n\\mathbf{x}^{\\prime}(t) & =\\mathbf{A x}(t)+\\mathbf{B u}(t) \\\\\n\\mathbf{y}(t) & =\\mathbf{C x}(t)+\\mathbf{D u}(t)\n\\end{aligned}\n$$\n\\]\n\nwhere $\\mathbf{A} \\in \\mathbb{C}^{n \\times n}, \\mathbf{B} \\in \\mathbb{C}^{n \\times m}, \\mathbf{C} \\in \\mathbb{C}^{p \\times n}, \\mathbf{D} \\in \\mathbb{C}^{p \\times m}$ are the state, input, output and feedthrough matrices; and $\\mathbf{u}(t) \\in \\mathbb{C}^{m}, \\mathbf{x}(t) \\in \\mathbb{C}^{n}, \\mathbf{y}(t) \\in \\mathbb{C}^{p}$ are the inputs, states, and outputs of the system, respectively. The system can be discretized at time steps $j \\Delta t$, where $\\Delta t>0$ and $j=1, \\ldots, L$, to be fed with sequential inputs of length $L$. To store and process the information of the long sequential inputs online, the SSMs are often initialized by a pre-designed LTI system. One of the most popular schemes is called \"HiPPO initialization\" [35, 15], in which the Legendre coefficients of the input history at time $t$, i.e., $\\mathbf{u} \\cdot \\mathbb{1}_{[0, t]}$, are stored and updated in the state vector $\\mathbf{x}(t)$. This initialization is specifically designed to model long-range dependencies in sequential data. The recently proposed S4 model [17] leverages the HiPPO initialization and accelerates training and inference by decomposing $\\mathbf{A}$ into the sum of a diagonal matrix and a low-rank one. The diagonal-plus-low-rank (DPLR) structure yields a barycentric representation [1] of the transfer function of eq. (1) that maps inputs to outputs in the frequency domain, enabling fast computation in the frequency domain [3]. While the DPLR structure achieves an asymptotic speed-up of the model, considering A to be a diagonal matrix results in a simpler structure. Compared to a DPLR matrix A, a diagonal SSM is not only faster to compute and easier to implement, but it also allows integrating channel communication via parallel scans [32], thereby improving its performance on long-range tasks. Unfortunately, the problem of diagonalizing the HiPPO framework is exponentially illconditioned, as $n$ increases. Hence, while [17] shows analytic forms of the eigenvalues and eigenvectors of HiPPO matrices, they suffer from an exponentially large variance and cannot be used in practice. So far, the most popular way of obtaining a diagonal SSM is to simply discard the low-rank part from the DPLR structure, leveraging a stable diagonalization algorithm for a normal matrix. Discarding the low-rank component changes the underlying diagonalization problem, however; and it abandons the theoretical insights about HiPPO. Still, the resulting model almost matches S4's performance, in practice. Such diagonal models are called S4D [16] when the systems are single-input/single-output (i.e., $m=p=1$ ) and S5 [32] when the systems are multiple-input/multiple-output (i.e., $m=p>1$ ), which enables channel communication. The issue of ill-posed diagonalization problems is not merely specific to SSMs. For example, it is known that non-normal matrices make RNNs more expressive [22, 27]. More generally, non-normality plays an important role in the training of certain neural networks [31, 25]. While the ill-posedness of the diagonalization problem essentially prevents accurate computation of eigenvalues and eigenvectors (i.e., we cannot have a small forward error) - in fact, the true spectral information becomes meaningless in this case - using a backward stable eigensolver, one can recover the non-normal matrix accurately (i.e., we can have a small backward error) from the wrong eigenvalues and eigenvectors. In this paper, we propose a generic \"perturb-then-diagonalize\" (PTD) methodology as a backward stable eigensolver. PTD is based on the idea that a small random perturbation remedies the problem of the blowing up of eigenvector condition number $[11,10,6]$, regularizing the ill-posed problem into a close but well-posed one. It is based on the pseudospectral theory of non-normal operators [34] and may be interpreted as the approximate diagonalization of the non-normal matrices. In the context of SSMs, our PTD method can be used to diagonalize the highly non-normal HiPPO framework. Based on this, we introduce the S4-PTD and S5-PTD models. Our method is flexible, and it can be used to diagonalize many SSM initialization schemes that may be invented in the future. Contribution. Here are our main contributions:\n\n1. We propose a \"perturb-then-diagonalize\" (PTD) methodology that solves ill-posed diagonalization problems in machine learning when only the backward error is important.\n```\n\n#### 5. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.11)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 19/31 (Score: 0.11)*\n\n```\nIEEE Trans. Automat. Contr., 32(2):115-122, 1987. [21] Fusheng Liu and Qianxiao Li. From generalization analysis to optimization designs for state space models. arXiv preprint arXiv:2405.02670, 2024. [22] Charles H Martin and Michael W Mahoney. Implicit self-regularization in deep neural networks: Evidence from random matrix theory and implications for learning. Journal of Machine Learning Research, 22(165):1-73, 2021. [23] Charles H Martin, Tongsu Peng, and Michael W Mahoney. Predicting trends in the quality of state-of-the-art neural networks without access to training or testing data.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: selective state space model stability optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization\n\n*From Search Query: selective state space model stability optimization*\n\n*Shida Wang, Qianxiao Li*\n\n**TL;DR:** This paper introduces a class of reparameterization techniques for SSMs that effectively lift its memory limitations and illustrates that a principled choice of reparameterization scheme can also enhance optimization stability.\n\n**Abstract:** In this paper, we investigate the long-term memory learning capabilities of state-space models (SSMs) from the perspective of parameterization. We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory. Our analysis identifies this\"curse of memory\"as a result of the recurrent weights converging to a stability boundary, suggesting that a reparameterization technique can be effective. To this end, we introduce a class of reparameterization techniques for SSMs that effectively lift its memory limitations. Besides improving approximation capabilities, we further illustrate that a principled choice of reparameterization scheme can also enhance optimization stability. We validate our findings using synthetic datasets, language models and image classifications.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 9  (*Influential: 1*)\n\n#### 2. From Generalization Analysis to Optimization Designs for State Space Models\n\n*From Search Query: selective state space model stability optimization*\n\n*Fusheng Liu, Qianxiao Li*\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 3. Deep Latent State Space Models for Time-Series Generation\n\n*From Search Query: selective state space model stability optimization*\n\n*Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, Stefano Ermon*\n\n**TL;DR:** LS4 is proposed, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity, and sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets.\n\n**Abstract:** Methods based on ordinary differential equations (ODEs) are widely used to build generative models of time-series. In addition to high computational overhead due to explicitly computing hidden states recurrence, existing ODE-based models fall short in learning sequence data with sharp transitions - common in many real-world systems - due to numerical challenges during optimization. In this work, we propose LS4, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity. Inspired by recent deep state space models (S4), we achieve speedups by leveraging a convolutional representation of LS4 which bypasses the explicit evaluation of hidden states. We show that LS4 significantly outperforms previous continuous-time generative models in terms of marginal distribution, classification, and prediction scores on real-world datasets in the Monash Forecasting Repository, and is capable of modeling highly stochastic data with sharp temporal transitions. LS4 sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets, while also being x100 faster than other baselines on long sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 3*)\n\n#### 4. Self-Supervised Inference in State-Space Models\n\n*From Search Query: selective state space model stability optimization*\n\n*David Ruhe, Patrick Forr'e*\n\n**TL;DR:** This work performs approximate inference in state-space models with nonlinear state transitions using a local linearity approximation parameterized by neural networks, accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states.\n\n**Abstract:** We perform approximate inference in state-space models with nonlinear state transitions. Without parameterizing a generative model, we apply Bayesian update formulas using a local linearity approximation parameterized by neural networks. This comes accompanied by a maximum likelihood objective that requires no supervision via uncorrupt observations or ground truth latent states. The optimization backpropagates through a recursion similar to the classical Kalman filter and smoother. Additionally, using an approximate conditional independence, we can perform smoothing without having to parameterize a separate model. In scientific applications, domain knowledge can give a linear approximation of the latent transition maps, which we can easily incorporate into our model. Usage of such domain knowledge is reflected in excellent results (despite our model's simplicity) on the chaotic Lorenz system compared to fully supervised and variational inference methods. Finally, we show competitive results on an audio denoising experiment.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Span-Selective Linear Attention Transformers for Effective and Robust Schema-Guided Dialogue State Tracking\n\n*From Search Query: selective state space model stability optimization*\n\n*Bj\u00f6rn Bebensee, Haejun Lee*\n\n**Abstract:** In schema-guided dialogue state tracking models estimate the current state of a conversation using natural language descriptions of the service schema for generalization to unseen services. Prior generative approaches which decode slot values sequentially do not generalize well to variations in schema, while discriminative approaches separately encode history and schema and fail to account for inter-slot and intent-slot dependencies. We introduce SPLAT, a novel architecture which achieves better generalization and efficiency than prior approaches by constraining outputs to a limited prediction space. At the same time, our model allows for rich attention among descriptions and history while keeping computation costs constrained by incorporating linear-time attention. We demonstrate the effectiveness of our model on the Schema-Guided Dialogue (SGD) and MultiWOZ datasets. Our approach significantly improves upon existing models achieving 85.3 JGA on the SGD dataset. Further, we show increased robustness on the SGD-X benchmark: our model outperforms the more than 30x larger D3ST-XXL model by 5.0 points.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Sibling-Attack: Rethinking Transferable Adversarial Attacks against Face Recognition\n\n*From Search Query: selective state space model stability optimization*\n\n*Cong Liu, Simin Chen, Shouhong Ding, Juefeng Guo, Taiping Yao, Bangjie Yin, Zexin Li*\n\n**Abstract:** A hard challenge in developing practical face recognition (FR) attacks is due to the black-box nature of the target FR model, i.e., inaccessible gradient and parameter information to attackers. While recent research took an important step towards attacking black-box FR models through leveraging transferability, their performance is still limited, especially against online commercial FR systems that can be pessimistic (e.g., a less than 50% ASR--attack success rate on average). Motivated by this, we present Sibling-Attack, a new FR attack technique for the first time explores a novel multi-task perspective (i.e., leveraging extra information from multi-correlated tasks to boost attacking transferability). Intuitively, Sibling-Attack selects a set of tasks correlated with FR and picks the Attribute Recognition (AR) task as the task used in Sibling-Attack based on theoretical and quantitative analysis. Sibling-Attack then develops an optimization framework that fuses adversarial gradient information through (1) constraining the cross-task features to be under the same space, (2) a joint-task meta optimization framework that enhances the gradient compatibility among tasks, and (3) a cross-task gradient stabilization method which mitigates the oscillation effect during attacking. Extensive experiments demonstrate that Sibling-Attack outperforms state-of-the-art FR attack techniques by a non-trivial margin, boosting ASR by 12.61% and 55.77% on average on state-of-the-art pre-trained FR models and two well-known, widely used commercial FR systems.\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2023-03-22\n\n\n\n#### 2. GroupMamba: Parameter-Efficient and Accurate Group Visual State Space Model\n\n*From Search Query: selective state space model stability optimization*\n\n*Fahad Shahbaz Khan, Juergen Gall, Salman Khan, Syed Talal Wasim, Abdelrahman Shaker*\n\n**Abstract:** Recent advancements in state-space models (SSMs) have showcased effective performance in modeling long-range dependencies with subquadratic complexity. However, pure SSM-based models still face challenges related to stability and achieving optimal performance on computer vision tasks. Our paper addresses the challenges of scaling SSM-based models for computer vision, particularly the instability and inefficiency of large model sizes. To address this, we introduce a Modulated Group Mamba layer which divides the input channels into four groups and applies our proposed SSM-based efficient Visual Single Selective Scanning (VSSS) block independently to each group, with each VSSS block scanning in one of the four spatial directions. The Modulated Group Mamba layer also wraps the four VSSS blocks into a channel modulation operator to improve cross-channel communication. Furthermore, we introduce a distillation-based training objective to stabilize the training of large models, leading to consistent performance gains. Our comprehensive experiments demonstrate the merits of the proposed contributions, leading to superior performance over existing methods for image classification on ImageNet-1K, object detection, instance segmentation on MS-COCO, and semantic segmentation on ADE20K. Our tiny variant with 23M parameters achieves state-of-the-art performance with a classification top-1 accuracy of 83.3% on ImageNet-1K, while being 26% efficient in terms of parameters, compared to the best existing Mamba design of same model size. Our code and models are available at: https://github.com/Amshaker/GroupMamba.\n\n**Published:** 2024-07-18\n\n\n\n#### 3. Diffusing to the Top: Boost Graph Neural Networks with Minimal Hyperparameter Tuning\n\n*From Search Query: selective state space model stability optimization*\n\n*Junbin Gao, Zhiyong Wang, Andi Han, Dai Shi, Lequan Lin*\n\n**Abstract:** Graph Neural Networks (GNNs) are proficient in graph representation learning and achieve promising performance on versatile tasks such as node classification and link prediction. Usually, a comprehensive hyperparameter tuning is essential for fully unlocking GNN's top performance, especially for complicated tasks such as node classification on large graphs and long-range graphs. This is usually associated with high computational and time costs and careful design of appropriate search spaces. This work introduces a graph-conditioned latent diffusion framework (GNN-Diff) to generate high-performing GNNs based on the model checkpoints of sub-optimal hyperparameters selected by a light-tuning coarse search. We validate our method through 166 experiments across four graph tasks: node classification on small, large, and long-range graphs, as well as link prediction. Our experiments involve 10 classic and state-of-the-art target models and 20 publicly available datasets. The results consistently demonstrate that GNN-Diff: (1) boosts the performance of GNNs with efficient hyperparameter tuning; and (2) presents high stability and generalizability on unseen data across multiple generation runs. The code is available at https://github.com/lequanlin/GNN-Diff.\n\n**Published:** 2024-10-08\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using selective state space models, focusing on stability and optimization, here are some key technical details and methods that can be particularly useful:\n\n## Input-Dependent Dynamics and Stable Reparameterization\n\nThe S7 model, as described in the references, introduces several critical components that can be adapted for improving autoregressive language models:\n\n- **Input-Dependent Transition Matrices**: By making the transition matrices a function of the input, the model can adapt to the current input at each time step dynamically. This is achieved in S7 by defining the transition matrix \\(\\Lambda_k\\), along with the input matrices \\(B_k\\), \\(C_k\\), and \\(D_k\\), as functions of the input \\(u_k\\). This allows the model to filter information selectively, enhancing its ability to capture long-term dependencies and perform content-based reasoning.\n\n## Stability Ensuring Techniques\n\nTo ensure stability in long-sequence modeling, the S7 model employs several techniques:\n\n- **Stable Reparameterization**: The transition matrix is reparameterized to ensure that its eigenvalues remain within a stable range. This is done by modifying the recurrent matrix using a stability function, which prevents the system from exhibiting unstable behavior over time. Specifically, the reparameterization ensures that the eigenvalues of the matrix stay within a range that promotes stable dynamics.\n\n- **Lipschitz Continuity and Sobolev Norms**: The model assumes that the mappings are Lipschitz continuous, which helps in preventing uncontrolled behavior and ensures that the system provides accurate approximations of target functionals. The use of Sobolev norms allows for measuring approximation quality in terms of both the function value and its derivative.\n\n## Initialization and Regularization\n\nFor ensuring stability and optimization during training, the following methods are crucial:\n\n- **Careful Initialization**: Proper initialization of the model parameters is essential for convergence. Ensuring that the initial parameters are well-chosen can help in avoiding issues like exploding or vanishing gradients.\n\n- **Regularization Techniques**: Regularization is vital for maintaining stability, especially during test-time training. Techniques such as constraint-based updates can help in keeping the model stable and preventing overfitting. The S7 model's use of a gating mechanism, represented by a sigmoid activation, helps regulate the flow of information and can be seen as a form of regularization.\n\n## Gradient Norm Control\n\nControlling the gradient norms is essential for efficient training and preventing issues like exploding gradients. The S7 model analyzes how the gradient of the loss function with respect to the raw parameters scales with the derivative of the reparameterization function. This ensures that gradients are well-behaved during optimization, which is critical for stability and convergence.\n\n## Hardware and Memory Efficiency\n\nFor practical implementation, ensuring that the model is hardware-efficient and memory-efficient is crucial:\n\n- **Linear Complexity**: Maintaining linear complexity in the model's architecture is important for efficient processing. The S7 model and other state space models like Mamba ensure that the state processing remains efficient while supporting adaptability.\n\n- **Selective State Updates**: Memory efficiency can be achieved through selective state updates, which allow the model to update only the necessary parts of the state, reducing memory usage and improving performance.\n\nBy integrating these techniques\u2014input-dependent dynamics, stable reparameterization, careful initialization, regularization, gradient norm control, and hardware/memory efficiency\u2014the researchers can significantly improve the stability, adaptability, and overall performance of their autoregressive language model.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis:\n\nThe search results provide valuable insights into combining state space models with test-time training capabilities. Key findings include:\n\na) Integration Opportunities:\n- Mamba2's SSD framework offers efficient state processing\n- FastTTTLinear's test-time training can enhance adaptability\n- Linear complexity can be maintained through careful design\n\nb) Technical Challenges:\n- Balancing stability and adaptivity in test-time updates\n- Hardware optimization requirements\n- Memory efficiency considerations\n\n2. Useful References with Excerpts:\n\nFrom \"State Space Models as Foundation Models\":\n\"SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas.\"\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages:\n1. Provable robustness properties independent of spectrum and dimensionality\n2. Fixed convolutional filters that don't require learning while still outperforming SSMs\"\n\nFrom \"Mamba: Linear-Time Sequence Modeling\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities.\"\n\n3. Key Insights:\n\na) Architectural Integration:\n- Need to preserve linear complexity when combining mechanisms\n- State space representation must support both efficient processing and adaptability\n- Hardware-aware implementation crucial for performance\n\nb) Stability Considerations:\n- Regularization techniques needed for test-time training\n- Constraint-based updates can help maintain stability\n- Careful initialization important for convergence\n\nc) Performance Optimization:\n- Parallel processing capabilities important\n- Memory efficiency through selective state updates\n- Hardware-specific optimizations needed\n\n4. Future Search Plan:\nNeed to investigate:\n1. Specific techniques for combining selective state spaces with test-time training\n2. Methods for ensuring stability in adaptive updates\n3. Hardware optimization strategies for combined architectures\n\n5. References:\n\n[1] Alonso, C.A., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv:2401.12184.\n\n[2] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv:2306.11270.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752.\n\n[4] Bonassi, F., Andersson, C.R., Mattsson, P., & Sch\u00f6n, T.B. (2023). Structured state-space models are deep Wiener models. arXiv:2312.03227.\n\n[5] Gu, A., et al. (2021). Combining Recurrent, Convolutional, and Continuous-time Models with Linear State-Space Layers. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "adaptive state space model optimization hardware",
            "detail": "Find technical details about hardware-efficient implementations of adaptive state space models, focusing on optimization techniques and memory management.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind technical details about hardware-efficient implementations of adaptive state space models, focusing on optimization techniques and memory management.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. From generalization analysis to optimization designs for state space models (Avg. Score: 0.79)\n\n*Fusheng Liu, Qianxiao Li*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.79)*\n\n```\n# From Generalization Analysis to Optimization Designs for State Space Models \n\nFusheng Liu<br>National University of Singapore<br>fusheng@u.nus.edu\n\nQianxiao Li<br>National University of Singapore<br>qianxiao@nus.edu.sg\n\n\n#### Abstract\n\nA State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.37)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 2/74 (Score: 0.47)*\n\n```\nMany subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference ( $5 \\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation. ## 1 Introduction\n\nFoundation models (FMs), or large models pretrained on massive data then adapted for downstream tasks, have emerged as an effective paradigm in modern machine learning. The backbone of these FMs are often sequence models, operating on arbitrary sequences of inputs from a wide variety of domains such as language, images, speech, audio, time series, and genomics (Brown et al. 2020; Dosovitskiy et al. 2020; Ismail Fawaz et al. 2019; Oord et al. 2016; Poli et al. 2023; Sutskever, Vinyals, and Quoc V Le 2014). While this concept is agnostic to a particular choice of model architecture, modern FMs are predominantly based on a single type of sequence model: the Transformer (Vaswani et al. 2017) and its core attention layer (Bahdanau, Cho, and Bengio 2015) The efficacy of self-attention is attributed to its ability to route information densely within a context window, allowing it to model complex data. However, this property brings fundamental drawbacks: an inability to model anything outside of a finite window, and quadratic scaling with respect to the window length. An enormous body of research has appeared on more efficient variants of attention to overcome these drawbacks (Tay, Dehghani, Bahri, et al. 2022), but often at the expense of the very properties that makes it effective. As of yet, none of these variants have been shown to be empirically effective at scale across domains. Recently, structured state space sequence models (SSMs) (Gu, Goel, and R\u00e9 2022; Gu, Johnson, Goel, et al. 2021) have emerged as a promising class of architectures for sequence modeling. These models can be interpreted as a combination of recurrent neural networks (RNNs) and convolutional neural networks ( CNNs ), with inspiration from classical state space models (Kalman 1960). This class of models can be computed very efficiently as either a recurrence or convolution, with linear or near-linear scaling in sequence length. Additionally, they have principled mechanisms for modeling long-range dependencies (Gu, Dao, et al. 2020) in certain data modalities, and have dominated benchmarks such as the long Range\n\n[^0]Arena (Tay, Dehghani, Abnar, et al. 2021). Many flavors of SSMs (Gu, Goel, and R\u00e9 2022; Gu, Gupta, et al. 2022; Gupta, Gu, and Berant 2022; Y. Li et al. 2023; Ma et al. 2023; Orvieto et al. 2023; Smith, Warrington, and Linderman 2023) have been successful in domains involving continuous signal data such as audio and vision (Goel et al. 2022; Nguyen, Goel, et al. 2022; Saon, Gupta, and Cui 2023). However, they have been less effective at modeling discrete and information-dense data such as text. We propose a new class of selective state space models, that improves on prior work on several axes to achieve the modeling power of Transformers while scaling linearly in sequence length. Selection Mechanism. First, we identify a key limitation of prior models: the ability to efficiently select data in an input-dependent manner (i.e. focus on or ignore particular inputs). Building on intuition based on important synthetic tasks such as selective copy and induction heads, we design a simple selection mechanism by parameterizing the SSM parameters based on the input. This allows the model to filter out irrelevant information and remember relevant information indefinitely. Hardware-aware Algorithm. This simple change poses a technical challenge for the computation of the model; in fact, all prior SSMs models must be time- and input-invariant in order to be computationally efficient. We overcome this with a hardware-aware algorithm that computes the model recurrently with a scan instead of convolution, but does not materialize the expanded state in order to avoid IO access between different levels of the GPU memory hierarchy. The resulting implementation is faster than previous methods both in theory (scaling linearly in sequence length, compared to pseudo-linear for all convolution-based SSMs) and on modern hardware (up to $3 \\times$ faster on A100 GPUs). Architecture. We simplify prior deep sequence model architectures by combining the design of prior SSM architectures (Dao, Fu, Saab, et al. 2023) with the MLP block of Transformers into a single block, leading to a simple and homogenous architecture design (Mamba) incorporating selective state spaces. Selective SSMs, and by extension the Mamba architecture, are fully recurrent models with key properties that make them suitable as the backbone of general foundation models operating on sequences. (i) High quality: selectivity brings strong performance on dense modalities such as language and genomics. (ii) Fast training and inference: computation and memory scales linearly in sequence length during training, and unrolling the model autoregressively during inference requires only constant time per step since it does not require a cache of previous elements. (iii) Long context: the quality and efficiency together yield performance improvements on real data up to sequence length 1 M . We empirically validate Mamba's potential as a general sequence FM backbone, in both pretraining quality and domainspecific task performance, on several types of modalities and settings:\n\n- Synthetics. On important synthetic tasks such as copying and induction heads that have been proposed as being key to large language models, Mamba not only solves them easily but can extrapolate solutions indefinitely long ( $>1 \\mathrm{M}$ tokens). - Audio and Genomics. Mamba out-performs prior state-of-the-art models such as SaShiMi, Hyena, and Transformers on modeling audio waveforms and DNA sequences, both in pretraining quality and downstream metrics (e.g. reducing FID on a challenging speech generation dataset by more than half). In both settings, its performance improves with longer context up to million-length sequences. - Language Modeling. Mamba is the first linear-time sequence model that truly achieves Transformer-quality performance, both in pretraining perplexity and downstream evaluations. With scaling laws up to 1B parameters, we show that Mamba exceeds the performance of a large range of baselines, including very strong modern Transformer training recipes based on LLaMa (Touvron et al. 2023). Our Mamba language model has $5 \\times$ generation throughput compared to Transformers of similar size, and Mamba-3B's quality matches that of Transformers twice its size (e.g. 4 points higher avg. on common sense reasoning compared to Pythia-3B and even exceeding Pythia-7B). Model code and pre-trained checkpoints are open-sourced at https://github.com/state-spaces/mamba. ## 2 State Space Models\n\nStructured state space sequence models (S4) are a recent class of sequence models for deep learning that are broadly related to RNNs, and CNNs, and classical state space models. They are inspired by a particular continuous system (1) that maps a\n\n## Selective State Space Model\n\nwith Hardware-aware State Expansion\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_9db7b10d0e19303048adg-03.jpg?height=535&width=1722&top_left_y=356&top_left_x=234)\n\nFigure 1: (Overview.) Structured SSMs independently map each channel (e.g. $D=5$ ) of an input $x$ to output $y$ through a higher dimensional latent state $h($ e.g. $N=4$ ). Prior SSMs avoid materializing this large effective state ( $D N$, times batch size $B$ and sequence length $L$ ) through clever alternate computation paths requiring time-invariance: the ( $\\triangle, A, B, C$ ) parameters are constant across time. Our selection mechanism adds back input-dependent dynamics, which also requires a careful hardware-aware algorithm to only materialize the expanded states in more efficient levels of the GPU memory hierarchy. 1-dimensional function or sequence $x(t) \\in \\mathbb{R} \\mapsto y(t) \\in \\mathbb{R}$ through an implicit latent state $h(t) \\in \\mathbb{R}^{N}$. Concretely, S 4 models are defined with four parameters $(\\Delta, A, B, C)$, which define a sequence-to-sequence transformation in two stages. $$\n\\begin{aligned}\n& h^{\\prime}(t)=A h(t)+B x(t) \\quad \\text { (1a) } \\quad h_{t}=\\bar{A} h_{t-1}+\\bar{B} x_{t} \\\\\n& \\bar{K}=\\left(C \\bar{B}, C \\overline{A B}, \\ldots, C \\bar{A}^{k} \\bar{B}, \\ldots\\right) \\\\\n& y(t)=\\operatorname{Ch}(t)\n\\end{aligned}\n$$\n\nDiscretization. The first stage transforms the \"continuous parameters\" $(\\Delta, A, B)$ to \"discrete parameters\" $(\\bar{A}, \\bar{B})$ through fixed formulas $\\overline{\\boldsymbol{A}}=f_{A}(\\Delta, \\boldsymbol{A})$ and $\\overline{\\boldsymbol{B}}=f_{B}(\\Delta, \\boldsymbol{A}, \\boldsymbol{B})$, where the pair $\\left(f_{A}, f_{B}\\right)$ is called a discretization rule. Various rules can be used such as the zero-order hold $(\\mathrm{ZOH})$ defined in equation (4). $$\n\\bar{A}=\\exp (\\Delta A) \\quad \\bar{B}=(\\Delta A)^{-1}(\\exp (\\Delta A)-I) \\cdot \\Delta B\n$$\n\nDiscretization has deep connections to continuous-time systems which can endow them with additional properties such as resolution invariance (Nguyen, Goel, et al.\n```\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.27)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.24)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 1/30 (Score: 0.24)*\n\n```\n# B\u2019MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory \n\nLuca Zancato* Arjun Seshadri Yonatan Dukler Aditya Golatkar Yantao Shen<br>Benjamin Bowman Matthew Trager Alessandro Achille Stefano Soatto\n\nAWS AI Labs\n\n\n#### Abstract\n\nWe describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference.\n```\n\n#### 4. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.16)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.16)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: adaptive state space model optimization hardware\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 352  (*Influential: 57*)\n\n#### 2. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 270  (*Influential: 21*)\n\n#### 3. Increasing the Scope as You Learn: Adaptive Bayesian Optimization in Nested Subspaces\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Leonard Papenmeier, Luigi Nardi, Matthias Poloczek*\n\n**TL;DR:** This paper proposes BAxUS that leverages a novel family of nested random subspaces to adapt the space it optimizes over to the problem and ensures high performance while removing the risk of failure, which it asserts via theoretical guarantees.\n\n**Abstract:** Recent advances have extended the scope of Bayesian optimization (BO) to expensive-to-evaluate black-box functions with dozens of dimensions, aspiring to unlock impactful applications, for example, in the life sciences, neural architecture search, and robotics. However, a closer examination reveals that the state-of-the-art methods for high-dimensional Bayesian optimization (HDBO) suffer from degrading performance as the number of dimensions increases or even risk failure if certain unverifiable assumptions are not met. This paper proposes BAxUS that leverages a novel family of nested random subspaces to adapt the space it optimizes over to the problem. This ensures high performance while removing the risk of failure, which we assert via theoretical guarantees. A comprehensive evaluation demonstrates that BAxUS achieves better results than the state-of-the-art methods for a broad set of applications.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 25  (*Influential: 4*)\n\n#### 4. From Generalization Analysis to Optimization Designs for State Space Models\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Fusheng Liu, Qianxiao Li*\n\n**TL;DR:** This paper gives a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences, and introduces a new regularization method for training SSMs to enhance the generalization performance.\n\n**Abstract:** A State Space Model (SSM) is a foundation model in time series analysis, which has recently been shown as an alternative to transformers in sequence modeling. In this paper, we theoretically study the generalization of SSMs and propose improvements to training algorithms based on the generalization results. Specifically, we give a \\textit{data-dependent} generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data; (2) introduce a new regularization method for training SSMs to enhance the generalization performance. Numerical results are conducted to validate our results.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 5. Deep Latent State Space Models for Time-Series Generation\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, Stefano Ermon*\n\n**TL;DR:** LS4 is proposed, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity, and sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets.\n\n**Abstract:** Methods based on ordinary differential equations (ODEs) are widely used to build generative models of time-series. In addition to high computational overhead due to explicitly computing hidden states recurrence, existing ODE-based models fall short in learning sequence data with sharp transitions - common in many real-world systems - due to numerical challenges during optimization. In this work, we propose LS4, a generative model for sequences with latent variables evolving according to a state space ODE to increase modeling capacity. Inspired by recent deep state space models (S4), we achieve speedups by leveraging a convolutional representation of LS4 which bypasses the explicit evaluation of hidden states. We show that LS4 significantly outperforms previous continuous-time generative models in terms of marginal distribution, classification, and prediction scores on real-world datasets in the Monash Forecasting Repository, and is capable of modeling highly stochastic data with sharp temporal transitions. LS4 sets state-of-the-art for continuous-time latent generative models, with significant improvement of mean squared error and tighter variational lower bounds on irregularly-sampled datasets, while also being x100 faster than other baselines on long sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 3*)\n\n### 3 related papers from Papers with Code\n\n#### 1. MCUFormer: Deploying Vision Transformers on Microcontrollers with Limited Memory\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Jiwen Lu, Jie zhou, Yansong Tang, Xiuwei Xu, Ziwei Wang, Yinan Liang*\n\n**Abstract:** Due to the high price and heavy energy consumption of GPUs, deploying deep models on IoT devices such as microcontrollers makes significant contributions for ecological AI. Conventional methods successfully enable convolutional neural network inference of high resolution images on microcontrollers, while the framework for vision transformers that achieve the state-of-the-art performance in many vision applications still remains unexplored. In this paper, we propose a hardware-algorithm co-optimizations method called MCUFormer to deploy vision transformers on microcontrollers with extremely limited memory, where we jointly design transformer architecture and construct the inference operator library to fit the memory resource constraint. More specifically, we generalize the one-shot network architecture search (NAS) to discover the optimal architecture with highest task performance given the memory budget from the microcontrollers, where we enlarge the existing search space of vision transformers by considering the low-rank decomposition dimensions and patch resolution for memory reduction. For the construction of the inference operator library of vision transformers, we schedule the memory buffer during inference through operator integration, patch embedding decomposition, and token overwriting, allowing the memory buffer to be fully utilized to adapt to the forward pass of the vision transformer. Experimental results demonstrate that our MCUFormer achieves 73.62\\% top-1 accuracy on ImageNet for image classification with 320KB memory on STM32F746 microcontroller. Code is available at https://github.com/liangyn22/MCUFormer.\n\n**Conference:** mcuformer-deploying-vision-tranformers-on\n\n**Published:** 2023-10-25\n\n\n\n#### 2. Bayesian Optimization in Variational Latent Spaces with Dynamic Compression\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Rika Antonova, Tianyu Li, Akshara Rai, Danica Kragic*\n\n**Abstract:** Data-efficiency is crucial for autonomous robots to adapt to new tasks and environments. In this work we focus on robotics problems with a budget of only 10-20 trials. This is a very challenging setting even for data-efficient approaches like Bayesian optimization (BO), especially when optimizing higher-dimensional controllers. Simulated trajectories can be used to construct informed kernels for BO. However, previous work employed supervised ways of extracting low-dimensional features for these. We propose a model and architecture for a sequential variational autoencoder that embeds the space of simulated trajectories into a lower-dimensional space of latent paths in an unsupervised way. We further compress the search space for BO by reducing exploration in parts of the state space that are undesirable, without requiring explicit constraints on controller parameters. We validate our approach with hardware experiments on a Daisy hexapod robot and an ABB Yumi manipulator. We also present simulation experiments with further comparisons to several baselines on Daisy and two manipulators. Our experiments indicate the proposed trajectory-based kernel with dynamic compression can offer ultra data-efficient optimization.\n\n**Published:** 2019-07-10\n\n\n\n#### 3. DaCapo: Accelerating Continuous Learning in Autonomous Systems for Video Analytics\n\n*From Search Query: adaptive state space model optimization hardware*\n\n*Jongse Park, Amir Yazdanbakhsh, Hardik Sharma, Yubin Lee, Seongryong Oh, Wonung Kim, Jinwoo Hwang, Changhun Oh, Yoonsung Kim*\n\n**Abstract:** Deep neural network (DNN) video analytics is crucial for autonomous systems such as self-driving vehicles, unmanned aerial vehicles (UAVs), and security robots. However, real-world deployment faces challenges due to their limited computational resources and battery power. To tackle these challenges, continuous learning exploits a lightweight \"student\" model at deployment (inference), leverages a larger \"teacher\" model for labeling sampled data (labeling), and continuously retrains the student model to adapt to changing scenarios (retraining). This paper highlights the limitations in state-of-the-art continuous learning systems: (1) they focus on computations for retraining, while overlooking the compute needs for inference and labeling, (2) they rely on power-hungry GPUs, unsuitable for battery-operated autonomous systems, and (3) they are located on a remote centralized server, intended for multi-tenant scenarios, again unsuitable for autonomous systems due to privacy, network availability, and latency concerns. We propose a hardware-algorithm co-designed solution for continuous learning, DaCapo, that enables autonomous systems to perform concurrent executions of inference, labeling, and training in a performant and energy-efficient manner. DaCapo comprises (1) a spatially-partitionable and precision-flexible accelerator enabling parallel execution of kernels on sub-accelerators at their respective precisions, and (2) a spatiotemporal resource allocation algorithm that strategically navigates the resource-accuracy tradeoff space, facilitating optimal decisions for resource allocation to achieve maximal accuracy. Our evaluation shows that DaCapo achieves 6.5% and 5.5% higher accuracy than a state-of-the-art GPU-based continuous learning systems, Ekya and EOMU, respectively, while consuming 254x less power.\n\n**Published:** 2024-03-21\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using adaptive state space models (SSMs), with a focus on hardware-efficient implementations, optimization techniques, and memory management, here are some key technical details and strategies:\n\n## Optimization Techniques for SSMs\n\n### Token Reduction and Pruning\nThe paper on \"Rethinking Token Reduction for State Space Models\" introduces a tailored, unified post-training token reduction method for SSMs. This approach integrates token importance and similarity, using a hybrid strategy of pruning and merging tokens. This method improves accuracy by 5.7% to 13.1% while reducing computational demands and memory requirements, which is crucial for maintaining hardware efficiency.\n\n### Structure-Aware State Fusion\nThe \"Spatial-Mamba\" model incorporates a structure-aware state fusion (SASF) mechanism, which enhances the capture of spatial dependencies in the latent state space. This involves using dilated convolutions to re-weight and merge nearby state variables, improving the model's adaptability and performance. Additionally, optimizations such as re-parameterization techniques and CUDA kernel optimization accelerate the model by at least 30%.\n\n## Hardware-Aware Implementations\n\n### Quantization and System Co-design\nThe work on \"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving\" highlights several hardware-aware optimization techniques. These include:\n- **W4A8 and 4-bit KV-cache quantization**: Reducing the precision of weights and activations to lower bits while maintaining performance.\n- **Progressive quantization of weights**: Complying with 8-bit compute after dequantizing 4-bit weights to 8-bits.\n- **SmoothAttention method**: Reducing the error of 4-bit quantization of Key cache compatible with RoPE operations.\nThese methods are designed to optimize memory usage and latency, making the model more efficient for hardware deployment.\n\n### Reparameterization for Efficiency\nAnother approach involves reparameterizing traditional LLM layers with MatMul operations into Shift-Add and LUT query-based operations. This method quantizes weight matrices into binary matrices paired with group-wise scaling factors, reducing accuracy loss through multi-objective optimization and automated bit allocation strategies. This reparameterization technique can significantly reduce memory usage and latency.\n\n## Memory Management and Efficiency\n\n### Selective State Updates\nThe Mamba model, which incorporates a data-dependent selection mechanism, simplifies computation and architecture in a hardware-friendly way. This selective mechanism allows for efficient handling of long-range dependencies while maintaining linear complexity, which is beneficial for memory management and computational efficiency.\n\n### Overlapped Convolutions and Local Enhancement\nIn the Spatial-Mamba model, using overlapped convolutions and local enhancement techniques (such as LPU) enriches the local information available between image patches before processing by the SASF module. This approach improves accuracy and captures structural dependencies more effectively, contributing to better memory and computational efficiency.\n\n## Stability and Robustness\n\n### Stable Reparameterization Techniques\nTo ensure stability, using stable reparameterization techniques such as the 'perturb-then-diagonalize' (PTD) methodology can be beneficial. This approach, based on pseudospectral theory, helps in the approximate diagonalization of non-normal matrices defining SSMs, enhancing the robustness of the model[Analysis Note, References].\n\n### Careful Initialization and Regularization\nCareful initialization schemes and regularization methods are crucial for maintaining stability and robustness in SSMs. Data-dependent generalization bounds can guide the initialization and scaling of model parameters, improving the model's robustness to different temporal patterns in sequence data[Analysis Note, References].\n\nBy integrating these strategies, researchers can develop an adaptive state space model that is not only highly efficient in terms of hardware resources but also robust and adaptable, thereby improving the overall performance of autoregressive language models.\n",
            "analysis": "Analysis Note\n\n1. Summary of Analysis:\n\nAfter analyzing the search results and parent architectures, several key themes emerge for combining Mamba2 and FastTTTLinear:\n\na) Stability Considerations:\n- Both architectures require careful handling of stability\n- Initialization and regularization are crucial\n- Need to balance adaptivity with stability\n\nb) Optimization Approaches:\n- Data-dependent generalization bounds\n- Careful parameter initialization schemes\n- Regularization methods for enhanced performance\n\nc) Integration Opportunities:\n- Combine SSD framework with test-time training\n- Leverage both architectures' linear complexity advantages\n- Maintain hardware efficiency while enhancing adaptability\n\n2. Useful References with Excerpts:\n\nFrom \"From generalization analysis to optimization designs for state space models\":\n\"We give a data-dependent generalization bound for SSMs, showing an interplay between the SSM parameters and the temporal dependencies of the training sequences. Leveraging the generalization bound, we (1) set up a scaling rule for model initialization based on the proposed generalization measure, which significantly improves the robustness of the output value scales on SSMs to different temporal patterns in the sequence data.\"\n\nFrom \"StableSSM\":\n\"We prove that state-space models without any reparameterization exhibit a memory limitation similar to that of traditional RNNs: the target relationships that can be stably approximated by state-space models must have an exponential decaying memory.\"\n\nFrom \"Robustifying State-space Models\":\n\"We introduce a generic, backward-stable 'perturb-then-diagonalize' (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs.\"\n\n3. Key Insights:\n\na) Stability Enhancement:\n- Use stable reparameterization techniques\n- Implement careful initialization schemes\n- Apply regularization methods for robustness\n\nb) Performance Optimization:\n- Leverage data-dependent generalization bounds\n- Use selective state updates for efficiency\n- Maintain hardware-aware implementation\n\nc) Integration Strategy:\n- Combine SSD framework with test-time training\n- Preserve linear complexity of both architectures\n- Ensure stable parameter updates\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific techniques for combining selective SSMs with test-time training\n2. Hardware optimization strategies for the combined architecture\n3. Methods for ensuring stability in adaptive updates\n\n5. References:\n\n[1] Liu, F., & Li, Q. (2024). From generalization analysis to optimization designs for state space models. arXiv preprint arXiv:2405.02670.\n\n[2] Wang, S., & Li, Q. (2023). StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization. International Conference on Machine Learning.\n\n[3] Yu, A., et al. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. arXiv preprint.\n\n[4] Gu, A., et al. (2022). On the Parameterization and Initialization of Diagonal State Space Models. Neural Information Processing Systems.\n\n[5] Alonso, C.A., et al. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint arXiv:2401.12184."
        },
        {
            "ready": false,
            "query": "\"state space models adaptive mechanisms\", \"memory efficient parameter updates neural networks\", \"practical implementation adaptive neural networks\"",
            "detail": "Find papers discussing practical implementations and performance evaluations of adaptive state space models, focusing on:\n1. Integration techniques for SSMs and adaptive mechanisms\n2. Memory-efficient parameter update strategies\n3. Real-world performance benchmarks and metrics\n4. Implementation challenges and solutions",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing practical implementations and performance evaluations of adaptive state space models, focusing on:\n1. Integration techniques for SSMs and adaptive mechanisms\n2. Memory-efficient parameter update strategies\n3. Real-world performance benchmarks and metrics\n4. Implementation challenges and solutions\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.95)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 1/27 (Score: 0.98)*\n\n```\n# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.\n```\n\n##### *Relevant Chunk: No. 4/27 (Score: 0.93)*\n\n```\nThis allows to remove a mixing layer present in the postprocessing of the S4 and S4D output. ## D. Linear Recurrent Unit (LRU)\n\nThe LRU model attempts to simplify previous SSM proposals by unveiling their essential components. One of the main contributions of LRU is to explicitly encode long-range memory through eigenvalues. This allows to move away from the HiPPO theory and directly use a discrete-time model together with concepts of marginal stability from control theory. a) Parameterization: The LRU model directly parameterizes the discrete-time dynamics (4), i.e.,\n\n$$\n\\bar{A}=e^{-e^{\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)}+i \\operatorname{idiag}\\left(\\theta_{1}, \\ldots, \\theta_{p}\\right)}, \\quad \\bar{B}=e^{\\gamma} \\Gamma\n$$\n\nwith $i$ the complex unit, $\\lambda_{j}, \\theta_{j} \\in \\mathbb{R} \\forall j=1, \\ldots, p, \\Gamma \\in \\mathbb{C}^{p \\times q}$ a dense complex-valued matrix, and $\\gamma \\in \\mathbb{R}$. Notice that this parameterization directly represents the diagonal entries of $\\bar{A}$, and therefore the eigenvalues in polar coordinates, i.e. $a_{j}=$ $r_{j}+i \\theta_{j}$ where $r_{j}=e^{-e^{\\lambda_{j}}}$, is constrained to the interval $[0,1]$ by construction. This is also the first parameterization that does not have shared parameters between $\\bar{A}$ and $\\bar{B}$. b) Discretization: The LRU model is the first of the SSMs that is not seen as a discretization of a continuous-time model. Instead, a discrete parametrization of $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ is directly used. c) Structure and Initialization: The structure of the model is identical to S5, where a MIMO system -as opposed to $q$ SISO subsystems- is considered. Given the parametrization 10p, Lemma 2.2 is automatically enforced by constraining the eigenvalues of $\\bar{A}$ to lie in the unitdisk. Hence, the initialization is directly performed in polar coordinates by defining a range for $r$ and $\\theta$ in which $r$ and $\\theta$ are uniformly sampled, resulting in the eigenvalues shown in Figure 2\nd) Implementation: Similar to LRU, the model is implemented using a parallel scan algorithm [16] for both training and inference. e) Scaffolding: The scaffolding used in LRU is identical to the one used in S 5 . ## E. Scan Selective Structured State Space Sequence Model (S6)\n\nThe S6 parametrization [10] introduces for the first time a linear time-varying representation of the dynamics (3). The time-varying nature of the system stems from the matrices $\\bar{A}_{k}, \\bar{B}_{k}$ and $\\bar{C}_{k}$ being functions of the input $u(k)$ at every timestep $k$, which the authors refer to as selectivity. Although more expressive, the time-varying representation presents computational challenges. The main contribution of this paper is to address those so the more expressive time-varying nature of the system can be exploited in practice. a) Parametrization: Similar to S4D, the S6 parametrization relies on a time-invariant diagonal $A$ matrix (8) as per Assumption 2.1. The novelty of the S6 parametrization is that $B$ and $C$ are parameterized to be time-varying given their input-dependent nature:\n\n$$\nB_{k}=W_{B} u(k) \\quad C_{k}=W_{C} u(k)\n$$\n\nwhere $W_{B}$ and $W_{C}$ are linear projection matrices of appropriate dimensions. ![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-6.jpg?height=467&width=1781&top_left_y=162&top_left_x=172)\n\nFig. 2: Complex plane representation of the unit disk and the eigenvalues of discrete-time dynamics matrix $\\bar{A}$ (4) resulting from the initialization method in each of the models S4, S4D, S5, LRU, S6, and RG-LRU. Since the initialization of S6 and RG-LRU are input dependent, we plot the initialization for two sample inputs (blue and orange). b) Discretization: Similar to S4D, the S6 model also uses exact discretization to compute the discrete-time dynamics (4). However, in this case the time-step $\\Delta_{k}$ is itself time-varying since it is a function of the input\n$\\Delta_{k}=\\sigma\\left(W_{\\Delta} u(k)\\right), \\quad \\bar{A}_{k}=e^{\\Delta_{k} A}, \\quad \\bar{B}_{k}=\\left(\\Delta_{k} A\\right)^{-1}\\left(\\bar{A}_{k}-I\\right) \\Delta_{k} B_{k}$,\n$\\bar{C}_{k}=C_{k}$ and $\\bar{D}_{k}=D_{k}$, with $W_{\\Delta} \\in \\mathbb{R}^{1 \\times q}$ and $\\sigma(\\cdot)$ the softplus function. c) Structure and Initialization: Similar to S5, the model is structured in a MIMO manner. In order to initialize the dynamic matrix $A$, its diagonal parametrization is exploited: $\\lambda_{i}=-i \\forall i=1, \\ldots, p$, ensuring that the eigenvalues lie in the negative halfplane. Due to the time-varying nature of the discretization step $\\Delta_{k}$, the eigenvalues of the discrete-time matrices $\\bar{A}_{k}$ have an initialization that is input-dependent as depicted in Figure 2 However, in order to enforce Lemma 2.2 the resulting eigenvalues are guaranteed to lie in the unit disk since $\\Delta_{k}$ and $A$ in (12) are positive and negative, respectively. d) Implementation: One of the main contributions of the work in [10] is to provide an efficient implementation of the time-varying dynamics (4) with matrices (11) and 12 both at inference and training time. In general, the timevarying nature of the S 6 model renders the convolutional representation too computationally expensive for practical use. To overcome these limitations, the S6 paper presents a highly customized variation of the parallel scan algorithm [16] for both training and inference. e) Scaffolding: Another innovation of the work in [10] is the introduction of a new scaffolding: the Mamba scaffolding. Here, the pre-processing relies on both linear and nonlinear maps. The map of the upper signal (linear map) is a linear projection followed by a causal convolution, while the map of the lower signal (nonlinear map) is a linear projection followed by a SiLU nonlinearity. The post-processing is once again a gating function similar to previous scaffolding proposals. ## F. Real-Gated Linear Recurrent Unit (RG-LRU)\n\nThe RG-LRU model is a derivative of the well-known long short-term memory (LSTM) model [27] and therefore offers a different perspective on SSM models. The RG-LRU model fuses ideas from LSTMs, LRU, and S6. a) Parametrization: Following S6, RG-LRU also relies on a time-varying parametrization of the linear dynamics. However, while all previous SSM proposals rely on output feedback dynamics, the RG-LRU model introduces for the first time a state feedback model where $C$ and $D$ are not present. The $A$ and $B$ matrices are then parameterized as\n\n$$\n\\bar{A}_{k}=e^{-c \\phi\\left(W_{A}\\right) \\sigma\\left(W_{\\Delta} u(k)\\right)}, \\quad \\bar{B}_{k}=\\sqrt{1-A_{k}^{2}} \\sigma\\left(W_{B} u(k)\\right)\n$$\n\nwhere $W_{\\Delta}, W_{A}, W_{B}$ are linear projection matrices of appropriate dimensions, $c \\in \\mathbb{R}$ is a scalar constant ${ }_{4}^{4} \\phi(\\cdot)$ is the softplus function, and $\\sigma(\\cdot)$ is the sigmoid function. The operation $\\sqrt{1-A_{k}^{2}}$ is computed element-wise for each entry of $A_{k}$. b) Discretization: Similar to the LRU model, the RGLRU model does not rely on a continuous-time representation and instead directly parametrizes the discrete matrices $\\bar{A}_{k}, \\bar{B}_{k}$. c) Structure and Initialization: Similar to LRU, the RG-LRU model is structured as a MIMO system. Taking inspiration from LSTMs, this models assumes the state dimension to be equal to the input dimension, i.e., $p=q$. The linear projection matrices $W_{\\Delta}, W_{A}, W_{B}$ are initialized with standard initialization methods, e.g. Glorot [14], resulting in the eigenvalues shown in Figure 2 Given the parameterization of $\\bar{A}_{k}$ in 13), its eigenvalues are restricted to the unit disk by construction. d) Implementation: Due to the time-varying nature of the RG-LRU model, it faces the same challenges as the S6 model. Therefore, it also uses a customized variation of the parallel scan algorithm [16] to compute the outputs at both training and inference time. e) Scaffolding: The RG-LRU model uses the same scaffolding as the S6 model, Mamba. However, this work also introduces two additional task-specific scaffoldings around the basic Mamba scaffolding that are tailored to language modelling: Hawk and Griffin [21, Section 2]. [^3]| Model | LRA Task [\\%] |  |  |  |  |  |  |\n| :--- | :---: | :---: | :---: | :---: | :---: | :---: | :---: | :---: |\n|  | ListOps | Text | Retrieval | Image | Pathfinder | Path-X | avg. |\n| Random | 10.00 | 50.00 | 50.00 | 10.00 | 50.00 | 50.00 | 36.67 |\n| Transformer [2] (paper results) | 36.37 | 64.27 | 57.46 | 42.44 | 71.40 | FAIL | 53.66 |\n| S4 [12] (paper results) | 59.60 | 86.82 | 90.90 | 88.65 | 94.20 | 96.35 | 86.09 |\n| S4D [18] (paper results) | 60.52 | 87.34 | 91.09 | 88.19 | 93.96 | 92.80 | 85.65 |\n| S5 [19] (paper results) | $\\mathbf{6 2 . 1 5}$ | 89.31 | $\\mathbf{9 1 . 4 0}$ | 88.00 | $\\mathbf{9 5 . 3 3}$ | $\\mathbf{9 8 . 5 8}$ | $\\mathbf{8 7 . 4 6}$ |\n| LRU [20] (paper results) | 60.20 | $\\mathbf{8 9 . 4 0}$ | 89.90 | $\\mathbf{8 9 . 0 0}$ | 95.10 | 94.20 | 86.30 |\n| S6 [10] | 38.02 | 82.98 | 72.14 | 69.82 | 69.26 | 67.32 | 66.59 |\n| RG-LRU [21] | 32.34 | 71.75 | 66.58 | 61.15 | 73.38 | 69.53 | 62.45 |\n\nTABLE II: Model performance in terms of test accuracy on the LRA benchmark. The first entry (Random) represents the performance of random guessing on the task, i.e., indicating the baseline above which a model is considered to have learned a meaningful representation. Models failing to exceed this baseline on a task are marked as FAIL. The best model on each task is highlighted in bold. ## IV. Performance in Practice\n\nIn this section, we evaluate the SSM proposals reviewed in Section III on the long-range arena (LRA) benchmark [2]. This benchmark evaluates the models in terms of their reasoning ability and their handling of diverse data types, which is one of the strengths of SSMs. We first introduce the benchmark, before presenting the empirical evaluation. ## A. Long-Range Arena (LRA) Benchmark\n\nThe goal of the LRA benchmark is to evaluate the reasoning capabilities of sequence models in diverse ways. The benchmark consists of 5 different tasks, which we summarize in the following. For more details on the benchmark and the individual tasks, we refer to [2]. a) List Operations (ListOps): This task evaluates a model's ability to capture hierarchical dependencies over long contexts. The goal is to predict the result of a mathematical operation consisting of nested mean, median, max, and $\\min$ operations ${ }^{5}$ The task is a ten-way classification task with maximal input lengths of 2 k . b) Text Classification (Text): This task evaluates a model's ability to capture the tone of long tokenized texts. The dataset consists of IMDb movie reviews, which need to be classified as negative or positive in tone. The task is a binary classification task with maximal input lengths of 4 k . c) Document Retrieval (Retrieval): This task evaluates a model's ability to compress long sequences into representations that are suitable for similarity matching. The dataset consists of tokenized papers published by the American Academy of Neurology (AAN), which need to be classified in having a citation link or not. The task is a binary classification task with maximal input lengths of 8 k . d) Image Classification (Image): This task evaluates a model's ability to learn 2D spatial relations from a 1D vector. The dataset consists of vectorized images, which depict one of ten possible classes, e.g. a horse or a car. The task is a ten-way classification task with maximal input lengths of 1 k . [^4]e) Long-Range Spacial Dependency: This task evaluates a model's ability to learn spacial dependencies in a vectorized image. The dataset consists of images, which depict two circles and multiple dashed paths. The goal is to evaluate whether the two circles are connected by any of the present paths or not. The task is therefore a binary classification task and is divided into two subtasks, which only differ in the size of the image. The first subtask has inputs of length 2 k and we will refer to it as Pathfinder; the second subtask has a maximal input length of 16 k and we will refer to it as Path-X. ## B. Empirical Evaluation of SSM Proposals\n\nThe empirical performance of the reviewed SSM proposals, the Transformer [1], and random guessing are reported in Table II We include the performance of the Transformer as a baseline, since they are the dominant architecture in large language models and sequence modelling. For S4, S4D, S5, and LRU we report the performance of the best variant from the original papers in order to present the most competitive results. Other variants of these models might perform better on tasks not included in the LRA benchmark; for more details on these variants we refer to the original papers. Since performance on the LRA benchmark of S6 and RGLRU have not been reported in the literature, we provide the results of our own implementation of these architectures, which we make available here ${ }^{6}$ The hyperparameters of the models and training details of our implementation are stated in the public code repository. On the LRA benchmark, the LTI-based models S4, S4D, S5, LRU outperform the LTV-based models S6, RG-LRU and the Transformer. From a control theoretic perspective this is surprising, since a general LTV definition encompasses LTI systems as a special case, i.e., a LTV system should perform at least as well as a LTI system. However, this is not the case for the particular time-varying parametrization of S6 or RG-LRU, since e.g. $\\bar{A}=\\bar{A}_{k} \\forall k$ cannot be achieved. We attempted to improve the performance of the LTV-based\n\n[^5]models by changing the initialization of S6 and RG-LRU and forcing the input-dependent eigenvalues of $\\bar{A}_{k} \\forall k$ closer to marginal stability according to Lemma 2.2 However, this resulted in both models to perform considerably worse or fail to learn anything meaningful at all. While marginally stable eigenvalues appear to be important for the LTI-based models, the same is not true for LTV-based models. To date, this behavior is not well understood. Lastly, even though the LTVbased models are closely related to the Transformer [28], they generally perform better on the LRA benchmark. ## V. CONCLUSION AND Future Opportunities\n\nIn this paper, we have provided an overview of stateof-the-art state space models (SSM) and explored their features from a control theoretic perspective. In doing this, we highlighted the many connections to standard control theoretic concepts such as the connection between memory and marginal-stability. Additionally, we compared the reviewed SSMs on the long-range arena (LRA) benchmark, finding that the more recent LTV-based SSMs perform worse than their LTI-based counterparts. From a control theoretic perspective, this raises many interesting research questions concerning a LTV parametrization that attains the same performance as the LTI models, as well as a deeper understanding on the role of the eigenvalues in the LTVbased models. SSMs, particularly the LTV versions, rely on dynamics where the dynamic matrices depend on the input (excitation) to the system. However, in the SSM literature the theoretical properties arising from these dynamics remains poorly understood. The evident connections between SSMs and linearsystem theory give rise to ample opportunities to provide explainability to large foundational models. Moreover, as seen with the LRU model, control theoretic insights have the potential to inform better designs for SSMs.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.92)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.92)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.90)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.90)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.90)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.90)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"state space models adaptive mechanisms\", \"memory efficient parameter updates neural networks\", \"practical implementation adaptive neural networks\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Structured State Space Models for In-Context Reinforcement Learning\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Chris Xiaoxuan Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, J. Foerster, Satinder Singh, Feryal M. P. Behbahani*\n\n**TL;DR:** The results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks.\n\n**Abstract:** Structured state space sequence (S4) models have recently achieved state-of-the-art performance on long-range sequence modeling tasks. These models also have fast inference speeds and parallelisable training, making them potentially useful in many reinforcement learning settings. We propose a modification to a variant of S4 that enables us to initialise and reset the hidden state in parallel, allowing us to tackle reinforcement learning tasks. We show that our modified architecture runs asymptotically faster than Transformers in sequence length and performs better than RNN's on a simple memory-based task. We evaluate our modified architecture on a set of partially-observable environments and find that, in practice, our model outperforms RNN's while also running over five times faster. Then, by leveraging the model's ability to handle long-range sequences, we achieve strong performance on a challenging meta-learning task in which the agent is given a randomly-sampled continuous control environment, combined with a randomly-sampled linear projection of the environment's observations and actions. Furthermore, we show the resulting model can adapt to out-of-distribution held-out tasks. Overall, the results presented in this paper show that structured state space models are fast and performant for in-context reinforcement learning tasks. We provide code at https://github.com/luchris429/popjaxrl.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 61  (*Influential: 7*)\n\n#### 2. On the Parameterization and Initialization of Diagonal State Space Models\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Albert Gu, Ankit Gupta, Karan Goel, Christopher R\u00e9*\n\n**TL;DR:** This work systematically describes various design choices in parameterizing and computing diagonal SSMs, and performs a controlled empirical study ablating the effects of these choices.\n\n**Abstract:** State space models (SSM) have recently been shown to be very effective as a deep learning layer as a promising alternative to sequence models such as RNNs, CNNs, or Transformers. The first version to show this potential was the S4 model, which is particularly effective on tasks involving long-range dependencies by using a prescribed state matrix called the HiPPO matrix. While this has an interpretable mathematical mechanism for modeling long dependencies, it introduces a custom representation and algorithm that can be difficult to implement. On the other hand, a recent variant of S4 called DSS showed that restricting the state matrix to be fully diagonal can still preserve the performance of the original model when using a specific initialization based on approximating S4's matrix. This work seeks to systematically understand how to parameterize and initialize such diagonal state space models. While it follows from classical results that almost all SSMs have an equivalent diagonal form, we show that the initialization is critical for performance. We explain why DSS works mathematically, by showing that the diagonal restriction of S4's matrix surprisingly recovers the same kernel in the limit of infinite state dimension. We also systematically describe various design choices in parameterizing and computing diagonal SSMs, and perform a controlled empirical study ablating the effects of these choices. Our final model S4D is a simple diagonal version of S4 whose kernel computation requires just 2 lines of code and performs comparably to S4 in almost all settings, with state-of-the-art results for image, audio, and medical time-series domains, and averaging 85\\% on the Long Range Arena benchmark.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 212  (*Influential: 37*)\n\n#### 3. How to Train Your HiPPO: State Space Models with Generalized Orthogonal Basis Projections\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Albert Gu, Isys Johnson, Aman Timalsina, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A more general and intuitive formulation of the HiPPO framework is derived, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies.\n\n**Abstract:** Linear time-invariant state space models (SSM) are a classical model from engineering and statistics, that have recently been shown to be very promising in machine learning through the Structured State Space sequence model (S4). A core component of S4 involves initializing the SSM state matrix to a particular matrix called a HiPPO matrix, which was empirically important for S4's ability to handle long sequences. However, the specific matrix that S4 uses was actually derived in previous work for a particular time-varying dynamical system, and the use of this matrix as a time-invariant SSM had no known mathematical interpretation. Consequently, the theoretical mechanism by which S4 models long-range dependencies actually remains unexplained. We derive a more general and intuitive formulation of the HiPPO framework, which provides a simple mathematical interpretation of S4 as a decomposition onto exponentially-warped Legendre polynomials, explaining its ability to capture long dependencies. Our generalization introduces a theoretically rich class of SSMs that also lets us derive more intuitive S4 variants for other bases such as the Fourier basis, and explains other aspects of training S4, such as how to initialize the important timescale parameter. These insights improve S4's performance to 86% on the Long Range Arena benchmark, with 96% on the most difficult Path-X task.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 66  (*Influential: 5*)\n\n#### 4. Training Neural Networks with Fixed Sparse Masks\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Yi-Lin Sung, Varun Nair, Colin Raffel*\n\n**TL;DR:** This paper shows that it is possible to induce a fixed sparse mask on the model's parameters that selects a subset to update over many iterations that matches or exceeds the performance of other methods for training with sparse updates while being more efficient in terms of memory usage and communication costs.\n\n**Abstract:** During typical gradient-based training of deep neural networks, all of the model's parameters are updated at each iteration. Recent work has shown that it is possible to update only a small subset of the model's parameters during training, which can alleviate storage and communication requirements. In this paper, we show that it is possible to induce a fixed sparse mask on the model's parameters that selects a subset to update over many iterations. Our method constructs the mask out of the $k$ parameters with the largest Fisher information as a simple approximation as to which parameters are most important for the task at hand. In experiments on parameter-efficient transfer learning and distributed training, we show that our approach matches or exceeds the performance of other methods for training with sparse updates while being more efficient in terms of memory usage and communication costs. We release our code publicly to promote further applications of our approach.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 159  (*Influential: 13*)\n\n#### 5. LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Yi-Lin Sung, Jaemin Cho, Mohit Bansal*\n\n**TL;DR:** LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections, and achieves higher accuracy than Adapter and LoRA in a low-memory regime.\n\n**Abstract:** Fine-tuning large pre-trained models on downstream tasks has been adopted in a variety of domains recently. However, it is costly to update the entire parameter set of large pre-trained models. Although recently proposed parameter-efficient transfer learning (PETL) techniques allow updating a small subset of parameters (e.g. only using 2% of parameters) inside a pre-trained backbone network for a new task, they only reduce the training memory requirement by up to 30%. This is because the gradient computation for the trainable parameters still requires backpropagation through the large pre-trained backbone model. To address this, we propose Ladder Side-Tuning (LST), a new PETL technique that can reduce training memory requirements by more substantial amounts. Unlike existing parameter-efficient methods that insert additional parameters inside backbone networks, we train a ladder side network, a small and separate network that takes intermediate activations as input via shortcut connections (called ladders) from backbone networks and makes predictions. LST has significantly lower memory requirements than previous methods, because it does not require backpropagation through the backbone network, but instead only through the side network and ladder connections. We evaluate our method with various models (T5 and CLIP-T5) on both NLP (GLUE) and vision-and-language (VQA, GQA, NLVR2 , MSCOCO) tasks. LST saves 69% of the memory costs to fine-tune the whole network, while other methods only save 26% of that in similar parameter usages (hence, 2.7x more memory savings). Moreover, LST achieves higher accuracy than Adapter and LoRA in a low-memory regime. To further show the advantage of this better memory efficiency, we also apply LST to larger T5 models, attaining better GLUE performance than full fine-tuning and other PETL methods. The accuracy-efficiency trade-off also holds on VL tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 177  (*Influential: 26*)\n\n#### 6. Low-rank lottery tickets: finding efficient low-rank neural networks via matrix differential equations\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Steffen Schotth\u00f6fer, Emanuele Zangrando, J. Kusch, Gianluca Ceruti, Francesco Tudisco*\n\n**TL;DR:** A novel algorithm to find efficient low-rank subnetworks that are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced.\n\n**Abstract:** Neural networks have achieved tremendous success in a large variety of applications. However, their memory footprint and computational demand can render them impractical in application settings with limited hardware or energy resources. In this work, we propose a novel algorithm to find efficient low-rank subnetworks. Remarkably, these subnetworks are determined and adapted already during the training phase and the overall time and memory resources required by both training and evaluating them are significantly reduced. The main idea is to restrict the weight matrices to a low-rank manifold and to update the low-rank factors rather than the full matrix during training. To derive training updates that are restricted to the prescribed manifold, we employ techniques from dynamic model order reduction for matrix differential equations. This allows us to provide approximation, stability, and descent guarantees. Moreover, our method automatically and dynamically adapts the ranks during training to achieve the desired approximation accuracy. The efficiency of the proposed method is demonstrated through a variety of numerical experiments on fully-connected and convolutional networks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 32  (*Influential: 0*)\n\n#### 7. Quantum Ridgelet Transform: Winning Lottery Ticket of Neural Networks with Quantum Computation\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*H. Yamasaki, Sathyawageeswar Subramanian, Satoshi Hayakawa, Sho Sonoda*\n\n**TL;DR:** A quantum Ridgelet transform (QRT) is developed, which implements the ridgelet transform of a quantum state within a linear runtime $O(D)$ of quantum computation and it is shown that one can use QRT as a fundamental subroutine for QML to efficiently find a sparse trainable subnetwork of large shallow wide neural networks without conducting large-scale optimization of the original network.\n\n**Abstract:** A significant challenge in the field of quantum machine learning (QML) is to establish applications of quantum computation to accelerate common tasks in machine learning such as those for neural networks. Ridgelet transform has been a fundamental mathematical tool in the theoretical studies of neural networks, but the practical applicability of ridgelet transform to conducting learning tasks was limited since its numerical implementation by conventional classical computation requires an exponential runtime $\\exp(O(D))$ as data dimension $D$ increases. To address this problem, we develop a quantum ridgelet transform (QRT), which implements the ridgelet transform of a quantum state within a linear runtime $O(D)$ of quantum computation. As an application, we also show that one can use QRT as a fundamental subroutine for QML to efficiently find a sparse trainable subnetwork of large shallow wide neural networks without conducting large-scale optimization of the original network. This application discovers an efficient way in this regime to demonstrate the lottery ticket hypothesis on finding such a sparse trainable neural network. These results open an avenue of QML for accelerating learning tasks with commonly used classical neural networks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 8. Practical Quasi-Newton Methods for Training Deep Neural Networks\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*D. Goldfarb, Yi Ren, Achraf Bahamou*\n\n**TL;DR:** This work proposes a new damping approach to keep the upper as well as the lower bounds of the BFGS and L-BFGS approximations bounded, and outperformed or performed comparably to KFAC and state-of-the-art first-order stochastic methods.\n\n**Abstract:** We consider the development of practical stochastic quasi-Newton, and in particular Kronecker-factored block-diagonal BFGS and L-BFGS methods, for training deep neural networks (DNNs). In DNN training, the number of variables and components of the gradient $n$ is often of the order of tens of millions and the Hessian has $n^2$ elements. Consequently, computing and storing a full $n \\times n$ BFGS approximation or storing a modest number of (step, change in gradient) vector pairs for use in an L-BFGS implementation is out of the question. In our proposed methods, we approximate the Hessian by a block-diagonal matrix and use the structure of the gradient and Hessian to further approximate these blocks, each of which corresponds to a layer, as the Kronecker product of two much smaller matrices. This is analogous to the approach in KFAC, which computes a Kronecker-factored block-diagonal approximation to the Fisher matrix in a stochastic natural gradient method. Because the indefinite and highly variable nature of the Hessian in a DNN, we also propose a new damping approach to keep the upper as well as the lower bounds of the BFGS and L-BFGS approximations bounded. In tests on autoencoder feed-forward neural network models with either nine or thirteen layers applied to three datasets, our methods outperformed or performed comparably to KFAC and state-of-the-art first-order stochastic methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 88  (*Influential: 8*)\n\n#### 9. Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*Juanhui Li, Harry Shomer, Haitao Mao, Shenglai Zeng, Yao Ma, Neil Shah, Jiliang Tang, Dawei Yin*\n\n**TL;DR:** A more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT) is created, which samples hard negative samples via multiple heuristics and helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations.\n\n**Abstract:** Link prediction attempts to predict whether an unseen edge exists based on only a portion of edges of a graph. A flurry of methods have been introduced in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple pitfalls currently exist that hinder our ability to properly evaluate these new methods. These pitfalls mainly include: (1) Lower than actual performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, and (3) An unrealistic evaluation setting that uses easy negative samples. To overcome these challenges, we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset and hyperparameter search settings. We then create a more practical evaluation setting based on a Heuristic Related Sampling Technique (HeaRT), which samples hard negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations. Our implementation and data are available at https://github.com/Juanhui28/HeaRT\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 30  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. Meta Dialogue Policy Learning\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Yumo Xu, Chenguang Zhu, Baolin Peng, Michael Zeng*\n\n**Abstract:** Dialog policy determines the next-step actions for agents and hence is\ncentral to a dialogue system. However, when migrated to novel domains with\nlittle data, a policy model can fail to adapt due to insufficient interactions\nwith the new environment. We propose Deep Transferable Q-Network (DTQN) to\nutilize shareable low-level signals between domains, such as dialogue acts and\nslots. We decompose the state and action representation space into feature\nsubspaces corresponding to these low-level components to facilitate\ncross-domain knowledge transfer. Furthermore, we embed DTQN in a meta-learning\nframework and introduce Meta-DTQN with a dual-replay mechanism to enable\neffective off-policy training and adaptation. In experiments, our model\noutperforms baseline models in terms of both success rate and dialogue\nefficiency on the multi-domain dialogue dataset MultiWOZ 2.0.\n\n**Published:** 2020-06-03T23:53:06Z  (*Updated: 2020-06-03T23:53:06Z*)\n\n\n\n#### 2. MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News\n  Detection\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Abhijit Suprem, Calton Pu*\n\n**Abstract:** COVID-19 related misinformation and fake news, coined an 'infodemic', has\ndramatically increased over the past few years. This misinformation exhibits\nconcept drift, where the distribution of fake news changes over time, reducing\neffectiveness of previously trained models for fake news detection. Given a set\nof fake news models trained on multiple domains, we propose an adaptive\ndecision module to select the best-fit model for a new sample. We propose\nMiDAS, a multi-domain adaptative approach for fake news detection that ranks\nrelevancy of existing models to new samples. MiDAS contains 2 components: a\ndoman-invariant encoder, and an adaptive model selector. MiDAS integrates\nmultiple pre-trained and fine-tuned models with their training data to create a\ndomain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of\nthe invariant embedding space to estimate each model's relevance to a new\nsample. Higher ranked models provide predictions, and lower ranked models\nabstain. We evaluate MiDAS on generalization to drifted data with 9 fake news\ndatasets, each obtained from different domains and modalities. MiDAS achieves\nnew state-of-the-art performance on multi-domain adaptation for\nout-of-distribution fake news classification.\n\n**Published:** 2022-05-19T19:36:08Z  (*Updated: 2022-05-19T19:36:08Z*)\n\n\n\n#### 3. Efficient Fine-Tuning of BERT Models on the Edge\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Danilo Vucetic, Mohammadreza Tayaranian, Maryam Ziaeefard, James J. Clark, Brett H. Meyer, Warren J. Gross*\n\n**Abstract:** Resource-constrained devices are increasingly the deployment targets of\nmachine learning applications. Static models, however, do not always suffice\nfor dynamic environments. On-device training of models allows for quick\nadaptability to new scenarios. With the increasing size of deep neural\nnetworks, as noted with the likes of BERT and other natural language processing\nmodels, comes increased resource requirements, namely memory, computation,\nenergy, and time. Furthermore, training is far more resource intensive than\ninference. Resource-constrained on-device learning is thus doubly difficult,\nespecially with large BERT-like models. By reducing the memory usage of\nfine-tuning, pre-trained BERT models can become efficient enough to fine-tune\non resource-constrained devices. We propose Freeze And Reconfigure (FAR), a\nmemory-efficient training regime for BERT-like models that reduces the memory\nusage of activation maps during fine-tuning by avoiding unnecessary parameter\nupdates. FAR reduces fine-tuning time on the DistilBERT model and CoLA dataset\nby 30%, and time spent on memory operations by 47%. More broadly, reductions in\nmetric performance on the GLUE and SQuAD datasets are around 1% on average.\n\n**Published:** 2022-05-03T14:51:53Z  (*Updated: 2022-05-03T14:51:53Z*)\n\n\n\n#### 4. Learning to Ignore: Long Document Coreference with Bounded Memory Neural\n  Networks\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Shubham Toshniwal, Sam Wiseman, Allyson Ettinger, Karen Livescu, Kevin Gimpel*\n\n**Abstract:** Long document coreference resolution remains a challenging task due to the\nlarge memory and runtime requirements of current models. Recent work doing\nincremental coreference resolution using just the global representation of\nentities shows practical benefits but requires keeping all entities in memory,\nwhich can be impractical for long documents. We argue that keeping all entities\nin memory is unnecessary, and we propose a memory-augmented neural network that\ntracks only a small bounded number of entities at a time, thus guaranteeing a\nlinear runtime in length of document. We show that (a) the model remains\ncompetitive with models with high memory and computational requirements on\nOntoNotes and LitBank, and (b) the model learns an efficient memory management\nstrategy easily outperforming a rule-based strategy.\n\n**Published:** 2020-10-06T15:16:31Z  (*Updated: 2020-11-17T02:31:30Z*)\n\n\n\n#### 5. Adapting by Pruning: A Case Study on BERT\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*Yang Gao, Nicolo Colombo, Wei Wang*\n\n**Abstract:** Adapting pre-trained neural models to downstream tasks has become the\nstandard practice for obtaining high-quality models. In this work, we propose a\nnovel model adaptation paradigm, adapting by pruning, which prunes neural\nconnections in the pre-trained model to optimise the performance on the target\ntask; all remaining connections have their weights intact. We formulate\nadapting-by-pruning as an optimisation problem with a differentiable loss and\npropose an efficient algorithm to prune the model. We prove that the algorithm\nis near-optimal under standard assumptions and apply the algorithm to adapt\nBERT to some GLUE tasks. Results suggest that our method can prune up to 50%\nweights in BERT while yielding similar performance compared to the fine-tuned\nfull model. We also compare our method with other state-of-the-art pruning\nmethods and study the topological differences of their obtained sub-networks.\n\n**Published:** 2021-05-07T15:51:08Z  (*Updated: 2021-05-07T15:51:08Z*)\n\n\n\n#### 6. A Conceptual Framework For Trie-Augmented Neural Networks (TANNS)\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*Temitayo Adefemi*\n\n**Abstract:** Trie-Augmented Neural Networks (TANNs) combine trie structures with neural\nnetworks, forming a hierarchical design that enhances decision-making\ntransparency and efficiency in machine learning. This paper investigates the\nuse of TANNs for text and document classification, applying Recurrent Neural\nNetworks (RNNs) and Feed forward Neural Networks (FNNs). We evaluated TANNs on\nthe 20 NewsGroup and SMS Spam Collection datasets, comparing their performance\nwith traditional RNN and FFN Networks with and without dropout regularization.\nThe results show that TANNs achieve similar or slightly better performance in\ntext classification. The primary advantage of TANNs is their structured\ndecision-making process, which improves interpretability. We discuss\nimplementation challenges and practical limitations. Future work will aim to\nrefine the TANNs architecture for more complex classification tasks.\n\n**Published:** 2024-06-11T17:08:16Z  (*Updated: 2024-06-11T17:08:16Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Learning Mesh-Based Simulation with Graph Networks\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Peter W. Battaglia, Alvaro Sanchez-Gonzalez, Meire Fortunato, Tobias Pfaff*\n\n**Abstract:** Mesh-based simulations are central to modeling complex physical systems in many disciplines across science and engineering. Mesh representations support powerful numerical integration methods and their resolution can be adapted to strike favorable trade-offs between accuracy and efficiency. However, high-dimensional scientific simulations are very expensive to run, and solvers and parameters must often be tuned individually to each system studied. Here we introduce MeshGraphNets, a framework for learning mesh-based simulations using graph neural networks. Our model can be trained to pass messages on a mesh graph and to adapt the mesh discretization during forward simulation. Our results show it can accurately predict the dynamics of a wide range of physical systems, including aerodynamics, structural mechanics, and cloth. The model's adaptivity supports learning resolution-independent dynamics and can scale to more complex state spaces at test time. Our method is also highly efficient, running 1-2 orders of magnitude faster than the simulation on which it is trained. Our approach broadens the range of problems on which neural network simulators can operate and promises to improve the efficiency of complex, scientific modeling tasks.\n\n**Conference:** learning-mesh-based-simulation-with-graph\n\n**Published:** 2020-10-07\n\n\n\n#### 2. Towards Relation-centered Pooling and Convolution for Heterogeneous Graph Learning Networks\n\n*From Search Query: \"state space models adaptive mechanisms\"*\n\n*Jiong Jin, Xiaowei Huang, Xin Chen, Youhua Xia, Yao Yao, Yuze Liu, Tiehua Zhang*\n\n**Abstract:** Heterogeneous graph neural network has unleashed great potential on graph representation learning and shown superior performance on downstream tasks such as node classification and clustering. Existing heterogeneous graph learning networks are primarily designed to either rely on pre-defined meta-paths or use attention mechanisms for type-specific attentive message propagation on different nodes/edges, incurring many customization efforts and computational costs. To this end, we design a relation-centered Pooling and Convolution for Heterogeneous Graph learning Network, namely PC-HGN, to enable relation-specific sampling and cross-relation convolutions, from which the structural heterogeneity of the graph can be better encoded into the embedding space through the adaptive training process. We evaluate the performance of the proposed model by comparing with state-of-the-art graph learning models on three different real-world datasets, and the results show that PC-HGN consistently outperforms all the baseline and improves the performance maximumly up by 17.8%.\n\n**Published:** 2022-10-31\n\n\n\n#### 3. Efficient Training of Retrieval Models using Negative Cache\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Sanjiv Kumar, Ruiqi Guo, Sashank Reddi, Erik Lindgren*\n\n**Abstract:** Factorized models, such as two tower neural network models, are widely used for scoring (query, document) pairs in information retrieval tasks. These models are typically trained by optimizing the model parameters to score relevant positive\" pairs higher than the irrelevantnegative\" ones. While a large set of negatives typically improves the model performance, limited computation and memory budgets place constraints on the number of negatives used during training. In this paper, we develop a novel negative sampling technique for accelerating training with softmax cross-entropy loss. By using cached (possibly stale) item embeddings, our technique enables training with a large pool of negatives with reduced memory and computation. We also develop a streaming variant of our algorithm geared towards very large datasets. Furthermore, we establish a theoretical basis for our approach by showing that updating a very small fraction of the cache at each iteration can still ensure fast convergence. Finally, we experimentally validate our approach and show that it is efficient and compares favorably with more complex, state-of-the-art approaches.\n\n**Proceeding:** neurips-2021-12\n\n**Published:** 2021-12-01\n\n\n\n#### 4. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: \"memory efficient parameter updates neural networks\"*\n\n*Quoc V. Le, Yifeng Lu, Cho-Jui Hsieh, Thang Luong, Xuanyi Dong, Hieu Pham, Yao Liu, Kaiyuan Wang, Esteban Real, Da Huang, Chen Liang, Xiangning Chen*\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Conference:** symbolic-discovery-of-optimization-algorithms\n\n**Published:** 2023-02-13\n\n\n\n#### 5. Quasi-Oracle Estimation of Heterogeneous Treatment Effects\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*Xinkun Nie, Stefan Wager*\n\n**Abstract:** Flexible estimation of heterogeneous treatment effects lies at the heart of many statistical challenges, such as personalized medicine and optimal resource allocation. In this paper, we develop a general class of two-step algorithms for heterogeneous treatment effect estimation in observational studies. We first estimate marginal effects and treatment propensities in order to form an objective function that isolates the causal component of the signal. Then, we optimize this data-adaptive objective function. Our approach has several advantages over existing methods. From a practical perspective, our method is flexible and easy to use: In both steps, we can use any loss-minimization method, e.g., penalized regression, deep neural networks, or boosting; moreover, these methods can be fine-tuned by cross validation. Meanwhile, in the case of penalized kernel regression, we show that our method has a quasi-oracle property: Even if the pilot estimates for marginal effects and treatment propensities are not particularly accurate, we achieve the same error bounds as an oracle who has a priori knowledge of these two nuisance components. We implement variants of our approach based on penalized regression, kernel ridge regression, and boosting in a variety of simulation setups, and find promising performance relative to existing baselines.\n\n**Published:** 2017-12-13\n\n\n\n#### 6. Fixed-Point Convolutional Neural Network for Real-Time Video Processing in FPGA\n\n*From Search Query: \"practical implementation adaptive neural networks\"*\n\n*Alexandr Kalinin, Vladimir Rukhlov, Dmitry Telpukhov, Alexander Kustov, Roman Solovyev*\n\n**Abstract:** Modern mobile neural networks with a reduced number of weights and parameters do a good job with image classification tasks, but even they may be too complex to be implemented in an FPGA for video processing tasks. The article proposes neural network architecture for the practical task of recognizing images from a camera, which has several advantages in terms of speed. This is achieved by reducing the number of weights, moving from a floating-point to a fixed-point arithmetic, and due to a number of hardware-level optimizations associated with storing weights in blocks, a shift register, and an adjustable number of convolutional blocks that work in parallel. The article also proposed methods for adapting the existing data set for solving a different task. As the experiments showed, the proposed neural network copes well with real-time video processing even on the cheap FPGAs.\n\n**Published:** 2018-08-29\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using state space models (SSMs) and adaptive mechanisms, here are some key findings and references that align with your requirements:\n\n## Integration Techniques for SSMs and Adaptive Mechanisms\n\nThe paper on **MatMamba** introduces a promising integration of SSMs with adaptive mechanisms. MatMamba combines the Matryoshka representation learning with Mamba2 state space models, allowing for joint training and adaptive inference across various model sizes. This approach enables the extraction of hundreds of submodels that preserve the metric space of the original model, which is crucial for adaptive deployment.\n\n## Memory-Efficient Parameter Update Strategies\n\n**MatMamba** also addresses memory-efficient parameter updates by allowing the training of a single large model from which multiple smaller submodels can be extracted. This approach ensures that the smaller submodels share the metric space of the larger model, reducing the computational cost and memory requirements for parameter updates and inference.\n\n## Real-World Performance Benchmarks and Metrics\n\nThe **MatMamba** paper provides performance benchmarks showing that MatMamba models scale as well as the baseline Mamba2 models across various model sizes (from 35M to 1.4B parameters) on both language and vision tasks. The models demonstrate comparable accuracy and significant speed improvements, especially for long context lengths and high-resolution visual processing.\n\n## Implementation Challenges and Solutions\n\n### Hardware-Efficient Implementation\nThe **Spatial-Mamba** paper addresses the challenges of applying SSMs to 2D vision tasks by introducing a structure-aware state fusion (SASF) mechanism. This approach captures spatial dependencies effectively, reducing the computational cost associated with traditional scanning methods. The use of dilated convolutions and depth-wise convolutions enhances local neighborhood connectivity in the state space, making the model more efficient.\n\n### Memory Management\nThe **MatMamba** and **Spatial-Mamba** papers highlight the importance of memory-efficient designs. MatMamba's ability to extract submodels and Spatial-Mamba's use of structure-aware state fusion help in managing memory efficiently. These models are designed to work with minimal loss in accuracy even with reduced computational resources.\n\n### Stability and Adaptation\nThe integration of Matryoshka-style learning with SSMs in **MatMamba** ensures stability and adaptability. The model's ability to jointly optimize all nested granularities helps in maintaining stability while allowing for adaptive inference. This balance between adaptability and computational efficiency is crucial for practical deployment.\n\n## Additional Insights\n\n- **IO-Aware Algorithms and Memory Optimization**: While not directly related to SSMs, papers like **FlashAttention** and **StreamNet** provide valuable insights into IO-aware algorithms and memory optimization techniques that can be applied to improve the efficiency of SSM-based models.\n\nIn summary, the **MatMamba** and **Spatial-Mamba** papers offer practical implementations and performance evaluations that align well with the goals of improving autoregressive language model design using adaptive state space models. These models address integration techniques, memory-efficient parameter updates, real-world performance metrics, and implementation challenges, making them highly relevant for the researcher's intent.\n",
            "analysis": "Analysis Note for AdaptiveSSMamba Proposal - Round 3\n\n1. Summary of Analysis:\nAfter three rounds of searches, we have gathered comprehensive insights about:\n\na) Hardware-Efficient Implementation:\n- IO-aware algorithms and memory optimization techniques\n- Hardware-specific optimizations for different deployment scenarios\n- Trade-offs between computational efficiency and model expressiveness\n\nb) Memory Management:\n- Streaming and patch-based inference techniques\n- Memory-efficient activation management\n- Efficient parameter storage and update strategies\n\nc) Stability and Adaptation:\n- Theoretical frameworks for stability analysis\n- Adaptive normalization and parameter update mechanisms\n- Balance between adaptability and computational efficiency\n\n2. Useful References with Excerpts:\n\nFrom \"FlashAttention\":\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM [...] achieving 15% end-to-end wall-clock speedup on BERT-large\"\n\nFrom \"StreamNet\":\n\"StreamNet uses 1D and 2D streaming processing and provides a parameter selection algorithm that automatically improve the performance of patch-based inference with minimal requirements on the MCU's SRAM memory space.\"\n\nFrom \"State Space Models as Foundation Models\":\n\"SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas [...] highlighting their main features from a control theoretic perspective.\"\n\n3. Key Insights:\n\na) Memory Optimization:\n- IO-aware algorithms can significantly reduce memory bandwidth requirements\n- Streaming and patch-based processing enable efficient memory usage\n- Memory hierarchy-aware design is crucial for performance\n\nb) Hardware Efficiency:\n- Tiling and blocking strategies can improve hardware utilization\n- Hardware-specific optimizations can lead to significant speedups\n- Trade-off between model complexity and hardware efficiency needs careful consideration\n\nc) Stability and Adaptation:\n- Control theoretic perspectives provide valuable insights for stability\n- Adaptive mechanisms need to balance effectiveness with computational cost\n- Memory-efficient parameter updates are crucial for practical deployment\n\n4. Future Search Plan:\n\nNeed one final round focusing on:\n1. Integration techniques for combining SSMs with adaptive mechanisms\n2. Practical implementations of memory-efficient adaptation\n3. Real-world performance metrics and benchmarks\n\n5. References:\n\n[1] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[2] Zheng, H. S., Liu, Y. Y., Hsu, C. F., & Yeh, T. T. (2023). StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller. Neural Information Processing Systems.\n\n[3] Lin, J., Chen, W. M., Cai, H., Gan, C., & Han, S. (2021). MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning. Neural Information Processing Systems.\n\n[4] Alonso, C. A., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv.org.\n\n[5] Smith, J. T. H., et al. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "\"adaptive state space model\", \"test-time training neural networks\", \"hardware efficient language models\"",
            "detail": "Find papers discussing adaptive mechanisms in state space models, particularly those focusing on state compression, dynamic parameter updates, or hardware efficiency. Include papers that combine state space models with test-time training or online adaptation techniques.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing adaptive mechanisms in state space models, particularly those focusing on state compression, dynamic parameter updates, or hardware efficiency. Include papers that combine state space models with test-time training or online adaptation techniques.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.96)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.96)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.94)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 15/37 (Score: 0.94)*\n\n```\nAdvances in neural information processing systems, 33:1474-1487, 2020. [16] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35:35971-35983, 2022. [17] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2022. [18] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in neural information processing systems, 34:572-585, 2021. [19] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher R\u00e9. How to train your hippo: State space models with generalized orthogonal basis projections. International Conference on Learning Representations, 2023. [20] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. International Conference on Learning Representations, 2023. [21] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. Convolutional State Space Models for Long-Range Spatiotemporal Modeling (Avg. Score: 0.85)\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 9  (*Influential: 0*)\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n##### *Relevant Chunk: No. 15/44 (Score: 0.85)*\n\n```\nIn International conference on machine learning, pages 4651-4664. PMLR, 2021. [40] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long Range Arena: A benchmark for efficient Transformers. In International Conference on Learning Representations, 2021. [41] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, 2022. [42] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. [43] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In International Conference on Learning Representations, 2023. [44] Karan Goel, Albert Gu, Chris Donahue, and Christopher Re. It's raw! Audio generation with state-space models. In Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 7616-7633. PMLR, 17-23 Jul 2022. [45] Eric Nguyen, Karan Goel, Albert Gu, Gordon Downs, Preey Shah, Tri Dao, Stephen Baccus, and Christopher R\u00e9. S4ND: Modeling images and videos as multidimensional signals with state spaces. In Advances in Neural Information Processing Systems, 2022. [46] Md Mohaiminul Islam and Gedas Bertasius. Long movie clip classification with state-space video models. In Computer Vision-ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXV, pages 87-104, 2022. [47] Shmuel Bar David, Itamar Zimerman, Eliya Nachmani, and Lior Wolf. Decision S4: Efficient sequence-based RL via state spaces layers. In The Eleventh International Conference on Learning Representations, 2023. [48] Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. arXiv preprint arXiv:2303.03982, 2023. [49] Linqi Zhou, Michael Poli, Winnie Xu, Stefano Massaroli, and Stefano Ermon. Deep latent state space models for time-series generation. arXiv preprint arXiv:2212.12749, 2022. [50] Daniel Y Fu, Tri Dao, Khaled Kamal Saab, Armin W Thomas, Atri Rudra, and Christopher Re. Hungry hungry hippos: Towards language modeling with state space models. In The Eleventh International Conference on Learning Representations, 2023. [51] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In The Eleventh International Conference on Learning Representations, 2023. [52] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [53] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models.\n```\n\n#### 5. The Illusion of State in State-Space Models (Avg. Score: 0.83)\n\n*William Merrill, Jackson Petty, Ashish Sabharwal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Analysis of state-space models reveals that SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.\n\n**Abstract:** State-space models (SSMs) have emerged as a potential alternative architecture for building large language models (LLMs) compared to the previously ubiquitous transformer architecture. One theoretical weakness of transformers is that they cannot express certain kinds of sequential computation and state tracking (Merrill&Sabharwal, 2023), which SSMs are explicitly designed to address via their close architectural similarity to recurrent neural networks (RNNs). But do SSMs truly have an advantage (over transformers) in expressive power for state tracking? Surprisingly, the answer is no. Our analysis reveals that the expressive power of SSMs is limited very similarly to transformers: SSMs cannot express computation outside the complexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve simple state-tracking problems like permutation composition. It follows that SSMs are provably unable to accurately track chess moves with certain notation, evaluate code, or track entities in a long narrative. To supplement our formal analysis, we report experiments showing that Mamba-style SSMs indeed struggle with state tracking. Thus, despite its recurrent formulation, the\"state\"in an SSM is an illusion: SSMs have similar expressiveness limitations to non-recurrent models like transformers, which may fundamentally limit their ability to solve real-world state-tracking problems.\n\n##### *Relevant Chunk: No. 13/39 (Score: 0.83)*\n\n```\narXiv:2312.00752. Gu, A., Johnson, I., Goel, K., Saab, K. K., Dao, T., Rudra, A., and Re, C. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In NeurIPS, 2021. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In ICLR, 2022a. Gu, A., Goel, K., Saab, K., and R\u00e9, C. Structured state spaces: Combining continuous-time, recurrent, and convolutional models, January 2022b. URL https://hazyresearch.stanford.edu/ blog/2022-01-14-s 4-3. Blog post accessed January $31,2024$. Hao, S., Angluin, D., and Frank, R. Formal language recognition by hard attention transformers: Perspectives from circuit complexity. TACL, 10:800-810, 2022. Hasani, R., Lechner, M., Wang, T.-H., Chahine, M., Amini, A., and Rus, D. Liquid structural state-space models.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive state space model\", \"test-time training neural networks\", \"hardware efficient language models\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. TADAM: Task dependent adaptive metric for improved few-shot learning\n\n*From Search Query: \"adaptive state space model\"*\n\n*Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, Alexandre Lacoste*\n\n**TL;DR:** This work identifies that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms and proposes and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space.\n\n**Abstract:** Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1234  (*Influential: 202*)\n\n#### 2. Adaptive Discretization for Model-Based Reinforcement Learning\n\n*From Search Query: \"adaptive state space model\"*\n\n*Sean R. Sinclair, Tianyu Wang, Gauri Jain, Siddhartha Banerjee, C. Yu*\n\n**TL;DR:** This work introduces the technique of adaptive discretization to design efficient model-based episodic reinforcement learning algorithms in large (potentially continuous) state-action spaces and provides worst-case regret bounds for this algorithm, which are competitive compared to the state-of-the-art model- based algorithms.\n\n**Abstract:** We introduce the technique of adaptive discretization to design efficient model-based episodic reinforcement learning algorithms in large (potentially continuous) state-action spaces. Our algorithm is based on optimistic one-step value iteration extended to maintain an adaptive discretization of the space. From a theoretical perspective, we provide worst-case regret bounds for our algorithm, which are competitive compared to the state-of-the-art model-based algorithms; moreover, our bounds are obtained via a modular proof technique, which can potentially extend to incorporate additional structure on the problem. \nFrom an implementation standpoint, our algorithm has much lower storage and computational requirements, due to maintaining a more efficient partition of the state and action spaces. We illustrate this via experiments on several canonical control problems, which shows that our algorithm empirically performs significantly better than fixed discretization in terms of both faster convergence and lower memory usage. Interestingly, we observe empirically that while fixed-discretization model-based algorithms vastly outperform their model-free counterparts, the two achieve comparable performance with adaptive discretization.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 20  (*Influential: 0*)\n\n#### 3. Towards Robust Pruning: An Adaptive Knowledge-Retention Pruning Strategy for Language Models\n\n*From Search Query: \"adaptive state space model\"*\n\n*Jianwei Li, Qi Lei, Wei Cheng, Dongkuan Xu*\n\n**TL;DR:** This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass, and introduces a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre- trained knowledge during the pruning process.\n\n**Abstract:** The pruning objective has recently extended beyond accuracy and sparsity to robustness in language models. Despite this, existing methods struggle to enhance robustness against adversarial attacks when continually increasing model sparsity and require a retraining process. As humans step into the era of large language models, these issues become increasingly prominent. This paper proposes that the robustness of language models is proportional to the extent of pre-trained knowledge they encompass. Accordingly, we introduce a post-training pruning strategy designed to faithfully replicate the embedding space and feature space of dense language models, aiming to conserve more pre-trained knowledge during the pruning process. In this setup, each layer's reconstruction error not only originates from itself but also includes cumulative error from preceding layers, followed by an adaptive rectification. Compared to other state-of-art baselines, our approach demonstrates a superior balance between accuracy, sparsity, robustness, and pruning cost with BERT on datasets SST2, IMDB, and AGNews, marking a significant stride towards robust pruning in language models.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 5. GraphPatcher: Mitigating Degree Bias for Graph Neural Networks via Test-time Augmentation\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Mingxuan Ju, Tong Zhao, Wenhao Yu, Neil Shah, Yanfang Ye*\n\n**TL;DR:** GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes, and can also mitigate the degree bias for either self-supervised or supervised GNNs.\n\n**Abstract:** Recent studies have shown that graph neural networks (GNNs) exhibit strong biases towards the node degree: they usually perform satisfactorily on high-degree nodes with rich neighbor information but struggle with low-degree nodes. Existing works tackle this problem by deriving either designated GNN architectures or training strategies specifically for low-degree nodes. Though effective, these approaches unintentionally create an artificial out-of-distribution scenario, where models mainly or even only observe low-degree nodes during the training, leading to a downgraded performance for high-degree nodes that GNNs originally perform well at. In light of this, we propose a test-time augmentation framework, namely GraphPatcher, to enhance test-time generalization of any GNNs on low-degree nodes. Specifically, GraphPatcher iteratively generates virtual nodes to patch artificially created low-degree nodes via corruptions, aiming at progressively reconstructing target GNN's predictions over a sequence of increasingly corrupted nodes. Through this scheme, GraphPatcher not only learns how to enhance low-degree nodes (when the neighborhoods are heavily corrupted) but also preserves the original superior performance of GNNs on high-degree nodes (when lightly corrupted). Additionally, GraphPatcher is model-agnostic and can also mitigate the degree bias for either self-supervised or supervised GNNs. Comprehensive experiments are conducted over seven benchmark datasets and GraphPatcher consistently enhances common GNNs' overall performance by up to 3.6% and low-degree performance by up to 6.5%, significantly outperforming state-of-the-art baselines. The source code is publicly available at https://github.com/jumxglhf/GraphPatcher.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 6. LiteTransformerSearch: Training-free Neural Architecture Search for Efficient Language Models\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Mojan Javaheripi, Gustavo de Rosa, Subhabrata Mukherjee, S. Shah, T. L. Religa, C. C. T. Mendes, S\u00e9bastien Bubeck, F. Koushanfar, Debadeepta Dey*\n\n**TL;DR:** The search phase of this training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs and effectively removes the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.\n\n**Abstract:** The Transformer architecture is ubiquitously used as the building block of large-scale autoregressive language models. However, finding architectures with the optimal trade-off between task performance (perplexity) and hardware constraints like peak memory utilization and latency is non-trivial. This is exacerbated by the proliferation of various hardware. We leverage the somewhat surprising empirical observation that the number of decoder parameters in autoregressive Transformers has a high rank correlation with task performance, irrespective of the architecture topology. This observation organically induces a simple Neural Architecture Search (NAS) algorithm that uses decoder parameters as a proxy for perplexity without need for any model training. The search phase of our training-free algorithm, dubbed Lightweight Transformer Search (LTS), can be run directly on target devices since it does not require GPUs. Using on-target-device measurements, LTS extracts the Pareto-frontier of perplexity versus any hardware performance cost. We evaluate LTS on diverse devices from ARM CPUs to NVIDIA GPUs and two popular autoregressive Transformer backbones: GPT-2 and Transformer-XL. Results show that the perplexity of 16-layer GPT-2 and Transformer-XL can be achieved with up to 1.5x, 2.5x faster runtime and 1.2x, 2.0x lower peak memory utilization. When evaluated in zero and one-shot settings, LTS Pareto-frontier models achieve higher average accuracy compared to the 350M parameter OPT across 14 tasks, with up to 1.6x lower latency. LTS extracts the Pareto-frontier in under 3 hours while running on a commodity laptop. We effectively remove the carbon footprint of hundreds of GPU hours of training during search, offering a strong simple baseline for future NAS methods in autoregressive language modeling.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 7. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han*\n\n**TL;DR:** SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs, offers a turn-key solution that reduces hardware costs and democratizes LLMs.\n\n**Abstract:** Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 486  (*Influential: 80*)\n\n#### 8. High-throughput Generative Inference of Large Language Models with a Single GPU\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher R\u00e9, Ion Stoica, Ce Zhang*\n\n**TL;DR:** FlexGen is presented, a high-throughput generation engine for running LLMs with limited GPU memory that compresses the weights and the attention cache to 4 bits with negligible accuracy loss, enabling FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput.\n\n**Abstract:** The high computational and memory requirements of large language model (LLM) inference make it feasible only with multiple high-end accelerators. Motivated by the emerging demand for latency-insensitive tasks with batched processing, this paper initiates the study of high-throughput LLM inference using limited resources, such as a single commodity GPU. We present FlexGen, a high-throughput generation engine for running LLMs with limited GPU memory. FlexGen can be flexibly configured under various hardware resource constraints by aggregating memory and computation from the GPU, CPU, and disk. By solving a linear programming problem, it searches for efficient patterns to store and access tensors. FlexGen further compresses the weights and the attention cache to 4 bits with negligible accuracy loss. These techniques enable FlexGen to have a larger space of batch size choices and thus significantly increase maximum throughput. As a result, when running OPT-175B on a single 16GB GPU, FlexGen achieves significantly higher throughput compared to state-of-the-art offloading systems, reaching a generation throughput of 1 token/s for the first time with an effective batch size of 144. On the HELM benchmark, FlexGen can benchmark a 30B model with a 16GB GPU on 7 representative sub-scenarios in 21 hours. The code is available at https://github.com/FMInference/FlexGen\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 236  (*Influential: 28*)\n\n### 6 related papers from ArXiv\n\n#### 1. MiDAS: Multi-integrated Domain Adaptive Supervision for Fake News\n  Detection\n\n*From Search Query: \"adaptive state space model\"*\n\n*Abhijit Suprem, Calton Pu*\n\n**Abstract:** COVID-19 related misinformation and fake news, coined an 'infodemic', has\ndramatically increased over the past few years. This misinformation exhibits\nconcept drift, where the distribution of fake news changes over time, reducing\neffectiveness of previously trained models for fake news detection. Given a set\nof fake news models trained on multiple domains, we propose an adaptive\ndecision module to select the best-fit model for a new sample. We propose\nMiDAS, a multi-domain adaptative approach for fake news detection that ranks\nrelevancy of existing models to new samples. MiDAS contains 2 components: a\ndoman-invariant encoder, and an adaptive model selector. MiDAS integrates\nmultiple pre-trained and fine-tuned models with their training data to create a\ndomain-invariant representation. Then, MiDAS uses local Lipschitz smoothness of\nthe invariant embedding space to estimate each model's relevance to a new\nsample. Higher ranked models provide predictions, and lower ranked models\nabstain. We evaluate MiDAS on generalization to drifted data with 9 fake news\ndatasets, each obtained from different domains and modalities. MiDAS achieves\nnew state-of-the-art performance on multi-domain adaptation for\nout-of-distribution fake news classification.\n\n**Published:** 2022-05-19T19:36:08Z  (*Updated: 2022-05-19T19:36:08Z*)\n\n\n\n#### 2. DAN: Dual-View Representation Learning for Adapting Stance Classifiers\n  to New Domains\n\n*From Search Query: \"adaptive state space model\"*\n\n*Chang Xu, Cecile Paris, Surya Nepal, Ross Sparks, Chong Long, Yafang Wang*\n\n**Abstract:** We address the issue of having a limited number of annotations for stance\nclassification in a new domain, by adapting out-of-domain classifiers with\ndomain adaptation. Existing approaches often align different domains in a\nsingle, global feature space (or view), which may fail to fully capture the\nrichness of the languages used for expressing stances, leading to reduced\nadaptability on stance data. In this paper, we identify two major types of\nstance expressions that are linguistically distinct, and we propose a tailored\ndual-view adaptation network (DAN) to adapt these expressions across domains.\nThe proposed model first learns a separate view for domain transfer in each\nexpression channel and then selects the best adapted parts of both views for\noptimal transfer. We find that the learned view features can be more easily\naligned and more stance-discriminative in either or both views, leading to more\ntransferable overall features after combining the views. Results from extensive\nexperiments show that our method can enhance the state-of-the-art single-view\nmethods in matching stance data across different domains, and that it\nconsistently improves those methods on various adaptation tasks.\n\n**Published:** 2020-03-13T23:56:37Z  (*Updated: 2020-03-13T23:56:37Z*)\n\n\n\n#### 3. Optimizing the Neural Network Training for OCR Error Correction of\n  Historical Hebrew Texts\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Omri Suissa, Avshalom Elmalech, Maayan Zhitomirsky-Geffet*\n\n**Abstract:** Over the past few decades, large archives of paper-based documents such as\nbooks and newspapers have been digitized using Optical Character Recognition.\nThis technology is error-prone, especially for historical documents. To correct\nOCR errors, post-processing algorithms have been proposed based on natural\nlanguage analysis and machine learning techniques such as neural networks.\nNeural network's disadvantage is the vast amount of manually labeled data\nrequired for training, which is often unavailable. This paper proposes an\ninnovative method for training a light-weight neural network for Hebrew OCR\npost-correction using significantly less manually created data. The main\nresearch goal is to develop a method for automatically generating language and\ntask-specific training data to improve the neural network results for OCR\npost-correction, and to investigate which type of dataset is the most effective\nfor OCR post-correction of historical documents. To this end, a series of\nexperiments using several datasets was conducted. The evaluation corpus was\nbased on Hebrew newspapers from the JPress project. An analysis of historical\nOCRed newspapers was done to learn common language and corpus-specific OCR\nerrors. We found that training the network using the proposed method is more\neffective than using randomly generated errors. The results also show that the\nperformance of the neural network for OCR post-correction strongly depends on\nthe genre and area of the training data. Moreover, neural networks that were\ntrained with the proposed method outperform other state-of-the-art neural\nnetworks for OCR post-correction and complex spellcheckers. These results may\nhave practical implications for many digital humanities projects.\n\n**Published:** 2023-07-30T12:59:06Z  (*Updated: 2023-07-30T12:59:06Z*)\n\n\n\n#### 4. Performance of Transfer Learning Model vs. Traditional Neural Network in\n  Low System Resource Environment\n\n*From Search Query: \"test-time training neural networks\"*\n\n*William Hui*\n\n**Abstract:** Recently, the use of pre-trained model to build neural network based on\ntransfer learning methodology is increasingly popular. These pre-trained models\npresent the benefit of using less computing resources to train model with\nsmaller amount of training data. The rise of state-of-the-art models such as\nBERT, XLNet and GPT boost accuracy and benefit as a base model for transfer\nleanring. However, these models are still too complex and consume many\ncomputing resource to train for transfer learning with low GPU memory. We will\ncompare the performance and cost between lighter transfer learning model and\npurposely built neural network for NLP application of text classification and\nNER model.\n\n**Published:** 2020-10-20T08:12:56Z  (*Updated: 2020-10-20T08:12:56Z*)\n\n\n\n#### 5. Scalable Training of Language Models using JAX pjit and TPUv4\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Joanna Yoo, Kuba Perlin, Siddhartha Rao Kamalakara, Jo\u00e3o G. M. Ara\u00fajo*\n\n**Abstract:** Modern large language models require distributed training strategies due to\ntheir size. The challenges of efficiently and robustly training them are met\nwith rapid developments on both software and hardware frontiers. In this\ntechnical report, we explore challenges and design decisions associated with\ndeveloping a scalable training framework, and present a quantitative analysis\nof efficiency improvements coming from adopting new software and hardware\nsolutions.\n\n**Published:** 2022-04-13T17:08:58Z  (*Updated: 2022-04-13T17:08:58Z*)\n\n\n\n#### 6. Simple Hardware-Efficient PCFGs with Independent Left and Right\n  Productions\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu*\n\n**Abstract:** Scaling dense PCFGs to thousands of nonterminals via a low-rank\nparameterization of the rule probability tensor has been shown to be beneficial\nfor unsupervised parsing. However, PCFGs scaled this way still perform poorly\nas a language model, and even underperform similarly-sized HMMs. This work\nintroduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and\nright productions. Despite imposing a stronger independence assumption than the\nlow-rank approach, we find that this formalism scales more effectively both as\na language model and as an unsupervised parser. As an unsupervised parser, our\nsimple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language\nmodel, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank\nPCFGs. We further introduce \\emph{FlashInside}, a hardware IO-aware\nimplementation of the inside algorithm for efficiently scaling simple PCFGs.\n\n**Published:** 2023-10-23T14:48:51Z  (*Updated: 2023-10-23T14:48:51Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Mamba-FSCIL: Dynamic Adaptation with Selective State Space Model for Few-Shot Class-Incremental Learning\n\n*From Search Query: \"adaptive state space model\"*\n\n*Min Zhang, Liqiang Nie, Bernard Ghanem, Jianlong Wu, Yibo Yang, Xiaojie Li*\n\n**Abstract:** Few-shot class-incremental learning (FSCIL) confronts the challenge of integrating new classes into a model with minimal training samples while preserving the knowledge of previously learned classes. Traditional methods widely adopt static adaptation relying on a fixed parameter space to learn from data that arrive sequentially, prone to overfitting to the current session. Existing dynamic strategies require the expansion of the parameter space continually, leading to increased complexity. In this study, we explore the potential of Selective State Space Models (SSMs) for FSCIL, leveraging its dynamic weights and strong ability in sequence modeling to address these challenges. Concretely, we propose a dual selective SSM projector that dynamically adjusts the projection parameters based on the intermediate features for dynamic adaptation. The dual design enables the model to maintain the robust features of base classes, while adaptively learning distinctive feature shifts for novel classes. Additionally, we develop a class-sensitive selective scan mechanism to guide dynamic adaptation. It minimizes the disruption to base-class representations caused by training on novel data, and meanwhile, forces the selective scan to perform in distinct patterns between base and novel classes. Experiments on miniImageNet, CUB-200, and CIFAR-100 demonstrate that our framework outperforms the existing state-of-the-art methods. The code is available at \\url{https://github.com/xiaojieli0903/Mamba-FSCIL}.\n\n**Published:** 2024-07-08\n\n\n\n#### 2. Adaptive Probabilistic ODE Solvers Without Adaptive Memory Requirements\n\n*From Search Query: \"adaptive state space model\"*\n\n*Nicholas Kr\u00e4mer*\n\n**Abstract:** Despite substantial progress in recent years, probabilistic solvers with adaptive step sizes can still not solve memory-demanding differential equations -- unless we care only about a single point in time (which is far too restrictive; we want the whole time series). Counterintuitively, the culprit is the adaptivity itself: Its unpredictable memory demands easily exceed our machine's capabilities, making our simulations fail unexpectedly and without warning. Still, dropping adaptivity would abandon years of progress, which can't be the answer. In this work, we solve this conundrum. We develop an adaptive probabilistic solver with fixed memory demands building on recent developments in robust state estimation. Switching to our method (i) eliminates memory issues for long time series, (ii) accelerates simulations by orders of magnitude through unlocking just-in-time compilation, and (iii) makes adaptive probabilistic solvers compatible with scientific computing in JAX.\n\n**Published:** 2024-10-14\n\n\n\n#### 3. The Dual Form of Neural Networks Revisited: Connecting Test Time Predictions to Training Patterns via Spotlights of Attention\n\n*From Search Query: \"test-time training neural networks\"*\n\n*J\u00fcrgen Schmidhuber, R\u00f3bert Csord\u00e1s, Kazuki Irie*\n\n**Abstract:** Linear layers in neural networks (NNs) trained by gradient descent can be expressed as a key-value memory system which stores all training datapoints and the initial weights, and produces outputs using unnormalised dot attention over the entire training experience. While this has been technically known since the 1960s, no prior work has effectively studied the operations of NNs in such a form, presumably due to prohibitive time and space complexities and impractical model sizes, all of them growing linearly with the number of training patterns which may get very large. However, this dual formulation offers a possibility of directly visualising how an NN makes use of training patterns at test time, by examining the corresponding attention weights. We conduct experiments on small scale supervised image classification tasks in single-task, multi-task, and continual learning settings, as well as language modelling, and discuss potentials and limits of this view for better understanding and interpreting how NNs exploit training patterns. Our code is public.\n\n**Published:** 2022-02-11\n\n\n\n#### 4. Layer Normalization\n\n*From Search Query: \"test-time training neural networks\"*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 5. Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Dhabaleswar K. Panda, Hari Subramoni, Aamir Shafi, Olatunji Ruwase, Masahiro Tanaka, Sam Ade Jacobs, Jinghan Yao*\n\n**Abstract:** Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.\n\n**Published:** 2024-08-30\n\n\n\n#### 6. Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training\n\n*From Search Query: \"hardware efficient language models\"*\n\n*Minjia Zhang, Olatunji Ruwase, Stas Bekman, Masahiro Tanaka, Lev Kurilenko, Sam Ade Jacobs, Xinyu Lian*\n\n**Abstract:** Existing checkpointing approaches seem ill-suited for distributed training even though hardware limitations make model parallelism, i.e., sharding model state across multiple accelerators, a requirement for model scaling. Consolidating distributed model state into a single checkpoint unacceptably slows down training, and is impractical at extreme scales. Distributed checkpoints, in contrast, are tightly coupled to the model parallelism and hardware configurations of the training run, and thus unusable on different configurations. To address this problem, we propose Universal Checkpointing, a technique that enables efficient checkpoint creation while providing the flexibility of resuming on arbitrary parallelism strategy and hardware configurations. Universal Checkpointing unlocks unprecedented capabilities for large-scale training such as improved resilience to hardware failures through continued training on remaining healthy hardware, and reduced training time through opportunistic exploitation of elastic capacity. The key insight of Universal Checkpointing is the selection of the optimal representation in each phase of the checkpointing life cycle: distributed representation for saving, and consolidated representation for loading. This is achieved using two key mechanisms. First, the universal checkpoint format, which consists of a consolidated representation of each model parameter and metadata for mapping parameter fragments into training ranks of arbitrary model-parallelism configuration. Second, the universal checkpoint language, a simple but powerful specification language for converting distributed checkpoints into the universal checkpoint format. Our evaluation demonstrates the effectiveness and generality of Universal Checkpointing on state-of-the-art model architectures and a wide range of parallelism techniques.\n\n**Published:** 2024-06-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on adaptive state space models, test-time training, and hardware efficiency, here are some key findings and references that align with your requirements:\n\n## Adaptive State Space Models\n\nThe concept of combining Matryoshka-style learning with state space models like Mamba2 is highly relevant. The work on **MatMamba** introduces a state space model that integrates Matryoshka-style learning, enabling adaptive inference and efficient deployment across various model sizes.\n\n- **MatMamba** allows for the training of a single large model from which hundreds of smaller nested models can be extracted without additional training. This approach optimizes multiple nested models within the same weight space, making it efficient and adaptable for different computational constraints.\n\n## State Compression and Dynamic Parameter Updates\n\n**MatMamba** incorporates state compression by slicing the model along hidden dimensions, allowing for the joint optimization of multiple nested models. This approach ensures that the smallest dimensions capture the most salient representations, which is crucial for maintaining model expressiveness while reducing computational requirements.\n\n## Test-Time Training Integration\n\nWhile the specific papers on **MatMamba** do not explicitly discuss test-time training, the concept of adaptive inference in **MatMamba** can be seen as a form of online adaptation. However, to integrate test-time training like FastTTTLinear, you would need to look into how other models combine state space models with online adaptation techniques.\n\n- For example, the idea of dynamic parameter updates and adaptive state compression in **MatMamba** could be extended by incorporating test-time training methods. This would involve researching how to adapt the model's parameters at test time to new data, potentially leveraging techniques from other adaptive models.\n\n## Hardware Efficiency\n\n**MatMamba** is designed to be hardware-efficient, retaining the speed advantages of Mamba2 models, especially at longer context lengths. The model's ability to generate smaller submodels from a single large model makes it highly efficient in terms of compute resources and memory management.\n\n- The practical implications of **MatMamba** include efficient inference characteristics, making it a viable option for deploying large-scale models in an elastic way based on available inference compute. This aligns well with the goal of hardware-efficient implementation.\n\n## Comparison with Existing Approaches\n\nTo compare **MatMamba** with existing adaptive SSM approaches and hybrid architectures, you would need to investigate other recent works in state space models and their adaptations.\n\n- For instance, comparing **MatMamba** with other models like **Mamba2** and **MatFormer** can provide insights into the trade-offs between compression, model expressiveness, and computational efficiency. The stability issues in adaptive state space models can also be analyzed by comparing different approaches to state compression and dynamic parameter updates.\n\n## Additional Considerations\n\n- **State Space Model Integration**: The stability of adaptive state compression in **MatMamba** can be evaluated by analyzing its performance across different tasks and comparing it with other adaptive SSM approaches.\n- **Test-Time Training Integration**: Investigating how **MatMamba** could be modified to include test-time training, similar to FastTTTLinear, would involve looking into existing works that combine SSMs with online adaptation techniques.\n- **Hardware Efficiency**: The unified memory management in **MatMamba** can be compared with other hardware-optimized architectures to understand the practical implications and potential improvements.\n\nBy focusing on these aspects, the researchers can further develop and refine the **AdaptiveSSMamba** proposal, ensuring it leverages the best of adaptive state space models, test-time training, and hardware efficiency.\n",
            "analysis": "Initial Analysis of AdaptiveSSMamba Proposal:\n\n1. Summary of Key Components:\n- Combines Mamba2's state space processing with FastTTTLinear's test-time training\n- Introduces adaptive state compression and dynamic parameter updates\n- Focuses on hardware-efficient implementation\n- Maintains linear complexity while enabling adaptability\n\n2. Key Areas Requiring Investigation:\n\na) State Space Model Integration:\n- How does adaptive state compression affect SSM stability?\n- What are the trade-offs between compression and model expressiveness?\n- How does this compare to existing adaptive SSM approaches?\n\nb) Test-Time Training Integration:\n- How does the proposal's approach to test-time training differ from FastTTTLinear?\n- What are the memory and computational implications?\n- Are there existing works combining SSMs with test-time training?\n\nc) Hardware Efficiency:\n- What are the practical implications of the proposed optimizations?\n- How does the unified memory management compare to existing approaches?\n- Are there similar hardware-optimized architectures?\n\n3. Investigation Plan:\nRound 1: Focus on state space models and adaptive mechanisms\n- Search for recent advances in adaptive SSMs\n- Look for papers discussing state compression in SSMs\n- Investigate stability issues in adaptive state space models\n\nRound 2: Examine test-time training and hardware optimization\n- Research papers combining SSMs with online adaptation\n- Look for hardware-efficient implementations of similar architectures\n- Study memory management techniques in language models\n\nRound 3: Compare with existing approaches\n- Search for similar hybrid architectures\n- Investigate performance benchmarks of related models\n- Look for theoretical analyses of similar approaches\n\n4. Initial Novelty Assessment:\nThe proposal appears to be distinct from the found siblings and similar designs:\n- densessmamba focuses on dense state connections\n- hierarchical_gated_graph_attenti uses graph-based attention\n- rwkv_lightning emphasizes intra/inter-block attention\n- geogate_gpt uses geometric priors for attention routing\n\nThe current proposal's focus on adaptive state compression and hardware-efficient test-time training appears novel, but requires further investigation to confirm.\n\n5. References from Initial Review:\n1. Dao, T., & Gu, A. (2024). \"Transformers are SSMs\"\n2. Liu, F., & Li, Q. (2024). \"From generalization analysis to optimization designs for state space models\"\n3. Gu, A., & Dao, T. (2023). \"Mamba: Linear-Time Sequence Modeling with Selective State Spaces\""
        },
        {
            "ready": false,
            "query": "\"adaptive parameter updates state space models\", \"recurrent neural networks test time adaptation\", \"memory efficient adaptive neural networks\"",
            "detail": "Find papers discussing adaptive parameter update mechanisms in state space models and recurrent architectures, particularly focusing on:\n1. Methods for dynamic parameter adaptation\n2. Stability analysis of adaptive updates\n3. Memory-efficient implementations\n4. Integration with test-time training approaches",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing adaptive parameter update mechanisms in state space models and recurrent architectures, particularly focusing on:\n1. Methods for dynamic parameter adaptation\n2. Stability analysis of adaptive updates\n3. Memory-efficient implementations\n4. Integration with test-time training approaches\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.85)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 23/31 (Score: 0.85)*\n\n```\n[27] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [29] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [30] Shida Wang and Qianxiao Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization.\n```\n\n#### 2. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.44)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.44)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 3. Structured state-space models are deep Wiener models (Avg. Score: 0.35)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 16/22 (Score: 0.35)*\n\n```\nForgione, M. and Piga, D. (2021). dynoNet: A neural network architecture for learning dynamical systems. International Journal of Adaptive Control and Signal Processing, 35(4), 612-626. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. (2020). Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 1474-1487. Gu, A., Goel, K., Gupta, A., and R\u00e9, C. (2022). On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 35971-35983. Gu, A., Goel, K., and R\u00e9, C. (2021). Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Gupta, A., Gu, A., and Berant, J. (2022). Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35, $22982-22994$. Kumar, S.K. (2017). On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863. Lanzetti, N. et al. (2019). Recurrent neural network based MPC for process industries. In 2019 18th European Control Conference (ECC), 1005-1010. IEEE. Ljung, L., Zhang, Q., Lindskog, P., and Juditski, A. (2004). Estimation of grey box and black box models for nonlinear circuit data. IFAC Proceedings Volumes, 37(13), $399-404$. Marconato, A., Sj\u00f6berg, J., Suykens, J.A., and Schoukens, J. (2013). Improved initialization for nonlinear statespace modeling. IEEE Transactions on instrumentation and Measurement, 63(4), 972-980.\n```\n\n#### 4. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.29)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 12/30 (Score: 0.29)*\n\n```\nZenodo, Sept. 2021. [17] Felix A. Gers, J\u00fcrgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000. [18] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571-575, 2021. [19] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, and J\u00fcrgen Schmidhuber. Lstm: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:2222-2232, 2015. [20] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [23] Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3800-3809. PMLR, 2020. [24] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572-585, 2021. [25] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers, 2021. [26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [27] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns. CoRR, abs/2212.00768, 2022. [28] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyu He and Marko Kabic. A unified view of long-sequence models towards modeling million-scale dependencies. CoRR, abs/2302.06218, 2023. [30] Sepp Hochreiter and Yoshua Bengio. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n```\n\n#### 5. Sequence Modeling with Multiresolution Convolutional Memory  (Avg. Score: 0.28)\n\n*Jiaxin Shi, Ke Alexander Wang, E. Fox*\n\n**Published in:** International Conference on Machine Learning (2023)\t**Cited by** 6  (*Influential: 1*)\n\n**TL;DR:** The key component of the model is the multiresolution convolution, capturing multiscale trends in the input sequence, and can be implemented with shared filters across a dilated causal convolution tree, garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions.\n\n**Abstract:** Efficiently capturing the long-range patterns in sequential data sources salient to a given task -- such as classification and generative modeling -- poses a fundamental challenge. Popular approaches in the space tradeoff between the memory burden of brute-force enumeration and comparison, as in transformers, the computational burden of complicated sequential dependencies, as in recurrent neural networks, or the parameter burden of convolutional networks with many or large filters. We instead take inspiration from wavelet-based multiresolution analysis to define a new building block for sequence modeling, which we call a MultiresLayer. The key component of our model is the multiresolution convolution, capturing multiscale trends in the input sequence. Our MultiresConv can be implemented with shared filters across a dilated causal convolution tree. Thus it garners the computational advantages of convolutional networks and the principled theoretical motivation of wavelet decompositions. Our MultiresLayer is straightforward to implement, requires significantly fewer parameters, and maintains at most a $\\mathcal{O}(N\\log N)$ memory footprint for a length $N$ sequence. Yet, by stacking such layers, our model yields state-of-the-art performance on a number of sequence classification and autoregressive density estimation tasks using CIFAR-10, ListOps, and PTB-XL datasets.\n\n##### *Relevant Chunk: No. 20/37 (Score: 0.28)*\n\n```\nB., Azencot, O., Queiruga, A., Hodgkinson, L., and Mahoney, M. W. Lipschitz recurrent neural networks. In International Conference on Learning Representations, 2021. Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Iintelligence and Statistics, pp. 249-256, 2010. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. HiPPO: Recurrent memory with optimal polynomial projections. Advances in Neural Information Processing Systems, 33: $1474-1487,2020 a$. Gu, A., Gulcehre, C., Paine, T., Hoffman, M., and Pascanu, R. Improving the gating mechanism of recurrent neural networks. In International Conference on Machine Learning, pp. 3800-3809. PMLR, 2020b. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. In International Conference on Learning Representations, 2021. Gu, A., Goel, K., Gupta, A., and R\u00e9, C. On the parameterization and initialization of diagonal state space models. In Advances in Neural Information Processing Systems, 2022. Gupta, A., Gu, A., and Berant, J. Diagonal state spaces are as effective as structured state spaces.\n```\n\n\n\n---\n## Found 20 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"adaptive parameter updates state space models\", \"recurrent neural networks test time adaptation\", \"memory efficient adaptive neural networks\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. TADAM: Task dependent adaptive metric for improved few-shot learning\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Boris N. Oreshkin, Pau Rodr\u00edguez L\u00f3pez, Alexandre Lacoste*\n\n**TL;DR:** This work identifies that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms and proposes and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space.\n\n**Abstract:** Few-shot learning has become essential for producing models that generalize from few examples. In this work, we identify that metric scaling and metric task conditioning are important to improve the performance of few-shot algorithms. Our analysis reveals that simple metric scaling completely changes the nature of few-shot algorithm parameter updates. Metric scaling provides improvements up to 14% in accuracy for certain metrics on the mini-Imagenet 5-way 5-shot classification task. We further propose a simple and effective way of conditioning a learner on the task sample set, resulting in learning a task-dependent metric space. Moreover, we propose and empirically test a practical end-to-end optimization procedure based on auxiliary task co-training to learn a task-dependent metric space. The resulting few-shot learning model based on the task-dependent scaled metric achieves state of the art on mini-Imagenet. We confirm these results on another few-shot dataset that we introduce in this paper based on CIFAR100. Our code is publicly available at this https URL.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1234  (*Influential: 202*)\n\n#### 2. Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Vaisakh Shaj, Dieter Buchler, Rohit Sonker, P. Becker, G. Neumann*\n\n**TL;DR:** This work introduces the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors that outperforms RSSMs and competing multi-task models on several challenging robotic benchmarks both on real-world systems and simulations.\n\n**Abstract:** Recurrent State-space models (RSSMs) are highly expressive models for learning patterns in time series data and system identification. However, these models assume that the dynamics are fixed and unchanging, which is rarely the case in real-world scenarios. Many control applications often exhibit tasks with similar but not identical dynamics which can be modeled as a latent variable. We introduce the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors. We present a simple and effective way of learning and performing inference over this Gaussian graphical model that avoids approximations like variational inference. We show that HiP-RSSMs outperforms RSSMs and competing multi-task models on several challenging robotic benchmarks both on real-world systems and simulations.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 3. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Tri Dao, Daniel Y. Fu, Khaled Kamal Saab, A. Thomas, A. Rudra, Christopher R\u00e9*\n\n**TL;DR:** A new SSM layer, H3, is proposed that is explicitly designed for the impact on language modeling and achieves promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 270  (*Influential: 21*)\n\n#### 4. Professor Forcing: A New Algorithm for Training Recurrent Networks\n\n*From Search Query: \"recurrent neural networks test time adaptation\"*\n\n*Anirudh Goyal, Alex Lamb, Ying Zhang, Saizheng Zhang, Aaron C. Courville, Yoshua Bengio*\n\n**TL;DR:** The Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps, is introduced.\n\n**Abstract:** The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network\u2019s own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2016\n\n**Citations:** 550  (*Influential: 38*)\n\n#### 5. Test-Time Adaptation via Conjugate Pseudo-labels\n\n*From Search Query: \"recurrent neural networks test time adaptation\"*\n\n*Sachin Goyal, Mingjie Sun, Aditi Raghunathan, Zico Kolter*\n\n**TL;DR:** A generic recipe is presented that can be used to find a good TTA loss for any given supervised training loss function of a general class, and can also be interpreted as a kind of self-training using a very specific soft label, which is referred to as the conjugate pseudolabel.\n\n**Abstract:** Test-time adaptation (TTA) refers to adapting neural networks to distribution shifts, with access to only the unlabeled test samples from the new domain at test-time. Prior TTA methods optimize over unsupervised objectives such as the entropy of model predictions in TENT [Wang et al., 2021], but it is unclear what exactly makes a good TTA loss. In this paper, we start by presenting a surprising phenomenon: if we attempt to meta-learn the best possible TTA loss over a wide class of functions, then we recover a function that is remarkably similar to (a temperature-scaled version of) the softmax-entropy employed by TENT. This only holds, however, if the classifier we are adapting is trained via cross-entropy; if trained via squared loss, a different best TTA loss emerges. To explain this phenomenon, we analyze TTA through the lens of the training losses's convex conjugate. We show that under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss and indeed, it recovers the best losses found by meta-learning. This leads to a generic recipe that can be used to find a good TTA loss for any given supervised training loss function of a general class. Empirically, our approach consistently dominates other baselines over a wide range of benchmarks. Our approach is particularly of interest when applied to classifiers trained with novel loss functions, e.g., the recently-proposed PolyLoss, where it differs substantially from (and outperforms) an entropy-based loss. Further, we show that our approach can also be interpreted as a kind of self-training using a very specific soft label, which we refer to as the conjugate pseudolabel. Overall, our method provides a broad framework for better understanding and improving test-time adaptation. Code is available at https://github.com/locuslab/tta_conjugate.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 73  (*Influential: 5*)\n\n#### 6. Interneurons accelerate learning dynamics in recurrent neural networks for statistical adaptation\n\n*From Search Query: \"recurrent neural networks test time adaptation\"*\n\n*David Lipshutz, C. Pehlevan, D. Chklovskii*\n\n**TL;DR:** The results suggest that interneurons are computationally useful for rapid adaptation to changing input statistics and can be viewed as a recurrent linear neural network analogue of the implicit acceleration phenomenon observed in overparameterized feedforward linear neural networks.\n\n**Abstract:** Early sensory systems in the brain rapidly adapt to fluctuating input statistics, which requires recurrent communication between neurons. Mechanistically, such recurrent communication is often indirect and mediated by local interneurons. In this work, we explore the computational benefits of mediating recurrent communication via interneurons compared with direct recurrent connections. To this end, we consider two mathematically tractable recurrent linear neural networks that statistically whiten their inputs -- one with direct recurrent connections and the other with interneurons that mediate recurrent communication. By analyzing the corresponding continuous synaptic dynamics and numerically simulating the networks, we show that the network with interneurons is more robust to initialization than the network with direct recurrent connections in the sense that the convergence time for the synaptic dynamics in the network with interneurons (resp. direct recurrent connections) scales logarithmically (resp. linearly) with the spectrum of their initialization. Our results suggest that interneurons are computationally useful for rapid adaptation to changing input statistics. Interestingly, the network with interneurons is an overparameterized solution of the whitening objective for the network with direct recurrent connections, so our results can be viewed as a recurrent linear neural network analogue of the implicit acceleration phenomenon observed in overparameterized feedforward linear neural networks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 10  (*Influential: 0*)\n\n#### 7. Enhancing Adaptive History Reserving by Spiking Convolutional Block Attention Module in Recurrent Neural Networks\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*Qi Xu, Yuyuan Gao, Jiangrong Shen, Yaxin Li, Xuming Ran, Huajin Tang, Gang Pan*\n\n**TL;DR:** This paper develops a recurrent spiking neural network model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns.\n\n**Abstract:** Spiking neural networks (SNNs) serve as one type of efficient model to process spatio-temporal patterns in time series, such as the Address-Event Representation data collected from Dynamic Vision Sensor (DVS). Although convolutional SNNs have achieved remarkable performance on these AER datasets, benefiting from the predominant spatial feature extraction ability of convolutional structure, they ignore temporal features related to sequential time points. In this paper, we develop a recurrent spiking neural network (RSNN) model embedded with an advanced spiking convolutional block attention module (SCBAM) component to combine both spatial and temporal features of spatio-temporal patterns. It invokes the history information in spatial and temporal channels adaptively through SCBAM, which brings the advantages of efficient memory calling and history redundancy elimination. The performance of our model was evaluated in DVS128-Gesture dataset and other time-series datasets. The experimental results show that the proposed SRNN-SCBAM model makes better use of the history information in spatial and temporal dimensions with less memory space, and achieves higher accuracy compared to other models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2024\n\n**Citations:** 7  (*Influential: 0*)\n\n#### 8. Robust Training of Neural Networks using Scale Invariant Architectures\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*Zhiyuan Li, Srinadh Bhojanapalli, M. Zaheer, Sashank J. Reddi, Surinder Kumar*\n\n**TL;DR:** A scale invariant version of BERT is designed, called SIBERT, which when trained simply by vanilla SGD achieves performance comparable to BERT trained by adaptive methods like Adam on downstream tasks.\n\n**Abstract:** In contrast to SGD, adaptive gradient methods like Adam allow robust training of modern deep networks, especially large language models. However, the use of adaptivity not only comes at the cost of extra memory but also raises the fundamental question: can non-adaptive methods like SGD enjoy similar benefits? In this paper, we provide an affirmative answer to this question by proposing to achieve both robust and memory-efficient training via the following general recipe: (1) modify the architecture and make it scale invariant, i.e. the scale of parameter doesn't affect the output of the network, (2) train with SGD and weight decay, and optionally (3) clip the global gradient norm proportional to weight norm multiplied by $\\sqrt{\\tfrac{2\\lambda}{\\eta}}$, where $\\eta$ is learning rate and $\\lambda$ is weight decay. We show that this general approach is robust to rescaling of parameter and loss by proving that its convergence only depends logarithmically on the scale of initialization and loss, whereas the standard SGD might not even converge for many initializations. Following our recipe, we design a scale invariant version of BERT, called SIBERT, which when trained simply by vanilla SGD achieves performance comparable to BERT trained by adaptive methods like Adam on downstream tasks.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 23  (*Influential: 1*)\n\n#### 9. Normalizing flow neural networks by JKO scheme\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*Chen Xu, Xiuyuan Cheng, Yao Xie*\n\n**TL;DR:** A neural ODE flow network inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which unfolds the discrete-time dynamic of the Wasserstein gradient flow is developed, achieving competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost.\n\n**Abstract:** Normalizing flow is a class of deep generative models for efficient sampling and likelihood estimation, which achieves attractive performance, particularly in high dimensions. The flow is often implemented using a sequence of invertible residual blocks. Existing works adopt special network architectures and regularization of flow trajectories. In this paper, we develop a neural ODE flow network called JKO-iFlow, inspired by the Jordan-Kinderleherer-Otto (JKO) scheme, which unfolds the discrete-time dynamic of the Wasserstein gradient flow. The proposed method stacks residual blocks one after another, allowing efficient block-wise training of the residual blocks, avoiding sampling SDE trajectories and score matching or variational learning, thus reducing the memory load and difficulty in end-to-end training. We also develop adaptive time reparameterization of the flow network with a progressive refinement of the induced trajectory in probability space to improve the model accuracy further. Experiments with synthetic and real data show that the proposed JKO-iFlow network achieves competitive performance compared with existing flow and diffusion models at a significantly reduced computational and memory cost.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 15  (*Influential: 2*)\n\n### 6 related papers from ArXiv\n\n#### 1. NEAT: Nonlinear Parameter-efficient Adaptation of Pre-trained Models\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Yibo Zhong, Haoxiang Jiang, Lincan Li, Ryumei Nakada, Tianci Liu, Linjun Zhang, Huaxiu Yao, Haoyu Wang*\n\n**Abstract:** Fine-tuning pre-trained models is crucial for adapting large models to\ndownstream tasks, often delivering state-of-the-art performance. However,\nfine-tuning all model parameters is resource-intensive and laborious, leading\nto the emergence of parameter-efficient fine-tuning (PEFT) methods. One widely\nadopted PEFT technique, Low-Rank Adaptation (LoRA), freezes the pre-trained\nmodel weights and introduces two low-rank matrices whose ranks are\nsignificantly smaller than the dimensions of the original weight matrices. This\nenables efficient fine-tuning by adjusting only a small number of parameters.\nDespite its efficiency, LoRA approximates weight updates using low-rank\ndecomposition, which struggles to capture complex, non-linear components and\nefficient optimization trajectories. As a result, LoRA-based methods often\nexhibit a significant performance gap compared to full fine-tuning. Closing\nthis gap requires higher ranks, which increases the number of parameters. To\naddress these limitations, we propose a nonlinear parameter-efficient\nadaptation method (NEAT). NEAT introduces a lightweight neural network that\ntakes pre-trained weights as input and learns a nonlinear transformation to\napproximate cumulative weight updates. These updates can be interpreted as\nfunctions of the corresponding pre-trained weights. The nonlinear approximation\ndirectly models the cumulative updates, effectively capturing complex and\nnon-linear structures in the weight updates. Our theoretical analysis\ndemonstrates taht NEAT can be more efficient than LoRA while having equal or\ngreater expressivity. Extensive evaluations across four benchmarks and over\ntwenty datasets demonstrate that NEAT significantly outperforms baselines in\nboth vision and text tasks.\n\n**Published:** 2024-10-02T17:29:23Z  (*Updated: 2024-10-02T17:29:23Z*)\n\n\n\n#### 2. Revisiting Dynamic Evaluation: Online Adaptation for Large Language\n  Models\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Amal Rannen-Triki, Jorg Bornschein, Razvan Pascanu, Marcus Hutter, Andras Gy\u00f6rgy, Alexandre Galashov, Yee Whye Teh, Michalis K. Titsias*\n\n**Abstract:** We consider the problem of online fine tuning the parameters of a language\nmodel at test time, also known as dynamic evaluation. While it is generally\nknown that this approach improves the overall predictive performance,\nespecially when considering distributional shift between training and\nevaluation data, we here emphasize the perspective that online adaptation turns\nparameters into temporally changing states and provides a form of\ncontext-length extension with memory in weights, more in line with the concept\nof memory in neuroscience. We pay particular attention to the speed of\nadaptation (in terms of sample efficiency),sensitivity to the overall\ndistributional drift, and the computational overhead for performing gradient\ncomputations and parameter updates. Our empirical study provides insights on\nwhen online adaptation is particularly interesting. We highlight that with\nonline adaptation the conceptual distinction between in-context learning and\nfine tuning blurs: both are methods to condition the model on previously\nobserved tokens.\n\n**Published:** 2024-03-03T14:03:48Z  (*Updated: 2024-03-03T14:03:48Z*)\n\n\n\n#### 3. Attending to Mathematical Language with Transformers\n\n*From Search Query: \"recurrent neural networks test time adaptation\"*\n\n*Artit Wangperawong*\n\n**Abstract:** Mathematical expressions were generated, evaluated and used to train neural\nnetwork models based on the transformer architecture. The expressions and their\ntargets were analyzed as a character-level sequence transduction task in which\nthe encoder and decoder are built on attention mechanisms. Three models were\ntrained to understand and evaluate symbolic variables and expressions in\nmathematics: (1) the self-attentive and feed-forward transformer without\nrecurrence or convolution, (2) the universal transformer with recurrence, and\n(3) the adaptive universal transformer with recurrence and adaptive computation\ntime. The models respectively achieved test accuracies as high as 76.1%, 78.8%\nand 84.9% in evaluating the expressions to match the target values. For the\ncases inferred incorrectly, the results differed from the targets by only one\nor two characters. The models notably learned to add, subtract and multiply\nboth positive and negative decimal numbers of variable digits assigned to\nsymbolic variables.\n\n**Published:** 2018-12-05T03:05:08Z  (*Updated: 2019-09-14T20:03:50Z*)\n\n\n\n#### 4. Training Integer-Only Deep Recurrent Neural Networks\n\n*From Search Query: \"recurrent neural networks test time adaptation\"*\n\n*Vahid Partovi Nia, Eyy\u00fcb Sari, Vanessa Courville, Masoud Asgharian*\n\n**Abstract:** Recurrent neural networks (RNN) are the backbone of many text and speech\napplications. These architectures are typically made up of several\ncomputationally complex components such as; non-linear activation functions,\nnormalization, bi-directional dependence and attention. In order to maintain\ngood accuracy, these components are frequently run using full-precision\nfloating-point computation, making them slow, inefficient and difficult to\ndeploy on edge devices. In addition, the complex nature of these operations\nmakes them challenging to quantize using standard quantization methods without\na significant performance drop. We present a quantization-aware training method\nfor obtaining a highly accurate integer-only recurrent neural network (iRNN).\nOur approach supports layer normalization, attention, and an adaptive piecewise\nlinear (PWL) approximation of activation functions, to serve a wide range of\nstate-of-the-art RNNs. The proposed method enables RNN-based language models to\nrun on edge devices with $2\\times$ improvement in runtime, and $4\\times$\nreduction in model size while maintaining similar accuracy as its\nfull-precision counterpart.\n\n**Published:** 2022-12-22T15:22:36Z  (*Updated: 2022-12-22T15:22:36Z*)\n\n\n\n#### 5. Improving Neural Language Models with a Continuous Cache\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*Edouard Grave, Armand Joulin, Nicolas Usunier*\n\n**Abstract:** We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n\n**Published:** 2016-12-13T23:09:49Z  (*Updated: 2016-12-13T23:09:49Z*)\n\n\n\n#### 6. Efficient Second-Order Neural Network Optimization via Adaptive Trust\n  Region Methods\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*James Vo*\n\n**Abstract:** Second-order optimization methods offer notable advantages in training deep\nneural networks by utilizing curvature information to achieve faster\nconvergence. However, traditional second-order techniques are computationally\nprohibitive, primarily due to the large matrix inversions and high memory\ndemands they require. While adaptive trust-region methods have been developed\nto mitigate these issues, their performance is often hindered by conservative\nestimates of key parameters, such as the Lipschitz constant of the Hessian,\nresulting in suboptimal outcomes. In this paper, we introduce\nSecondOrderAdaptiveAdam (SOAA), a novel optimization algorithm designed to\novercome these limitations. SOAA approximates the Fisher information matrix\nusing a diagonal representation, reducing computational complexity from\n\\(O(n^{2})\\) to \\(O(n)\\), thereby making it suitable for large-scale deep\nlearning models, including large language models (LLMs). Additionally, the\nalgorithm integrates an adaptive trust-region mechanism that dynamically\nadjusts the trust region size based on observed loss reduction, ensuring both\nrobust convergence and computational efficiency. We empirically demonstrate\nthat SOAA achieves faster and more stable convergence compared to first-order\noptimizers, such as Adam, under similar computational constraints. However, the\ndiagonal approximation of the Fisher information matrix may be less effective\nin capturing higher-order interactions between gradients, suggesting potential\nareas for further refinement and future research.\n\n**Published:** 2024-10-03T08:23:06Z  (*Updated: 2024-10-03T08:23:06Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Medical SAM Adapter: Adapting Segment Anything Model for Medical Image Segmentation\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Min Xu, Huazhu Fu, Yuanpei Liu, Wei Ji, Yueming Jin, Yanwu Xu, Junde Wu*\n\n**Abstract:** The Segment Anything Model (SAM) has recently gained popularity in the field of image segmentation due to its impressive capabilities in various segmentation tasks and its prompt-based interface. However, recent studies and individual experiments have shown that SAM underperforms in medical image segmentation, since the lack of the medical specific knowledge. This raises the question of how to enhance SAM's segmentation capability for medical images. In this paper, instead of fine-tuning the SAM model, we propose the Medical SAM Adapter (Med-SA), which incorporates domain-specific medical knowledge into the segmentation model using a light yet effective adaptation technique. In Med-SA, we propose Space-Depth Transpose (SD-Trans) to adapt 2D SAM to 3D medical images and Hyper-Prompting Adapter (HyP-Adpt) to achieve prompt-conditioned adaptation. We conduct comprehensive evaluation experiments on 17 medical image segmentation tasks across various image modalities. Med-SA outperforms several state-of-the-art (SOTA) medical image segmentation methods, while updating only 2\\% of the parameters. Our code is released at https://github.com/KidsWithTokens/Medical-SAM-Adapter.\n\n**Published:** 2023-04-25\n\n\n\n#### 2. Multigrid Backprojection Super-Resolution and Deep Filter Visualization\n\n*From Search Query: \"adaptive parameter updates state space models\"*\n\n*Pablo Navarrete Michelini, Hanwen Liu, Dan Zhu*\n\n**Abstract:** We introduce a novel deep-learning architecture for image upscaling by large\nfactors (e.g. 4x, 8x) based on examples of pristine high-resolution images. Our\ntarget is to reconstruct high-resolution images from their downscale versions.\nThe proposed system performs a multi-level progressive upscaling, starting from\nsmall factors (2x) and updating for higher factors (4x and 8x). The system is\nrecursive as it repeats the same procedure at each level. It is also residual\nsince we use the network to update the outputs of a classic upscaler. The\nnetwork residuals are improved by Iterative Back-Projections (IBP) computed in\nthe features of a convolutional network. To work in multiple levels we extend\nthe standard back-projection algorithm using a recursion analogous to\nMulti-Grid algorithms commonly used as solvers of large systems of linear\nequations. We finally show how the network can be interpreted as a standard\nupsampling-and-filter upscaler with a space-variant filter that adapts to the\ngeometry. This approach allows us to visualize how the network learns to\nupscale. Finally, our system reaches state of the art quality for models with\nrelatively few number of parameters.\n\n**Published:** 2018-09-25\n\n\n\n#### 3. Layer Normalization\n\n*From Search Query: \"recurrent neural networks test time adaptation\"*\n\n*Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton*\n\n**Abstract:** Training state-of-the-art, deep neural networks is computationally expensive.\nOne way to reduce the training time is to normalize the activities of the\nneurons. A recently introduced technique called batch normalization uses the\ndistribution of the summed input to a neuron over a mini-batch of training\ncases to compute a mean and variance which are then used to normalize the\nsummed input to that neuron on each training case. This significantly reduces\nthe training time in feed-forward neural networks. However, the effect of batch\nnormalization is dependent on the mini-batch size and it is not obvious how to\napply it to recurrent neural networks. In this paper, we transpose batch\nnormalization into layer normalization by computing the mean and variance used\nfor normalization from all of the summed inputs to the neurons in a layer on a\nsingle training case. Like batch normalization, we also give each neuron its\nown adaptive bias and gain which are applied after the normalization but before\nthe non-linearity. Unlike batch normalization, layer normalization performs\nexactly the same computation at training and test times. It is also\nstraightforward to apply to recurrent neural networks by computing the\nnormalization statistics separately at each time step. Layer normalization is\nvery effective at stabilizing the hidden state dynamics in recurrent networks.\nEmpirically, we show that layer normalization can substantially reduce the\ntraining time compared with previously published techniques.\n\n**Published:** 2016-07-21\n\n\n\n#### 4. Do We Need Anisotropic Graph Neural Networks?\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*Nicholas D. Lane, Pietro Li\u00f2, Felix L. Opolka, Shyam A. Tailor*\n\n**Abstract:** Common wisdom in the graph neural network (GNN) community dictates that anisotropic models -- in which messages sent between nodes are a function of both the source and target node -- are required to achieve state-of-the-art performance. Benchmarks to date have demonstrated that these models perform better than comparable isotropic models -- where messages are a function of the source node only. In this work we provide empirical evidence challenging this narrative: we propose an isotropic GNN, which we call Efficient Graph Convolution (EGC), that consistently outperforms comparable anisotropic models, including the popular GAT or PNA architectures by using spatially-varying adaptive filters. In addition to raising important questions for the GNN community, our work has significant real-world implications for efficiency. EGC achieves higher model accuracy, with lower memory consumption and latency, along with characteristics suited to accelerator implementation, while being a drop-in replacement for existing architectures. As an isotropic model, it requires memory proportional to the number of vertices in the graph ($\\mathcal{O}(V)$); in contrast, anisotropic models require memory proportional to the number of edges ($\\mathcal{O}(E)$). We demonstrate that EGC outperforms existing approaches across 6 large and diverse benchmark datasets, and conclude by discussing questions that our work raise for the community going forward. Code and pretrained models for our experiments are provided at https://github.com/shyam196/egc.\n\n**Published:** 2021-04-03\n\n\n\n#### 5. PointRend: Image Segmentation as Rendering\n\n*From Search Query: \"memory efficient adaptive neural networks\"*\n\n*Ross Girshick, Yuxin Wu, Alexander Kirillov, Kaiming He*\n\n**Abstract:** We present a new method for efficient high-quality image segmentation of objects and scenes. By analogizing classical computer graphics methods for efficient rendering with over- and undersampling challenges faced in pixel labeling tasks, we develop a unique perspective of image segmentation as a rendering problem. From this vantage, we present the PointRend (Point-based Rendering) neural network module: a module that performs point-based segmentation predictions at adaptively selected locations based on an iterative subdivision algorithm. PointRend can be flexibly applied to both instance and semantic segmentation tasks by building on top of existing state-of-the-art models. While many concrete implementations of the general idea are possible, we show that a simple design already achieves excellent results. Qualitatively, PointRend outputs crisp object boundaries in regions that are over-smoothed by previous methods. Quantitatively, PointRend yields significant gains on COCO and Cityscapes, for both instance and semantic segmentation. PointRend's efficiency enables output resolutions that are otherwise impractical in terms of memory or computation compared to existing approaches. Code has been made available at https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend.\n\n**Conference:** pointrend-image-segmentation-as-rendering-1\n\n**Published:** 2019-12-17\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design with a focus on adaptive parameter updates, test-time adaptation, and memory efficiency, here are some key findings and references that align with your requirements:\n\n## Adaptive Parameter Updates in State Space Models\n\n- The paper on **Composite Learning Units (CLUs)** introduces a framework where the model can learn during inference without traditional parameter updates. CLUs continuously refine their knowledge spaces through feedback, enabling dynamic adaptability and real-time learning. This approach is particularly relevant for state space models as it allows for adaptive parameter updates without the need for costly retraining.\n\n- **MatMamba**, a state space model, combines Matryoshka-style learning with Mamba2, enabling joint training and adaptive inference. This model allows for efficient deployment across various model sizes and incorporates nested dimensions for adaptive processing. While it does not explicitly discuss parameter updates during inference, it highlights the adaptivity and efficiency of state space models, which can be extended to include dynamic parameter adaptation.\n\n## Recurrent Neural Networks and Test-Time Adaptation\n\n- The concept of **test-time training** is crucial for adapting models to new or changing inputs. A method proposed in the context of domain adaptation shows that self-supervision during training paired with test-time training at inference can close the performance gap due to natural distribution shifts. This approach can be integrated with recurrent neural networks to enable adaptive updates during test time.\n\n- **CLU** also integrates a feedback-driven learning approach, which can be seen as a form of test-time training. By continuously refining its knowledge spaces through feedback, CLU adapts to new information without the need for retraining, making it a promising candidate for integrating with recurrent architectures.\n\n## Memory-Efficient Implementations\n\n- **MatMamba** emphasizes the efficiency of state space models, particularly in terms of memory and computational resources. By using nested granularities of smaller submodels within a larger model, MatMamba allows for adaptive inference at intermediate granularities without significant loss in accuracy. This approach is memory-efficient and can be scaled to various model sizes.\n\n- **DenseMamba**, another state space model, retains fine-grained information through dense connections while maintaining training parallelizability and inference efficiency. This design ensures that the model remains memory-efficient while improving information flow and expressiveness in the Analysis Note.\n\n## Stability Analysis of Adaptive Updates\n\n- While the papers mentioned do not delve deeply into stability analysis specifically, the **CLU** system's ability to learn from feedback and adapt dynamically suggests a robust mechanism for handling stability. The system's persistence through negative feedback and its ability to refine internal representations until a breakthrough is achieved indicate a stable adaptation process.\n\n## Integration with Test-Time Training Approaches\n\n- The integration of **test-time training** with state space models and recurrent architectures is a promising area. The method of self-supervision during training paired with test-time training can be applied to these models to enhance their adaptability to new or changing inputs. This approach ensures that the models can adapt dynamically without extensive retraining, aligning with the goals of improving autoregressive language models.\n\nIn summary, the CLU and MatMamba models offer significant insights into adaptive parameter updates, test-time adaptation, and memory-efficient implementations, which can be crucial for improving the design of autoregressive language models.\n",
            "analysis": "Analysis Note for AdaptiveSSMamba Proposal\n\n1. Summary of Analysis:\nThe initial search reveals several key aspects that require deeper investigation:\n\na) State Space Model Innovations:\n- Mamba architecture introduces selective state propagation for content-based reasoning\n- Recent work on adaptive discretization and compression techniques\n- Hardware-aware parallel algorithms for efficient implementation\n\nb) Test-Time Training Integration:\n- Limited research on combining SSMs with test-time adaptation\n- Some promising results in domain adaptation scenarios\n- Need to explore stability and efficiency trade-offs\n\nc) Hardware Efficiency:\n- Emerging techniques for efficient LLM deployment\n- Memory management optimizations\n- Hardware-aware algorithm design\n\n2. Useful References with Excerpts:\n\nFrom \"Mamba: Linear-Time Sequence Modeling\":\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nFrom \"DenseMamba\":\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom \"Test-Time Training Can Close the Natural Distribution Shift\":\n\"We propose a domain adaptation method [...] that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap.\"\n\n3. Key Insights:\n\na) State Space Model Design:\n- Selective state propagation is crucial for model expressiveness\n- Dense connections can improve information flow without sacrificing efficiency\n- Hardware-aware algorithm design is essential for practical implementation\n\nb) Adaptation Mechanisms:\n- Self-supervision during training can enhance test-time adaptation\n- Domain adaptation techniques might be applicable to SSM architectures\n- Need to balance adaptation capability with computational efficiency\n\nc) Hardware Considerations:\n- Memory management is critical for efficient implementation\n- Parallel algorithms can significantly improve performance\n- Trade-offs between model complexity and hardware efficiency need careful consideration\n\n4. Future Search Plan:\n\nRound 2 Focus:\n- Search for papers specifically on adaptive parameter updates in SSMs\n- Look for work combining recurrent architectures with test-time training\n- Investigate memory-efficient implementations of adaptive models\n\nSearch queries needed:\n1. \"adaptive parameter updates state space models\"\n2. \"recurrent neural networks test time adaptation\"\n3. \"memory efficient adaptive neural networks\"\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752.\n\n[2] He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Yu, A., Nigmetov, A., Morozov, D., Mahoney, M. W., & Erichson, N. B. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. arXiv preprint.\n\n[4] Darestani, M. Z., Liu, J., & Heckel, R. (2022). Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing. International Conference on Machine Learning.\n\n[5] Smith, J. T. H., De Mello, S., Kautz, J., Linderman, S. W., & Byeon, W. (2023). Convolutional State Space Models for Long-Range Spatiotemporal Modeling. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "\"hardware optimization state space models neural networks\", \"stability analysis adaptive neural networks\", \"efficient memory management deep learning\"",
            "detail": "Find papers discussing hardware optimization and stability analysis of state space models and adaptive neural networks, particularly focusing on:\n1. Hardware-aware implementation strategies\n2. Memory management techniques\n3. Stability analysis frameworks\n4. Efficient parameter update mechanisms",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing hardware optimization and stability analysis of state space models and adaptive neural networks, particularly focusing on:\n1. Hardware-aware implementation strategies\n2. Memory management techniques\n3. Stability analysis frameworks\n4. Efficient parameter update mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.38)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.38)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. Structured state-space models are deep Wiener models (Avg. Score: 0.29)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 16/22 (Score: 0.29)*\n\n```\nForgione, M. and Piga, D. (2021). dynoNet: A neural network architecture for learning dynamical systems. International Journal of Adaptive Control and Signal Processing, 35(4), 612-626. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. (2020). Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 1474-1487. Gu, A., Goel, K., Gupta, A., and R\u00e9, C. (2022). On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 35971-35983. Gu, A., Goel, K., and R\u00e9, C. (2021). Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Gupta, A., Gu, A., and Berant, J. (2022). Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35, $22982-22994$. Kumar, S.K. (2017). On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863. Lanzetti, N. et al. (2019). Recurrent neural network based MPC for process industries. In 2019 18th European Control Conference (ECC), 1005-1010. IEEE. Ljung, L., Zhang, Q., Lindskog, P., and Juditski, A. (2004). Estimation of grey box and black box models for nonlinear circuit data. IFAC Proceedings Volumes, 37(13), $399-404$. Marconato, A., Sj\u00f6berg, J., Suykens, J.A., and Schoukens, J. (2013). Improved initialization for nonlinear statespace modeling. IEEE Transactions on instrumentation and Measurement, 63(4), 972-980.\n```\n\n#### 3. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.13)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 5/20 (Score: 0.13)*\n\n```\nThe definition is given in one-dimensional case, but it can be generalized to multi-variable case by taking different unit inputs in various coordinates. The motivation for the above definition is based on the idea of measuring the earlier input at the later output. In our experiment, we construct various RNN models and SSM with random generated weights. It can be seen in Figure 5 that the memory of naive state-space models also has an exponentially decay memory. It is consistent with the previous theorem that state-space model has an asymptotic exponentially decaying memory. Notice that here the naive SSM is simply adding a tanh activation across layers without specially tuning the weights. Such random initialization can expose the memory issue more significantly as the S 4 layer is constructed with several parameterization techniques. However, the manually constructed S 4 still has an asymptotic exponential decaying memory as is shown in Figure 6\n\n## 5 Related Work\n\nIn this section, we introduce the previous works on state-space models. As the single-layer statespace model is a linear RNN, we summarize the related approximation work on RNN. In particular, the approximation result and memory result is emphasized as this paper works on the universal approximation property and memory decay property of SSM. State-space models State-space models originate from the HIPPO matrix which is optimal in the online function approximation sense [2-4]. The Hippo matrix initialization for recurrent matrix $W$ enables the state-space model to have a slow decaying memory.\n```\n\n#### 4. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.06)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.06)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n#### 5. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.06)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 21/27 (Score: 0.06)*\n\n```\n[17] D. P. Kingma and J. Ba, \"Adam: A method for stochastic optimization,\" 2014. [Online]. Available: https://arxiv.org/abs/1412. 6980\n[18] A. Gu, A. Gupta, K. Goel, and C. R\u00e9, \"On the Parameterization and Initialization of Diagonal State Space Models,\" 2022. [Online]. Available: https://arxiv.org/abs/2206.11893\n[19] J. T. Smith, A. Warrington, and S. Linderman, \"Simplified State Space Layers for Sequence Modeling,\" in The Eleventh International Conference on Learning Representations, 2023.\n```\n\n\n\n---\n## Found 21 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: \"hardware optimization state space models neural networks\", \"stability analysis adaptive neural networks\", \"efficient memory management deep learning\"\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. ATFormer: A Learned Performance Model with Transfer Learning Across Devices for Deep Learning Tensor Programs\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Yang Bai, Wenqian Zhao, Shuo Yin, Zixiao Wang, Bei Yu*\n\n**TL;DR:** ATFormer is presented, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space and can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks.\n\n**Abstract:** The training and inference ef\ufb01ciency of ever-larger deep neural networks highly rely on the performance of tensor operators on speci\ufb01c hardware platforms. Therefore, a compilation-based optimization \ufb02ow with automatic tensor generation and parameter tuning is necessary for ef\ufb01cient model deployment. While compilation-based methods with performance models can provide dynamic and suitable code optimization, they suffer from a large design space exploration with rough measurement accuracy and poor transferability among different hardware platforms. This paper presents ATFormer, a simple yet ef\ufb01cient design with attention-inspired modules to accurately predict the performance of optimized operators by capturing global and long-range dependencies within a complete scheduling space. Compared with state-of-the-arts, ATFormer can predict the optimal implementation of tensor operators to reduce inference time with minimal effort on modern DNN benchmarks. Furthermore, AT-Former with pre-trained parameters can quickly adapt to different workloads and hardware via transfer learning.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Convolutional State Space Models for Long-Range Spatiotemporal Modeling\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Jimmy T.H. Smith, Shalini De Mello, Jan Kautz, Scott W. Linderman, Wonmin Byeon*\n\n**TL;DR:** This work addresses the challenges of prior methods and introduces convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5 and develops an equivalence between ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies.\n\n**Abstract:** Effectively modeling long spatiotemporal sequences is challenging due to the need to model complex spatial correlations and long-range temporal dependencies simultaneously. ConvLSTMs attempt to address this by updating tensor-valued states with recurrent neural networks, but their sequential computation makes them slow to train. In contrast, Transformers can process an entire spatiotemporal sequence, compressed into tokens, in parallel. However, the cost of attention scales quadratically in length, limiting their scalability to longer sequences. Here, we address the challenges of prior methods and introduce convolutional state space models (ConvSSM) that combine the tensor modeling ideas of ConvLSTM with the long sequence modeling approaches of state space methods such as S4 and S5. First, we demonstrate how parallel scans can be applied to convolutional recurrences to achieve subquadratic parallelization and fast autoregressive generation. We then establish an equivalence between the dynamics of ConvSSMs and SSMs, which motivates parameterization and initialization strategies for modeling long-range dependencies. The result is ConvS5, an efficient ConvSSM variant for long-range spatiotemporal modeling. ConvS5 significantly outperforms Transformers and ConvLSTM on a long horizon Moving-MNIST experiment while training 3X faster than ConvLSTM and generating samples 400X faster than Transformers. In addition, ConvS5 matches or exceeds the performance of state-of-the-art methods on challenging DMLab, Minecraft and Habitat prediction benchmarks and enables new directions for modeling long spatiotemporal sequences.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 0*)\n\n#### 3. Liquid Structural State-Space Models\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 67  (*Influential: 10*)\n\n#### 4. Overcoming Recency Bias of Normalization Statistics in Continual Learning: Balance and Adaptation\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Yilin Lyu, Liyuan Wang, Xingxing Zhang, Zicheng Sun, Hang Su, Jun Zhu, Liping Jing*\n\n**TL;DR:** This work focuses on the most popular Batch Normalization and provides an in-depth theoretical analysis of its sub-optimality in continual learning, and proposes Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages.\n\n**Abstract:** Continual learning entails learning a sequence of tasks and balancing their knowledge appropriately. With limited access to old training samples, much of the current work in deep neural networks has focused on overcoming catastrophic forgetting of old tasks in gradient-based optimization. However, the normalization layers provide an exception, as they are updated interdependently by the gradient and statistics of currently observed training samples, which require specialized strategies to mitigate recency bias. In this work, we focus on the most popular Batch Normalization (BN) and provide an in-depth theoretical analysis of its sub-optimality in continual learning. Our analysis demonstrates the dilemma between balance and adaptation of BN statistics for incremental tasks, which potentially affects training stability and generalization. Targeting on these particular challenges, we propose Adaptive Balance of BN (AdaB$^2$N), which incorporates appropriately a Bayesian-based strategy to adapt task-wise contributions and a modified momentum to balance BN statistics, corresponding to the training and testing stages. By implementing BN in a continual learning fashion, our approach achieves significant performance gains across a wide range of benchmarks, particularly for the challenging yet realistic online scenarios (e.g., up to 7.68%, 6.86% and 4.26% on Split CIFAR-10, Split CIFAR-100 and Split Mini-ImageNet, respectively). Our code is available at https://github.com/lvyilin/AdaB2N.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 5. Adaptive Smoothing Gradient Learning for Spiking Neural Networks\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Ziming Wang, Runhao Jiang, S. Lian, Rui Yan, Huajin Tang*\n\n**TL;DR:** This work proposes a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise, in this way, the network learns adaptively the accurate gradients of loss landscape in SNNs.\n\n**Abstract:** Spiking neural networks (SNNs) with biologically inspired spatio-temporal dynamics demonstrate superior energy efficiency on neuromorphic architectures. Error backpropagation in SNNs is pro-hibited by the all-or-none nature of spikes. The existing solution circumvents this problem by a relaxation on the gradient calculation using a continuous function with a constant relaxation degree, so-called surrogate gradient learning. Nevertheless, such a solution introduces additional smoothing error on spike firing which leads to the gradients being estimated inaccurately. Thus, how to adaptively adjust the relaxation degree and eliminate smoothing error progressively is crucial. Here, we propose a methodology such that training a prototype neural network will evolve into training an SNN gradually by fusing the learnable relaxation degree into the network with random spike noise. In this way, the network learns adaptively the accurate gradients of loss landscape in SNNs. The theoretical analysis further shows optimization on such a noisy network could be evolved into optimization on the embedded SNN with shared weights progressively. Moreover, The experiments on static images, dynamic event streams, speech, and instrumental sounds show the proposed method achieves state-of-the-art performance across all the datasets with remarkable robustness on different relaxation degrees.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 6. Finite-Time Analysis of Adaptive Temporal Difference Learning with Deep Neural Networks\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Tao Sun*\n\n**TL;DR:** This paper establishes the finite-time analysis for the adaptive TD with multi-layer ReLU networks approximation whose samples are generated from a Markov decision process and shows that if the width of the deep neural network is large enough, the adaptive TD using neural network approximation can find the (optimal) value function under the same iteration complexity as TD in general cases.\n\n**Abstract:** Temporal difference (TD) learning with function approximations (linear functions or neural networks) has achieved remarkable empirical success, giving impetus to the development of finite-time analysis. As an accelerated version of TD, the adaptive TD has been proposed and proved to enjoy finite-time convergence under the linear function approximation. Existing numerical results have demonstrated the superiority of adaptive algorithms to vanilla ones. Nevertheless, the performance guarantee of adaptive TD with neural network approximation remains widely unknown. This paper establishes the finite-time analysis for the adaptive TD with multi-layer ReLU networks approximation whose samples are generated from a Markov decision process. Our established theory shows that if the width of the deep neural network is large enough, the adaptive TD using neural network approximation can find the (optimal) value function with high probabilities under the same iteration complexity as TD in general cases. Furthermore, we show that the adaptive TD using neural network approximation, with the same width and searching area, can achieve theoretical acceleration when the stochastic semi-gradients decay fast.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 7. StreamNet: Memory-Efficient Streaming Tiny Deep Learning Inference on the Microcontroller\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Hong-Sheng Zheng, Yu-Yuan Liu, Chen-Fong Hsu, Tsung Tai Yeh*\n\n**TL;DR:** StreamNet uses 1D and 2D streaming processing and provides an parameter selection algorithm that automatically improve the performance of patch-based inference with minimal requirements on the MCU\u2019s SRAM memory space.\n\n**Abstract:** With the emerging Tiny Machine Learning (TinyML) inference applications, there is a growing interest when deploying TinyML models on the low-power Microcon-troller Unit (MCU). However, deploying TinyML models on MCUs reveals several challenges due to the MCU\u2019s resource constraints, such as small flash memory, tight SRAM memory budget, and slow CPU performance. Unlike typical layer-wise inference, patch-based inference reduces the peak usage of SRAM memory on MCUs by saving small patches rather than the entire tensor in the SRAM memory. However, the processing of patch-based inference tremendously increases the amount of MACs against the layer-wise method. Thus, this notoriously computational overhead makes patch-based inference undesirable on MCUs. This work designs StreamNet that employs the stream buffer to eliminate the redundant computation of patch-based inference. StreamNet uses 1D and 2D streaming processing and provides an parameter selection algorithm that automatically improve the performance of patch-based inference with minimal requirements on the MCU\u2019s SRAM memory space. In 10 TinyML models, StreamNet-2D achieves a geometric mean of 7.3X speedup and saves 81% of MACs over the state-of-the-art patch-based inference.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 8. MCUNetV2: Memory-Efficient Patch-based Inference for Tiny Deep Learning\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Ji Lin, Wei-Ming Chen, Han Cai, Chuang Gan, Song Han*\n\n**TL;DR:** This study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.\n\n**Abstract:** Tiny deep learning on microcontroller units (MCUs) is challenging due to the limited memory size. We find that the memory bottleneck is due to the imbalanced memory distribution in convolutional neural network (CNN) designs: the first several blocks have an order of magnitude larger memory usage than the rest of the network. To alleviate this issue, we propose a generic patch-by-patch inference scheduling, which operates only on a small spatial region of the feature map and significantly cuts down the peak memory. However, naive implementation brings overlapping patches and computation overhead. We further propose network redistribution to shift the receptive field and FLOPs to the later stage and reduce the computation overhead. Manually redistributing the receptive field is difficult. We automate the process with neural architecture search to jointly optimize the neural architecture and inference scheduling, leading to MCUNetV2. Patch-based inference effectively reduces the peak memory usage of existing networks by 4-8x. Co-designed with neural networks, MCUNetV2 sets a record ImageNet accuracy on MCU (71.8%), and achieves>90% accuracy on the visual wake words dataset under only 32kB SRAM. MCUNetV2 also unblocks object detection on tiny devices, achieving 16.9% higher mAP on Pascal VOC compared to the state-of-the-art result. Our study largely addressed the memory bottleneck in tinyML and paved the way for various vision applications beyond image classification.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 123  (*Influential: 17*)\n\n#### 9. AutoChunk: Automated Activation Chunk for Memory-Efficient Deep Learning Inference\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Xuanlei Zhao, Shenggan Cheng, Guangyang Lu, Hao Zhou, Bin Jia, Yang You*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n### 6 related papers from ArXiv\n\n#### 1. FlexiBERT: Are Current Transformer Architectures too Homogeneous and\n  Rigid?\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Shikhar Tuli, Bhishma Dedhia, Shreshth Tuli, Niraj K. Jha*\n\n**Abstract:** The existence of a plethora of language models makes the problem of selecting\nthe best one for a custom task challenging. Most state-of-the-art methods\nleverage transformer-based models (e.g., BERT) or their variants. Training such\nmodels and exploring their hyperparameter space, however, is computationally\nexpensive. Prior work proposes several neural architecture search (NAS) methods\nthat employ performance predictors (e.g., surrogate models) to address this\nissue; however, analysis has been limited to homogeneous models that use fixed\ndimensionality throughout the network. This leads to sub-optimal architectures.\nTo address this limitation, we propose a suite of heterogeneous and flexible\nmodels, namely FlexiBERT, that have varied encoder layers with a diverse set of\npossible operations and different hidden dimensions. For better-posed surrogate\nmodeling in this expanded design space, we propose a new graph-similarity-based\nembedding scheme. We also propose a novel NAS policy, called BOSHNAS, that\nleverages this new scheme, Bayesian modeling, and second-order optimization, to\nquickly train and use a neural surrogate model to converge to the optimal\narchitecture. A comprehensive set of experiments shows that the proposed\npolicy, when applied to the FlexiBERT design space, pushes the performance\nfrontier upwards compared to traditional models. FlexiBERT-Mini, one of our\nproposed models, has 3% fewer parameters than BERT-Mini and achieves 8.9%\nhigher GLUE score. A FlexiBERT model with equivalent performance as the best\nhomogeneous model achieves 2.6x smaller size. FlexiBERT-Large, another proposed\nmodel, achieves state-of-the-art results, outperforming the baseline models by\nat least 5.7% on the GLUE benchmark.\n\n**Published:** 2022-05-23T22:44:34Z  (*Updated: 2022-05-23T22:44:34Z*)\n\n\n\n#### 2. Sparse Modular Activation for Efficient Sequence Modeling\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Liliang Ren, Yang Liu, Shuohang Wang, Yichong Xu, Chenguang Zhu, ChengXiang Zhai*\n\n**Abstract:** Recent hybrid models combining Linear State Space Models (SSMs) with\nself-attention mechanisms have demonstrated impressive results across a range\nof sequence modeling tasks. However, current approaches apply attention modules\nstatically and uniformly to all elements in the input sequences, leading to\nsub-optimal quality-efficiency trade-offs. To address this limitation, we\nintroduce Sparse Modular Activation (SMA), a general mechanism enabling neural\nnetworks to sparsely and dynamically activate sub-modules for sequence elements\nin a differentiable manner. Through allowing each element to skip non-activated\nsub-modules, SMA reduces computation and memory consumption of neural networks\nat both training and inference stages. To validate the effectiveness of SMA on\nsequence modeling, we design a novel neural architecture, SeqBoat, which\nemploys SMA to sparsely activate a Gated Attention Unit (GAU) based on the\nstate representations learned from an SSM. By constraining the GAU to only\nconduct local attention on the activated inputs, SeqBoat can achieve linear\ninference complexity with theoretically infinite attention span, and provide\nsubstantially better quality-efficiency trade-off than the chunking-based\nmodels. With experiments on a wide range of tasks, including long sequence\nmodeling, speech classification and language modeling, SeqBoat brings new\nstate-of-the-art results among hybrid models with linear complexity, and\nreveals the amount of attention needed for each task through the learned sparse\nactivation patterns. Our code is publicly available at\nhttps://github.com/renll/SeqBoat.\n\n**Published:** 2023-06-19T23:10:02Z  (*Updated: 2023-11-04T21:26:03Z*)\n\n\n\n#### 3. Dynamic Layer Normalization for Adaptive Neural Acoustic Modeling in\n  Speech Recognition\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Taesup Kim, Inchul Song, Yoshua Bengio*\n\n**Abstract:** Layer normalization is a recently introduced technique for normalizing the\nactivities of neurons in deep neural networks to improve the training speed and\nstability. In this paper, we introduce a new layer normalization technique\ncalled Dynamic Layer Normalization (DLN) for adaptive neural acoustic modeling\nin speech recognition. By dynamically generating the scaling and shifting\nparameters in layer normalization, DLN adapts neural acoustic models to the\nacoustic variability arising from various factors such as speakers, channel\nnoises, and environments. Unlike other adaptive acoustic models, our proposed\napproach does not require additional adaptation data or speaker information\nsuch as i-vectors. Moreover, the model size is fixed as it dynamically\ngenerates adaptation parameters. We apply our proposed DLN to deep\nbidirectional LSTM acoustic models and evaluate them on two benchmark datasets\nfor large vocabulary ASR experiments: WSJ and TED-LIUM release 2. The\nexperimental results show that our DLN improves neural acoustic models in terms\nof transcription accuracy by dynamically adapting to various speakers and\nenvironments.\n\n**Published:** 2017-07-19T13:04:09Z  (*Updated: 2017-07-19T13:04:09Z*)\n\n\n\n#### 4. Differentiable Pooling for Unsupervised Acoustic Model Adaptation\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Pawel Swietojanski, Steve Renals*\n\n**Abstract:** We present a deep neural network (DNN) acoustic model that includes\nparametrised and differentiable pooling operators. Unsupervised acoustic model\nadaptation is cast as the problem of updating the decision boundaries\nimplemented by each pooling operator. In particular, we experiment with two\ntypes of pooling parametrisations: learned $L_p$-norm pooling and weighted\nGaussian pooling, in which the weights of both operators are treated as\nspeaker-dependent. We perform investigations using three different large\nvocabulary speech recognition corpora: AMI meetings, TED talks and Switchboard\nconversational telephone speech. We demonstrate that differentiable pooling\noperators provide a robust and relatively low-dimensional way to adapt acoustic\nmodels, with relative word error rates reductions ranging from 5--20% with\nrespect to unadapted systems, which themselves are better than the baseline\nfully-connected DNN-based acoustic models. We also investigate how the proposed\ntechniques work under various adaptation conditions including the quality of\nadaptation data and complementarity to other feature- and model-space\nadaptation methods, as well as providing an analysis of the characteristics of\neach of the proposed approaches.\n\n**Published:** 2016-03-31T15:10:40Z  (*Updated: 2016-07-13T18:12:49Z*)\n\n\n\n#### 5. Parallel Long Short-Term Memory for Multi-stream Classification\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Mohamed Bouaziz, Mohamed Morchid, Richard Dufour, Georges Linar\u00e8s, Renato De Mori*\n\n**Abstract:** Recently, machine learning methods have provided a broad spectrum of original\nand efficient algorithms based on Deep Neural Networks (DNN) to automatically\npredict an outcome with respect to a sequence of inputs. Recurrent hidden cells\nallow these DNN-based models to manage long-term dependencies such as Recurrent\nNeural Networks (RNN) and Long Short-Term Memory (LSTM). Nevertheless, these\nRNNs process a single input stream in one (LSTM) or two (Bidirectional LSTM)\ndirections. But most of the information available nowadays is from multistreams\nor multimedia documents, and require RNNs to process these information\nsynchronously during the training. This paper presents an original LSTM-based\narchitecture, named Parallel LSTM (PLSTM), that carries out multiple parallel\nsynchronized input sequences in order to predict a common output. The proposed\nPLSTM method could be used for parallel sequence classification purposes. The\nPLSTM approach is evaluated on an automatic telecast genre sequences\nclassification task and compared with different state-of-the-art architectures.\nResults show that the proposed PLSTM method outperforms the baseline n-gram\nmodels as well as the state-of-the-art LSTM approach.\n\n**Published:** 2017-02-11T09:50:40Z  (*Updated: 2017-02-11T09:50:40Z*)\n\n\n\n#### 6. M6-10T: A Sharing-Delinking Paradigm for Efficient Multi-Trillion\n  Parameter Pretraining\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Junyang Lin, An Yang, Jinze Bai, Chang Zhou, Le Jiang, Xianyan Jia, Ang Wang, Jie Zhang, Yong Li, Wei Lin, Jingren Zhou, Hongxia Yang*\n\n**Abstract:** Recent expeditious developments in deep learning algorithms, distributed\ntraining, and even hardware design for large models have enabled training\nextreme-scale models, say GPT-3 and Switch Transformer possessing hundreds of\nbillions or even trillions of parameters. However, under limited resources,\nextreme-scale model training that requires enormous amounts of computes and\nmemory footprint suffers from frustratingly low efficiency in model\nconvergence. In this paper, we propose a simple training strategy called\n\"Pseudo-to-Real\" for high-memory-footprint-required large models.\nPseudo-to-Real is compatible with large models with architecture of sequential\nlayers. We demonstrate a practice of pretraining unprecedented\n10-trillion-parameter model, an order of magnitude larger than the\nstate-of-the-art, on solely 512 GPUs within 10 days. Besides demonstrating the\napplication of Pseudo-to-Real, we also provide a technique, Granular CPU\noffloading, to manage CPU memory for training large model and maintain high GPU\nutilities. Fast training of extreme-scale models on a decent amount of\nresources can bring much smaller carbon footprint and contribute to greener AI.\n\n**Published:** 2021-10-08T04:24:51Z  (*Updated: 2021-10-25T06:24:41Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Searching Efficient 3D Architectures with Sparse Point-Voxel Convolution\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Song Han, Yujun Lin, Haotian Tang, Hanrui Wang, Shengyu Zhao, Zhijian Liu, Ji Lin*\n\n**Abstract:** Self-driving cars need to understand 3D scenes efficiently and accurately in order to drive safely. Given the limited hardware resources, existing 3D perception models are not able to recognize small instances (e.g., pedestrians, cyclists) very well due to the low-resolution voxelization and aggressive downsampling. To this end, we propose Sparse Point-Voxel Convolution (SPVConv), a lightweight 3D module that equips the vanilla Sparse Convolution with the high-resolution point-based branch. With negligible overhead, this point-based branch is able to preserve the fine details even from large outdoor scenes. To explore the spectrum of efficient 3D models, we first define a flexible architecture design space based on SPVConv, and we then present 3D Neural Architecture Search (3D-NAS) to search the optimal network architecture over this diverse design space efficiently and effectively. Experimental results validate that the resulting SPVNAS model is fast and accurate: it outperforms the state-of-the-art MinkowskiNet by 3.3%, ranking 1st on the competitive SemanticKITTI leaderboard. It also achieves 8x computation reduction and 3x measured speedup over MinkowskiNet with higher accuracy. Finally, we transfer our method to 3D object detection, and it achieves consistent improvements over the one-stage detection baseline on KITTI.\n\n**Proceeding:** eccv-2020-8\n\n**Published:** 2020-07-31\n\n\n\n#### 2. DeFiNES: Enabling Fast Exploration of the Depth-first Scheduling Space for DNN Accelerators through Analytical Modeling\n\n*From Search Query: \"hardware optimization state space models neural networks\"*\n\n*Anonymous*\n\n**Abstract:** DNN workloads can be scheduled onto DNN accelerators in many different ways: from layer-by-layer scheduling to cross-layer depth-first scheduling (a.k.a. layer fusion, or cascaded execution). This results in a very broad scheduling space, with each schedule leading to varying hardware (HW) costs in terms of energy and latency. To rapidly explore this vast space for a wide variety of hardware architectures, analytical cost models are crucial to estimate scheduling effects on the HW level. However, state-of-the-art cost models are lacking support for exploring the complete depth-first scheduling space, for instance focusing only on activations while ignoring weights, or modeling only DRAM accesses while overlooking on-chip data movements. These limitations prevent researchers from systematically and accurately understanding the depth-first scheduling space. After formalizing this design space, this work proposes a unified modeling framework, DeFiNES, for layer-by-layer and depth-first scheduling to fill in the gaps. DeFiNES enables analytically estimating the hardware cost for possible schedules in terms of both energy and latency, while considering data access at every memory level. This is done for each schedule and HW architecture under study by optimally choosing the active part of the memory hierarchy per unique combination of operand, layer, and feature map tile. The hardware costs are estimated, taking into account both data computation and data copy phases. The analytical cost model is validated against measured data from a taped-out depth-first DNN accelerator, DepFiN, showing good modeling accuracy at the end-to-end neural network level. A comparison with generalized state-of-the-art demonstrates up to 10X better solutions found with DeFiNES.\n\n**Published:** 2022-12-10\n\n\n\n#### 3. HoSNN: Adversarially-Robust Homeostatic Spiking Neural Networks with Adaptive Firing Thresholds\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Peng Li, Hejia Geng*\n\n**Abstract:** While spiking neural networks (SNNs) offer a promising neurally-inspired model of computation, they are vulnerable to adversarial attacks. We present the first study that draws inspiration from neural homeostasis to design a threshold-adapting leaky integrate-and-fire (TA-LIF) neuron model and utilize TA-LIF neurons to construct the adversarially robust homeostatic SNNs (HoSNNs) for improved robustness. The TA-LIF model incorporates a self-stabilizing dynamic thresholding mechanism, offering a local feedback control solution to the minimization of each neuron's membrane potential error caused by adversarial disturbance. Theoretical analysis demonstrates favorable dynamic properties of TA-LIF neurons in terms of the bounded-input bounded-output stability and suppressed time growth of membrane potential error, underscoring their superior robustness compared with the standard LIF neurons. When trained with weak FGSM attacks (attack budget = 2/255) and tested with much stronger PGD attacks (attack budget = 8/255), our HoSNNs significantly improve model accuracy on several datasets: from 30.54% to 74.91% on FashionMNIST, from 0.44% to 35.06% on SVHN, from 0.56% to 42.63% on CIFAR10, from 0.04% to 16.66% on CIFAR100, over the conventional LIF-based SNNs.\n\n**Published:** 2023-08-20\n\n\n\n#### 4. Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks\n\n*From Search Query: \"stability analysis adaptive neural networks\"*\n\n*Manfred Morari, Mahyar Fazlyab, Hamed Hassani, George J. Pappas, Alexander Robey*\n\n**Abstract:** Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNIST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.\n\n**Conference:** efficient-and-accurate-estimation-of-1\n\n**Published:** 2019-06-12\n\n\n\n#### 5. A differentiable brain simulator bridging brain simulation and brain-inspired computing\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Shangyang Li, Hongyaoxing Gu, Si Wu, Sichao He, Tianqiu Zhang, ChaoMing Wang*\n\n**Abstract:** Brain simulation builds dynamical models to mimic the structure and functions of the brain, while brain-inspired computing (BIC) develops intelligent systems by learning from the structure and functions of the brain. The two fields are intertwined and should share a common programming framework to facilitate each other's development. However, none of the existing software in the fields can achieve this goal, because traditional brain simulators lack differentiability for training, while existing deep learning (DL) frameworks fail to capture the biophysical realism and complexity of brain dynamics. In this paper, we introduce BrainPy, a differentiable brain simulator developed using JAX and XLA, with the aim of bridging the gap between brain simulation and BIC. BrainPy expands upon the functionalities of JAX, a powerful AI framework, by introducing complete capabilities for flexible, efficient, and scalable brain simulation. It offers a range of sparse and event-driven operators for efficient and scalable brain simulation, an abstraction for managing the intricacies of synaptic computations, a modular and flexible interface for constructing multi-scale brain models, and an object-oriented just-in-time compilation approach to handle the memory-intensive nature of brain dynamics. We showcase the efficiency and scalability of BrainPy on benchmark tasks, highlight its differentiable simulation for biologically plausible spiking models, and discuss its potential to support research at the intersection of brain simulation and BIC.\n\n**Published:** 2023-11-09\n\n\n\n#### 6. Reinforcement Learning for Portfolio Management\n\n*From Search Query: \"efficient memory management deep learning\"*\n\n*Angelos Filos*\n\n**Abstract:** In this thesis, we develop a comprehensive account of the expressive power, modelling efficiency, and performance advantages of so-called trading agents (i.e., Deep Soft Recurrent Q-Network (DSRQN) and Mixture of Score Machines (MSM)), based on both traditional system identification (model-based approach) as well as on context-independent agents (model-free approach). The analysis provides conclusive support for the ability of model-free reinforcement learning methods to act as universal trading agents, which are not only capable of reducing the computational and memory complexity (owing to their linear scaling with the size of the universe), but also serve as generalizing strategies across assets and markets, regardless of the trading universe on which they have been trained. The relatively low volume of daily returns in financial market data is addressed via data augmentation (a generative approach) and a choice of pre-training strategies, both of which are validated against current state-of-the-art models. For rigour, a risk-sensitive framework which includes transaction costs is considered, and its performance advantages are demonstrated in a variety of scenarios, from synthetic time-series (sinusoidal, sawtooth and chirp waves), simulated market series (surrogate data based), through to real market data (S\\&P 500 and EURO STOXX 50). The analysis and simulations confirm the superiority of universal model-free reinforcement learning agents over current portfolio management model in asset allocation strategies, with the achieved performance advantage of as much as 9.2\\% in annualized cumulative returns and 13.4\\% in annualized Sharpe Ratio.\n\n**Published:** 2019-09-12\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on hardware optimization, stability analysis, and efficient memory management, here are some key insights and relevant information from the provided sources and additional context:\n\n## Hardware-Aware Implementation Strategies\n\n- **Hardware-Efficient Neural Networks**: The design of neural networks can be significantly optimized for hardware efficiency. For instance, the use of approximate computing techniques can reduce hardware area and power consumption. A study on hardware MLP neural networks with approximate MAC units shows a 13.33% improvement in power consumption with only a 0.92% drop in accuracy.\n- **Edge Computing Optimizations**: SCAN-Edge incorporates device-friendly operations and a search algorithm that uses real latency metrics to optimize neural networks for edge devices. This approach ensures that the models are optimized for specific hardware properties, which can be beneficial for deploying autoregressive models on various devices.\n\n## Memory Management Techniques\n\n- **Efficient Memory Layouts**: Efficient memory management is crucial for practical deployment. Techniques such as using offset-based tensor representations can help manage memory more efficiently, especially when dealing with sequences of varying lengths. This is relevant for autoregressive models that process sequential data.\n- **Resource-Saving Techniques**: Hardware-efficient designs often involve reducing the number of neurons and applying operations across multiple clock cycles. These techniques can help in minimizing execution clock cycles and reducing memory usage, making them suitable for autoregressive models.\n\n## Stability Analysis Frameworks\n\n- **Stability in Adaptive Models**: Stability analysis is critical for adaptive neural networks. The use of low-dimensional latent parameterizations, such as in Hidden Parameter Recurrent State Space Models (HiP-RSSMs), can help in maintaining stability by avoiding approximations like variational inference.\n- **Test-Time Adaptation**: Stability during test-time adaptation can be improved using conjugate pseudo-labels, which provide a local approximation to the original supervised loss. This framework helps in understanding and improving test-time adaptation stability [Analysis Note].\n\n## Efficient Parameter Update Mechanisms\n\n- **Efficient Parameter Updates**: Efficient parameter update mechanisms are essential for adaptive models. Techniques such as using conjugate functions for test-time adaptation can provide efficient approximations and maintain stability. Additionally, careful parameterization schemes, such as those in HiP-RSSMs, can capture system variations effectively [Analysis Note].\n- **Parallel and Distributed Training**: For large models, parallel and distributed training methods can be employed. Techniques like data parallel training, pipeline parallelism, and model-level parallelism can help in updating parameters efficiently across multiple GPUs or devices.\n\n## Additional Considerations\n\n- **Hybrid Approaches**: Combining state space models (SSMs) with adaptive mechanisms and attention can improve performance. For example, the \"Hungry Hungry Hippos\" model shows promise by addressing token recall and comparison capabilities, which are crucial for autoregressive models [Analysis Note].\n- **Trade-Offs Between Adaptability and Efficiency**: Balancing adaptability and hardware constraints is crucial. Novel approaches to parameter updates that maintain efficiency while adapting to changing dynamics are essential for practical deployment[Analysis Note].\n\nBy integrating these insights, researchers can develop autoregressive language models that are optimized for hardware efficiency, stability, and efficient memory management, ultimately leading to better performance and scalability.\n",
            "analysis": "Analysis Note for AdaptiveSSMamba Proposal - Round 2\n\n1. Summary of Analysis:\nThe second round of searches reveals important insights about:\n\na) Hardware-Efficient Implementations:\n- Importance of memory management and computational efficiency\n- Trade-offs between adaptability and hardware constraints\n- Novel approaches to parameter updates that maintain efficiency\n\nb) Test-Time Adaptation:\n- Various approaches to dynamic model adaptation\n- Integration of self-supervised learning during inference\n- Stability considerations for adaptive updates\n\nc) State Space Model Innovations:\n- Recent advances in efficient SSM architectures\n- Novel parameterization schemes for stability\n- Approaches to combining SSMs with adaptive mechanisms\n\n2. Useful References with Excerpts:\n\nFrom \"Hidden Parameter Recurrent State Space Models\":\n\"We introduce the Hidden Parameter Recurrent State Space Models (HiP-RSSMs), a framework that parametrizes a family of related dynamical systems with a low-dimensional set of latent factors [...] avoiding approximations like variational inference.\"\n\nFrom \"Hungry Hungry Hippos\":\n\"We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence [...] H3 matches attention on synthetic languages and comes within 0.4 PPL of Transformers.\"\n\nFrom \"Test-Time Adaptation via Conjugate Pseudo-labels\":\n\"Under natural conditions, this (unsupervised) conjugate function can be viewed as a good local approximation to the original supervised loss [...] provides a broad framework for better understanding and improving test-time adaptation.\"\n\n3. Key Insights:\n\na) Adaptive Mechanisms:\n- Low-dimensional latent parameterizations can effectively capture system variations\n- Conjugate functions provide efficient approximations for test-time adaptation\n- Memory-efficient implementations require careful consideration of hardware constraints\n\nb) State Space Model Design:\n- Token recall and comparison capabilities are crucial challenges\n- Hybrid approaches combining SSMs with attention show promise\n- Stability can be improved through careful parameterization\n\nc) Implementation Considerations:\n- Hardware-aware algorithm design is essential\n- Memory management is critical for practical deployment\n- Trade-offs between adaptability and efficiency need careful balance\n\n4. Future Search Plan:\n\nRound 3 Focus:\n- Investigate specific hardware optimization techniques for SSMs\n- Look for stability analysis of adaptive state space models\n- Search for papers on efficient memory management in adaptive models\n\nSearch queries needed:\n1. \"hardware optimization state space models neural networks\"\n2. \"stability analysis adaptive neural networks\"\n3. \"efficient memory management deep learning\"\n\n5. References:\n\n[1] Shaj, V., Buchler, D., Sonker, R., Becker, P., & Neumann, G. (2022). Hidden Parameter Recurrent State Space Models For Changing Dynamics Scenarios. International Conference on Learning Representations.\n\n[2] Dao, T., Fu, D. Y., Saab, K. K., Thomas, A., Rudra, A., & R\u00e9, C. (2022). Hungry Hungry Hippos: Towards Language Modeling with State Space Models. International Conference on Learning Representations.\n\n[3] Goyal, S., Sun, M., Raghunathan, A., & Kolter, Z. (2022). Test-Time Adaptation via Conjugate Pseudo-labels. Neural Information Processing Systems.\n\n[4] Yu, A., et al. (2024). There is HOPE to Avoid HiPPOs for Long-memory State Space Models. arXiv.org.\n\n[5] Sieber, J., et al. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv.org."
        }
    ],
    "rating": 4.3,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.624021,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.75465,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": null,
    "proposal": "# AdaptiveSSMamba: Hardware-Efficient Adaptive State Space Model with Test-Time Training\n\n## Motivation\n\nModern language models face the challenge of balancing computational efficiency with model expressiveness, particularly when processing long sequences. The parent architectures - Mamba2 and FastTTTLinear - represent two different approaches to this challenge:\n\n1. Mamba2 leverages state space duality (SSD) for efficient processing but lacks adaptability\n2. FastTTTLinear provides test-time training capabilities but faces memory I/O challenges\n\nAdaptiveSSMamba aims to combine the strengths of both approaches while addressing their limitations through hardware-aware design and implementation.\n\n## Problem Analysis\n\n### Parent Architecture Analysis\n\n#### Mamba2 Strengths:\n- Efficient state processing through SSD framework\n- Linear complexity in sequence length\n- Hardware-optimized implementation\n- 2-8X faster than original Mamba\n\n#### Mamba2 Limitations:\n- Fixed state representation\n- Limited adaptability to varying contexts\n- No test-time training capability\n\n#### FastTTTLinear Strengths:\n- Adaptive computation through test-time training\n- Linear attention with data-dependent gates\n- Continuous model refinement during inference\n\n#### FastTTTLinear Limitations:\n- Memory I/O challenges\n- Complex integration requirements\n- Hardware efficiency concerns\n\n### Integration Opportunities\n\n1. Combine SSD framework with adaptive gates\n2. Leverage test-time training for state updates\n3. Maintain hardware efficiency through careful implementation\n4. Preserve linear complexity of both architectures\n\n## Core Idea and Philosophy\n\nAdaptiveSSMamba introduces a novel approach to combining efficient state space processing with adaptive computation through three key innovations:\n\n1. **Adaptive State Compression**: Dynamically compress state representations based on input content\n2. **Hardware-Aware Parameter Updates**: Efficient test-time training through selective parameter updates\n3. **Unified Memory Management**: Integrated memory handling for both state processing and adaptation\n\n## Design Plan\n\n### 1. Architecture Overview\n\n```python\nclass AdaptiveSSMamba(GAUBase):\n    def __init__(self, embed_dim, block_loc, kwarg_all, device=None, dtype=None):\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        \n        # Core components\n        self.ssd_layer = SSDMinimalDiscrete(embed_dim, block_loc, kwarg_all)\n        self.adaptive_gate = AdaptiveStateGate(embed_dim, block_loc, kwarg_all)\n        self.state_compress = StateCompression(embed_dim, block_loc, kwarg_all)\n        self.norm = RMSNorm(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, **Z):\n        # Compute state compression\n        compressed_state = self.state_compress(X)\n        \n        # Apply SSD with adaptive gating\n        gated_state = self.adaptive_gate(compressed_state)\n        output, Z = self.ssd_layer(gated_state, **Z)\n        \n        # Apply normalization\n        output = self.norm(output)[0]\n        \n        return output, Z\n```\n\n### 2. Core Components\n\n#### a. Adaptive State Gate\n```python\nclass AdaptiveStateGate(nn.Module):\n    def __init__(self, embed_dim, block_loc, kwarg_all):\n        super().__init__()\n        self.gate_proj = nn.Linear(embed_dim, embed_dim)\n        self.update_proj = nn.Linear(embed_dim, embed_dim)\n        \n    def forward(self, x):\n        gate = torch.sigmoid(self.gate_proj(x))\n        update = self.update_proj(x)\n        return gate * update + (1 - gate) * x\n```\n\n#### b. State Compression\n```python\nclass StateCompression(nn.Module):\n    def __init__(self, embed_dim, block_loc, kwarg_all):\n        super().__init__()\n        self.compress_proj = nn.Linear(embed_dim, embed_dim)\n        self.scale = nn.Parameter(torch.ones(embed_dim))\n        \n    def forward(self, x):\n        compressed = self.compress_proj(x)\n        return compressed * self.scale.unsqueeze(0).unsqueeze(0)\n```\n\n### 3. Hardware Optimizations\n\n1. **Memory Management**:\n   - Use shared memory buffers for state and parameter updates\n   - Implement efficient memory access patterns\n   - Minimize data movement between GPU memory hierarchies\n\n2. **Computation Optimization**:\n   - Fuse operations where possible\n   - Utilize hardware-specific CUDA kernels\n   - Implement parallel processing for independent operations\n\n3. **Parameter Updates**:\n   - Selective parameter updates based on importance\n   - Efficient gradient computation and propagation\n   - Hardware-aware update scheduling\n\n### 4. Stability Measures\n\n1. **Initialization**:\n   ```python\n   def initialize_parameters(self):\n       # Scale initialization based on embedding dimension\n       scale = math.sqrt(1.0 / self.embed_dim)\n       \n       # Initialize gate parameters\n       nn.init.normal_(self.adaptive_gate.gate_proj.weight, std=scale)\n       nn.init.zeros_(self.adaptive_gate.gate_proj.bias)\n       \n       # Initialize compression parameters\n       nn.init.normal_(self.state_compress.compress_proj.weight, std=scale)\n       nn.init.ones_(self.state_compress.scale)\n   ```\n\n2. **Regularization**:\n   - L2 regularization on parameter updates\n   - Gradient clipping for stability\n   - Adaptive learning rate scheduling\n\n## Implementation Guidelines\n\n### 1. Core Implementation\n\n```python\ndef forward_implementation(self, X, **Z):\n    # 1. Apply state compression\n    batch_size, seq_len, dim = X.shape\n    compressed_state = self.state_compress(X)\n    \n    # 2. Compute adaptive gates\n    gates = self.adaptive_gate(compressed_state)\n    \n    # 3. Apply SSD processing\n    ssd_output, new_state = self.ssd_layer(gates * compressed_state)\n    \n    # 4. Update parameters during inference\n    if self.training:\n        self.update_parameters(ssd_output, X)\n    \n    return ssd_output, new_state\n```\n\n### 2. Hardware-Aware Optimization\n\n```python\nclass HardwareOptimizer:\n    def __init__(self, model, device):\n        self.model = model\n        self.device = device\n        self.memory_buffer = torch.zeros(\n            (MAX_BATCH_SIZE, MAX_SEQ_LEN, EMBED_DIM),\n            device=device\n        )\n    \n    def optimize_memory_access(self):\n        # Implement efficient memory access patterns\n        pass\n    \n    def fuse_operations(self):\n        # Implement operation fusion\n        pass\n```\n\n### 3. Test-Time Training\n\n```python\ndef update_parameters(self, output, target):\n    # Compute loss\n    loss = self.compute_loss(output, target)\n    \n    # Selective parameter update\n    grads = torch.autograd.grad(loss, self.parameters())\n    with torch.no_grad():\n        for param, grad in zip(self.parameters(), grads):\n            if grad is not None:\n                param.sub_(self.lr * grad)\n```\n\n## Research Summary\n\nKey research findings that influenced the design:\n\n1. State Space Models:\n   - Linear complexity achievable through careful implementation\n   - Hardware-aware algorithms crucial for performance\n   - Selective state updates improve efficiency\n\n2. Test-Time Training:\n   - Adaptive computation enhances model expressiveness\n   - Parameter updates can be optimized for hardware\n   - Memory management crucial for efficiency\n\n3. Hardware Optimization:\n   - Memory access patterns significantly impact performance\n   - Operation fusion reduces overhead\n   - Parallel processing opportunities exist\n\n## Theoretical Analysis\n\n### 1. Complexity Analysis\n\n- Time Complexity: O(n) for sequence length n\n- Space Complexity: O(d) for embedding dimension d\n- Parameter Updates: O(p) for p selected parameters\n\n### 2. Stability Analysis\n\n- Bounded parameter updates through careful initialization\n- Gradient clipping prevents exploding gradients\n- Adaptive learning rates ensure stable convergence\n\n## Conclusion\n\nAdaptiveSSMamba combines the strengths of Mamba2 and FastTTTLinear while addressing their limitations through:\n\n1. Efficient state processing with adaptive updates\n2. Hardware-optimized implementation\n3. Stable test-time training capability\n4. Linear complexity maintenance\n\nThe design achieves this while maintaining hardware efficiency and numerical stability, making it suitable for practical deployment in modern language models.\n\n## References\n\n1. Dao, T., & Gu, A. (2024). Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality.\n\n2. Liu, F., & Li, Q. (2024). From generalization analysis to optimization designs for state space models.\n\n3. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces.\n\n4. Wang, S., & Li, Q. (2023). StableSSM: Alleviating the Curse of Memory in State-space Models through Stable Reparameterization.\n\n5. Yu, A., et al. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "claude3.5_sonnet",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel architecture that combines Mamba2's efficient state space processing with FastTTTLinear's adaptive computation through a hardware-optimized adaptive state update mechanism. The design leverages state space duality to integrate selective state updates with test-time training capabilities, while maintaining linear complexity through careful hardware-aware implementation. The architecture introduces adaptive state compression and dynamic parameter updates to enhance model expressiveness without sacrificing computational efficiency.",
    "ideation": null,
    "modelname": "AdaptiveSSMamba",
    "suggestions": "1. Stability Analysis:\n- Provide more detailed analysis of adaptive update stability\n- Include theoretical bounds on parameter updates\n- Consider adding regularization mechanisms\n\n2. Memory Management:\n- Elaborate on memory allocation strategies\n- Include specific optimization techniques for different hardware\n- Consider adding cache-aware algorithms\n\n3. Implementation Details:\n- Add more specific initialization guidelines\n- Include pseudo-code for critical components\n- Provide more detailed parameter selection strategies\n\n4. Evaluation Metrics:\n- Define specific benchmarks for performance evaluation\n- Include expected complexity analysis\n- Add theoretical bounds on memory usage\n\n5. Hardware Optimization:\n- Add more specific hardware-aware optimizations\n- Include CUDA kernel implementations\n- Consider different hardware architectures",
    "user_input": ""
}