{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = SelectiveGatedMHA(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.mlp = GatedMLP(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass GatedMLP(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, hidden_features=None, out_features=None,\n        activation=None, bias=False, multiple_of=128, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        out_features = out_features if out_features is not None else embed_dim\n        hidden_features = (hidden_features if hidden_features is not None else\n            int(8 * embed_dim / 3))\n        hidden_features = (hidden_features + multiple_of - 1\n            ) // multiple_of * multiple_of\n        self.fc1 = nn.Linear(embed_dim, 2 * hidden_features, bias=bias, **\n            self.factory_kwargs)\n        self.activation = activation if activation is not None else F.silu\n        self.fc2 = nn.Linear(hidden_features, out_features, bias=bias, **\n            self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        y = self.fc1(X)\n        y, gate = y.chunk(2, dim=-1)\n        y = y * self.activation(gate)\n        y = self.fc2(y)\n        return y\n\n\nimport torch.nn.functional as F\nimport math\n\n\nclass SelectiveGatedMHA(GAUBase):\n    \"\"\"\n    SelectiveGatedMHA: Hierarchical Selective Attention with Dynamic Parameter Generation\n\n    This module implements a Multi-Head Attention mechanism with selective gating and dynamic parameter generation.\n    It introduces content-dependent gating to selectively focus computation on important inputs and dynamically generates\n    parameters based on the input content. It also incorporates hierarchical memory management for efficient processing\n    of long sequences.\n\n    **Key Components:**\n    - **SelectiveGate**: Computes importance scores and generates binary gates to select important inputs.\n    - **DynamicParamGen**: Generates dynamic parameters conditioned on the input content.\n    - **HierMemManager**: Manages memory efficiently by processing inputs in blocks.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head. If None, calculated as embed_dim // num_heads.\n        block_size (int, optional): Size of blocks for hierarchical memory management. Default: 64.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        Y (Tensor): Output tensor of shape (batch_size, seq_length, embed_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None,\n        block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.block_size = block_size\n        self.selective_gate = SelectiveGate(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.param_gen = DynamicParamGen(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mem_manager = HierMemManager(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.to_qkv = nn.Linear(self.embed_dim, 3 * self.embed_dim, **self.\n            factory_kwargs)\n        self.to_out = nn.Linear(self.embed_dim, self.embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        input_dtype = X.dtype\n        X = X.to(**self.factory_kwargs)\n        _, Z_ = self.selective_gate(X, **Z)\n        Z.update(Z_)\n        gates = Z_['gates'].to(**self.factory_kwargs)\n        _, Z_ = self.param_gen(X, **Z)\n        Z.update(Z_)\n        params = Z_['params'].to(**self.factory_kwargs)\n        _, Z_ = self.mem_manager(X, **Z)\n        Z.update(Z_)\n        X_blocks = Z_['X_blocks']\n        L_orig = Z_['L_orig']\n        block_size = Z_['block_size']\n        num_blocks = X_blocks.size(1)\n        outputs = []\n        past_k = []\n        past_v = []\n        cumulative_len = 0\n        for block_idx in range(num_blocks):\n            X_block = X_blocks[:, block_idx, :, :]\n            block_seq_len = X_block.size(1)\n            block_start = block_idx * block_size\n            block_end = min(block_start + block_seq_len, L_orig)\n            seq_in_block = block_end - block_start\n            qkv = self.to_qkv(X_block[:, :seq_in_block])\n            qkv = qkv.chunk(3, dim=-1)\n            q, k, v = [t.view(B, seq_in_block, self.num_heads, self.\n                head_dim) for t in qkv]\n            block_gates = gates[:, block_start:block_end, :, :]\n            block_params = params[:, block_start:block_end, :, :]\n            q = q.transpose(1, 2)\n            k = k.transpose(1, 2)\n            v = v.transpose(1, 2)\n            block_gates = block_gates.transpose(1, 2)\n            block_params = block_params.transpose(1, 2)\n            k = k * block_gates\n            v = v * block_params * block_gates\n            all_k = torch.cat(past_k + [k], dim=2)\n            all_v = torch.cat(past_v + [v], dim=2)\n            attn_scores = torch.matmul(q, all_k.transpose(-2, -1)) / math.sqrt(\n                self.head_dim)\n            total_len = cumulative_len + seq_in_block\n            causal_mask = torch.tril(torch.ones(seq_in_block, total_len,\n                device=X.device, dtype=torch.bool))\n            attn_scores = attn_scores.masked_fill(~causal_mask.unsqueeze(0)\n                .unsqueeze(0), float('-inf'))\n            attn = F.softmax(attn_scores, dim=-1)\n            out = torch.matmul(attn, all_v)\n            out = out.transpose(1, 2).contiguous().view(B, seq_in_block,\n                self.embed_dim)\n            outputs.append(out)\n            past_k.append(k)\n            past_v.append(v)\n            cumulative_len += seq_in_block\n        Y = torch.cat(outputs, dim=1)[:, :L_orig, :]\n        Y = self.to_out(Y)\n        Y = Y.to(dtype=input_dtype)\n        return Y, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HierMemManager(GAUBase):\n    \"\"\"\n    HierMemManager Module\n\n    Processes input X in blocks for efficient memory management, particularly useful for handling long sequences.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        block_size (int, optional): Size of blocks for hierarchical memory management.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        X_blocks (Tensor): Blocked input tensor of shape (batch_size, num_blocks, block_size, embed_dim).\n        L_orig (int): Original sequence length before padding.\n        block_size (int): Block size used for blocking.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, block_size: int=64, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.block_size = block_size\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        X = X.to(**self.factory_kwargs)\n        pad_len = (self.block_size - L % self.block_size) % self.block_size\n        if pad_len > 0:\n            X_padded = F.pad(X, (0, 0, 0, pad_len))\n        else:\n            X_padded = X\n        L_padded = X_padded.size(1)\n        num_blocks = L_padded // self.block_size\n        X_blocks = X_padded.view(B, num_blocks, self.block_size, D)\n        return X, {'X_blocks': X_blocks, 'L_orig': L, 'block_size': self.\n            block_size}\n\n\nclass DynamicParamGen(GAUBase):\n    \"\"\"\n    DynamicParamGen Module\n\n    Generates dynamic parameters based on input X, enabling content-dependent parameter generation for the attention mechanism.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n        head_dim (int, optional): Dimension of each attention head.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        params (Tensor): Dynamic parameters tensor of shape (batch_size, seq_length, num_heads, head_dim).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, head_dim: int=None, **kwargs\n        ):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.head_dim = head_dim or embed_dim // num_heads\n        self.param_proj = nn.Linear(embed_dim, self.num_heads * self.\n            head_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        X = X.to(**self.factory_kwargs)\n        params = self.param_proj(X)\n        params = params.view(X.size(0), X.size(1), self.num_heads, self.\n            head_dim)\n        return X, {'params': params.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\n\n\nclass SelectiveGate(GAUBase):\n    \"\"\"\n    SelectiveGate Module\n\n    Computes importance scores and generates binary gates based on input X, allowing the model to selectively focus\n    computation on important inputs.\n\n    **Args:**\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the module.\n        device (torch.device, optional): The device to allocate parameters to.\n        dtype (torch.dtype, optional): The data type of parameters.\n        num_heads (int, optional): Number of attention heads. Default: 8.\n\n    **Inputs:**\n        X (Tensor): Input tensor of shape (batch_size, seq_length, embed_dim).\n\n    **Outputs:**\n        gates (Tensor): Binary gate tensor of shape (batch_size, seq_length, num_heads, 1).\n        scores (Tensor): Importance scores tensor of shape (batch_size, seq_length, num_heads).\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads: int=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        self.gate_proj = nn.Linear(embed_dim, num_heads, **self.factory_kwargs)\n        self.threshold = nn.Parameter(torch.zeros(1, **self.factory_kwargs))\n\n    def _forward(self, X, **Z):\n        B, L, _ = X.shape\n        X = X.to(**self.factory_kwargs)\n        scores = torch.sigmoid(self.gate_proj(X))\n        temperature = 1.1\n        hard_gates = (scores > self.threshold).float()\n        soft_gates = torch.sigmoid((scores - self.threshold) / temperature)\n        gates = hard_gates.detach() + soft_gates - soft_gates.detach()\n        gates = gates.view(B, L, self.num_heads, 1)\n        return X, {'gates': gates.to(**self.factory_kwargs), 'scores':\n            scores.to(**self.factory_kwargs)}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'hidden_features': None, 'out_features': None,\n    'activation': None, 'bias': False, 'multiple_of': 128, 'num_heads': 8,\n    'head_dim': None, 'block_size': 64}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}