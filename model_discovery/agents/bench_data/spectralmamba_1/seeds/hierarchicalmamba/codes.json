{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 128,\n    'n_block': 16\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 46\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 256,\n    'n_block': 14\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 46\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 384,\n    'n_block': 24\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 128,\n    'n_block': 5\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = Mamba2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass Mamba2(GAUBase):\n    \"\"\"\n    Mamba2: A Generalized Autoregressive Unit (GAU) implementing a double-layer Mamba architecture.\n\n    This class represents a Mamba2 block, which consists of two Mamba layers with normalization.\n    It's designed to process sequential data in a causal, differentiable, and parallelizable manner.\n\n    Architecture:\n        1. Input Normalization (RMSNorm)\n        2. First Mamba Layer\n        3. Residual Connection\n        4. Second Normalization (RMSNorm)\n        5. Second Mamba Layer\n        6. Final Residual Connection\n\n    Args:\n        embed_dim (int): The dimensionality of the input and output embeddings.\n        block_loc (tuple): The location of this block within the larger model architecture.\n        kwarg_all (dict): Additional keyword arguments to be passed to child components.\n        device (torch.device, optional): The device on which to allocate tensors.\n        dtype (torch.dtype, optional): The default dtype for tensors in this module.\n\n    Inputs:\n        X (torch.Tensor): Input tensor of shape (batch_size, sequence_length, embed_dim).\n        **Z: Additional keyword arguments for potential future extensions.\n\n    Outputs:\n        X (torch.Tensor): Output tensor of shape (batch_size, sequence_length, embed_dim).\n        Z (dict): Updated keyword arguments.\n\n    Note:\n        This implementation adheres to the GAU (Generalized Autoregressive Unit) interface\n        and maintains causal properties for autoregressive processing.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mamba1 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.mamba2 = HierarchicalMambaLayer(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mamba1(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mamba2(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass HierarchicalMambaLayer(GAUBase):\n    \"\"\"\n    HierarchicalMambaLayer: An enhanced implementation of the Mamba architecture layer with hierarchical state space modeling.\n    \n    This layer extends the Mamba2Layer by incorporating hierarchical processing capabilities,\n    allowing it to capture dependencies at multiple temporal scales. It maintains the efficient\n    processing of long sequences while adding the ability to model complex hierarchical patterns.\n\n    Args:\n        embed_dim (int): Dimension of the input embeddings.\n        block_loc (tuple): Location of the block within the model.\n        kwarg_all (dict): Additional keyword arguments.\n        d_state (int, optional): Dimension of the state. Defaults to 64.\n        d_conv (int, optional): Kernel size for the 1D convolution. Defaults to 4.\n        expand (int, optional): Expansion factor for the inner dimension. Defaults to 2.\n        headdim (int, optional): Dimension of each head. Defaults to 128.\n        ngroups (int, optional): Number of groups for group linear operators. Defaults to 1.\n        A_init_range (tuple, optional): Range for initializing the A parameter. Defaults to (1, 16).\n        dt_min (float, optional): Minimum value for dt initialization. Defaults to 0.001.\n        dt_max (float, optional): Maximum value for dt initialization. Defaults to 0.1.\n        dt_init_floor (float, optional): Floor value for dt initialization. Defaults to 1e-4.\n        chunk_size (int, optional): Size of chunks for processing. Defaults to 256.\n        device (torch.device, optional): Device to use for computations.\n        dtype (torch.dtype, optional): Data type to use for computations.\n        n_scales (int, optional): Number of temporal scales to model. Defaults to 2.\n\n    The layer processes sequences through multiple temporal scales:\n    1. Input projection with scale-specific parameters\n    2. Multi-scale convolution processing\n    3. Hierarchical state space modeling\n    4. Scale fusion and output projection\n\n    Each scale captures patterns at different temporal resolutions, allowing the model\n    to effectively process both local and global dependencies.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        d_state=64, d_conv=4, expand=2, headdim=128, ngroups=1,\n        A_init_range=(1, 16), dt_min=0.001, dt_max=0.1, dt_init_floor=\n        0.0001, chunk_size=256, n_scales=2, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.d_model = embed_dim\n        self.d_state = d_state\n        self.d_conv = d_conv\n        self.expand = expand\n        self.d_inner = self.expand * self.d_model\n        self.headdim = headdim\n        self.ngroups = ngroups\n        self.n_scales = n_scales\n        assert self.d_inner % self.headdim == 0\n        self.nheads = self.d_inner // self.headdim\n        self.chunk_size = chunk_size\n        d_in_proj = (2 * self.d_inner + 2 * self.ngroups * self.d_state +\n            self.nheads)\n        self.in_proj = nn.ModuleList([nn.Linear(self.d_model, d_in_proj,\n            bias=True, **self.factory_kwargs) for _ in range(self.n_scales)])\n        conv_dim = self.d_inner + 2 * self.ngroups * self.d_state\n        self.conv1d = nn.ModuleList([nn.Conv1d(in_channels=conv_dim,\n            out_channels=conv_dim, kernel_size=d_conv * 2 ** i, groups=\n            conv_dim, padding=d_conv * 2 ** i - 1, **self.factory_kwargs) for\n            i in range(self.n_scales)])\n        self.act = nn.SiLU()\n        self.silu = nn.SiLU()\n        self.dt_biases = nn.ParameterList([self._init_dt_bias(dt_min,\n            dt_max, dt_init_floor) for _ in range(self.n_scales)])\n        self.A_logs = nn.ParameterList([self._init_A_log(A_init_range) for\n            _ in range(self.n_scales)])\n        self.norm = nn.LayerNorm(self.d_inner, eps=1e-05, **self.factory_kwargs\n            )\n        self.scale_weights = nn.Parameter(torch.ones(self.n_scales) / self.\n            n_scales)\n        self.out_proj = nn.Linear(self.d_inner, self.d_model, bias=True, **\n            self.factory_kwargs)\n        self.ssd_minimal_discrete = SSDMinimalDiscrete(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n\n    def _init_dt_bias(self, dt_min, dt_max, dt_init_floor):\n        dt = torch.exp(torch.rand(self.nheads, **self.factory_kwargs) * (\n            math.log(dt_max) - math.log(dt_min)) + math.log(dt_min))\n        dt = torch.clamp(dt, min=dt_init_floor)\n        inv_dt = dt + torch.log(-torch.expm1(-dt))\n        dt_bias = nn.Parameter(inv_dt)\n        dt_bias._no_weight_decay = True\n        return dt_bias\n\n    def _init_A_log(self, A_init_range):\n        A = torch.empty(self.nheads, **self.factory_kwargs).uniform_(*\n            A_init_range)\n        A_log = nn.Parameter(torch.log(A))\n        A_log._no_weight_decay = True\n        return A_log\n\n    def pad_to_block_length(self, X, block_len):\n        pad_len = (block_len - X.shape[1] % block_len) % block_len\n        if pad_len > 0:\n            padding = torch.zeros(X.shape[0], pad_len, *X.shape[2:], dtype=\n                X.dtype, device=X.device)\n            X = torch.cat([X, padding], dim=1)\n        return X\n\n    def _process_scale(self, u, scale_idx, seqlen):\n        \"\"\"Process input at a specific scale.\"\"\"\n        zxbcdt = self.in_proj[scale_idx](u)\n        A = -torch.exp(self.A_logs[scale_idx])\n        z, xBC, dt = torch.split(zxbcdt, [self.d_inner, self.d_inner + 2 *\n            self.ngroups * self.d_state, self.nheads], dim=-1)\n        dt = F.softplus(dt + self.dt_biases[scale_idx])\n        xBC = self.act(self.conv1d[scale_idx](xBC.transpose(1, 2)).\n            transpose(1, 2))\n        xBC = xBC[:, :seqlen, :]\n        x, B, C = torch.split(xBC, [self.d_inner, self.ngroups * self.\n            d_state, self.ngroups * self.d_state], dim=-1)\n        x = rearrange(x, 'b l (h p) -> b l h p', p=self.headdim)\n        B = rearrange(B, 'b l (g n) -> b l g n', g=self.ngroups)\n        C = rearrange(C, 'b l (g n) -> b l g n', g=self.ngroups)\n        return x, A, B, C, dt, z\n\n    def _forward(self, u, **kwargs):\n        \"\"\"\n        Forward pass with hierarchical processing.\n        \n        Args:\n            u: Input tensor of shape (B, L, D)\n            \n        Returns:\n            Tensor of same shape as input\n        \"\"\"\n        batch, _seqlen, dim = u.shape\n        u = self.pad_to_block_length(u, self.chunk_size)\n        seqlen = u.shape[1]\n        scale_outputs = []\n        for scale_idx in range(self.n_scales):\n            x, A, B, C, dt, z = self._process_scale(u, scale_idx, seqlen)\n            Z = {'x': x, 'A': A, 'B': B, 'C': C, 'dt': dt, 'chunk_size':\n                self.chunk_size}\n            _, Z_ = self.ssd_minimal_discrete(u, **Z)\n            y = Z_.get('y')\n            y = rearrange(y, 'b l h p -> b l (h p)')\n            y = self.norm(y * self.silu(z))\n            scale_outputs.append(y)\n        scale_weights = F.softmax(self.scale_weights, dim=0)\n        combined_output = sum(w * y for w, y in zip(scale_weights,\n            scale_outputs))\n        out = self.out_proj(combined_output)\n        out = out[:, :_seqlen, :]\n        return out\n\n\nimport torch.nn.functional as F\nfrom einops import rearrange, repeat\n\n\nclass SSDMinimalDiscrete(GAUBase):\n    \"\"\"\n    SSDMinimalDiscrete (State Space Discrete Minimal) implements a discrete-time state space model.\n\n    This class provides an efficient implementation of the SSM algorithm, particularly\n    suited for processing sequential data in chunks. It uses a minimal discrete-time\n    formulation that is both memory-efficient and computationally effective.\n\n    Args:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block within the larger model structure.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device to run the module on.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n\n    Inputs:\n        X (torch.Tensor): The input tensor of shape (batch, length, n_heads, d_head).\n        A (torch.Tensor): The state transition tensor of shape (batch, length, n_heads).\n        B (torch.Tensor): The input-to-state tensor of shape (batch, length, n_heads, d_state).\n        C (torch.Tensor): The state-to-output tensor of shape (batch, length, n_heads, d_state).\n        dt (torch.Tensor): The time step tensor of shape (batch, length, n_heads).\n        chunk_size (int): The size of chunks for processing the sequence.\n\n    Outputs:\n        Y (torch.Tensor): The output tensor of shape (batch, length, n_heads, d_head).\n\n    The class implements the forward pass of the SSM algorithm, including:\n    1. Intra-chunk computations (diagonal blocks)\n    2. Inter-chunk state propagation\n    3. State-to-output conversion\n\n    This implementation is designed to be efficient for long sequences by processing\n    the input in chunks, which allows for better parallelization and memory usage.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n\n    def _forward(self, X, x, A, B, C, dt, chunk_size):\n        y, _ = self.ssd_minimal_discrete(x * dt.unsqueeze(-1), A * dt, B, C,\n            chunk_size)\n        Z_ = {'y': y}\n        return X, Z_\n\n    def segsum(self, x):\n        \"\"\"More stable segment sum calculation.\"\"\"\n        T = x.size(-1)\n        x = repeat(x, '... d -> ... d e', e=T)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=-1)\n        x = x.masked_fill(~mask, 0)\n        x_segsum = torch.cumsum(x, dim=-2)\n        mask = torch.tril(torch.ones(T, T, device=x.device, dtype=bool),\n            diagonal=0)\n        x_segsum = x_segsum.masked_fill(~mask, -torch.inf)\n        return x_segsum\n\n    def ssd_minimal_discrete(self, X, A, B, C, block_len, initial_states=None):\n        \"\"\"\n        Arguments:\n            X: (batch, length, n_heads, d_head)\n            A: (batch, length, n_heads)\n            B: (batch, length, n_heads, d_state)\n            C: (batch, length, n_heads, d_state)\n        Return:\n            Y: (batch, length, n_heads, d_head)\n        \"\"\"\n        assert X.dtype == A.dtype == B.dtype == C.dtype\n        X, A, B, C = [rearrange(x, 'b (c l) ... -> b c l ...', l=block_len) for\n            x in (X, A, B, C)]\n        A = rearrange(A, 'b c l h -> b h c l')\n        A_cumsum = torch.cumsum(A, dim=-1)\n        L = torch.exp(self.segsum(A))\n        Y_diag = torch.einsum('bclhn,bcshn,bhcls,bcshp->bclhp', C, B, L, X)\n        decay_states = torch.exp(A_cumsum[:, :, :, -1:] - A_cumsum)\n        states = torch.einsum('bclhn,bhcl,bclhp->bchpn', B, decay_states, X)\n        if initial_states is None:\n            initial_states = torch.zeros_like(states[:, :1])\n        states = torch.cat([initial_states, states], dim=1)\n        decay_chunk = torch.exp(self.segsum(F.pad(A_cumsum[:, :, :, -1], (1,\n            0))))\n        new_states = torch.einsum('bhzc,bchpn->bzhpn', decay_chunk, states)\n        states, final_state = new_states[:, :-1], new_states[:, -1]\n        state_decay_out = torch.exp(A_cumsum)\n        Y_off = torch.einsum('bclhn,bchpn,bhcl->bclhp', C, states,\n            state_decay_out)\n        Y = rearrange(Y_diag + Y_off, 'b c l h p -> b (c l) h p')\n        return Y, final_state\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'd_state': 64, 'd_conv': 4, 'expand': 2,\n    'headdim': 128, 'ngroups': 1, 'A_init_range': (1, 16), 'dt_min': 0.001,\n    'dt_max': 0.1, 'dt_init_floor': 0.0001, 'chunk_size': 256, 'n_scales': 2}\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 46\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}