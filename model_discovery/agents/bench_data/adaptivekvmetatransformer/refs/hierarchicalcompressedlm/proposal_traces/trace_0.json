{
    "variantname": "HierarchicalCompressedFastTTTLinear",
    "review": "The HierarchicalCompressedFastTTTLinear proposal presents an innovative approach to enhancing FastTTTLinear through hierarchical compression and adaptive sparse attention. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel integration of hierarchical compression with adaptive sparse attention\n- Well-grounded theoretical framework for multi-tier compression\n- Clear differentiation from existing approaches (e.g., TRAMS, Ring Attention)\n- Unique approach to memory management through tiered storage\n\n2. Design Clarity:\n- Well-structured mathematical formulation\n- Clear architectural components and interactions\n- Detailed implementation guidelines\n- Thoughtful consideration of hardware efficiency\n\n3. Efficiency Considerations:\n- Hierarchical compression reduces memory footprint\n- Adaptive sparse attention optimizes computation\n- Hardware-aware implementation strategies\n- IO-efficient memory access patterns\n\n4. Scalability:\n- Linear complexity through careful design\n- Efficient handling of long sequences\n- Distributed processing capabilities\n- Memory-efficient state management\n\nCONCERNS:\n\n1. Implementation Complexity:\n- Complex coordination between compression tiers\n- Potential challenges in maintaining test-time training capabilities\n- Need for careful management of state transitions\n- Integration complexity with existing FastTTTLinear components\n\n2. Memory Management:\n- Overhead from maintaining multiple compression tiers\n- Dynamic compression rate adaptation complexity\n- Cache pressure from hierarchical structures\n- State synchronization in distributed settings\n\n3. Training Stability:\n- Impact of compression on gradient flow\n- Potential instability in adaptive mechanisms\n- Complexity of multi-tier optimization\n- Balance between compression and model quality\n\n4. Hardware Optimization:\n- Need for hardware-specific implementations\n- Potential bottlenecks in memory access\n- Complexity of distributed processing\n- Optimization requirements for different hardware architectures",
    "search_stack": [
        {
            "ready": false,
            "query": "hierarchical memory language models, hardware efficient sparse attention",
            "detail": "Find papers discussing:\n1. Hierarchical memory architectures in language models, particularly those that use different levels of compression or multi-scale representations\n2. Hardware-efficient implementations of sparse attention mechanisms, especially those that consider modern accelerator architectures\n3. Techniques for combining local and global attention patterns efficiently",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Hierarchical memory architectures in language models, particularly those that use different levels of compression or multi-scale representations\n2. Hardware-efficient implementations of sparse attention mechanisms, especially those that consider modern accelerator architectures\n3. Techniques for combining local and global attention patterns efficiently\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.90)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 5/50 (Score: 0.90)*\n\n```\nWe also report competitive results on enwik-8 ( 0.99 vs 0.98 perplexity) and present ablations on CIFAR-10. ## 2 Related Work\n\nAttention with Temporal Sparsity: Research on efficient attention neural models parallels the advent of attention-based architectures. In the context of speech recognition, Jaitly et al. (2016) proposed the Neural Transducer which segments sequences in non-overlapping chunks and attention is performed in each chunk independently. Limiting attention to a fixed temporal context around the current prediction has also been explored in Chorowski et al. (2015), while ? dynamically segment the sequence into variable sized-chunks. Hierarchical attention strategies have also been explored: the model first considers which part of the inputs should be attended to before computing full attention in a contiguous neighborhood of the selected area (Gregor et al., 2015; Xu et al., 2015; Luong et al., 2015). Later, hierarchical attention has been simplified by Liu et al. (2018) that alternates coarse layers (attending to the whole sequence at a lower temporal resolution) with local layers (attending to a neighborhood of the current prediction). This alternating strategy is also employed by Child et al. (2019), which introduces bounded and strided attention, i.e. attending to a fixed context in the past at a sub-sampled temporal resolution. This work formalizes such a strategy using a sparse attention formalism, showing how it relates to full attention with a specific sparsity pattern in the attention matrix. It shows that sparse attention is sufficient to get state-of-the-art results in modeling long sequences over language modeling, image generation and music generation. Sukhbaatar et al. (2019) build upon this work and show that is it is possible to obtain further sparsity by letting the model learn the length of the temporal context for each attention module. This work also makes use of the attention cache introduced in Dai et al. (2019), a memory mechanism to train models over temporal contexts which extend beyond the length of the training batches. Attention with Content-Based Sparsity: The above work mainly relies on two efficient ideas: attending to less elements by only considering a fixed bounded local context in the past, and attending to less elements by decreasing the temporal resolution of context. These ideas do not allow arbitrary sparsity patterns in attention matrices. Content-based sparse attention has been introduced to allow for richer patterns and more expressive models. Martins and Kreutzer (2017); Malaviya et al. (2018) propose to compute attention weights with variants of sparsemax. Correia et al. (2019) generalizes this approach to every layer in a Transformer using entmax which allows for more efficient inference. This line of work allows for learning arbitrary sparsity attention patterns from data, based\non the content of the current query and past context. However, sparsity here cannot be leveraged to improve space and time complexity since sparse$\\max /$ entmax formulations require instantiating the full attention matrix prior to sparsification. This is a drawback compared to temporal sparsity approaches. Our work is motivated by bridging this gap and allows for arbitrary sparsity patterns while avoiding having to instantiate non-zero entries of attention matrices. Contemporaneous to our work, Kitaev et al. (2020) proposed to use Locality Sensitive Hashing (LSH) using random hyper-planes to infer content based sparsity patterns for attention: tokens that fall into the same hash bucket, get to attend to each other. While similar in spirit to our approach, the approach of Kitaev et al. (2020) keeps the randomly initialized hyper-planes fixed throughout, while we use mini-batch spherical $k$-means to learn the space-partitioning centroids. The motivation in both approaches is to approximate Maximum Inner Product Search (MIPS) in the context of dot product attention, for which both LSH and spherical $k$-means have been used in literature. However, typically spherical $k$-means is known to outperform LSH for MIPS (see e.g.\n```\n\n#### 2. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.82)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.82)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 3. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.56)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 28/38 (Score: 0.56)*\n\n```\nArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n##### *Relevant Chunk: No. 22/38 (Score: 0.56)*\n\n```\narXiv preprint arXiv:2306.00978, 2023. [41] Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, and Anshumali Shrivastava. Scissorhands: Exploiting the persistence of importance hypothesis for llm kv cache compression at test time. ArXiv, abs/2305.17118, 2023. [42] Zichang Liu, Jue Wang, Tri Dao, Tianyi Zhou, Binhang Yuan, Zhao Song, Anshumali Shrivastava, Ce Zhang, Yuandong Tian, Christopher Re, et al. Deja vu: Contextual sparsity for efficient llms at inference time. In International Conference on Machine Learning, pages 22137-22176. PMLR, 2023. [43] Liqiang Lu, Yicheng Jin, Hangrui Bi, Zizhang Luo, Peng Li, Tao Wang, and Yun Liang. Sanger: A co-design framework for enabling sparse attention using reconfigurable architecture. In MICRO-54: 54th Annual IEEE/ACM International Symposium on Microarchitecture, MICRO '21, page 977-991, New York, NY, USA, 2021. Association for Computing Machinery. [44] Xin Men, Mingyu Xu, Qingyu Zhang, Bingning Wang, Hongyu Lin, Yaojie Lu, Xianpei Han, and Weipeng Chen. Shortgpt: Layers in large language models are more redundant than you expect.\n```\n\n#### 4. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.48)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 2/16 (Score: 0.48)*\n\n```\n## 2. Related Works\n\n### 2.1. Sparse Attention\n\nIn recent times, the computational burden of long contexts has been effectively alleviated with the introduction of various sparsified attention mechanisms. (Zaheer et al., 2021) integrating random attention, windowed attention, and global attention achieved commendable results. (Zhao et al., 2019), (Gupta et al., 2021) posits that the plethora of irrelevant information within the attention mechanism can be distracting for the model, and thus zeroes out the less significant positions within the attention matrix to focus the model's attention. Subsequently, (Zhang et al., 2023) proposed a method to filter tokens of importance by summing up attention scores. Going a step further, (Ribar et al., 2023) estimated attention scores in the embedding dimension using the top-r values to then select the top- k largest KV pairs. The recently prominent Mistral architecture(Jiang et al., 2023a), employs windowed attention akin to the receptive fields of CNNs(O'Shea \\& Nash, 2015), theoretically enabling the effortless handling of text sequences up to the length of $32 \\times 4096$. However, none of these works can achieve lossless compression of context.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: hierarchical memory language models, hardware efficient sparse attention\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models\n\n*From Search Query: hierarchical memory language models*\n\n*Xihang Yue, Linchao Zhu, Yi Yang*\n\n**TL;DR:** A relation-aware fragment assessment criteria is introduced and the fragment-connected Hierarchical Memory based LLM is presented, which validates the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.\n\n**Abstract:** To process contexts with unlimited length using Large Language Models (LLMs), recent studies explore hierarchically managing the long text. Only several text fragments are taken from the external memory and passed into the temporary working memory, i.e., LLM's context window. However, existing approaches isolatedly handle the text fragments without considering their structural connections, thereby suffering limited capability on texts with intensive inter-relations, e.g., coherent stories and code repositories. This work attempts to resolve this by exploiting the fragment-level relations in external memory. First, we formulate the fragment-level relations and present several instantiations for different text types. Next, we introduce a relation-aware fragment assessment criteria upon previous independent fragment assessment. Finally, we present the fragment-connected Hierarchical Memory based LLM. We validate the benefits of involving these relations on long story understanding, repository-level code generation, and long-term chatting.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 2. Dior-CVAE: Pre-trained Language Models and Diffusion Priors for Variational Dialog Generation\n\n*From Search Query: hierarchical memory language models*\n\n*Tianyu Yang, Thy Thy Tran, Iryna Gurevych*\n\n**TL;DR:** This work employs a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM, and proposes memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation.\n\n**Abstract:** Current variational dialog models have employed pre-trained language models (PLMs) to parameterize the likelihood and posterior distributions. However, the Gaussian assumption made on the prior distribution is incompatible with these distributions, thus restricting the diversity of generated responses. These models also suffer from posterior collapse, i.e., the decoder tends to ignore latent variables and directly access information captured in the encoder through the cross-attention mechanism. In this work, we propose Dior-CVAE, a hierarchical conditional variational autoencoder (CVAE) with diffusion priors to address these challenges. We employ a diffusion model to increase the complexity of the prior distribution and its compatibility with the distributions produced by a PLM. Also, we propose memory dropout to the cross-attention mechanism, which actively encourages the use of latent variables for response generation. Overall, experiments across two commonly used open-domain dialog datasets show that our method can generate more diverse responses without large-scale dialog pre-training. Code is available at https://github.com/UKPLab/dior-cvae.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. History-Aware Hierarchical Transformer for Multi-session Open-domain Dialogue System\n\n*From Search Query: hierarchical memory language models*\n\n*Tong Zhang, Yong Liu, Boyang Albert Li, Zhiwei Zeng, Pengwei Wang, Yuan You, Chun Miao, Li-zhen Cui*\n\n**TL;DR:** Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models and human evaluation results support that HAHT generates more human-like, context-relevant and history-relevant responses than baseline models.\n\n**Abstract:** With the evolution of pre-trained language models, current open-domain dialogue systems have achieved great progress in conducting one-session conversations. In contrast, Multi-Session Conversation (MSC), which consists of multiple sessions over a long term with the same user, is under-investigated. In this paper, we propose History-Aware Hierarchical Transformer (HAHT) for multi-session open-domain dialogue. HAHT maintains a long-term memory of history conversations and utilizes history information to understand current conversation context and generate well-informed and context-relevant responses. Specifically, HAHT first encodes history conversation sessions hierarchically into a history memory. Then, HAHT leverages historical information to facilitate the understanding of the current conversation context by encoding the history memory together with the current context with attention-based mechanisms. Finally, to explicitly utilize historical information, HAHT uses a history-aware response generator that switches between a generic vocabulary and a history-aware vocabulary. Experimental results on a large-scale MSC dataset suggest that the proposed HAHT model consistently outperforms baseline models. Human evaluation results support that HAHT generates more human-like, context-relevant and history-relevant responses than baseline models.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 11  (*Influential: 1*)\n\n#### 4. Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models\n\n*From Search Query: hardware efficient sparse attention*\n\n*Beidi Chen, Tri Dao, Kaizhao Liang, Jiaming Yang, Zhao Song, A. Rudra, C. R\u00e9*\n\n**TL;DR:** This work uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers and empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs.\n\n**Abstract:** Overparameterized neural networks generalize well but are expensive to train. Ideally, one would like to reduce their computational cost while retaining their generalization benefits. Sparse model training is a simple and promising approach to achieve this, but there remain challenges as existing methods struggle with accuracy loss, slow training runtime, or difficulty in sparsifying all model components. The core problem is that searching for a sparsity mask over a discrete set of sparse matrices is difficult and expensive. To address this, our main insight is to optimize over a continuous superset of sparse matrices with a fixed structure known as products of butterfly matrices. As butterfly matrices are not hardware efficient, we propose simple variants of butterfly (block and flat) to take advantage of modern hardware. Our method (Pixelated Butterfly) uses a simple fixed sparsity pattern based on flat block butterfly and low-rank matrices to sparsify most network layers (e.g., attention, MLP). We empirically validate that Pixelated Butterfly is 3x faster than butterfly and speeds up training to achieve favorable accuracy--efficiency tradeoffs. On the ImageNet classification and WikiText-103 language modeling tasks, our sparse models train up to 2.5x faster than the dense MLP-Mixer, Vision Transformer, and GPT-2 medium with no drop in accuracy.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 68  (*Influential: 14*)\n\n#### 5. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification\n\n*From Search Query: hardware efficient sparse attention*\n\n*Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, Cho-Jui Hsieh*\n\n**TL;DR:** A dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input and an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens is proposed.\n\n**Abstract:** Attention is sparse in vision transformers. We observe the final prediction in vision transformers is only based on a subset of most informative tokens, which is sufficient for accurate image recognition. Based on this observation, we propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input. Specifically, we devise a lightweight prediction module to estimate the importance score of each token given the current features. The module is added to different layers to prune redundant tokens hierarchically. To optimize the prediction module in an end-to-end manner, we propose an attention masking strategy to differentiably prune a token by blocking its interactions with other tokens. Benefiting from the nature of self-attention, the unstructured sparse tokens are still hardware friendly, which makes our framework easy to achieve actual speed-up. By hierarchically pruning 66% of the input tokens, our method greatly reduces 31%~37% FLOPs and improves the throughput by over 40% while the drop of accuracy is within 0.5% for various vision transformers. Equipped with the dynamic token sparsification framework, DynamicViT models can achieve very competitive complexity/accuracy trade-offs compared to state-of-the-art CNNs and vision transformers on ImageNet. Code is available at https://github.com/raoyongming/DynamicViT\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 527  (*Influential: 103*)\n\n#### 6. Winning the Lottery Ahead of Time: Efficient Early Network Pruning\n\n*From Search Query: hardware efficient sparse attention*\n\n*John Rachwan, Daniel Zugner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, Stephan Gunnemann*\n\n**TL;DR:** Early Compression via Gradient Flow Preservation (EarlyCroP) is proposed, which efficiently extracts state-of-the-art sparse models before or early in training addressing challenge, and can be applied in a structured manner addressing challenge.\n\n**Abstract:** Pruning, the task of sparsifying deep neural networks, received increasing attention recently. Although state-of-the-art pruning methods extract highly sparse models, they neglect two main challenges: (1) the process of finding these sparse models is often very expensive; (2) unstructured pruning does not provide benefits in terms of GPU memory, training time, or carbon emissions. We propose Early Compression via Gradient Flow Preservation (EarlyCroP), which efficiently extracts state-of-the-art sparse models before or early in training addressing challenge (1), and can be applied in a structured manner addressing challenge (2). This enables us to train sparse networks on commodity GPUs whose dense versions would be too large, thereby saving costs and reducing hardware requirements. We empirically show that EarlyCroP outperforms a rich set of baselines for many tasks (incl. classification, regression) and domains (incl. computer vision, natural language processing, and reinforcment learning). EarlyCroP leads to accuracy comparable to dense training while outperforming pruning baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 22  (*Influential: 1*)\n\n### 4 related papers from Papers with Code\n\n#### 1. NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation\n\n*From Search Query: hierarchical memory language models*\n\n*Qiang Li, Hao Zhang, Tong Xiao, Jingbo Zhu*\n\n**Abstract:** \n\n**Proceeding:** acl-2012-7\n\n**Published:** 2012-07-01\n\n\n\n#### 2. MemGPT: Towards LLMs as Operating Systems\n\n*From Search Query: hierarchical memory language models*\n\n*Ion Stoica, Kevin Lin, Joseph E. Gonzalez, Sarah Wooders, Shishir G. Patil, Vivian Fang, Charles Packer*\n\n**Abstract:** Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.\n\n**Published:** 2023-10-12\n\n\n\n#### 3. TorchSparse: Efficient Point Cloud Inference Engine\n\n*From Search Query: hardware efficient sparse attention*\n\n*Song Han, Yujun Lin, Xiuyu Li, Zhijian Liu, Haotian Tang*\n\n**Abstract:** Deep learning on point clouds has received increased attention thanks to its wide applications in AR/VR and autonomous driving. These applications require low latency and high accuracy to provide real-time user experience and ensure user safety. Unlike conventional dense workloads, the sparse and irregular nature of point clouds poses severe challenges to running sparse CNNs efficiently on the general-purpose hardware. Furthermore, existing sparse acceleration techniques for 2D images do not translate to 3D point clouds. In this paper, we introduce TorchSparse, a high-performance point cloud inference engine that accelerates the sparse convolution computation on GPUs. TorchSparse directly optimizes the two bottlenecks of sparse convolution: irregular computation and data movement. It applies adaptive matrix multiplication grouping to trade computation for better regularity, achieving 1.4-1.5x speedup for matrix multiplication. It also optimizes the data movement by adopting vectorized, quantized and fused locality-aware memory access, reducing the memory movement cost by 2.7x. Evaluated on seven representative models across three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv, respectively.\n\n**Published:** 2022-04-21\n\n\n\n#### 4. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks\n\n*From Search Query: hardware efficient sparse attention*\n\n*Guoqi Li, Jason K. Eshraghian, Qihang Zhao, Rui-Jie Zhu*\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n**Published:** 2023-02-27\n\n\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, focusing on hierarchical memory architectures, hardware-efficient implementations of sparse attention, and techniques for combining local and global attention patterns, here are some key findings and references:\n\n## Hierarchical Memory Architectures\n\n1. **MemTree Algorithm**:\n   The MemTree algorithm introduces a dynamic, tree-structured memory representation that optimizes the organization, retrieval, and integration of information. It organizes memory hierarchically, with each node encapsulating aggregated textual content, corresponding semantic embeddings, and varying abstraction levels. This approach is similar to human cognitive schemas and enhances performance in scenarios requiring structured memory management, such as multi-turn dialogue understanding and document question answering.\n\n2. **Fragment-level Relations in External Memory**:\n   The FragRel approach exploits fragment-level relations in the external memory of large language models. It hierarchically manages long text by considering the structural connections between text fragments, which is particularly beneficial for texts with intensive inter-relations like coherent stories and code repositories. This method involves a relation-aware fragment assessment and a fragment-connected hierarchical memory, improving performance in long story understanding and long-term chatting.\n\n## Hardware-Efficient Implementations of Sparse Attention\n\n1. **Adaptive Structured Sparse Attention**:\n   The \"Near-Lossless Acceleration\" approach employs a two-stage query-guided key-value filtering method that adaptively selects a minimum set of key-values with low overhead. This technique maintains accuracy while reducing computational costs, making it hardware-efficient. It also emphasizes the importance of local window patterns and structured sparsity for better hardware utilization (referenced in the analysis).\n\n2. **Ring Attention with Blockwise Transformers**:\n   Ring Attention leverages blockwise computation of self-attention to distribute long sequences across multiple devices, fully overlapping communication with computation. This method is hardware-efficient and can reduce overhead by using a ring-style communication pattern. It suggests combining distributed processing with adaptive sparsity to enhance efficiency (referenced in the analysis).\n\n## Combining Local and Global Attention Patterns\n\n1. **Multi-Scale Attention Mechanisms**:\n   The MemTree algorithm inherently supports multi-scale attention by maintaining a hierarchical structure where higher-level nodes capture more abstract information and deeper nodes store finer details. This allows the model to balance abstract representations with specific details, effectively combining local and global attention patterns.\n\n2. **Adaptive Sparse Attention Patterns**:\n   The \"Near-Lossless Acceleration\" method and the concept of Ring Attention both suggest using adaptive sparse attention patterns. These patterns can be combined with local window attention to ensure that the model focuses on relevant information while maintaining efficiency. This approach can be integrated with hierarchical memory architectures to optimize both memory management and attention mechanisms (referenced in the analysis).\n\n## Additional Considerations\n\n- **Online Hierarchical Clustering Algorithms**:\n  MemTree's memory update algorithm is inspired by online hierarchical clustering algorithms, which ensures efficient and effective hierarchical memory management. This alignment with OTD (Online Top-Down Clustering) algorithms provides a theoretical justification for the approach.\n\n- **Hardware-Aware Optimization**:\n  To better utilize modern hardware capabilities, it is crucial to optimize memory access patterns and implement structured sparsity. Techniques like blockwise computation and overlapping communication with computation can significantly enhance hardware efficiency.\n\nBy integrating these concepts, the researchers can develop a more sophisticated autoregressive language model that leverages hierarchical memory architectures, hardware-efficient sparse attention mechanisms, and efficient combination of local and global attention patterns.\n",
            "analysis": "Summary of Analysis:\n\nThe seed design FastTTTLinear focuses on enhancing TTTLinear with Gated Linear Attention (GLA) and RWKV concepts to improve computational efficiency for long sequences. From analyzing the seed design and references, I've identified several key areas for potential innovation:\n\n1. Memory Management and State Representation:\n- The current design uses standard key-value caching\n- Opportunity exists for more sophisticated memory mechanisms\n- References like DiffuSeq and Ring Attention suggest alternative approaches\n\n2. Attention Mechanism:\n- Current GLA implementation could be enhanced\n- Ring Attention offers insights into distributed processing\n- Adaptive sparse attention patterns show promise\n\n3. Efficiency and Scalability:\n- Current focus on linear complexity\n- Room for improvement in hardware utilization\n- Potential for better parallelization\n\nKey Insights from References:\n\n1. From Ring Attention:\n- Blockwise computation of self-attention enables better distribution\n- Ring-style communication pattern can reduce overhead\n- Overlapping computation with communication is crucial\nRelevant excerpt:\n\"leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention\"\n\n2. From DiffuSeq:\n- Diffusion models offer interesting approaches to sequence modeling\n- Continuous representations can be beneficial\n- Progressive refinement could improve quality\nRelevant excerpt:\n\"adapting diffusion models to natural language is under-explored due to the discrete nature of texts, especially for conditional generation\"\n\n3. From \"Near-Lossless Acceleration\":\n- Adaptive sparse attention patterns are effective\n- Two-stage filtering can maintain accuracy\n- Local window patterns are important\nRelevant excerpt:\n\"employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead\"\n\n4. From TRAMS:\n- Simple metrics can guide effective token selection\n- Training-free approaches are valuable\n- Memory selection can improve efficiency\nRelevant excerpt:\n\"selects tokens participating in attention calculation based on one simple metric\"\n\nAnalysis and Potential Directions:\n\n1. Memory Architecture:\n- Could integrate a hierarchical memory system\n- Different levels of compression for different temporal scales\n- Adaptive memory allocation based on importance\n\n2. Attention Enhancement:\n- Combine ring attention's distribution with adaptive sparsity\n- Implement multi-scale attention patterns\n- Use simple metrics for token selection\n\n3. Hardware Efficiency:\n- Better utilize modern hardware capabilities\n- Implement structured sparsity for hardware efficiency\n- Optimize memory access patterns\n\nFuture Search Plan:\n\n1. Need to investigate:\n- Hierarchical memory architectures in language models\n- Hardware-efficient implementations of sparse attention\n- Multi-scale attention mechanisms\n\n2. Specific areas to search:\n- Memory compression techniques\n- Structured sparsity patterns\n- Hardware-aware optimization strategies\n\nReferences:\n\n1. Liu, H., Zaharia, M., & Abbeel, P. (2023). Ring Attention with Blockwise Transformers for Near-Infinite Context. arXiv preprint.\n\n2. Gong, S., Li, M., Feng, J., Wu, Z., & Kong, L. (2022). DiffuSeq: Sequence to Sequence Text Generation with Diffusion Models. International Conference on Learning Representations.\n\n3. Zhu, Q., et al. (2024). Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention. arXiv preprint.\n\n4. Yu, H., Wang, C., Zhang, Y., & Bi, W. (2023). TRAMS: Training-free Memory Selection for Long-range Language Modeling. Conference on Empirical Methods in Natural Language Processing.\n\nBased on this analysis, I believe we need one more round of search focusing specifically on hierarchical memory architectures and hardware-efficient implementations before formulating a complete proposal. This will help ensure our design effectively addresses the identified opportunities while maintaining practical implementability."
        },
        {
            "ready": false,
            "query": "memory compression language models, hardware sparse matrix optimization",
            "detail": "Find papers discussing:\n1. Memory compression techniques for language models, particularly those that maintain accuracy while reducing memory footprint\n2. Hardware-specific optimizations for sparse matrix operations, especially those applicable to attention mechanisms\n3. Methods for efficient implementation of combined attention patterns",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Memory compression techniques for language models, particularly those that maintain accuracy while reducing memory footprint\n2. Hardware-specific optimizations for sparse matrix operations, especially those applicable to attention mechanisms\n3. Methods for efficient implementation of combined attention patterns\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.85)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.85)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 2. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.85)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.85)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n#### 3. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.83)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 1/16 (Score: 0.83)*\n\n```\n# LoMA: Lossless Compressed Memory Attention \n\nYumeng Wang* ${ }^{1}$ Zhenyang Xiao ${ }^{* 12}$\n\n\n#### Abstract\n\nLarge Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $t c$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression. ## 1. Introduction\n\nIn the field of Natural Language Processing (NLP), understanding and managing long context represents one of the significant challenges for achieving in-depth language comprehension. Research into long context not only enhances the model's capabilities in processing lengthy dialogues, document comprehension, and information retrieval tasks but also aids in achieving more precise language inference and knowledge extraction, thereby facilitating progress in\n\n[^0]applications such as machine translation, summarization, and question-answering systems(Yang et al., 2023). In these tasks, users expect language models to access as much information as possible, necessitating a method that can effectively store and retrieve information. An essential direction for improving long-context processing involves information compression, encapsulating prior key-value (KV) information within a few specialized tokens. Previous efforts, such as (Mu et al., 2023), have achieved this goal with relative efficacy. However, a notable limitation of these methods is their lossy nature of compression, which inevitably leads to the loss of vital information during the process. We propose a novel approach, the Lossless Compressed Memory Attention (LoMA), which divides sequence into multiple chunks of equal length, each chunk structured to include a reading zone, a memory zone and a repetition zone. The latter two zones incorporate newly introduced special tokens: ' $<\\mathrm{m}>$ ' and ' $<\\mathrm{r}>$ '. We also designed a unique attention matrix mask: the reading zone employs a conventional autoregressive lower triangular mask; in order to facilitate better internal information transmission and communication, the memory zone employs a bidirectional attention mechanism and they can attend to reading zone; tokens in the repetition zone can only observe the memory zone directly preceding it, as well as the token itself. With this masking strategy, the ' $<\\mathrm{r}>$ ' token in the repetition zone needs to faithfully reproduce the text content of the reading zone, while only being able to attend to the $<\\mathrm{m}>$ tokens in the memory zone. This implies that the ' $<\\mathrm{m}>$ ' tokens quickly learn to compress the entire content of the reading zone into their own KV. We have also mathematically demonstrated that the loss function generated in the repetition zone can indirectly supervise the training of the model in the memory zone, obviating the need for constructing labels and computing loss for the tokens in the memory zone. Through the generative algorithm of LoMA, transformer models acquire the ability to compress memory losslessly within the memory zone, substantially extending the length of the long-context they are capable of handling and significantly reducing computational and memory costs. Our experiments show that the Llama-2-7B model(Touvron et al.,\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_dce00b655b99311c4676g-02.jpg?height=519&width=1743&top_left_y=204&top_left_x=159)\n\nFigure 1: Comparison of the standard transformer model with the LoMA model in autoregressive generation: (a) In the standard transformer model's autoregressive generation, the input token and the previous context's KV cache are fed together into the attention module to compute and predict the next token. (b) In the LoMA model's autoregressive generation, the previous context's KV cache is first compressed, and the input token is processed with the compressed KV cache by the attention module. 2023), when fine-tuned with the LoMA training method, is capable of high-ratio lossless memory compression of its own KV cache. Importantly, our approach does not modify the model's architecture or rely on additional auxiliary models. Chapter 2 reviews several studies related to our methodology, Chapter 3 provides an in-depth explanation of the LoMA generation algorithm, Chapter 4 describes the training precedure for endowing the transformer model with memory compression capabilities, Chapter 5 discusses our experimental results, and Chapter 6 concludes with a summary of our work.\n```\n\n#### 4. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.83)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.83)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 5. Reformer: The Efficient Transformer (Avg. Score: 0.67)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 0.67)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: memory compression language models, hardware sparse matrix optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. A Comparative Study on the Impact of Model Compression Techniques on Fairness in Language Models\n\n*From Search Query: memory compression language models*\n\n*Krithika Ramesh, Arnav Chavan, Shrey Pandit, Sunayana Sitaram*\n\n**TL;DR:** This analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification and indicates that compression strategies can have an adverse effect on fairness measures.\n\n**Abstract:** Compression techniques for deep learning have become increasingly popular, particularly in settings where latency and memory constraints are imposed. Several methods, such as pruning, distillation, and quantization, have been adopted for compressing models, each providing distinct advantages. However, existing literature demonstrates that compressing deep learning models could affect their fairness. Our analysis involves a comprehensive evaluation of pruned, distilled, and quantized language models, which we benchmark across a range of intrinsic and extrinsic metrics for measuring bias in text classification. We also investigate the impact of using multilingual models and evaluation measures. Our findings highlight the significance of considering both the pre-trained model and the chosen compression strategy in developing equitable language technologies. The results also indicate that compression strategies can have an adverse effect on fairness measures.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 13  (*Influential: 0*)\n\n#### 2. The Cost of Compression: Investigating the Impact of Compression on Parametric Knowledge in Language Models\n\n*From Search Query: memory compression language models*\n\n*Satya Sai, Srinath Namburi, Makesh Narsimhan Sreedhar, Srinath Srinivasan, Frederic Sala*\n\n**TL;DR:** A comprehensive analysis across multiple model families using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance, with a particular focus on tradeoffs involving parametric knowledge.\n\n**Abstract:** Compressing large language models (LLMs), often consisting of billions of parameters, provides faster inference, smaller memory footprints, and enables local deployment. Two standard compression techniques are pruning and quantization, with the former eliminating redundant connections in model layers and the latter representing model parameters with fewer bits. The key tradeoff is between the degree of compression and the impact on the quality of the compressed model. Existing research on LLM compression primarily focuses on performance in terms of general metrics like perplexity or downstream task accuracy. More fine-grained metrics, such as those measuring parametric knowledge, remain significantly underexplored. To help bridge this gap, we present a comprehensive analysis across multiple model families (ENCODER, ENCODER-DECODER, and DECODER) using the LAMA and LM-HARNESS benchmarks in order to systematically quantify the effect of commonly employed compression techniques on model performance. A particular focus is on tradeoffs involving parametric knowledge, with the goal of providing practitioners with practical insights to help make informed decisions on compression. We release our codebase1 to enable further research.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. History Compression via Language Models in Reinforcement Learning\n\n*From Search Query: memory compression language models*\n\n*Fabian Paischer, Thomas Adler, Vihang Patil, Angela Bitto-Nemling, Markus Holzleitner, S. Lehner, Hamid Eghbalzadeh, Sepp Hochreiter*\n\n**TL;DR:** This work proposes to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency, and introduces FrozenHopfield, which automatically associates observations with pretrained token embeddings.\n\n**Abstract:** In a partially observable Markov decision process (POMDP), an agent typically uses a representation of the past to approximate the underlying MDP. We propose to utilize a frozen Pretrained Language Transformer (PLT) for history representation and compression to improve sample efficiency. To avoid training of the Transformer, we introduce FrozenHopfield, which automatically associates observations with pretrained token embeddings. To form these associations, a modern Hopfield network stores these token embeddings, which are retrieved by queries that are obtained by a random but fixed projection of observations. Our new method, HELM, enables actor-critic network architectures that contain a pretrained language Transformer for history representation as a memory module. Since a representation of the past need not be learned, HELM is much more sample efficient than competitors. On Minigrid and Procgen environments HELM achieves new state-of-the-art results. Our code is available at https://github.com/ml-jku/helm.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 39  (*Influential: 2*)\n\n#### 4. Few-Shot Data-Driven Algorithms for Low Rank Approximation\n\n*From Search Query: hardware sparse matrix optimization*\n\n*P. Indyk, Tal Wagner, David P. Woodruff*\n\n**TL;DR:** These algorithms are interpretable: while previous algorithms choose the sketching matrix either at random or by black-box learning, this work shows that it can be set to clearly interpretable values extracted from the dataset.\n\n**Abstract:** Recently, data-driven and learning-based algorithms for low rank matrix approximation were shown to outperform classical data-oblivious algorithms by wide margins in terms of accuracy. Those algorithms are based on the optimization of sparse sketching matrices, which lead to large savings in time and memory during testing. However, they require long training times on a large amount of existing data, and rely on access to specialized hardware and software. In this work, we develop new data-driven low rank approximation algorithms with better computational efficiency in the training phase, alleviating these drawbacks. Furthermore, our methods are interpretable: while previous algorithms choose the sketching matrix either at random or by black-box learning, we show that it can be set (or initialized) to clearly interpretable values extracted from the dataset. Our experiments show that our algorithms, either by themselves or in combination with previous methods, achieve significant empirical advantages over previous work, improving training times by up to an order of magnitude toward achieving the same target accuracy.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 9  (*Influential: 3*)\n\n#### 5. Discretely Relaxing Continuous Variables for tractable Variational Inference\n\n*From Search Query: hardware sparse matrix optimization*\n\n*Trefor W. Evans, P. Nair*\n\n**TL;DR:** This work explores a new research direction in Bayesian variational inference with discrete latent variable priors where Kronecker matrix algebra is exploited for efficient and exact computations of the evidence lower bound (ELBO).\n\n**Abstract:** We explore a new research direction in Bayesian variational inference with discrete latent variable priors where we exploit Kronecker matrix algebra for efficient and exact computations of the evidence lower bound (ELBO). The proposed \"DIRECT\" approach has several advantages over its predecessors; (i) it can exactly compute ELBO gradients (i.e. unbiased, zero-variance gradient estimates), eliminating the need for high-variance stochastic gradient estimators and enabling the use of quasi-Newton optimization methods; (ii) its training complexity is independent of the number of training points, permitting inference on large datasets; and (iii) its posterior samples consist of sparse and low-precision quantized integers which permit fast inference on hardware limited devices. In addition, our DIRECT models can exactly compute statistical moments of the parameterized predictive posterior without relying on Monte Carlo sampling. The DIRECT approach is not practical for all likelihoods, however, we identify a popular model structure which is practical, and demonstrate accurate inference using latent variables discretized as extremely low-precision 4-bit quantized integers. While the ELBO computations considered in the numerical studies require over 10^2352 log-likelihood evaluations, we train on datasets with over two-million points in just seconds.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 6. Hardness and Algorithms for Robust and Sparse Optimization\n\n*From Search Query: hardware sparse matrix optimization*\n\n*Eric Price, Sandeep Silwal, Samson Zhou*\n\n**TL;DR:** An algorithm for robust regression that achieves arbitrarily accurate additive error and uses runtime that closely matches the lower bound from the fine-grained hardness result, as well as an algorithm for sparse regression with similar runtime that is inspired by the 3SUM problem.\n\n**Abstract:** We explore algorithms and limitations for sparse optimization problems such as sparse linear regression and robust linear regression. The goal of the sparse linear regression problem is to identify a small number of key features, while the goal of the robust linear regression problem is to identify a small number of erroneous measurements. Specifically, the sparse linear regression problem seeks a $k$-sparse vector $x\\in\\mathbb{R}^d$ to minimize $\\|Ax-b\\|_2$, given an input matrix $A\\in\\mathbb{R}^{n\\times d}$ and a target vector $b\\in\\mathbb{R}^n$, while the robust linear regression problem seeks a set $S$ that ignores at most $k$ rows and a vector $x$ to minimize $\\|(Ax-b)_S\\|_2$. We first show bicriteria, NP-hardness of approximation for robust regression building on the work of [OWZ15] which implies a similar result for sparse regression. We further show fine-grained hardness of robust regression through a reduction from the minimum-weight $k$-clique conjecture. On the positive side, we give an algorithm for robust regression that achieves arbitrarily accurate additive error and uses runtime that closely matches the lower bound from the fine-grained hardness result, as well as an algorithm for sparse regression with similar runtime. Both our upper and lower bounds rely on a general reduction from robust linear regression to sparse regression that we introduce. Our algorithms, inspired by the 3SUM problem, use approximate nearest neighbor data structures and may be of independent interest for solving sparse optimization problems. For instance, we demonstrate that our techniques can also be used for the well-studied sparse PCA problem.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Compressed Context Memory For Online Language Model Interaction\n\n*From Search Query: memory compression language models*\n\n*Hyun Oh Song, Sangdoo Yun, Junyoung Yeom, Jang-Hyun Kim*\n\n**Abstract:** This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.\n\n**Published:** 2023-12-06\n\n\n\n#### 2. TT-Rec: Tensor Train Compression for Deep Learning Recommendation Models\n\n*From Search Query: memory compression language models*\n\n*Carole-Jean Wu, Xing Liu, Bilge Acun, Chunxing Yin*\n\n**Abstract:** The memory capacity of embedding tables in deep learning recommendation models (DLRMs) is increasing dramatically from tens of GBs to TBs across the industry. Given the fast growth in DLRMs, novel solutions are urgently needed, in order to enable fast and efficient DLRM innovations. At the same time, this must be done without having to exponentially increase infrastructure capacity demands. In this paper, we demonstrate the promising potential of Tensor Train decomposition for DLRMs (TT-Rec), an important yet under-investigated context. We design and implement optimized kernels (TT-EmbeddingBag) to evaluate the proposed TT-Rec design. TT-EmbeddingBag is 3 times faster than the SOTA TT implementation. The performance of TT-Rec is further optimized with the batched matrix multiplication and caching strategies for embedding vector lookup operations. In addition, we present mathematically and empirically the effect of weight initialization distribution on DLRM accuracy and propose to initialize the tensor cores of TT-Rec following the sampled Gaussian distribution. We evaluate TT-Rec across three important design space dimensions -- memory capacity, accuracy, and timing performance -- by training MLPerf-DLRM with Criteo's Kaggle and Terabyte data sets. TT-Rec achieves 117 times and 112 times model size compression, for Kaggle and Terabyte, respectively. This impressive model size reduction can come with no accuracy nor training time overhead as compared to the uncompressed baseline.\n\n**Published:** 2021-01-25\n\n\n\n#### 3. TorchSparse: Efficient Point Cloud Inference Engine\n\n*From Search Query: hardware sparse matrix optimization*\n\n*Song Han, Yujun Lin, Xiuyu Li, Zhijian Liu, Haotian Tang*\n\n**Abstract:** Deep learning on point clouds has received increased attention thanks to its wide applications in AR/VR and autonomous driving. These applications require low latency and high accuracy to provide real-time user experience and ensure user safety. Unlike conventional dense workloads, the sparse and irregular nature of point clouds poses severe challenges to running sparse CNNs efficiently on the general-purpose hardware. Furthermore, existing sparse acceleration techniques for 2D images do not translate to 3D point clouds. In this paper, we introduce TorchSparse, a high-performance point cloud inference engine that accelerates the sparse convolution computation on GPUs. TorchSparse directly optimizes the two bottlenecks of sparse convolution: irregular computation and data movement. It applies adaptive matrix multiplication grouping to trade computation for better regularity, achieving 1.4-1.5x speedup for matrix multiplication. It also optimizes the data movement by adopting vectorized, quantized and fused locality-aware memory access, reducing the memory movement cost by 2.7x. Evaluated on seven representative models across three benchmark datasets, TorchSparse achieves 1.6x and 1.5x measured end-to-end speedup over the state-of-the-art MinkowskiEngine and SpConv, respectively.\n\n**Published:** 2022-04-21\n\n\n\n#### 4. GE-SpMM: General-purpose Sparse Matrix-Matrix Multiplication on GPUs for Graph Neural Networks\n\n*From Search Query: hardware sparse matrix optimization*\n\n*Huazhong Yang, Guohao Dai, Yu Wang, Guyue Huang*\n\n**Abstract:** Graph Neural Networks (GNNs) have achieved significant improvements in various domains. Sparse Matrix-Matrix multiplication (SpMM) is a fundamental operator in GNNs, which performs a multiplication between a sparse matrix and a dense matrix. Accelerating SpMM on parallel hardware like GPUs can face the following challenges: From the GNN application perspective, the compatibility needs to be considered. General GNN algorithms require SpMM-like operations (e.g., pooling) between matrices, which are not supported in current high-performance GPU libraries (e.g., Nvidia cuSPARSE). Moreover, the sophisticated preprocessing in previous implementations will lead to heavy data format conversion overheads in GNN frameworks. From the GPU hardware perspective, optimizations in SpMV (Sparse Matrix-Vector) designs on GPUs do not apply well to SpMM. SpMM exposes the column-wise parallelism in the dense output matrix, but straightforward generalization from SpMV leads to inefficient, uncoalesced access to sparse matrix in global memory. The sparse row data can be reused among GPU threads, which is neither possible in SpMM designs inherited from SpMV. To tackle these challenges, we propose GE-SpMM. GE-SpMM performs SpMM-like operation on sparse matrices represented in the most common Compressed Sparse Row (CSR) format, so it can be embedded in GNN frameworks with no preprocessing overheads and support general GNN algorithms. We introduce the Coalesced Row Caching method to process columns in parallel and ensure coalesced access to sparse matrix data. We also present the Coarse-grained Warp Merging to reduce redundant data loading among GPU warps. Experiments on a real-world graph dataset show that GE-SpMM achieves up to 1.41X speedup over Nvidia cuSPARSE and up to 1.81X over GraphBLAST. We also embed GE-SpMM in GNN frameworks and get up to 3.67X speedup over popular GNN models like GCN and GraphSAGE.\n\n**Published:** 2020-07-07\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on memory compression techniques, hardware-specific optimizations for sparse matrix operations, and efficient implementation of combined attention patterns, here are some key findings and references:\n\n## Memory Compression Techniques\n\n### Dynamic Sparse Attention and KV-Cache Compression\nThe paper on CASAK-V introduces a dynamic sparse attention mechanism and adaptive KV-cache compression. This approach dynamically generates and applies head-specific sparse attention patterns, reducing memory usage by 40% and achieving near-linear runtime complexity. This method is particularly useful for long-context processing in memory-constrained environments.\n\n### Quantization Techniques\nQuantization is a significant technique for reducing the memory footprint of large language models (LLMs) while maintaining performance. Methods like GPTQ and bitsandbytes involve converting model weights to lower bit-widths (e.g., from 16-bit to 8-bit or lower), which can reduce model size and improve inference speed without significant performance loss.\n\n### Self-Data Distillation for Pruned Models\nSelf-data distillation can be used to recover the quality of pruned models. This method involves generating a distilled dataset from the original, unpruned model to fine-tune the pruned model, maintaining alignment with the base model's knowledge and reducing the model size while retaining accuracy.\n\n## Hardware-Specific Optimizations for Sparse Matrix Operations\n\n### Dynamic Sparse Attention\nThe CASAK-V approach also includes hardware-efficient sparse attention mechanisms. By dynamically adjusting sparse attention patterns based on attention scores, this method optimizes for both memory efficiency and computational speed, making it suitable for hardware-constrained environments.\n\n### Flash Attention\nFlash Attention is another technique that optimizes traditional attention mechanisms by leveraging optimized GPU memory utilization. This is particularly beneficial for processing long input sequences, which is common in many real-world applications.\n\n### Routing Transformer and Pixelated Butterfly\nThe Routing Transformer combines content-based sparse attention with local, temporal sparse attention, offering efficiency gains. Pixelated Butterfly uses simple fixed sparsity patterns, which are hardware-efficient and suitable for sparse training in neural network models.\n\n## Methods for Efficient Implementation of Combined Attention Patterns\n\n### Adaptive Structured Sparse Attention\nCombining content-based and temporal sparse attention, as suggested by the Routing Transformer, can be effective. Adaptive structured sparse attention mechanisms can balance local and global patterns, ensuring efficient computation and maintaining model accuracy.\n\n### Blockwise Computation\nRing Attention's blockwise computation approach for distributed processing can be integrated to enhance hardware utilization. This method allows for near-infinite context processing by dividing the attention mechanism into blockwise computations.\n\n### Dynamic Token Sparsification\nDynamicViT's dynamic token sparsification framework can be used to progressively prune redundant tokens based on the input. This approach ensures that only relevant tokens are processed, reducing computational and memory requirements.\n\nIn summary, the following techniques and methods can be integrated to improve the autoregressive language model design:\n\n- **CASAK-V**: Dynamic sparse attention and adaptive KV-cache compression for efficient long-context processing.\n- **Quantization Techniques**: GPTQ and bitsandbytes for reducing model size and improving inference speed.\n- **Self-Data Distillation**: For recovering quality in pruned models.\n- **Flash Attention**: Optimized GPU memory utilization for long input sequences.\n- **Routing Transformer and Pixelated Butterfly**: Efficient sparse attention mechanisms.\n- **Adaptive Structured Sparse Attention**: Balancing local and global patterns for efficient computation.\n- **Blockwise Computation**: Enhancing hardware utilization through distributed processing.\n- **Dynamic Token Sparsification**: Progressively pruning redundant tokens to reduce computational and memory requirements.\n\nThese approaches collectively address the need for memory compression, hardware-specific optimizations, and efficient attention mechanisms, aligning with the researcher's goals for improving autoregressive language model design.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising directions for enhancing FastTTTLinear, particularly in three key areas:\n\n1. Hierarchical Memory Management:\n- FragRel's approach to fragment-level relations offers insights for better memory organization\n- MemGPT's virtual context management provides inspiration for efficient memory tier handling\n- MemTree's dynamic tree-structured representation suggests ways to optimize memory organization\n\n2. Hardware-Efficient Sparse Attention:\n- Routing Transformer's content-based sparse attention offers efficient computation patterns\n- Pixelated Butterfly's simple fixed sparsity patterns show promise for hardware efficiency\n- DynamicViT's token sparsification framework suggests ways to progressively prune redundant information\n\n3. Multi-Scale Processing:\n- Ring Attention's blockwise computation approach for distributed processing\n- TRAMS's training-free memory selection for efficient token management\n- Adaptive structured sparse attention for balancing local and global patterns\n\nKey References and Excerpts:\n\n1. From Routing Transformer:\n\"combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention\"\nThis suggests combining content-based and temporal sparsity could be effective.\n\n2. From FragRel:\n\"exploiting the fragment-level relations in external memory [...] formulate the fragment-level relations and present several instantiations for different text types\"\nShows how to maintain structural relationships in memory management.\n\n3. From MemGPT:\n\"drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory\"\nOffers insights into efficient memory tier management.\n\n4. From DynamicViT:\n\"propose a dynamic token sparsification framework to prune redundant tokens progressively and dynamically based on the input\"\nDemonstrates effective dynamic sparsification strategies.\n\nKey Insights for Design:\n\n1. Memory Architecture:\n- Implement a hierarchical memory system with multiple tiers\n- Use fragment-level relations to maintain context coherence\n- Employ dynamic memory compression based on content importance\n\n2. Attention Mechanism:\n- Combine content-based and temporal sparse attention\n- Implement progressive token sparsification\n- Use hardware-efficient fixed sparsity patterns where appropriate\n\n3. Processing Strategy:\n- Adopt blockwise computation for better hardware utilization\n- Implement adaptive sparsity patterns based on content\n- Use multi-scale processing for different levels of context\n\nFuture Search Plan:\nNeed to investigate:\n1. Memory compression techniques and their efficiency-quality tradeoffs\n2. Hardware-specific optimizations for sparse operations\n3. Methods for combining different types of attention mechanisms\n\nReferences:\n\n1. Roy, A., Saffar, M., Vaswani, A., & Grangier, D. (2020). Efficient Content-Based Sparse Attention with Routing Transformers. Transactions of the Association for Computational Linguistics.\n\n2. Yue, X., Zhu, L., & Yang, Y. (2024). FragRel: Exploiting Fragment-level Relations in the External Memory of Large Language Models. Annual Meeting of the Association for Computational Linguistics.\n\n3. Stoica, I., Lin, K., Gonzalez, J.E., Wooders, S., Patil, S.G., Fang, V., & Packer, C. (2023). MemGPT: Towards LLMs as Operating Systems.\n\n4. Chen, B., Dao, T., Liang, K., Yang, J., Song, Z., Rudra, A., & R\u00e9, C. (2021). Pixelated Butterfly: Simple and Efficient Sparse training for Neural Network Models. International Conference on Learning Representations.\n\n5. Rao, Y., Zhao, W., Liu, B., Lu, J., Zhou, J., & Hsieh, C.J. (2021). DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. Neural Information Processing Systems.\n\n6. Liu, H., Zaharia, M., & Abbeel, P. (2023). Ring Attention with Blockwise Transformers for Near-Infinite Context. arXiv preprint.\n\nBased on this analysis, we need another round of search focusing on memory compression techniques and hardware-specific optimizations for sparse operations to develop a more complete understanding before formulating the final proposal."
        },
        {
            "ready": false,
            "query": "dynamic memory compression, hardware optimization techniques, adaptive attention mechanisms",
            "detail": "Find papers discussing:\n1. Dynamic memory compression techniques that can adapt to different types of content and computational requirements\n2. Hardware optimization techniques specifically designed for attention mechanisms\n3. Methods for combining different types of attention patterns efficiently",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Dynamic memory compression techniques that can adapt to different types of content and computational requirements\n2. Hardware optimization techniques specifically designed for attention mechanisms\n3. Methods for combining different types of attention patterns efficiently\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Self-attention Does Not Need $O(n^2)$ Memory (Avg. Score: 0.61)\n\n*M. Rabe, Charles Staats*\n\n**Published in:**  (2021)\t**Cited by** 94  (*Influential: 7*)\n\n**TL;DR:** A practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention is provided.\n\n**Abstract:** We present a very simple algorithm for attention that requires $O(1)$ memory with respect to sequence length and an extension to self-attention that requires $O(\\log n)$ memory. This is in contrast with the frequently stated belief that self-attention requires $O(n^2)$ memory. While the time complexity is still $O(n^2)$, device memory rather than compute capability is often the limiting factor on modern accelerators. Thus, reducing the memory requirements of attention allows processing of longer sequences than might otherwise be feasible. We provide a practical implementation for accelerators that requires $O(\\sqrt{n})$ memory, is numerically stable, and is within a few percent of the runtime of the standard implementation of attention. We also demonstrate how to differentiate the function while remaining memory-efficient. For sequence length 16384, the memory overhead of self-attention is reduced by 59X for inference and by 32X for differentiation.\n\n##### *Relevant Chunk: No. 7/12 (Score: 0.61)*\n\n```\nCoRR, abs/2106.01540, 2021. URL https://arxiv.org/abs/2106.01540. Jiezhong Qiu, Hao Ma, Omer Levy, Wen-tau Yih, Sinong Wang, and Jie Tang. Blockwise self-attention for long document understanding. In Findings of the Association for Computational Linguistics: EMNLP 2020, pp. 25552565, 2020. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. Combiner: Full attention transformer with sparse computation cost. arXiv preprint arXiv:2107.05768, 2021. Amin Rezaei. Memory efficient attention, 2021. URL https://github.com/AminRezaei0x443/memory-efficient-attention. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers.\n```\n\n#### 2. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.57)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 3/38 (Score: 0.57)*\n\n```\nThis ensures an accurate profiling of the attention influences to facilitate better compression results. Our contributions are summarized as follows. - Heterogeneous Elastic Rules. We propose heterogeneous elastic rules for masks of each attention head. We formulate MoA compression search space to include a diverse range of elastic rules that tailor the local attention span relative to the input length for each attention head. The heterogeneous elastic rules improve the fact retrieval accuracy of MoA from 25\\% to $98 \\%$ compared with masks with uniform span and scaling function for each head. - Calibration Dataset Construction We emphasize the importance of data engineering in LLM compression. Our findings demonstrate that using datasets with long-range dependencies and referencing the original LLM's responses are crucial for accurately profiling the influences of compression. - Automatic Optimization. We propose an automatic pipeline to find the optimal compression plan encompassing heterogeneous elastic rules for various attention heads. This pipeline can efficiently find the optimal plan within several hours, for example, two hours for compressing Vicuna-13B. Experiments show that MoA achieves $5.5 \\times$ to $6.7 \\times$ throughput improvements on 7 B and 13 B dense LLMs at a $50 \\%$ density (the average of KV-Cache length / input length), with only $1 \\%$ average relative degradation in retrieval accuracy. Additionally, MoA achieves over $90 \\%$ retrieval accuracy with just $25 \\%$ average density, far surpassing sparse attention baselines that need a density of $75 \\%$ to $100 \\%$ for similar performance. On long-context understanding benchmarks, MoA performs comparably to dense models, with a maximum relative performance drop of less than $5 \\%$, which is about one-sixth of that observed with the uniform sparse attention baseline. Our code is available at https://github.com/thu-nics/MoA\n\n## 2 Preliminary and Related work\n\n### 2.1 Attention mechanism\n\nThe Multi-Head Self Attention (MHA) mechanism [62] is crucial to the functionality of LLMs.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.50)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.50)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. Just read twice: closing the recall gap for recurrent language models (Avg. Score: 0.32)\n\n*Simran Arora, Aman Timalsina, Aaryan Singhal, Benjamin Spector, Sabri Eyuboglu, Xinyi Zhao, Ashish Rao, Atri Rudra, Christopher R'e*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This work empirically and theoretically shows that the recurrent memory required to solve set disjointness changes with set order, i.e., whether the smaller set appears first in-context, i.e., whether the smaller set appears first in-context.\n\n**Abstract:** Recurrent large language models that compete with Transformers in language modeling perplexity are emerging at a rapid rate (e.g., Mamba, RWKV). Excitingly, these architectures use a constant amount of memory during inference. However, due to the limited memory, recurrent LMs cannot recall and use all the information in long contexts leading to brittle in-context learning (ICL) quality. A key challenge for efficient LMs is selecting what information to store versus discard. In this work, we observe the order in which information is shown to the LM impacts the selection difficulty. To formalize this, we show that the hardness of information recall reduces to the hardness of a problem called set disjointness (SD), a quintessential problem in communication complexity that requires a streaming algorithm (e.g., recurrent model) to decide whether inputted sets are disjoint. We empirically and theoretically show that the recurrent memory required to solve SD changes with set order, i.e., whether the smaller set appears first in-context. Our analysis suggests, to mitigate the reliance on data order, we can put information in the right order in-context or process prompts non-causally. Towards that end, we propose: (1) JRT-Prompt, where context gets repeated multiple times in the prompt, effectively showing the model all data orders. This gives $11.0 \\pm 1.3$ points of improvement, averaged across $16$ recurrent LMs and the $6$ ICL tasks, with $11.9\\times$ higher throughput than FlashAttention-2 for generation prefill (length $32$k, batch size $16$, NVidia H100). We then propose (2) JRT-RNN, which uses non-causal prefix-linear-attention to process prompts and provides $99\\%$ of Transformer quality at $360$M params., $30$B tokens and $96\\%$ at $1.3$B params., $50$B tokens on average across the tasks, with $19.2\\times$ higher throughput for prefill than FA2.\n\n##### *Relevant Chunk: No. 23/71 (Score: 0.32)*\n\n```\n[64] A. Vyas, A. Katharopoulos, and F. Fleuret. Fast transformers with clustered attention. In Proceedings of the International Conference on Neural Information Processing Systems (NeurIPS), 2020. [65] Songlin Yang and Yu Zhang. Fla: A triton-based library for hardware-efficient implementations of linear attention mechanism, January 2024. URL https://github.com/sustcsonglin/ flash-linear-attention. [66] Soham De, Samuel L. Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, Guillaume Desjardins, Arnaud Doucet, David Budden, Yee Whye Teh, Razvan Pascanu, Nando De Freitas, and Caglar Gulcehre. Griffin: Mixing gated linear recurrences with local attention for efficient language models, 2024. [67] Michael Poli, Jue Wang, Stefano Massaroli, Jeffrey Quesnelle, Ryan Carlow, Eric Nguyen, and Armin Thomas. StripedHyena: Moving Beyond Transformers with Hybrid Signal Processing Models. 122023. doi:10.57967/hf/1595. URL https://github.com/togethercomputer/stripedhyena.\n```\n\n#### 5. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.32)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 24/25 (Score: 0.32)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training, 2023. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?, 2019. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. Opt: Open pre-trained transformer language models, 2022. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, J., Shen, X., Wang, J., Zhang, J., Sun, W., Zhang, J., Birchfield, S., Guo, D., Kong, L., Wang, M., and Zhong, Y. Audio-visual segmentation with semantics, 2023.\n```\n\n\n\n---\n## Found 17 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic memory compression, hardware optimization techniques, adaptive attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Efficient Latency-Aware CNN Depth Compression via Two-Stage Dynamic Programming\n\n*From Search Query: dynamic memory compression*\n\n*Jinuk Kim, Yeonwoo Jeong, Deokjae Lee, Hyun Oh Song*\n\n**TL;DR:** A subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent Convolution operations for efficient end-to-end inference latency is proposed.\n\n**Abstract:** Recent works on neural network pruning advocate that reducing the depth of the network is more effective in reducing run-time memory usage and accelerating inference latency than reducing the width of the network through channel pruning. In this regard, some recent works propose depth compression algorithms that merge convolution layers. However, the existing algorithms have a constricted search space and rely on human-engineered heuristics. In this paper, we propose a novel depth compression algorithm which targets general convolution operations. We propose a subset selection problem that replaces inefficient activation layers with identity functions and optimally merges consecutive convolution operations into shallow equivalent convolution operations for efficient end-to-end inference latency. Since the proposed subset selection problem is NP-hard, we formulate a surrogate optimization problem that can be solved exactly via two-stage dynamic programming within a few seconds. We evaluate our methods and baselines by TensorRT for a fair inference latency comparison. Our method outperforms the baseline method with higher accuracy and faster inference speed in MobileNetV2 on the ImageNet dataset. Specifically, we achieve $1.41\\times$ speed-up with $0.11$\\%p accuracy gain in MobileNetV2-1.0 on the ImageNet.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Clustering the Sketch: Dynamic Compression for Embedding Tables\n\n*From Search Query: dynamic memory compression*\n\n*Henry Ling-Hei Tsang, Thomas Dybdahl Ahle*\n\n**TL;DR:** Clustered Compositional Embeddings (CCE) is suggested which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and CompositionalEmbeddings to achieve the best of both worlds.\n\n**Abstract:** Embedding tables are used by machine learning systems to work with categorical features. In modern Recommendation Systems, these tables can be very large, necessitating the development of new methods for fitting them in memory, even during training. We suggest Clustered Compositional Embeddings (CCE) which combines clustering-based compression like quantization to codebooks with dynamic methods like The Hashing Trick and Compositional Embeddings (Shi et al., 2020). Experimentally CCE achieves the best of both worlds: The high compression rate of codebook-based quantization, but *dynamically* like hashing-based methods, so it can be used during training. Theoretically, we prove that CCE is guaranteed to converge to the optimal codebook and give a tight bound for the number of iterations required.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 3. Structured Evolution with Compact Architectures for Scalable Policy Optimization\n\n*From Search Query: hardware optimization techniques*\n\n*K. Choromanski, Mark Rowland, Vikas Sindhwani, Richard E. Turner, Adrian Weller*\n\n**TL;DR:** It is shown that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies algorithm.\n\n**Abstract:** We present a new method of blackbox optimization via gradient approximation with the use of structured random orthogonal matrices, providing more accurate estimators than baselines and with provable theoretical guarantees. We show that this algorithm can be successfully applied to learn better quality compact policies than those using standard gradient estimation techniques. The compact policies we learn have several advantages over unstructured ones, including faster training algorithms and faster inference. These benefits are important when the policy is deployed on real hardware with limited resources. Further, compact policies provide more scalable architectures for derivative-free optimization (DFO) in high-dimensional spaces. We show that most robotics tasks from the OpenAI Gym can be solved using neural networks with less than 300 parameters, with almost linear time complexity of the inference phase, with up to 13x fewer parameters relative to the Evolution Strategies (ES) algorithm introduced by Salimans et al. (2017). We do not need heuristics such as fitness shaping to learn good quality policies, resulting in a simple and theoretically motivated training mechanism.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2018\n\n**Citations:** 141  (*Influential: 13*)\n\n#### 4. SliceGPT: Compress Large Language Models by Deleting Rows and Columns\n\n*From Search Query: hardware optimization techniques*\n\n*Saleh Ashkboos, Maximilian L. Croci, Marcelo Gennari do Nascimento, Torsten Hoefler, James Hensman*\n\n**TL;DR:** SliceGPT is presented, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network.\n\n**Abstract:** Large language models have become the cornerstone of natural language processing, but their use comes with substantial costs in terms of compute and memory resources. Sparsification provides a solution to alleviate these resource constraints, and recent works have shown that trained models can be sparsified post-hoc. Existing sparsification techniques face challenges as they need additional data structures and offer constrained speedup with current hardware. In this paper we present SliceGPT, a new post-training sparsification scheme which replaces each weight matrix with a smaller (dense) matrix, reducing the embedding dimension of the network. Through extensive experimentation, we show that SliceGPT can remove up to 25% of the model parameters (including embeddings) for LLAMA2-70B, OPT 66B and Phi-2 models while maintaining 99%, 99% and 90% zero-shot task performance of the dense model respectively. Our sliced models run on fewer GPUs and run faster without any additional code optimization: on 24GB consumer GPUs we reduce the total compute for inference on LLAMA2-70B to 64% of that of the dense model; on 40GB A100 GPUs we reduce it to 66%. We offer a new insight, computational invariance in transformer networks, which enables SliceGPT and we hope it will inspire and enable future avenues to reduce memory and computation demands for pre-trained models. Code is available at: https://github.com/microsoft/TransformerCompression\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 67  (*Influential: 14*)\n\n#### 5. Pruning vs Quantization: Which is Better?\n\n*From Search Query: hardware optimization techniques*\n\n*Andrey Kuzmin, Markus Nagel, M. V. Baalen, A. Behboodi, Tijmen Blankevoort*\n\n**TL;DR:** The question on which is better: neural network quantization or pruning?\n\n**Abstract:** Neural network pruning and quantization techniques are almost as old as neural networks themselves. However, to date only ad-hoc comparisons between the two have been published. In this paper, we set out to answer the question on which is better: neural network quantization or pruning? By answering this question, we hope to inform design decisions made on neural network hardware going forward. We provide an extensive comparison between the two techniques for compressing deep neural networks. First, we give an analytical comparison of expected quantization and pruning error for general data distributions. Then, we provide lower bounds for the per-layer pruning and quantization error in trained networks, and compare these to empirical error after optimization. Finally, we provide an extensive experimental comparison for training 8 large-scale models on 3 tasks. Our results show that in most cases quantization outperforms pruning. Only in some scenarios with very high compression ratio, pruning might be beneficial from an accuracy standpoint.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 19  (*Influential: 0*)\n\n#### 6. Efficient Representation Learning via Adaptive Context Pooling\n\n*From Search Query: adaptive attention mechanisms*\n\n*Chen Huang, Walter A. Talbott, N. Jaitly, J. Susskind*\n\n**TL;DR:** Inspired by the success of ConvNets that are combined with pooling to capture long-range dependencies, this paper learns to pool neighboring features for each token before computing attention in a given attention layer, and makes attention models more expressive, achieving strong performance often with fewer layers and thus significantly reduced cost.\n\n**Abstract:** Self-attention mechanisms model long-range context by using pairwise attention between all input tokens. In doing so, they assume a fixed attention granularity defined by the individual tokens (e.g., text characters or image pixels), which may not be optimal for modeling complex dependencies at higher levels. In this paper, we propose ContextPool to address this problem by adapting the attention granularity for each token. Inspired by the success of ConvNets that are combined with pooling to capture long-range dependencies, we learn to pool neighboring features for each token before computing attention in a given attention layer. The pooling weights and support size are adaptively determined, allowing the pooled features to encode meaningful context with varying scale. We show that ContextPool makes attention models more expressive, achieving strong performance often with fewer layers and thus significantly reduced cost. Experiments validate that our ContextPool module, when plugged into transformer models, matches or surpasses state-of-the-art performance using less compute on several language and image benchmarks, outperforms recent works with learned context sizes or sparse attention patterns, and is also applicable to ConvNets for efficient feature learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 7. Adaptive Feature Abstraction for Translating Video to Language\n\n*From Search Query: adaptive attention mechanisms*\n\n*Yunchen Pu, Martin Renqiang Min, Zhe Gan, L. Carin*\n\n**TL;DR:** These experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.\n\n**Abstract:** A new model for video captioning is developed, using a deep three-dimensional Convolutional Neural Network (C3D) as an encoder for videos and a Recurrent Neural Network (RNN) as a decoder for captions. We consider both \"hard\" and \"soft\" attention mechanisms, to adaptively and sequentially focus on different layers of features (levels of feature \"abstraction\"), as well as local spatiotemporal regions of the feature maps at each layer. The proposed approach is evaluated on three benchmark datasets: YouTube2Text, M-VAD and MSR-VTT. Along with visualizing the results and how the model works, these experiments quantitatively demonstrate the effectiveness of the proposed adaptive spatiotemporal feature abstraction for translating videos to sentences with rich semantics.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2016\n\n**Citations:** 14  (*Influential: 1*)\n\n#### 8. BiSHop: Bi-Directional Cellular Learning for Tabular Data with Generalized Sparse Modern Hopfield Model\n\n*From Search Query: adaptive attention mechanisms*\n\n*Chenwei Xu, Yu-Chao Huang, Jerry Yao-Chieh Hu, Weijian Li, Ammar Gilani, H. Goan, Han Liu*\n\n**TL;DR:** The BiSHop framework, a novel end-to-end framework for deep tabular learning, is introduced, demonstrating that BiSHop surpasses current SOTA methods with significantly less HPO runs, marking it a robust solution for deep tabular learning.\n\n**Abstract:** We introduce the \\textbf{B}i-Directional \\textbf{S}parse \\textbf{Hop}field Network (\\textbf{BiSHop}), a novel end-to-end framework for deep tabular learning. BiSHop handles the two major challenges of deep tabular learning: non-rotationally invariant data structure and feature sparsity in tabular data. Our key motivation comes from the recent established connection between associative memory and attention mechanisms. Consequently, BiSHop uses a dual-component approach, sequentially processing data both column-wise and row-wise through two interconnected directional learning modules. Computationally, these modules house layers of generalized sparse modern Hopfield layers, a sparse extension of the modern Hopfield model with adaptable sparsity. Methodologically, BiSHop facilitates multi-scale representation learning, capturing both intra-feature and inter-feature interactions, with adaptive sparsity at each scale. Empirically, through experiments on diverse real-world datasets, we demonstrate that BiSHop surpasses current SOTA methods with significantly less HPO runs, marking it a robust solution for deep tabular learning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 11  (*Influential: 0*)\n\n### 3 related papers from ArXiv\n\n#### 1. Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference\n\n*From Search Query: dynamic memory compression*\n\n*Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, Edoardo M. Ponti*\n\n**Abstract:** Transformers have emerged as the backbone of large language models (LLMs).\nHowever, generation remains inefficient due to the need to store in memory a\ncache of key-value representations for past tokens, whose size scales linearly\nwith the input sequence length and batch size. As a solution, we propose\nDynamic Memory Compression (DMC), a method for online key-value cache\ncompression at inference time. Most importantly, the model learns to apply\ndifferent compression ratios in different heads and layers. We retrofit\npre-trained LLMs such as Llama 2 (7B, 13B and 70B) into DMC Transformers,\nachieving up to 7x throughput increase during auto-regressive inference on an\nNVIDIA H100 GPU. DMC is applied via continued pre-training on a negligible\npercentage of the original data without adding any extra parameters. DMC\npreserves the original downstream performance with up to 4x cache compression,\noutperforming up-trained grouped-query attention (GQA) and key-value eviction\npolicies (H$_2$O, TOVA). GQA and DMC can be even combined to obtain compounded\ngains. Hence, DMC can serve as a drop-in replacement for KV caching in existing\nLLMs to fit longer contexts and larger batches within any given memory budget.\n\n**Published:** 2024-03-14T17:59:26Z  (*Updated: 2024-07-23T17:55:30Z*)\n\n\n\n#### 2. Explain and Improve: LRP-Inference Fine-Tuning for Image Captioning\n  Models\n\n*From Search Query: adaptive attention mechanisms*\n\n*Jiamei Sun, Sebastian Lapuschkin, Wojciech Samek, Alexander Binder*\n\n**Abstract:** This paper analyzes the predictions of image captioning models with attention\nmechanisms beyond visualizing the attention itself. We develop variants of\nlayer-wise relevance propagation (LRP) and gradient-based explanation methods,\ntailored to image captioning models with attention mechanisms. We compare the\ninterpretability of attention heatmaps systematically against the explanations\nprovided by explanation methods such as LRP, Grad-CAM, and Guided Grad-CAM. We\nshow that explanation methods provide simultaneously pixel-wise image\nexplanations (supporting and opposing pixels of the input image) and linguistic\nexplanations (supporting and opposing words of the preceding sequence) for each\nword in the predicted captions. We demonstrate with extensive experiments that\nexplanation methods 1) can reveal additional evidence used by the model to make\ndecisions compared to attention; 2) correlate to object locations with high\nprecision; 3) are helpful to \"debug\" the model, e.g. by analyzing the reasons\nfor hallucinated object words. With the observed properties of explanations, we\nfurther design an LRP-inference fine-tuning strategy that reduces the issue of\nobject hallucination in image captioning models, and meanwhile, maintains the\nsentence fluency. We conduct experiments with two widely used attention\nmechanisms: the adaptive attention mechanism calculated with the additive\nattention and the multi-head attention mechanism calculated with the scaled dot\nproduct.\n\n**Published:** 2020-01-04T05:15:11Z  (*Updated: 2021-08-01T06:27:04Z*)\n\n\n\n#### 3. Density Adaptive Attention is All You Need: Robust Parameter-Efficient\n  Fine-Tuning Across Multiple Modalities\n\n*From Search Query: adaptive attention mechanisms*\n\n*Georgios Ioannides, Aman Chadha, Aaron Elkins*\n\n**Abstract:** We propose the Multi-Head Density Adaptive Attention Mechanism (DAAM), a\nnovel probabilistic attention framework that can be used for\nParameter-Efficient Fine-tuning (PEFT), and the Density Adaptive Transformer\n(DAT), designed to enhance information aggregation across multiple modalities,\nincluding Speech, Text, and Vision. DAAM integrates learnable mean and variance\ninto its attention mechanism, implemented in a multi-head framework, enabling\nit to collectively model any probability distribution for dynamic recalibration\nof feature significance. This method demonstrates significant improvements,\nespecially with highly non-stationary data, surpassing the state-of-the-art\nattention techniques in model performance, up to approximately +20% (abs.) in\naccuracy. Empirically, DAAM exhibits superior adaptability and efficacy across\na diverse range of tasks, including emotion recognition in speech, image\nclassification, and text classification, thereby establishing its robustness\nand versatility in handling data across multiple modalities. Furthermore, we\nintroduce the Importance Factor, a new learning-based metric that enhances the\nexplainability of models trained with DAAM-based methods.\n\n**Published:** 2024-01-20T06:42:32Z  (*Updated: 2024-09-29T00:45:46Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Build a Deep Neural Network model using CPUs Builds a feed-forward multilayer artificial neural network on an H2OFrame\n\n*From Search Query: dynamic memory compression*\n\n*Viraj Parmar, Erin LeDell, Jessica Lanford, Arno Candel, Anisha Arora*\n\n**Abstract:** H2O is fast, scalable, open-source machine learning and deep learning for\r\nsmarter applications. With H2O, enterprises like PayPal, Nielsen Catalina,\r\nCisco, and others can use all their data without sampling to get accurate\r\npredictions faster. Advanced algorithms such as deep learning, boosting, and\r\nbagging ensembles are built-in to help application designers create smarter\r\napplications through elegant APIs. Some of our initial customers have built\r\npowerful domain-speci\fc predictive engines for recommendations, customer\r\nchurn, propensity to buy, dynamic pricing, and fraud detection for the insurance,\r\nhealthcare, telecommunications, ad tech, retail, and payment systems industries.\r\nUsing in-memory compression, H2O handles billions of data rows in-memory,\r\neven with a small cluster. To make it easier for non-engineers to create complete\r\nanalytic work\r\nows, H2O's platform includes interfaces for R, Python, Scala,\r\nJava, JSON, and Co\u000beeScript/JavaScript, as well as a built-in web interface,\r\nFlow. H2O is designed to run in standalone mode, on Hadoop, or within a\r\nSpark Cluster, and typically deploys within minutes.\r\nH2O includes many common machine learning algorithms, such as generalized\r\nlinear modeling (linear regression, logistic regression, etc.), Na\u007f\u0010ve Bayes, principal\r\ncomponents analysis, k-means clustering, and others. H2O also implements\r\nbest-in-class algorithms at scale, such as distributed random forest, gradient\r\nboosting and deep learning. Customers can build thousands of models and\r\ncompare the results to get the best predictions.\r\nH2O is nurturing a grassroots movement of physicists, mathematicians, and\r\ncomputer scientists to herald the new wave of discovery with data science by\r\ncollaborating closely with academic researchers and industrial data scientists.\r\nStanford university giants Stephen Boyd, Trevor Hastie, Rob Tibshirani advise\r\nthe H2O team on building scalable machine learning algorithms. With hundreds\r\nof meetups over the past three years, H2O has become a word-of-mouth\r\nphenomenon, growing amongst the data community by a hundred-fold, and\r\nis now used by 30,000+ users and is deployed using R, Python, Hadoop, and\r\nSpark in 2000+ corporations.\n\n**Proceeding:** h2o-ai-inc-2015-9\n\n**Published:** 2015-09-01\n\n\n\n#### 2. DynaBERT: Dynamic BERT with Adaptive Width and Depth\n\n*From Search Query: dynamic memory compression*\n\n*Xiao Chen, Lifeng Shang, Zhiqi Huang, Qun Liu, Lu Hou, Xin Jiang*\n\n**Abstract:** The pre-trained language models like BERT, though powerful in many natural language processing tasks, are both computation and memory expensive. To alleviate this problem, one approach is to compress them for specific tasks before deployment. However, recent works on BERT compression usually compress the large BERT model to a fixed smaller size. They can not fully satisfy the requirements of different edge devices with various hardware performances. In this paper, we propose a novel dynamic BERT model (abbreviated as DynaBERT), which can flexibly adjust the size and latency by selecting adaptive width and depth. The training process of DynaBERT includes first training a width-adaptive BERT and then allowing both adaptive width and depth, by distilling knowledge from the full-sized model to small sub-networks. Network rewiring is also used to keep the more important attention heads and neurons shared by more sub-networks. Comprehensive experiments under various efficiency constraints demonstrate that our proposed dynamic BERT (or RoBERTa) at its largest size has comparable performance as BERT-base (or RoBERTa-base), while at smaller widths and depths consistently outperforms existing BERT compression methods. Code is available at https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/DynaBERT.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-04-08\n\n\n\n#### 3. PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel\n\n*From Search Query: hardware optimization techniques*\n\n*Shen Li, Ajit Mathews, Yuchen Hao, Geeta Chauhan, Pritam Damania, Bernard Nguyen, Can Balioglu, Alban Desmaison, Sam Shleifer, Myle Ott, Hamid Shojanazeri, Less Wright, Min Xu, Chien-chin Huang, Liang Luo, Rohan Varma, Andrew Gu, Yanli Zhao*\n\n**Abstract:** It is widely acknowledged that large models have the potential to deliver superior performance across a broad range of domains. Despite the remarkable progress made in the field of machine learning systems research, which has enabled the development and exploration of large models, such abilities remain confined to a small group of advanced users and industry leaders, resulting in an implicit technical barrier for the wider community to access and leverage these technologies. In this paper, we introduce PyTorch Fully Sharded Data Parallel (FSDP) as an industry-grade solution for large model training. FSDP has been closely co-designed with several key PyTorch core components including Tensor implementation, dispatcher system, and CUDA memory caching allocator, to provide non-intrusive user experiences and high training efficiency. Additionally, FSDP natively incorporates a range of techniques and settings to optimize resource utilization across a variety of hardware configurations. The experimental results demonstrate that FSDP is capable of achieving comparable performance to Distributed Data Parallel while providing support for significantly larger models with near-linear scalability in terms of TFLOPS.\n\n**Published:** 2023-04-21\n\n\n\n#### 4. Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training\n\n*From Search Query: hardware optimization techniques*\n\n*Minjia Zhang, Olatunji Ruwase, Stas Bekman, Masahiro Tanaka, Lev Kurilenko, Sam Ade Jacobs, Xinyu Lian*\n\n**Abstract:** Existing checkpointing approaches seem ill-suited for distributed training even though hardware limitations make model parallelism, i.e., sharding model state across multiple accelerators, a requirement for model scaling. Consolidating distributed model state into a single checkpoint unacceptably slows down training, and is impractical at extreme scales. Distributed checkpoints, in contrast, are tightly coupled to the model parallelism and hardware configurations of the training run, and thus unusable on different configurations. To address this problem, we propose Universal Checkpointing, a technique that enables efficient checkpoint creation while providing the flexibility of resuming on arbitrary parallelism strategy and hardware configurations. Universal Checkpointing unlocks unprecedented capabilities for large-scale training such as improved resilience to hardware failures through continued training on remaining healthy hardware, and reduced training time through opportunistic exploitation of elastic capacity. The key insight of Universal Checkpointing is the selection of the optimal representation in each phase of the checkpointing life cycle: distributed representation for saving, and consolidated representation for loading. This is achieved using two key mechanisms. First, the universal checkpoint format, which consists of a consolidated representation of each model parameter and metadata for mapping parameter fragments into training ranks of arbitrary model-parallelism configuration. Second, the universal checkpoint language, a simple but powerful specification language for converting distributed checkpoints into the universal checkpoint format. Our evaluation demonstrates the effectiveness and generality of Universal Checkpointing on state-of-the-art model architectures and a wide range of parallelism techniques.\n\n**Published:** 2024-06-27\n\n\n\n#### 5. Self-Adaptive Physics-Informed Neural Networks using a Soft Attention Mechanism\n\n*From Search Query: adaptive attention mechanisms*\n\n*Ulisses Braga-Neto, Levi McClenny*\n\n**Abstract:** Physics-Informed Neural Networks (PINNs) have emerged recently as a promising application of deep neural networks to the numerical solution of nonlinear partial differential equations (PDEs). However, it has been recognized that adaptive procedures are needed to force the neural network to fit accurately the stubborn spots in the solution of \"stiff\" PDEs. In this paper, we propose a fundamentally new way to train PINNs adaptively, where the adaptation weights are fully trainable and applied to each training point individually, so the neural network learns autonomously which regions of the solution are difficult and is forced to focus on them. The self-adaptation weights specify a soft multiplicative soft attention mask, which is reminiscent of similar mechanisms used in computer vision. The basic idea behind these SA-PINNs is to make the weights increase as the corresponding losses increase, which is accomplished by training the network to simultaneously minimize the losses and maximize the weights. In addition, we show how to build a continuous map of self-adaptive weights using Gaussian Process regression, which allows the use of stochastic gradient descent in problems where conventional gradient descent is not enough to produce accurate solutions. Finally, we derive the Neural Tangent Kernel matrix for SA-PINNs and use it to obtain a heuristic understanding of the effect of the self-adaptive weights on the dynamics of training in the limiting case of infinitely-wide PINNs, which suggests that SA-PINNs work by producing a smooth equalization of the eigenvalues of the NTK matrix corresponding to the different loss terms. In numerical experiments with several linear and nonlinear benchmark problems, the SA-PINN outperformed other state-of-the-art PINN algorithm in L2 error, while using a smaller number of training epochs.\n\n**Published:** 2020-09-07\n\n\n\n#### 6. AOE-Net: Entities Interactions Modeling with Adaptive Attention Mechanism for Temporal Action Proposals Generation\n\n*From Search Query: adaptive attention mechanisms*\n\n*Ngan Le, Minh-Triet Tran, Bhiksha Raj, Kashu Yamazaki, Sang Truong, Khoa Vo*\n\n**Abstract:** Temporal action proposal generation (TAPG) is a challenging task, which requires localizing action intervals in an untrimmed video. Intuitively, we as humans, perceive an action through the interactions between actors, relevant objects, and the surrounding environment. Despite the significant progress of TAPG, a vast majority of existing methods ignore the aforementioned principle of the human perceiving process by applying a backbone network into a given video as a black-box. In this paper, we propose to model these interactions with a multi-modal representation network, namely, Actors-Objects-Environment Interaction Network (AOE-Net). Our AOE-Net consists of two modules, i.e., perception-based multi-modal representation (PMR) and boundary-matching module (BMM). Additionally, we introduce adaptive attention mechanism (AAM) in PMR to focus only on main actors (or relevant objects) and model the relationships among them. PMR module represents each video snippet by a visual-linguistic feature, in which main actors and surrounding environment are represented by visual information, whereas relevant objects are depicted by linguistic features through an image-text model. BMM module processes the sequence of visual-linguistic features as its input and generates action proposals. Comprehensive experiments and extensive ablation studies on ActivityNet-1.3 and THUMOS-14 datasets show that our proposed AOE-Net outperforms previous state-of-the-art methods with remarkable performance and generalization for both TAPG and temporal action detection. To prove the robustness and effectiveness of AOE-Net, we further conduct an ablation study on egocentric videos, i.e. EPIC-KITCHENS 100 dataset. Source code is available upon acceptance.\n\n**Published:** 2022-10-05\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on dynamic memory compression, hardware optimization techniques, and efficient combination of attention patterns, here are some key findings and techniques from the provided sources:\n\n## Dynamic Memory Compression Techniques\n\n### CompAct: Compressed Activations\nThe CompAct technique introduced in reduces peak memory utilization during the training of Large Language Models (LLMs) by compressing activations. This method stores low-rank, compressed activations during the forward pass, which significantly reduces the memory required for the compute graph and optimizer states. This approach is adaptive as it adjusts the compression rate based on the layer's dimensionality, making it efficient for different types of content and computational requirements.\n\n### KV-Compress\nThe KV-Compress method described in involves evicting contiguous KV blocks within a PagedAttention framework, which reduces the memory footprint of the KV cache. This technique allows for variable compression rates across different attention heads, making it adaptable to various computational needs and content types.\n\n### CASAK-V: Adaptive KV-Cache Compression\nCASAK-V, as detailed in and, implements adaptive chunk-wise KV-cache compression using policies adapted from layer-wise sparse attention configurations. This method dynamically adjusts the compression rates and attention patterns based on the attention scores, ensuring efficient long-context processing with minimal performance degradation.\n\n## Hardware Optimization Techniques for Attention Mechanisms\n\n### PagedAttention and KV-Compress\nThe PagedAttention framework, enhanced by KV-Compress, is designed to handle the fragmentation introduced by variable-head-rate compression. This approach optimizes memory usage by representing and computing attention over a paged KV cache, which is particularly beneficial for hardware-constrained environments.\n\n### CASAK-V: Dynamic Sparse Attention\nCASAK-V uses a meta-learning framework to fine-tune a transformer for sparse pattern identification, which is then applied dynamically during token generation. This method reduces the quadratic complexity of attention mechanisms and the substantial memory requirements of KV-caches, making it suitable for on-device or on-premises deployments.\n\n## Methods for Combining Different Types of Attention Patterns Efficiently\n\n### CASAK-V: Context-Aware Adaptive Sparse Attention\nCASAK-V combines different sparse attention patterns (such as fixed local windows, dynamic column stripes, and block-sparse configurations) efficiently by dynamically adjusting these patterns based on attention scores. This approach ensures context-awareness and minimal performance degradation while reducing memory usage significantly.\n\n### CompAct and KV-Compress Integration\nWhile CompAct focuses on compressing activations during training, integrating it with KV-Compress or CASAK-V could provide a comprehensive approach to memory management. CompAct's low-rank compression during training can be complemented by the adaptive KV-cache compression and dynamic sparse attention mechanisms during inference, leading to a more efficient overall system.\n\n## Summary\n\n- **Dynamic Memory Compression**: Techniques like CompAct, KV-Compress, and CASAK-V offer adaptive compression methods that can adjust to different content types and computational requirements, significantly reducing memory footprint.\n- **Hardware Optimization**: PagedAttention, KV-Compress, and CASAK-V are optimized for hardware efficiency, particularly in handling attention mechanisms and reducing memory fragmentation.\n- **Combining Attention Patterns**: CASAK-V's dynamic sparse attention and adaptive KV-cache compression demonstrate how different attention patterns can be combined efficiently to maintain performance while reducing memory usage.\n\nThese approaches can be integrated to enhance the design of autoregressive language models, ensuring better scalability, efficiency, and performance.\n",
            "analysis": "Summary of Analysis:\n\nThe search results reveal several promising approaches for enhancing FastTTTLinear, particularly in three critical areas:\n\n1. Memory Compression and Management:\n- Loki's low-rank key approach for efficient sparse attention\n- LoMA's lossless KV cache compression\n- B'MOJO's hybrid state space realizations combining eidetic and fading memory\n\n2. Hardware-Efficient Sparse Operations:\n- Self-attention memory optimization techniques\n- Hardware-aware sparse matrix computations\n- Efficient implementation strategies for combined attention patterns\n\n3. Adaptive Computation:\n- Dynamic token selection mechanisms\n- Structured sparsity patterns\n- Memory-efficient attention implementations\n\nKey References and Excerpts:\n\n1. From Loki (Singhania et al., 2024):\n\"Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space\"\nThis suggests efficient ways to reduce memory footprint while maintaining performance.\n\n2. From LoMA (Wang & Xiao, 2024):\n\"enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation\"\nShows potential for efficient memory management without information loss.\n\n3. From B'MOJO (Zancato et al., 2024):\n\"seamlessly combine eidetic and fading memory within an elementary composable module\"\nOffers insights into hybrid memory architectures.\n\n4. From Self-attention Memory Paper (Rabe & Staats, 2021):\n\"requires O(\u221an) memory, is numerically stable, and is within a few percent of the runtime of the standard implementation\"\nDemonstrates practical memory optimization techniques.\n\nKey Insights:\n\n1. Memory Architecture:\n- Hierarchical memory systems can effectively balance speed and capacity\n- Lossless compression techniques can significantly reduce memory footprint\n- Hybrid approaches combining different memory types show promise\n\n2. Attention Optimization:\n- Low-rank approximations can maintain accuracy while reducing computation\n- Structured sparsity patterns can improve hardware efficiency\n- Dynamic token selection can reduce unnecessary computations\n\n3. Implementation Strategies:\n- Hardware-aware design is crucial for practical efficiency\n- Combining multiple optimization techniques can yield better results\n- Trade-offs between memory, computation, and accuracy need careful consideration\n\nFuture Search Plan:\nNeed to investigate:\n1. Techniques for combining different types of memory compression\n2. Methods for dynamic adaptation of compression rates\n3. Hardware-specific optimization strategies for different architectures\n\nReferences:\n\n1. Singhania, P., Singh, S., He, S., Feizi, S., & Bhatele, A. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv preprint.\n\n2. Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention. arXiv preprint.\n\n3. Zancato, L., et al. (2024). B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory.\n\n4. Rabe, M., & Staats, C. (2021). Self-attention Does Not Need O(n^2) Memory. arXiv preprint.\n\n5. Price, E., Silwal, S., & Zhou, S. (2022). Hardness and Algorithms for Robust and Sparse Optimization. International Conference on Machine Learning.\n\n6. Evans, T.W., & Nair, P. (2018). Discretely Relaxing Continuous Variables for tractable Variational Inference. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "compression attention integration, hardware efficient implementation",
            "detail": "Find papers discussing:\n1. Specific techniques and methods for integrating compression with attention mechanisms\n2. Hardware-efficient implementations of combined compression and attention patterns\n3. Practical examples of successful integration of multiple optimization techniques in language models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Specific techniques and methods for integrating compression with attention mechanisms\n2. Hardware-efficient implementations of combined compression and attention patterns\n3. Practical examples of successful integration of multiple optimization techniques in language models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.79)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.79)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 2. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models (Avg. Score: 0.59)\n\n*Haoran You, Yichao Fu, Zheng Wang, Amir Yazdanbakhsh, Y. Lin*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This work introduces an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs.\n\n**Abstract:** Autoregressive Large Language Models (LLMs) have achieved impressive performance in language tasks but face two significant bottlenecks: (1) quadratic complexity in the attention module as the number of tokens increases, and (2) limited efficiency due to the sequential processing nature of autoregressive LLMs during generation. While linear attention and speculative decoding offer potential solutions, their applicability and synergistic potential for enhancing autoregressive LLMs remain uncertain. We conduct the first comprehensive study on the efficacy of existing linear attention methods for autoregressive LLMs, integrating them with speculative decoding. We introduce an augmentation technique for linear attention that ensures compatibility with speculative decoding, enabling more efficient training and serving of LLMs. Extensive experiments and ablation studies involving seven existing linear attention models and five encoder/decoder-based LLMs consistently validate the effectiveness of our augmented linearized LLMs. Notably, our approach achieves up to a 6.67 reduction in perplexity on the LLaMA model and up to a 2$\\times$ speedup during generation compared to prior linear attention methods. Codes and models are available at https://github.com/GATECH-EIC/Linearized-LLM.\n\n##### *Relevant Chunk: No. 35/41 (Score: 0.59)*\n\n```\nIn NAACL, 2018. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. SmoothQuant: Accurate and Efficient Posttraining Quantization for Large Language Models. In ICML, 2023. Xiong, Y., Zeng, Z., Chakraborty, R., Tan, M., Fung, G., Li, Y., and Singh, V. Nystr\u00f6mformer: A Nystr\u00f6m-based Algorithm for Approximating Self-attention. In AAAI, 2021. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated Linear Attention Transformers with Hardware-efficient Training. arXiv preprint arXiv:2312.06635, 2023. You, H., Sun, Z., Shi, H., Yu, Z., Zhao, Y., Zhang, Y., Li, C., Li, B., and Lin, Y. ViTCoD: Vision Transformer Acceleration via Dedicated Algorithm and Accelerator Co-Design. In 2023 IEEE International Symposium on High-Performance Computer Architecture (HPCA), pp. 273-286. IEEE, 2023a. You, H., Xiong, Y., Dai, X., Wu, B., Zhang, P., Fan, H., Vajda, P., and Lin, Y. C. Castling-ViT: Compressing Self-Attention via Switching Towards Linear-Angular Attention at Vision Transformer Inference. In CVPR, 2023b. You, H., Shi, H., Guo, Y., and Lin, Y. ShiftAddViT: Mixture of Multiplication Primitives Towards Efficient Vision Transformer. Advances in Neural Information Processing Systems, 36, 2024. Zeng, Z., Xiong, Y., Ravi, S., Acharya, S., Fung, G. M., and Singh, V. You Only Sample (almost) Once: Linear Cost Self-attention via Bernoulli Sampling. In ICML, 2021. Zhang, X., Zhao, J., and LeCun, Y. Character-level Convolutional Networks for Text Classification.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.55)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.55)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 4. PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation (Avg. Score: 0.30)\n\n*Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yunhe Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work presents a new efficient model architecture for establishing modern language models, namely, PanGu-$\\pi$, and develops an LLM named YunShan for practical application, which can surpass other models with similar scales on benchmarks.\n\n**Abstract:** The recent trend of large language models (LLMs) is to increase the scale of both model size (\\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices. However, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art language model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the nonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The series informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further used to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the model nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using the same dataset and training strategy to compare PanGu-$\\pi$ with state-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a comparable performance to that of benchmarks with about 10\\% inference speed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical application. The results show that YunShan can surpass other models with similar scales on benchmarks.\n\n##### *Relevant Chunk: No. 21/62 (Score: 0.30)*\n\n```\n[36] J. W. Rae et al. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [37] G. Xiao et al. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.\n```\n\n#### 5. Extending Context Window of Large Language Models via Semantic Compression (Avg. Score: 0.22)\n\n*WeiZhi Fei, Xueyan Niu, Pingyi Zhou, Lu Hou, Bo Bai, Lei Deng, Wei Han*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** This work proposes a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning.\n\n**Abstract:** Transformer-based Large Language Models (LLMs) often impose limitations on the length of the text input to ensure the generation of fluent and relevant responses. This constraint restricts their applicability in scenarios involving long texts. We propose a novel semantic compression method that enables generalization to texts that are 6-8 times longer, without incurring significant computational costs or requiring fine-tuning. Our proposed framework draws inspiration from source coding in information theory and employs a pre-trained model to reduce the semantic redundancy of long inputs before passing them to the LLMs for downstream tasks. Experimental results demonstrate that our method effectively extends the context window of LLMs across a range of tasks including question answering, summarization, few-shot learning, and information retrieval. Furthermore, the proposed semantic compression method exhibits consistent fluency in text generation while reducing the associated computational overhead.\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.22)*\n\n```\nThis pattern arises from the tendency of language users to minimize effort in their daily conversations. Consequently, by utilizing an expanded vocabulary, sentences can be significantly shortened while preserving the same semantic meaning. Moreover, it is common for language users to include redundant words during communication (Strunk Jr, 2007). These language habits are prevalent among users, and we propose to include a semantic compression module to mitigate the redundancy associated with these habits. Our proposed semantic compression method, reminiscent of lossy source coding in information theory, extends the context window by equivalently shortening the long text while preserving the semantic meaning. This procedure is conducted before inputting the tokens into the pre-trained LLMs. As illustrated in Fig. 1, the input undergoes compression before being transmitted to the LLM for various potential tasks. The semantic compression method can be customized and optimized for downstream tasks, taking into consideration practical constraints such as time and memory resources. The implementation of the semantic compression module is straightforward and can easily be incorporated into other interpolation-based context window extension methods and black box APIs. It demonstrates enhanced performance compared to SoTA interpolation-based methods on a range of tasks, including single-document question answering, multi-document question answering, summarization, few-shot learning, and information retrieval, using real-world datasets while incurring no extra parameter updates or memory consumption. Empirically, the proposed method is computational efficient and achieves 6-8 times context window extension. ## Our contributions:\n\n- We introduce a context window extension framework for LLMs that utilizes semantic compression. This framework serves as a plug-and-play tool to mitigate redundancy in input texts by efficiently performing topic modeling. - We construct a graph representation of the input to identify distinct sections of the text that pertain to different topics. The result is the segmentation of long texts into separate chunks, each focusing on a specific topic. We then conquer each chunk independently, resulting in a concise version of the original texts. This compression technique helps to condense the information while preserving the key ideas and context. - We demonstrate the applicability of our proposed semantic compression method through extensive experiments. The results highlight the advantages of our method in several key applications, including single-document question answering, multi-document question answering, summarization, few-shot learning, and information retrieval. ## 2 RELATED WORK\n\nWith the advancement of SoTA LLMs, significant progress has been made in extending the context window lengths. ### 2.1 EXTRAPOLATION AND INTERPOLATION\n\nThe mainstream line of research aims to adapt existing language models trained on short texts to accommodate longer ones during inference (Anil et al., 2022). The key idea is to modify the positional embedding, which has only been trained on short texts. Several studies are based on the Rotary Position Embeddings (RoPE) of LLaMA and methods of adjusting it to the longer sequences. Chen et al. (2023a) develops the Position Interpolation (PI) method to linearly scale the input positional indices. Peng et al. (2023) presents YaRN, an efficient extrapolate mechanism inspired by the neural tangent kernel, to extend the context window to 64 k and 128 k . ### 2.2 EFFICIENT ATTENTION OpERATIONS\n\nDue to the self-attention mechanism, the inference cost of LLMs grows quadratically with the sequence length.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: compression attention integration, hardware efficient implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. LoCoCo: Dropping In Convolutions for Long Context Compression\n\n*From Search Query: compression attention integration*\n\n*Ruisi Cai, Yuandong Tian, Zhangyang Wang, Beidi Chen*\n\n**Abstract:** This paper tackles the memory hurdle of processing long context sequences in Large Language Models (LLMs), by presenting a novel approach, Dropping In Convolutions for Long Context Compression (LoCoCo). LoCoCo employs only a fixed-size Key-Value (KV) cache, and can enhance efficiency in both inference and fine-tuning stages. Diverging from prior methods that selectively drop KV pairs based on heuristics, LoCoCo leverages a data-driven adaptive fusion technique, blending previous KV pairs with incoming tokens to minimize the loss of contextual information and ensure accurate attention modeling. This token integration is achieved through injecting one-dimensional convolutional kernels that dynamically calculate mixing weights for each KV cache slot. Designed for broad compatibility with existing LLM frameworks, LoCoCo allows for straightforward\"drop-in\"integration without needing architectural modifications, while incurring minimal tuning overhead. Experiments demonstrate that LoCoCo maintains consistently outstanding performance across various context lengths and can achieve a high context compression rate during both inference and fine-tuning phases. During inference, we successfully compressed up to 3482 tokens into a 128-size KV cache, while retaining comparable performance to the full sequence - an accuracy improvement of up to 0.2791 compared to baselines at the same cache size. During post-training tuning, we also effectively extended the context length from 4K to 32K using a KV cache of fixed size 512, achieving performance similar to fine-tuning with entire sequences.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. COMCAT: Towards Efficient Compression and Customization of Attention-Based Vision Models\n\n*From Search Query: compression attention integration*\n\n*Jinqi Xiao, Miao Yin, Yu Gong, Xiao Zang, Jian Ren, Bo Yuan*\n\n**TL;DR:** This paper develops a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods and can be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training and lower extra storage cost than the existing works.\n\n**Abstract:** Attention-based vision models, such as Vision Transformer (ViT) and its variants, have shown promising performance in various computer vision tasks. However, these emerging architectures suffer from large model sizes and high computational costs, calling for efficient model compression solutions. To date, pruning ViTs has been well studied, while other compression strategies that have been widely applied in CNN compression, e.g., model factorization, is little explored in the context of ViT compression. This paper explores an efficient method for compressing vision transformers to enrich the toolset for obtaining compact attention-based vision models. Based on the new insight on the multi-head attention layer, we develop a highly efficient ViT compression solution, which outperforms the state-of-the-art pruning methods. For compressing DeiT-small and DeiT-base models on ImageNet, our proposed approach can achieve 0.45% and 0.76% higher top-1 accuracy even with fewer parameters. Our finding can also be applied to improve the customization efficiency of text-to-image diffusion models, with much faster training (up to $2.6\\times$ speedup) and lower extra storage cost (up to $1927.5\\times$ reduction) than the existing works.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 0*)\n\n#### 3. MiniLM: Deep Self-Attention Distillation for Task-Agnostic Compression of Pre-Trained Transformers\n\n*From Search Query: compression attention integration*\n\n*Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, Ming Zhou*\n\n**TL;DR:** This work presents a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation, and demonstrates that the monolingual model outperforms state-of-the-art baselines in different parameter size of student models.\n\n**Abstract:** Pre-trained language models (e.g., BERT (Devlin et al., 2018) and its variants) have achieved remarkable success in varieties of NLP tasks. However, these models usually consist of hundreds of millions of parameters which brings challenges for fine-tuning and online serving in real-life applications due to latency and capacity constraints. In this work, we present a simple and effective approach to compress large Transformer (Vaswani et al., 2017) based pre-trained models, termed as deep self-attention distillation. The small model (student) is trained by deeply mimicking the self-attention module, which plays a vital role in Transformer networks, of the large model (teacher). Specifically, we propose distilling the self-attention module of the last Transformer layer of the teacher, which is effective and flexible for the student. Furthermore, we introduce the scaled dot-product between values in the self-attention module as the new deep self-attention knowledge, in addition to the attention distributions (i.e., the scaled dot-product of queries and keys) that have been used in existing works. Moreover, we show that introducing a teacher assistant (Mirzadeh et al., 2019) also helps the distillation of large pre-trained Transformer models. Experimental results demonstrate that our monolingual model outperforms state-of-the-art baselines in different parameter size of student models. In particular, it retains more than 99% accuracy on SQuAD 2.0 and several GLUE benchmark tasks using 50% of the Transformer parameters and computations of the teacher model. We also obtain competitive results in applying deep self-attention distillation to multilingual pre-trained models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 992  (*Influential: 148*)\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient implementation*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 5. Simple Hardware-Efficient PCFGs with Independent Left and Right Productions\n\n*From Search Query: hardware efficient implementation*\n\n*Wei Liu, Songlin Yang, Yoon Kim, Kewei Tu*\n\n**TL;DR:** This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions, and finds that this formalism scales more effectively both as a language model and as an unsupervised parser.\n\n**Abstract:** Scaling dense PCFGs to thousands of nonterminals via a low-rank parameterization of the rule probability tensor has been shown to be beneficial for unsupervised parsing. However, PCFGs scaled this way still perform poorly as a language model, and even underperform similarly-sized HMMs. This work introduces \\emph{SimplePCFG}, a simple PCFG formalism with independent left and right productions. Despite imposing a stronger independence assumption than the low-rank approach, we find that this formalism scales more effectively both as a language model and as an unsupervised parser. As an unsupervised parser, our simple PCFG obtains an average F1 of 65.1 on the English PTB, and as a language model, it obtains a perplexity of 119.0, outperforming similarly-sized low-rank PCFGs. We further introduce \\emph{FlashInside}, a hardware IO-aware implementation of the inside algorithm for efficiently scaling simple PCFGs.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 6. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: hardware efficient implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. TAFIM: Targeted Adversarial Attacks against Facial Image Manipulations\n\n*From Search Query: compression attention integration*\n\n*Matthias Niessner, Lev Markhasin, Shivangi Aneja*\n\n**Abstract:** Face manipulation methods can be misused to affect an individual's privacy or to spread disinformation. To this end, we introduce a novel data-driven approach that produces image-specific perturbations which are embedded in the original images. The key idea is that these protected images prevent face manipulation by causing the manipulation model to produce a predefined manipulation target (uniformly colored output image in our case) instead of the actual manipulation. In addition, we propose to leverage differentiable compression approximation, hence making generated perturbations robust to common image compression. In order to prevent against multiple manipulation methods simultaneously, we further propose a novel attention-based fusion of manipulation-specific perturbations. Compared to traditional adversarial attacks that optimize noise patterns for each image individually, our generalized model only needs a single forward pass, thus running orders of magnitude faster and allowing for easy integration in image processing stacks, even on resource-constrained devices like smartphones.\n\n**Published:** 2021-12-16\n\n\n\n#### 2. Extremely Low Footprint End-to-End ASR System for Smart Device\n\n*From Search Query: compression attention integration*\n\n*Anonymous*\n\n**Abstract:** Recently, end-to-end (E2E) speech recognition has become popular, since it can integrate the acoustic, pronunciation and language models into a single neural network, which outperforms conventional models. Among E2E approaches, attention-based models, e.g. Transformer, have emerged as being superior. Such models have opened the door to deployment of ASR on smart devices, however they still suffer from requiring a large number of model parameters. We propose an extremely low footprint E2E ASR system for smart devices, to achieve the goal of satisfying resource constraints without sacrificing recognition accuracy. We design cross-layer weight sharing to improve parameter efficiency and further exploit model compression methods including sparsification and quantization, to reduce memory storage and boost decoding efficiency. We evaluate our approaches on the public AISHELL-1 and AISHELL-2 benchmarks. On the AISHELL-2 task, the proposed method achieves more than 10x compression (model size reduces from 248 to 24MB), at the cost of only minor performance loss (CER reduces from 6.49% to 6.92%).\n\n**Published:** 2021-04-06\n\n\n\n#### 3. Hardware-efficient entangled measurements for variational quantum algorithms\n\n*From Search Query: hardware efficient implementation*\n\n*Anonymous*\n\n**Abstract:** Variational algorithms have received significant attention in recent years due to their potential to solve practical problems using noisy intermediate-scale quantum (NISQ) devices. A fundamental step of these algorithms is the evaluation of the expected value of Hamiltonians, and hence efficient schemes to perform this task are required. The standard approach employs local measurements of Pauli operators and requires a large number of circuits. An alternative is to make use of entangled measurements, which might introduce additional gates between physically disconnected qubits that harm the performance. As a solution to this problem, we propose hardware-efficient entangled measurements (HEEM), that is, measurements that permit only entanglement between physically connected qubits. We show that this strategy enhances the evaluation of molecular Hamiltonians in NISQ devices by reducing the number of circuits required without increasing their depth. We provide quantitative metrics of how this approach offers better results than local measurements and arbitrarily entangled measurements. We estimate the ground-state energy of the H$_2$O molecule with classical simulators and quantum hardware using the variational quantum eigensolver with HEEM.\n\n**Published:** 2022-02-14\n\n\n\n#### 4. HELP: Hardware-Adaptive Efficient Latency Prediction for NAS via Meta-Learning\n\n*From Search Query: hardware efficient implementation*\n\n*Sung Ju Hwang, Song Chong, Sewoong Lee, Hayeon Lee*\n\n**Abstract:** For deployment, neural architecture search should be hardware-aware, in order to satisfy the device-specific constraints (e.g., memory usage, latency and energy consumption) and enhance the model efficiency. Existing methods on hardware-aware NAS collect a large number of samples (e.g., accuracy and latency) from a target device, either builds a lookup table or a latency estimator. However, such approach is impractical in real-world scenarios as there exist numerous devices with different hardware specifications, and collecting samples from such a large number of devices will require prohibitive computational and monetary cost. To overcome such limitations, we propose Hardware-adaptive Efficient Latency Predictor (HELP), which formulates the device-specific latency estimation problem as a meta-learning problem, such that we can estimate the latency of a model's performance for a given task on an unseen device with a few samples. To this end, we introduce novel hardware embeddings to embed any devices considering them as black-box functions that output latencies, and meta-learn the hardware-adaptive latency predictor in a device-dependent manner, using the hardware embeddings. We validate the proposed HELP for its latency estimation performance on unseen platforms, on which it achieves high estimation performance with as few as 10 measurement samples, outperforming all relevant baselines. We also validate end-to-end NAS frameworks using HELP against ones without it, and show that it largely reduces the total time cost of the base NAS method, in latency-constrained settings. Code is available at https://github.com/HayeonLee/HELP.\n\n**Published:** 2021-06-16\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models by integrating compression with attention mechanisms and ensuring hardware-efficient implementations, here are some key findings and techniques from the provided sources and additional insights:\n\n## Combining Compression with Attention Mechanisms\n\n### Dynamic Sparse Attention and KV-Cache Compression\nThe paper on \"Context-Aware adaptive Sparse Attention with Key-Value cache compression (CASAK-V)\" introduces a dynamic sparse attention mechanism that adapts to the input context and task requirements. This approach combines sparse attention patterns with adaptive KV-cache compression, which is crucial for reducing computational costs and memory usage while maintaining model accuracy. This method dynamically generates and applies head-specific sparse attention patterns, making it highly relevant for integrating compression with attention mechanisms.\n\n### Hierarchical Compression and Attention\nThe concept of hierarchical compression with dynamic rates, as hinted in the analysis, can be integrated with attention mechanisms. For instance, using structured sparsity patterns in attention calculations, as seen in \"Near-Lossless Acceleration\" and \"TRAMS,\" can be combined with hierarchical compression techniques to manage memory efficiently. This approach ensures that different types of information are compressed at different rates based on their importance, aligning with the dynamic state management and efficient KV-cache compression strategies.\n\n### Attention Optimization in Prompt Compression\nThe survey on \"Prompt Compression for Large Language Models\" discusses attention optimization techniques, such as soft prompt compression, which reduces the input length by storing key information in a small number of special tokens. This method limits attention to these compressed tokens, reducing computational costs. Integrating such attention optimization with compression techniques can enhance the efficiency of autoregressive models.\n\n## Hardware-Efficient Implementations\n\n### IO-Aware Implementations and Efficient Memory Access\nThe \"Lightning Attention-2\" paper highlights the importance of tiling and separately handling intra-block and inter-block components in linear attention calculations, which is IO-aware and efficient in terms of memory access patterns. This approach can be combined with dynamic sparse attention and KV-cache compression to ensure hardware-efficient implementations.\n\n### Hardware-Specific Sparse Operations\nThe \"CASAK-V\" paper emphasizes the use of a lightweight predictor network to estimate token pair importance in the attention matrix, which is hardware-specific and efficient for sparse operations. This approach allows for seamless integration with existing pre-trained LLMs without extensive retraining or architectural modifications, making it highly practical for hardware-constrained environments.\n\n### Distributed Processing and Blockwise Computation\nThe \"Ring Attention\" approach involves blockwise computation of self-attention and feedforward layers to distribute long sequences across multiple devices. This method provides insights into efficient distributed processing and can be integrated with compression and attention optimization techniques to enhance overall model efficiency and scalability.\n\n## Practical Examples of Successful Integration\n\n### Adaptive Structured Sparse Attention\nThe \"Near-Lossless Acceleration\" paper demonstrates the effectiveness of adaptive structured sparse attention in reducing computational costs while maintaining accuracy. This method can be combined with hierarchical compression and dynamic state management to achieve a balanced approach to memory and attention optimization.\n\n### Training-Free Memory Selection\nThe \"TRAMS\" method involves training-free memory selection based on a simple metric, which can be integrated with compression techniques to dynamically select tokens participating in attention calculations. This approach ensures efficient memory management and attention processing without compromising accuracy.\n\nBy combining these techniques, researchers can develop autoregressive language models that are highly efficient in terms of memory usage, computational costs, and hardware utilization, while maintaining or improving model accuracy and scalability.\n",
            "analysis": "Summary of Analysis:\n\nAfter three rounds of searches, I've identified several promising directions for enhancing FastTTTLinear, particularly in combining efficient memory management, hardware-aware optimization, and adaptive attention mechanisms:\n\n1. Memory Architecture:\n- Hierarchical compression with dynamic rates\n- Adaptive state management\n- Efficient KV-cache compression\n\n2. Attention Mechanisms:\n- Structured sparsity patterns\n- Dynamic token selection\n- Multi-scale attention processing\n\n3. Hardware Optimization:\n- IO-aware implementations\n- Efficient memory access patterns\n- Hardware-specific sparse operations\n\nKey References and Excerpts:\n\n1. From \"Dynamic Memory Compression\":\n\"CompAct technique reduces peak memory utilization during training of Large Language Models by compressing activations [...] adjusts compression rate based on layer's dimensionality\"\nThis suggests adaptive compression strategies can significantly reduce memory footprint.\n\n2. From \"Lightning Attention-2\":\n\"leverages the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation\"\nShows how to efficiently implement linear attention with hardware considerations.\n\n3. From \"Near-Lossless Acceleration\":\n\"employs a two-stage query-guided key-value filtering approach [...] adaptively select a minimum set of key-values with low overhead\"\nDemonstrates effective sparse attention implementation.\n\n4. From \"TRAMS\":\n\"selects tokens participating in attention calculation based on one simple metric\"\nShows efficient token selection can improve performance.\n\n5. From \"Ring Attention\":\n\"blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices\"\nProvides insights into efficient distributed processing.\n\nKey Insights:\n\n1. Memory Management:\n- Hierarchical compression with different rates for different types of information\n- Dynamic state management based on importance\n- Efficient KV-cache compression with minimal accuracy loss\n\n2. Attention Design:\n- Combine structured sparsity with dynamic token selection\n- Use multi-scale attention processing\n- Implement hardware-efficient attention patterns\n\n3. Implementation Strategy:\n- IO-aware design for better hardware utilization\n- Efficient memory access patterns\n- Hardware-specific optimizations for sparse operations\n\n4. Integration Approach:\n- Combine compression techniques with sparse attention\n- Use adaptive mechanisms for both memory and attention\n- Implement hardware-aware optimizations throughout\n\nFuture Search Plan:\nNeed to investigate:\n1. Specific techniques for combining compression with attention mechanisms\n2. Methods for dynamic adaptation of compression and attention patterns\n3. Hardware-specific implementation details for different architectures\n\nReferences:\n\n1. Qin, Z., et al. (2024). Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models. arXiv preprint.\n\n2. Zhu, Q., et al. (2024). Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention. arXiv preprint.\n\n3. Yu, H., et al. (2023). TRAMS: Training-free Memory Selection for Long-range Language Modeling. EMNLP.\n\n4. Liu, H., et al. (2023). Ring Attention with Blockwise Transformers for Near-Infinite Context. arXiv preprint.\n\n5. Nawrot, P., et al. (2024). Dynamic Memory Compression: Retrofitting LLMs for Accelerated Inference. arXiv preprint.\n\n6. Li, S., et al. (2023). PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel. arXiv preprint.\n\n7. Zhang, M., et al. (2024). Universal Checkpointing: Efficient and Flexible Checkpointing for Large Scale Distributed Training. arXiv preprint.\n\nBased on this analysis, we need one final round of search focusing on specific techniques for combining compression with attention mechanisms and hardware-specific implementation details before formulating the final proposal."
        },
        {
            "ready": false,
            "query": "hardware efficient compression language models, hierarchical compression implementation techniques",
            "detail": "Search for papers discussing:\n1. Hardware-efficient implementations of compressed attention\n2. Integration techniques for hierarchical compression in transformers\n3. Memory optimization strategies for compressed models\n4. Empirical evaluations of compression techniques\n5. Performance analysis of hierarchical attention architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hardware-efficient implementations of compressed attention\n2. Integration techniques for hierarchical compression in transformers\n3. Memory optimization strategies for compressed models\n4. Empirical evaluations of compression techniques\n5. Performance analysis of hierarchical attention architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. LoMA: Lossless Compressed Memory Attention (Avg. Score: 0.70)\n\n*Yumeng Wang, Zhenyang Xiao*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Lossless Compressed Memory Attention (LoMA) is introduced, a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation.\n\n**Abstract:** Large Language Models (LLMs) face limitations due to the high demand on GPU memory and computational resources when handling long contexts. While sparsify the Key-Value (KV) cache of transformer model is a typical strategy to alleviate resource usage, it unavoidably results in the loss of information. We introduce Lossless Compressed Memory Attention (LoMA), a novel approach that enables lossless compression of the KV cache, thereby reducing the memory and computational demands during autoregressive generation. LoMA incorporates a specialized training or fine-tuning precedure alongside an autoregressive generation algorithm optimized for the compressed context. Our method compresses the KV cache after every $tc$ generated tokens with a compression ratio of $c$ and a target compressed length $t$, and this process occurs within a single inference pass without dependency on auxiliary models. We engineered an efficient training scheme involving specific inputs, attention masks, and position identifiers to instill this compression capability. Experimental validation has demonstrated that LoMA significantly reducing computational consumption and memory usage through achieving lossless KV cache compression.\n\n##### *Relevant Chunk: No. 8/16 (Score: 0.70)*\n\n```\nSee Tab.2. ## 6. Conclusion\n\nWe propose the Lossless Compressed Memory Attention (LoMA), aimed at losslessly compressing information to reduce computational consumption in long text contexts. The advantages of this approach are: 1) It does not alter the model structure, allowing for an expansion of the model's contextual length to $c$ times its original size for most models; 2) It does not require additional annotated data and can be fine-tuned directly on pre-trained models; 3) It allows for segmental compression, and each compression only adds one inference process, avoiding a significant increase in generation time. We fine-tuned the LLaMA 7B model with LoMA on the C4 and GSM8K datasets, achieving convergence within 2000 iterations. Moreover, we found that information compression has good generalizability; models trained on C4 can be seamlessly generalized to the GSM8K dataset. We suggest adopting LoMA in pretraining to address the increasingly important scenarios of long texts in the future. ## References\n\nCobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., Hesse, C., and Schulman, J. Training Verifiers to Solve Math Word Problems, November 2021. URL http://arxiv.org/abs/2110. 14168. arXiv:2110.14168 [cs]. Gupta, A., Dar, G., Goodman, S., Ciprut, D., and Berant, J. Memory-efficient Transformers via Top-\\$k\\$ Attention, June 2021.\n```\n\n#### 2. Linear-Time Transformers via Vector Quantization (Avg. Score: 0.50)\n\n*Lucas D. Lingle*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** The optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput.\n\n**Abstract:** We introduce Transformer-VQ, a decoder-only transformer computing softmax-based dense self-attention in linear time. Transformer-VQ's efficient attention is enabled by vector-quantized keys and a novel caching mechanism. In our large-scale experiments, Transformer-VQ is shown highly competitive in quality, obtaining 0.99 bpb on Enwik8, 26.6 ppl on PG-19, and 3.16 bpb on ImageNet64. In addition, the optimized implementation of Transformer-VQ is over 3x faster than a comparable quadratic-time transformer at sequence length 8k, is over 12x faster at 32k, and can scale to 131k with similar throughput. Code available: \\url{https://github.com/transformer-vq/transformer_vq}\n\n##### *Relevant Chunk: No. 8/49 (Score: 0.50)*\n\n```\nFollowing van den Oord et al. (2017); Razavi et al. (2019), codebooks are parameterized using EMA-smoothed k-means. ### 3.4.2 TRAINING UPDATES\n\nInstead of updating on the full sequence loss given above, we generally update every $W / L$ query blocks, where $W \\ll T$, which resembles a strategy used in prior works Dai et al., 2019, Wu et al., 2022, Hutchins et al. 2022). Each update is obtained by backpropagating through a window of $W$ timesteps, with gradients computed on the corresponding terms in the per-token average losses above. Codebooks are also updated every $W / L$ query blocks. When $W / L=1$, using Theorem 3.7 is an efficient equivalent to a variable-length key-value cache. When $W / L>1$, a learning signal is sent through any value vectors added to the compressed cache within the backpropagation window. ## 4 RELATED WORK\n\n### 4.1 Hierarchical AtTENTION\n\nCombiner Ren et al. 2021) proposes an approximation of softmax using a simple graphical model, and parameterizes its internal probabilities using max-pooling over query/key features, enabling decoder-only self-attention in subquadratic time. H-Transformer-1D (Zhu \\& Soricut, 2021) uses average-pooling operations over queries/keys to reduce the complexity of encoder-only self-attention. Transformer-LS (Zhu et al. 2021) uses dynamic projections to downsample long-range features in transformers by a user-specified factor. Hourglass Transformer (Nawrot et al. 2021) and MegaByte (Yu et al. 2023) eschew pooling in favor of convolutions or reshaping for temporal downsampling, and apply these techniques to reduce computation in the interior layers of decoder-only transformers. Transformer-VQ differs from these works in that it uses vector quantization (VQ), a well-understood method for compression, instead of newly-designed heuristic methods. In addition, it does not rely on token contiguity to guide the compression process. Instead, it utilizes an equivalence to dense attention. Notably, Transformer-VQ is easier to sample from compared to previous hierarchical attention models; since the cache update logic can be equivalently applied every token instead of every $L$ tokens, there are no sporadic 'feature consolidation' operations required during sampling. ### 4.2 KERNELIZABLE ATTENTION\n\nKernelizable attention (Katharopoulos et al., 2020, Choromanski et al., 2021; Peng et al., 2021; Qin et al., 2022b) computes query and key features and applies the same nonlinearity to both of them separately, omitting additional nonlinearities when computing attention weights. By using the associativity of matrix multiplication, kernelized attention reduces attention to linear complexity. Transformer-VQ is distinguished from kernelizable attention through an asymmetric treatment of queries and keys, a deterministic equivalence to softmax-based attention, training stability, and strong quantitative results on long-context autoregressive modeling benchmarks. Clustering attention (Vyas et al. 2020) uses vector-quantized queries and is also kernelizable. However, it requires learning per-layer codebooks for each sequence and uses a modified form of Lloyd's iterations based on Hamming distance and locality-sensitive hashing. This yields a complex non-causal algorithm which is only suitable for non-causal attention and is slow on TPUs. Transformer-VQ is strongly differentiated from clustering attention by its simplicity, applicability to decoder-only tasks, efficiency on TPUs, and large-scale experimental validation. ### 4.3 COMPRESSIVE ATTENTION\n\nCompressive Transformers (Rae et al. 2020) directly learn a compression function for long-range features. LUNA (Ma et al., 2021) and Recurrent Transformers (Bulatov et al., 2022, Hutchins et al., 2022) use cross-attention to compress long-range features into a recurrent state. Notably, our model implements a kind of block-recurrent mechanism for its cache, but is significantly more parameter-efficient than the mechanisms proposed by Ma et al.\n```\n\n#### 3. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.48)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.48)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 4. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.37)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.37)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 5. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.37)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.37)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: hardware efficient compression language models, hierarchical compression implementation techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Boost Transformer-based Language Models with GPU-Friendly Sparsity and Quantization\n\n*From Search Query: hardware efficient compression language models*\n\n*Chong Yu, Tao Chen, Zhongxue Gan*\n\n**TL;DR:** GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and de-coder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks.\n\n**Abstract:** Along with the performance improvement in NLP domain, the sizes of transformer-based language models ( TLM ) are also dramatically increased. Some prior works intend to compress TLM models into more compact forms, but do not fully consider the hardware characters may not support the efficient execution for these forms, leading to the deployment of TLM on hardware with noticeable acceleration is still challenging. This paper thoroughly designs a compression scheme named GPUSQ-TLM to maximally utilize the GPU-friendly 2:4 fine-grained structured sparsity and quantization characters . Especially, a dense TLM model is first pruned to meet the GPU\u2019s acceleration constraint of sparse patterns with FP16 type, then it is further quantized into a fixed-point one by quantization-aware training, to provide an extra speedup for integer tensors on GPU. A mixed-strategy knowledge distillation of labels, logits and feature maps is used for best accuracy compensation during pruning and quantization process. Experiment results show GPUSQ-TLM scheme achieves state-of-the-art compression on TLM model of various encoder and de-coder blocks with negligible accuracy degradation on SQuAD, GLUE, CNN-DM & XSum and WikiText benchmarking tasks. Moreover, GPUSQ-TLM can boost actual deployment performance by up to 4.08-4.25 \u00d7 times latency and 6.18-6.79 \u00d7 throughput on A100 GPU.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 2. Winning the Lottery Ahead of Time: Efficient Early Network Pruning\n\n*From Search Query: hardware efficient compression language models*\n\n*John Rachwan, Daniel Zugner, Bertrand Charpentier, Simon Geisler, Morgane Ayle, Stephan Gunnemann*\n\n**TL;DR:** Early Compression via Gradient Flow Preservation (EarlyCroP) is proposed, which efficiently extracts state-of-the-art sparse models before or early in training addressing challenge, and can be applied in a structured manner addressing challenge.\n\n**Abstract:** Pruning, the task of sparsifying deep neural networks, received increasing attention recently. Although state-of-the-art pruning methods extract highly sparse models, they neglect two main challenges: (1) the process of finding these sparse models is often very expensive; (2) unstructured pruning does not provide benefits in terms of GPU memory, training time, or carbon emissions. We propose Early Compression via Gradient Flow Preservation (EarlyCroP), which efficiently extracts state-of-the-art sparse models before or early in training addressing challenge (1), and can be applied in a structured manner addressing challenge (2). This enables us to train sparse networks on commodity GPUs whose dense versions would be too large, thereby saving costs and reducing hardware requirements. We empirically show that EarlyCroP outperforms a rich set of baselines for many tasks (incl. classification, regression) and domains (incl. computer vision, natural language processing, and reinforcment learning). EarlyCroP leads to accuracy comparable to dense training while outperforming pruning baselines.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 22  (*Influential: 1*)\n\n#### 3. SmoothQuant: Accurate and Efficient Post-Training Quantization for Large Language Models\n\n*From Search Query: hardware efficient compression language models*\n\n*Guangxuan Xiao, Ji Lin, Mickael Seznec, Julien Demouth, Song Han*\n\n**TL;DR:** SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs, offers a turn-key solution that reduces hardware costs and democratizes LLMs.\n\n**Abstract:** Large language models (LLMs) show excellent performance but are compute- and memory-intensive. Quantization can reduce memory and accelerate inference. However, existing methods cannot maintain accuracy and hardware efficiency at the same time. We propose SmoothQuant, a training-free, accuracy-preserving, and general-purpose post-training quantization (PTQ) solution to enable 8-bit weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that weights are easy to quantize while activations are not, SmoothQuant smooths the activation outliers by offline migrating the quantization difficulty from activations to weights with a mathematically equivalent transformation. SmoothQuant enables an INT8 quantization of both weights and activations for all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, Llama-1/2, Falcon, Mistral, and Mixtral models. We demonstrate up to 1.56x speedup and 2x memory reduction for LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM within a single node. Our work offers a turn-key solution that reduces hardware costs and democratizes LLMs. Code is available at https://github.com/mit-han-lab/smoothquant.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 486  (*Influential: 80*)\n\n#### 4. Stochastic Controlled Averaging for Federated Learning with Communication Compression\n\n*From Search Query: hierarchical compression implementation techniques*\n\n*Xinmeng Huang, Ping Li, Xiaoyun Li*\n\n**TL;DR:** This paper revisits the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs and proposes two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression, respectively.\n\n**Abstract:** Communication compression, a technique aiming to reduce the information volume to be transmitted over the air, has gained great interests in Federated Learning (FL) for the potential of alleviating its communication overhead. However, communication compression brings forth new challenges in FL due to the interplay of compression-incurred information distortion and inherent characteristics of FL such as partial participation and data heterogeneity. Despite the recent development, the performance of compressed FL approaches has not been fully exploited. The existing approaches either cannot accommodate arbitrary data heterogeneity or partial participation, or require stringent conditions on compression. In this paper, we revisit the seminal stochastic controlled averaging method by proposing an equivalent but more efficient/simplified formulation with halved uplink communication costs. Building upon this implementation, we propose two compressed FL algorithms, SCALLION and SCAFCOM, to support unbiased and biased compression, respectively. Both the proposed methods outperform the existing compressed FL methods in terms of communication and computation complexities. Moreover, SCALLION and SCAFCOM accommodates arbitrary data heterogeneity and do not make any additional assumptions on compression errors. Experiments show that SCALLION and SCAFCOM can match the performance of corresponding full-precision FL approaches with substantially reduced uplink communication, and outperform recent compressed FL methods under the same communication budget.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 160  (*Influential: 36*)\n\n#### 5. HiLLoC: Lossless Image Compression with Hierarchical Latent Variable Models\n\n*From Search Query: hierarchical compression implementation techniques*\n\n*James Townsend, Thomas Bird, Julius Kunze, D. Barber*\n\n**TL;DR:** Full convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model, achieving state of the art for compression of full size ImageNet images.\n\n**Abstract:** We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 53  (*Influential: 13*)\n\n### 3 related papers from Papers with Code\n\n#### 1. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers\n\n*From Search Query: hardware efficient compression language models*\n\n*Yuxiong He, Conglong Li, Xiaoxia Wu, Minjia Zhang, Reza Yazdani Aminabadi, Zhewei Yao*\n\n**Abstract:** How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.\n\n**Published:** 2022-06-04\n\n\n\n#### 2. Understanding INT4 Quantization for Transformer Models: Latency Speedup, Composability, and Failure Cases\n\n*From Search Query: hardware efficient compression language models*\n\n*Yuxiong He, Zhewei Yao, Reza Yazdani Aminabadi, Cheng Li, Xiaoxia Wu*\n\n**Abstract:** Improving the deployment efficiency of transformer-based language models has been challenging given their high computation and memory cost. While INT8 quantization has recently been shown to be effective in reducing both the memory cost and latency while preserving model accuracy, it remains unclear whether we can leverage INT4 (which doubles peak hardware throughput) to achieve further latency improvement. In this study, we explore the feasibility of employing INT4 weight and activation (W4A4) quantization for language models. Our findings indicate that W4A4 quantization introduces no to negligible accuracy degradation for encoder-only and encoder-decoder models, but causes a significant accuracy drop for decoder-only models. To materialize the performance gain using W4A4, we develop a highly optimized end-to-end W4A4 encoder inference pipeline supporting different quantization strategies. Our INT4 pipeline is $8.5\\times$ faster for latency-oriented scenarios and up to $3\\times$ for throughput-oriented scenarios compared to the inference of FP16, and improves the SOTA BERT INT8 performance from FasterTransformer by up to $1.7\\times$. We provide insights into the failure cases when applying W4A4 to decoder-only models, and further explore the compatibility of INT4 quantization with other compression methods, like pruning and layer reduction.\n\n**Published:** 2023-01-27\n\n\n\n#### 3. Bit-Swap: Recursive Bits-Back Coding for Lossless Compression with Hierarchical Latent Variables\n\n*From Search Query: hierarchical compression implementation techniques*\n\n*Friso H. Kingma, Pieter Abbeel, Jonathan Ho*\n\n**Abstract:** The bits-back argument suggests that latent variable models can be turned into lossless compression schemes. Translating the bits-back argument into efficient and practical lossless compression schemes for general latent variable models, however, is still an open problem. Bits-Back with Asymmetric Numeral Systems (BB-ANS), recently proposed by Townsend et al. (2019), makes bits-back coding practically feasible for latent variable models with one latent layer, but it is inefficient for hierarchical latent variable models. In this paper we propose Bit-Swap, a new compression scheme that generalizes BB-ANS and achieves strictly better compression rates for hierarchical latent variable models with Markov chain structure. Through experiments we verify that Bit-Swap results in lossless compression rates that are empirically superior to existing techniques. Our implementation is available at https://github.com/fhkingma/bitswap.\n\n**Published:** 2019-05-16\n\n\n\n\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about hierarchical compression, test-time training, and hardware-aware optimizations for language models. Key findings include:\n- Various approaches to dynamic compression and pruning\n- Integration methods for hierarchical attention with efficient memory usage\n- Hardware-aware optimization techniques\n- Performance implications of compressed models\n\n2. Useful References with Excerpts:\n\nFrom \"Training LLMs over Neurally Compressed Text\":\n\"By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost... a model trained over $C \\times$ compressed text will see $C \\times$ more text during training compared to a model trained over raw text\"\n\nFrom \"CORM: Cache Optimization\":\n\"...there is a striking similarity between adjacent tokens' query vectors, and the attention calculation of the current query can rely exclusively on the attention information of a small fraction of preceding queries\"\n\nFrom \"Hierarchical Transformers\":\n\"...having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences\"\n\n3. Key Insights:\n\na) Compression Strategies:\n- Dynamic pruning shows promise for adaptive compression\n- Multi-tier compression can balance efficiency and expressiveness\n- Hierarchical structures can effectively integrate with attention mechanisms\n\nb) Memory Management:\n- Cache optimization through selective retention of key-value pairs\n- Efficient handling of long sequences through hierarchical structures\n- Hardware-aware memory access patterns\n\nc) Implementation Considerations:\n- Need for balanced trade-off between compression and model quality\n- Importance of hardware-efficient implementations\n- Integration with test-time training capabilities\n\n4. Future Search Plan:\nNeed additional information on:\n1. Specific integration techniques for hierarchical compression with FastTTTLinear\n2. Hardware optimization strategies for compressed attention\n3. Empirical evaluations of similar approaches\n\n5. References:\n\n[1] Lester, B., et al. (2024). Training LLMs over Neurally Compressed Text. arXiv preprint.\n\n[2] Dai, J., et al. (2024). CORM: Cache Optimization with Recent Message for Large Language Model Inference. arXiv preprint.\n\n[3] Nawrot, P., et al. (2021). Hierarchical Transformers Are More Efficient Language Models. NAACL-HLT.\n\n[4] Yu, H., et al. (2023). TRAMS: Training-free Memory Selection for Long-range Language Modeling. EMNLP.\n\n[5] Yang, Z., et al. (2016). Hierarchical Attention Networks for Document Classification. NAACL."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "hierarchical compression language models, adaptive sparse attention transformers",
            "detail": "Search for papers discussing:\n1. Hierarchical compression techniques in transformer architectures\n2. Integration of sparse attention with compression mechanisms\n3. Memory-efficient implementations of attention mechanisms\n4. Hardware-aware optimizations for compressed language models\n5. Performance analysis of compressed attention models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Hierarchical compression techniques in transformer architectures\n2. Integration of sparse attention with compression mechanisms\n3. Memory-efficient implementations of attention mechanisms\n4. Hardware-aware optimizations for compressed language models\n5. Performance analysis of compressed attention models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.99)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.99)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.91)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 12/24 (Score: 0.91)*\n\n```\narXiv preprint arXiv:1511.07289, 2015. Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc V Le, and Ruslan Salakhutdinov. Transformer-xl: Attentive language models beyond a fixed-length context. arXiv preprint arXiv:1901.02860, 2019. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:16344-16359, 2022. Jiayu Ding, Shuming Ma, Li Dong, Xingxing Zhang, Shaohan Huang, Wenhui Wang, Nanning Zheng, and Furu Wei. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint arXiv:2307.02486, 2023. Yao Fu, Rameswar Panda, Xinyao Niu, Xiang Yue, Hannaneh Hajishirzi, Yoon Kim, and Hao Peng. Data engineering for scaling language models to 128 k context. arXiv preprint arXiv:2402.10171, 2024. Tao Ge, Jing Hu, Xun Wang, Si-Qing Chen, and Furu Wei. In-context autoencoder for context compression in a large language model.\n```\n\n#### 3. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.90)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.90)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n#### 4. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.86)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.86)*\n\n```\narXiv preprint arXiv:2112.05682, 2021. [43] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. [44] Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, and Timothy P Lillicrap. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [45] Roshan M Rao, Jason Liu, Robert Verkuil, Joshua Meier, John Canny, Pieter Abbeel, Tom Sercu, and Alexander Rives. Msa transformer. In International Conference on Machine Learning, pages 8844 - 8856. PMLR, 2021. [46] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining, pages 3505-3506, 2020. [47] Kiersten M Ruff and Rohit V Pappu. Alphafold and implications for intrinsically disordered proteins. Journal of Molecular Biology, 433(20):167208, 2021. [48] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [49] Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. [50] Yi Tay, Mostafa Dehghani, Samira Abnar, Hyung Won Chung, William Fedus, Jinfeng Rao, Sharan Narang, Vinh Q Tran, Dani Yogatama, and Donald Metzler. Scaling laws vs model architectures: How does inductive bias influence scaling? arXiv preprint arXiv:2207.10551, 2022 . [51] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Computing Surveys, 55(6):1-28, 2022. [52] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [53] Junxiong Wang, Jing Nathan Yan, Albert Gu, and Alexander M Rush. Pretraining without attention. arXiv preprint arXiv:2212.10544, 2022. [54] Sinong Wang, Belinda Z Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity.\n```\n\n#### 5. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.82)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 28/38 (Score: 0.82)*\n\n```\nArXiv, abs/2307.03170, 2023. [61] Lucrezia Valeriani, Diego Doimo, Francesca Cuturello, Alessandro Laio, Alessio Ansuini, and Alberto Cazzaniga. The geometry of hidden representations of large transformer models. ArXiv, abs/2302.00294, 2023. [62] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [63] Zhongwei Wan, Xin Wang, Che Liu, Samiul Alam, Yu Zheng, Jiachen Liu, Zhongnan Qu, Shen Yan, Yi Zhu, Quanlu Zhang, Mosharaf Chowdhury, and Mi Zhang. Efficient large language models: A survey. ArXiv, abs/2312.03863, 2023. [64] Hanrui Wang, Zhekai Zhang, and Song Han. Spatten: Efficient sparse attention architecture with cascade token and head pruning.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: hierarchical compression language models, adaptive sparse attention transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. HRKD: Hierarchical Relational Knowledge Distillation for Cross-domain Language Model Compression\n\n*From Search Query: hierarchical compression language models*\n\n*Chenhe Dong, Yaliang Li, Ying Shen, Minghui Qiu*\n\n**TL;DR:** To enhance the model capability and transferability, the idea of meta-learning is leveraged and set up domain-relational graphs to capture the relational information across different domains, and to dynamically select the most representative prototypes for each domain, a hierarchical compare-aggregate mechanism to capture hierarchical relationships.\n\n**Abstract:** On many natural language processing tasks, large pre-trained language models (PLMs) have shown overwhelming performances compared with traditional neural network methods. Nevertheless, their huge model size and low inference speed have hindered the deployment on resource-limited devices in practice. In this paper, we target to compress PLMs with knowledge distillation, and propose a hierarchical relational knowledge distillation (HRKD) method to capture both hierarchical and domain relational information. Specifically, to enhance the model capability and transferability, we leverage the idea of meta-learning and set up domain-relational graphs to capture the relational information across different domains. And to dynamically select the most representative prototypes for each domain, we propose a hierarchical compare-aggregate mechanism to capture hierarchical relationships. Extensive experiments on public multi-domain datasets demonstrate the superior performance of our HRKD method as well as its strong few-shot learning ability. For reproducibility, we release the code at https://github.com/cheneydon/hrkd.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 2. Language-guided Skill Learning with Temporal Variational Inference\n\n*From Search Query: hierarchical compression language models*\n\n*Haotian Fu, Pratyusha Sharma, Elias Stengel-Eskin, G. Konidaris, Nicolas Le Roux, Marc-Alexandre Cot'e, Xingdi Yuan*\n\n**TL;DR:** The results demonstrate that agents equipped with the algorithm are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.\n\n**Abstract:** We present an algorithm for skill discovery from expert demonstrations. The algorithm first utilizes Large Language Models (LLMs) to propose an initial segmentation of the trajectories. Following that, a hierarchical variational inference framework incorporates the LLM-generated segmentation information to discover reusable skills by merging trajectory segments. To further control the trade-off between compression and reusability, we introduce a novel auxiliary objective based on the Minimum Description Length principle that helps guide this skill discovery process. Our results demonstrate that agents equipped with our method are able to discover skills that help accelerate learning and outperform baseline skill learning approaches on new long-horizon tasks in BabyAI, a grid world navigation environment, as well as ALFRED, a household simulation environment.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 3. LoSparse: Structured Compression of Large Language Models based on Low-Rank and Sparse Approximation\n\n*From Search Query: hierarchical compression language models*\n\n*Yixiao Li, Yifan Yu, Qingru Zhang, Chen Liang, Pengcheng He, Weizhu Chen, Tuo Zhao*\n\n**TL;DR:** LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix, which significantly outperforms existing compression methods.\n\n**Abstract:** Transformer models have achieved remarkable results in various natural language tasks, but they are often prohibitively large, requiring massive memories and computational resources. To reduce the size and complexity of these models, we propose LoSparse (Low-Rank and Sparse approximation), a novel model compression technique that approximates a weight matrix by the sum of a low-rank matrix and a sparse matrix. Our method combines the advantages of both low-rank approximations and pruning, while avoiding their limitations. Low-rank approximation compresses the coherent and expressive parts in neurons, while pruning removes the incoherent and non-expressive parts in neurons. Pruning enhances the diversity of low-rank approximations, and low-rank approximation prevents pruning from losing too many expressive neurons. We evaluate our method on natural language understanding, question answering, and natural language generation tasks. We show that it significantly outperforms existing compression methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 42  (*Influential: 4*)\n\n#### 4. Transformers meet Stochastic Block Models: Attention with Data-Adaptive Sparsity and Cost\n\n*From Search Query: adaptive sparse attention transformers*\n\n*Sungjun Cho, Seonwoo Min, Jinwoo Kim, Moontae Lee, Honglak Lee, Seunghoon Hong*\n\n**TL;DR:** Empirical evaluations demonstrate that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation, and theoretically shows that it outperforms previous efficient variants as well as the original Transformer with full attention.\n\n**Abstract:** To overcome the quadratic cost of self-attention, recent works have proposed various sparse attention modules, most of which fall under one of two groups: 1) sparse attention under a hand-crafted patterns and 2) full attention followed by a sparse variant of softmax such as $\\alpha$-entmax. Unfortunately, the first group lacks adaptability to data while the second still requires quadratic cost in training. In this work, we propose SBM-Transformer, a model that resolves both problems by endowing each attention head with a mixed-membership Stochastic Block Model (SBM). Then, each attention head data-adaptively samples a bipartite graph, the adjacency of which is used as an attention mask for each input. During backpropagation, a straight-through estimator is used to flow gradients beyond the discrete sampling step and adjust the probabilities of sampled edges based on the predictive loss. The forward and backward cost are thus linear to the number of edges, which each attention head can also choose flexibly based on the input. By assessing the distribution of graphs, we theoretically show that SBM-Transformer is a universal approximator for arbitrary sequence-to-sequence functions in expectation. Empirical evaluations under the LRA and GLUE benchmarks demonstrate that our model outperforms previous efficient variants as well as the original Transformer with full attention. Our implementation can be found in https://github.com/sc782/SBM-Transformer .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 5. Adaptive Transformers for Learning Multimodal Representations\n\n*From Search Query: adaptive sparse attention transformers*\n\n*Prajjwal Bhargava*\n\n**TL;DR:** This work study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks, and shows that these approaches can help to learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\n\n**Abstract:** The usage of transformers has grown from learning about language semantics to forming meaningful visiolinguistic representations. These architectures are often over-parametrized, requiring large amounts of computation. In this work, we extend adaptive approaches to learn more about model interpretability and computational efficiency. Specifically, we study attention spans, sparse, and structured dropout methods to help understand how their attention mechanism extends for vision and language tasks. We further show that these approaches can help us learn more about how the network perceives the complexity of input sequences, sparsity preferences for different modalities, and other related phenomena.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2020\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Exphormer: Sparse Transformers for Graphs\n\n*From Search Query: adaptive sparse attention transformers*\n\n*Hamed Shirzad, A. Velingker, B. Venkatachalam, Danica J. Sutherland, A. Sinop*\n\n**TL;DR:** It is shown that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets.\n\n**Abstract:** Graph transformers have emerged as a promising architecture for a variety of graph learning and representation tasks. Despite their successes, though, it remains challenging to scale graph transformers to large graphs while maintaining accuracy competitive with message-passing networks. In this paper, we introduce Exphormer, a framework for building powerful and scalable graph transformers. Exphormer consists of a sparse attention mechanism based on two mechanisms: virtual global nodes and expander graphs, whose mathematical characteristics, such as spectral expansion, pseduorandomness, and sparsity, yield graph transformers with complexity only linear in the size of the graph, while allowing us to prove desirable theoretical properties of the resulting transformer models. We show that incorporating Exphormer into the recently-proposed GraphGPS framework produces models with competitive empirical results on a wide variety of graph datasets, including state-of-the-art results on three datasets. We also show that Exphormer can scale to datasets on larger graphs than shown in previous graph transformer architectures. Code can be found at \\url{https://github.com/hamed1375/Exphormer}.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 66  (*Influential: 14*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Simple and Controllable Music Generation\n\n*From Search Query: hierarchical compression language models*\n\n*Alexandre D\u00e9fossez, Yossi Adi, Gabriel Synnaeve, David Kant, Tal Remez, Itai Gat, Felix Kreuk, Jade Copet*\n\n**Abstract:** We tackle the task of conditional music generation. We introduce MusicGen, a single Language Model (LM) that operates over several streams of compressed discrete music representation, i.e., tokens. Unlike prior work, MusicGen is comprised of a single-stage transformer LM together with efficient token interleaving patterns, which eliminates the need for cascading several models, e.g., hierarchically or upsampling. Following this approach, we demonstrate how MusicGen can generate high-quality samples, both mono and stereo, while being conditioned on textual description or melodic features, allowing better controls over the generated output. We conduct extensive empirical evaluation, considering both automatic and human studies, showing the proposed approach is superior to the evaluated baselines on a standard text-to-music benchmark. Through ablation studies, we shed light over the importance of each of the components comprising MusicGen. Music samples, code, and models are available at https://github.com/facebookresearch/audiocraft\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-06-08\n\n\n\n#### 2. Discrete Autoencoders for Sequence Models\n\n*From Search Query: hierarchical compression language models*\n\n*\u0141ukasz Kaiser, Samy Bengio*\n\n**Abstract:** Recurrent models for sequences have been recently successful at many tasks,\nespecially for language modeling and machine translation. Nevertheless, it\nremains challenging to extract good representations from these models. For\ninstance, even though language has a clear hierarchical structure going from\ncharacters through words to sentences, it is not apparent in current language\nmodels. We propose to improve the representation in sequence models by\naugmenting current approaches with an autoencoder that is forced to compress\nthe sequence through an intermediate discrete latent space. In order to\npropagate gradients though this discrete representation we introduce an\nimproved semantic hashing technique. We show that this technique performs well\non a newly proposed quantitative efficiency measure. We also analyze latent\ncodes produced by the model showing how they correspond to words and phrases.\nFinally, we present an application of the autoencoder-augmented model to\ngenerating diverse translations.\n\n**Conference:** discrete-autoencoders-for-sequence-models-1\n\n**Published:** 2018-01-29\n\n\n\n#### 3. Adapt or Perish: Adaptive Sparse Transformer with Attentive Feature Refinement for Image Restoration\n\n*From Search Query: adaptive sparse attention transformers*\n\n*Jufeng Yang, Jinglei Shi, Jinshan Pan, Duosheng Chen, Shihao Zhou*\n\n**Abstract:**     Transformer-based approaches have achieved promising performance in image restoration tasks given their ability to model long-range dependencies which is crucial for recovering clear images. Though diverse efficient attention mechanism designs have addressed the intensive computations associated with using transformers they often involve redundant information and noisy interactions from irrelevant regions by considering all available tokens. In this work we propose an Adaptive Sparse Transformer (AST) to mitigate the noisy interactions of irrelevant areas and remove feature redundancy in both spatial and channel domains. AST comprises two core designs i.e. an Adaptive Sparse Self-Attention (ASSA) block and a Feature Refinement Feed-forward Network (FRFN). Specifically ASSA is adaptively computed using a two-branch paradigm where the sparse branch is introduced to filter out the negative impacts of low query-key matching scores for aggregating features while the dense one ensures sufficient information flow through the network for learning discriminative representations. Meanwhile FRFN employs an enhance-and-ease scheme to eliminate feature redundancy in channels enhancing the restoration of clear latent images. Experimental results on commonly used benchmarks have demonstrated the versatility and competitive performance of our method in several tasks including rain streak removal real haze removal and raindrop removal. The code and pre-trained models are available at https://github.com/joshyZhou/AST.    \n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-01-01\n\n\n\n#### 4. OA-CNNs: Omni-Adaptive Sparse CNNs for 3D Semantic Segmentation\n\n*From Search Query: adaptive sparse attention transformers*\n\n*Jiaya Jia, Zhuotao Tian, Hengshuang Zhao, Yukang Chen, Li Jiang, Xiaoyang Wu, Bohao Peng*\n\n**Abstract:** The booming of 3D recognition in the 2020s began with the introduction of point cloud transformers. They quickly overwhelmed sparse CNNs and became state-of-the-art models, especially in 3D semantic segmentation. However, sparse CNNs are still valuable networks, due to their efficiency treasure, and ease of application. In this work, we reexamine the design distinctions and test the limits of what a sparse CNN can achieve. We discover that the key credit to the performance difference is adaptivity. Specifically, we propose two key components, i.e., adaptive receptive fields (spatially) and adaptive relation, to bridge the gap. This exploration led to the creation of Omni-Adaptive 3D CNNs (OA-CNNs), a family of networks that integrates a lightweight module to greatly enhance the adaptivity of sparse CNNs at minimal computational cost. Without any self-attention modules, OA-CNNs favorably surpass point transformers in terms of accuracy in both indoor and outdoor scenes, with much less latency and memory cost. Notably, it achieves 76.1%, 78.9%, and 70.6% mIoU on ScanNet v2, nuScenes, and SemanticKITTI validation benchmarks respectively, while maintaining at most 5x better speed than transformer counterparts. This revelation highlights the potential of pure sparse CNNs to outperform transformer-related networks.\n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-03-21\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design by incorporating hierarchical compression, adaptive sparse attention, and hardware-aware optimizations, here are some relevant findings from the provided sources:\n\n## Hierarchical Compression Techniques in Transformer Architectures\n\n- The paper on V:N:M sparsity for efficient Transformer inference discusses a hierarchical approach to sparsity, where weight matrices are divided into blocks and sparse patterns are applied within these blocks. This method allows for higher sparsity levels and significant reductions in parameters and FLOPs, which can be seen as a form of hierarchical compression.\n\n## Integration of Sparse Attention with Compression Mechanisms\n\n- The V:N:M sparsity paper also explores how sparse attention can be integrated with compression mechanisms. It describes a method where weights are zero-padded, converted into sparse storage formats, and then processed using sparse tensor cores. This approach combines sparsity with efficient matrix multiplications, which is crucial for maintaining performance while compressing the model.\n\n## Memory-Efficient Implementations of Attention Mechanisms\n\n- The memristor-based transformer design discusses an efficient implementation of self-attention mechanisms using memristor crossbars. This approach reduces computational latency and memory usage by leveraging the properties of memristors for matrix multiplications, which is a key component of attention mechanisms.\n\n## Hardware-Aware Optimizations for Compressed Language Models\n\n- The V:N:M sparsity paper highlights hardware-aware optimizations by utilizing GPU architectures that support sparse tensor cores. This allows for significant speedups in inference time by efficiently handling sparse matrix multiplications, which is essential for compressed language models.\n\n## Performance Analysis of Compressed Attention Models\n\n- The performance analysis of V:N:M-sparse Transformers shows that these models can achieve comparable accuracy to dense models while reducing parameters and FLOPs significantly. For example, the Swin Transformer with V:N:M sparsity achieved a 59.1% reduction in parameters and a 60.4% reduction in FLOPs without compromising accuracy.\n\n## Additional Insights\n\n- The DiT-SR architecture, while focused on image super-resolution, introduces an isotropic design for transformer blocks that could be adapted for hierarchical compression. This design reallocates computational resources to high-resolution layers, which might be beneficial for language models by focusing compression on less critical layers.\n\n- The LFformer model, although primarily for wind power prediction, uses a hierarchical interpolation structure and encoder-decoder architecture that could provide insights into multi-scale processing and hierarchical compression in language models.\n\n### Investigation Plan Alignment\n\n1. **Hierarchical Compression System**:\n   - The V:N:M sparsity approach and the isotropic design in DiT-SR provide novel multi-tier compression methods that can be integrated with FastTTTLinear architecture.\n\n2. **Adaptive Sparse Attention**:\n   - The V:N:M sparsity method integrates sparse attention with compression, offering a content-based token selection mechanism and hardware-efficient sparse patterns.\n\n3. **Hardware-Aware Implementation**:\n   - The memristor-based transformer and V:N:M sparsity papers offer insights into IO-efficient memory access patterns, optimization of sparse operations, and efficient distributed processing strategies.\n\n4. **Performance Analysis and Scalability**:\n   - The performance analysis of V:N:M-sparse Transformers and the scalability studies in these papers can help in understanding the real-world implications and benchmarks for compressed attention models.\n\nBy combining these insights, the researchers can develop a novel autoregressive language model that integrates hierarchical compression with adaptive sparse attention, optimized for hardware efficiency and scalability.\n",
            "analysis": "Initial Analysis of HierarchicalCompressedFastTTTLinear Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Hierarchical Compression System:\n- Novel multi-tier compression approach for different types of information\n- Dynamic compression rates based on content importance\n- Integration with FastTTTLinear architecture\n- Comparison with existing compression techniques in language models\n\n2. Adaptive Sparse Attention:\n- Content-based token selection mechanism\n- Hardware-efficient sparse patterns\n- Integration with hierarchical compression\n- Comparison with existing sparse attention methods\n\n3. Hardware-Aware Implementation:\n- IO-efficient memory access patterns\n- Optimization of sparse operations\n- Efficient distributed processing strategies\n- Real-world performance implications\n\nInvestigation Plan:\n1. First Round:\n- Search for existing hierarchical compression techniques in language models\n- Examine state-of-the-art sparse attention mechanisms\n- Look for similar approaches combining compression with sparse attention\n\n2. Second Round:\n- Investigate hardware-aware optimizations for language models\n- Research memory management techniques in long-sequence processing\n- Study distributed training approaches for compressed models\n\n3. Third Round:\n- Analyze implementation details of similar architectures\n- Research performance metrics and benchmarks\n- Examine scalability studies of related approaches\n\nNovelty Check:\nThe proposal appears to be distinct from the found siblings:\n- ssmfasttttlinear focuses on state-space models\n- liquidttt emphasizes liquid state space mechanisms\n- gatedmamba uses gated linear attention with Mamba architecture\n\nThe key innovation lies in combining hierarchical compression with adaptive sparse attention, which is not directly addressed by previous designs."
        },
        {
            "ready": false,
            "query": "dynamic compression rate adaptation language models, memory management compressed transformers",
            "detail": "Search for papers discussing:\n1. Dynamic compression rate adaptation techniques in neural networks\n2. Memory management strategies for compressed transformers\n3. Test-time training approaches with compressed models\n4. Efficient state management in compressed architectures\n5. Adaptive compression mechanisms for attention models",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Dynamic compression rate adaptation techniques in neural networks\n2. Memory management strategies for compressed transformers\n3. Test-time training approaches with compressed models\n4. Efficient state management in compressed architectures\n5. Adaptive compression mechanisms for attention models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.95)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.95)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n#### 2. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.88)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.88)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.87)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.87)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Extensible Embedding: A Flexible Multipler For LLM's Context Length (Avg. Score: 0.84)\n\n*Ninglu Shao, Shitao Xiao, Zheng Liu, Peitian Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.\n\n**Abstract:** Large language models (LLMs) call for extension of context to handle many critical applications. However, the existing approaches are prone to expensive costs and inferior quality of context extension. In this work, we propose Extensible Embedding, which realizes high-quality extension of LLM's context with strong flexibility and cost-effectiveness. Extensible embedding stand as an enhancement of typical token embedding, which represents the information for an extensible scope of context instead of a single token. By leveraging such compact input units of higher information density, the LLM can access to a vast scope of context even with a small context window. Extensible embedding is systematically optimized in architecture and training method, which leads to multiple advantages. 1) High flexibility of context extension, which flexibly supports ad-hoc extension of diverse context lengths. 2) Strong sample efficiency of training, which enables the embedding model to be learned in a cost-effective way. 3) Superior compatibility with the existing LLMs, where the extensible embedding can be seamlessly introduced as a plug-in component. Comprehensive evaluations on long-context language modeling and understanding tasks verify extensible embedding as an effective, efficient, flexible, and compatible method to extend the LLM's context.\n\n##### *Relevant Chunk: No. 17/19 (Score: 0.84)*\n\n```\nHuiqiang Jiang, Qianhui Wu, Chin-Yew Lin, Yuqing Yang, and Lili Qiu. 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv preprint arXiv:2310.05736. Jesse Mu, Xiang Lisa Li, and Noah Goodman. 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. arXiv preprint arXiv:2309.00071. Jack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2019. Compressive transformers for long-range sequence modelling. arXiv preprint. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, J\u00e9r\u00e9my Rapin, et al. 2023. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b. 2023. Focused transformer: Contrastive training for context scaling. arXiv preprint arXiv:2307.03170. Yuhuai Wu, Markus Norman Rabe, DeLesley Hutchins, and Christian Szegedy. 2022. Memorizing transformers. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023a. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. 2023b. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453. Peng Xu, Wei Ping, Xianchao Wu, Lawrence McAfee, Chen Zhu, Zihan Liu, Sandeep Subramanian, Evelina Bakhturina, Mohammad Shoeybi, and Bryan Catanzaro. 2023. Retrieval meets long context large language models. CoRR, abs/2310.03025.\n```\n\n#### 5. PanGu-\u03c0: Enhancing Language Model Architectures via Nonlinearity Compensation (Avg. Score: 0.82)\n\n*Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yunhe Wang, Fangcheng Liu, Zhicheng Liu, Jianyuan Guo, Sinan Zeng, Yinchen Zhang, Qinghua Xu, Qun Liu, Jun Yao, Chao Xu, Dacheng Tao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 11  (*Influential: 0*)\n\n**TL;DR:** This work presents a new efficient model architecture for establishing modern language models, namely, PanGu-$\\pi$, and develops an LLM named YunShan for practical application, which can surpass other models with similar scales on benchmarks.\n\n**Abstract:** The recent trend of large language models (LLMs) is to increase the scale of both model size (\\aka the number of parameters) and dataset to achieve better generative ability, which is definitely proved by a lot of work such as the famous GPT and Llama. However, large models often involve massive computational costs, and practical applications cannot afford such high prices. However, the method of constructing a strong model architecture for LLMs is rarely discussed. We first analyze the state-of-the-art language model architectures and observe the feature collapse problem. Based on the theoretical analysis, we propose that the nonlinearity is also very important for language models, which is usually studied in convolutional neural networks for vision tasks. The series informed activation function is then introduced with tiny calculations that can be ignored, and an augmented shortcut is further used to enhance the model nonlinearity. We then demonstrate that the proposed approach is significantly effective for enhancing the model nonlinearity through carefully designed ablations; thus, we present a new efficient model architecture for establishing modern, namely, PanGu-$\\pi$. Experiments are then conducted using the same dataset and training strategy to compare PanGu-$\\pi$ with state-of-the-art LLMs. The results show that PanGu-$\\pi$-7B can achieve a comparable performance to that of benchmarks with about 10\\% inference speed-up, and PanGu-$\\pi$-1B can achieve state-of-the-art performance in terms of accuracy and efficiency. In addition, we have deployed PanGu-$\\pi$-7B in the high-value domains of finance and law, developing an LLM named YunShan for practical application. The results show that YunShan can surpass other models with similar scales on benchmarks.\n\n##### *Relevant Chunk: No. 21/62 (Score: 0.82)*\n\n```\n[36] J. W. Rae et al. Compressive transformers for long-range sequence modelling. arXiv preprint arXiv:1911.05507, 2019. [37] G. Xiao et al. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: dynamic compression rate adaptation language models, memory management compressed transformers\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Meta-Learning Online Adaptation of Language Models\n\n*From Search Query: dynamic compression rate adaptation language models*\n\n*Nathan J. Hu, E. Mitchell, Christopher D. Manning, Chelsea Finn*\n\n**TL;DR:** This work meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step.\n\n**Abstract:** Large language models encode impressively broad world knowledge in their parameters. However, the knowledge in static language models falls out of date, limiting the model's effective\"shelf life.\"While online fine-tuning can reduce this degradation, we find that naively fine-tuning on a stream of documents leads to a low level of information uptake. We hypothesize that online fine-tuning does not sufficiently attend to important information. That is, the gradient signal from important tokens representing factual information is drowned out by the gradient from inherently noisy tokens, suggesting that a dynamic, context-aware learning rate may be beneficial. We therefore propose learning which tokens to upweight. We meta-train a small, autoregressive model to reweight the language modeling loss for each token during online fine-tuning, with the objective of maximizing the out-of-date base question-answering model's ability to answer questions about a document after a single weighted gradient step. We call this approach Context-aware Meta-learned Loss Scaling (CaMeLS). Across three different distributions of documents, our experiments find that CaMeLS provides substantially improved information uptake on streams of thousands of documents compared with standard fine-tuning and baseline heuristics for reweighting token losses.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 26  (*Influential: 3*)\n\n#### 2. Compression of Generative Pre-trained Language Models via Quantization\n\n*From Search Query: dynamic compression rate adaptation language models*\n\n*Chaofan Tao, Lu Hou, Wei Zhang, Lifeng Shang, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong*\n\n**TL;DR:** This paper compress generative PLMs by quantization with comparable performance with the full-precision models, and proposes a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules.\n\n**Abstract:** The increasing size of generative Pre-trained Language Models (PLMs) have greatly increased the demand for model compression. Despite various methods to compress BERT or its variants, there are few attempts to compress generative PLMs, and the underlying difficulty remains unclear. In this paper, we compress generative PLMs by quantization. We find that previous quantization methods fail on generative tasks due to the homogeneous word embeddings caused by reduced capacity and the varied distribution of weights. Correspondingly, we propose a token-level contrastive distillation to learn distinguishable word embeddings, and a module-wise dynamic scaling to make quantizers adaptive to different modules. Empirical results on various tasks show that our proposed method outperforms the state-of-the-art compression methods on generative PLMs by a clear margin. With comparable performance with the full-precision models, we achieve 14.4x and 13.4x compression rate on GPT-2 and BART, respectively.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 88  (*Influential: 6*)\n\n#### 3. Sparse Low-rank Adaptation of Pre-trained Language Models\n\n*From Search Query: dynamic compression rate adaptation language models*\n\n*Ning Ding, Xingtai Lv, Qiaosen Wang, Yulin Chen, Bowen Zhou, Zhiyuan Liu, Maosong Sun*\n\n**TL;DR:** This work extends the methodology of LoRA to an innovative approach the authors call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process, and introduces a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization.\n\n**Abstract:** Fine-tuning pre-trained large language models in a parameter-efficient manner is widely studied for its effectiveness and efficiency. The popular method of low-rank adaptation (LoRA) offers a notable approach, hypothesizing that the adaptation process is intrinsically low-dimensional. Although LoRA has demonstrated commendable performance, it is implemented with a fixed and unalterable intrinsic rank that might not always be the ideal choice. Recognizing the need for more flexible adaptation, we extend the methodology of LoRA to an innovative approach we call sparse low-rank adaptation (SoRA) that enables dynamic adjustments to the intrinsic rank during the adaptation process. We achieve this through the incorporation of a gate unit optimized with proximal gradient method in the training stage, controlling the cardinality of rank under the sparsity of the gate. In the subsequent inference stage, we eliminate the parameter blocks corresponding to the zeroed-out ranks, to reduce each SoRA module back to a concise yet rank-optimal LoRA. Our approach strengthens the representation power of LoRA by initializing it with a higher rank, while efficiently taming a temporarily increased number of parameters via updating in a sparse way. We further introduce a sparsifying scheduler for SoRA, aiming to examine the impact of the number of non-zero parameters on the model's memorization and generalization. Our experimental results demonstrate that SoRA can outperform other baselines even with 70% retained parameters and 70% training time.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 21  (*Influential: 2*)\n\n#### 4. Compressed Context Memory For Online Language Model Interaction\n\n*From Search Query: memory management compressed transformers*\n\n*Jang-Hyun Kim, Junyoung Yeom, Sangdoo Yun, Hyun Oh Song*\n\n**TL;DR:** This paper proposes a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments.\n\n**Abstract:** This paper presents a context key/value compression method for Transformer language models in online scenarios, where the context continually expands. As the context lengthens, the attention process demands increasing memory and computations, which in turn reduces the throughput of the language model. To address this challenge, we propose a compressed context memory system that continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space of computing environments. Our compression process involves integrating a lightweight conditional LoRA into the language model's forward pass during inference, without the need for fine-tuning the model's entire set of weights. We achieve efficient training by modeling the recursive compression process as a single parallelized forward computation. Through evaluations on conversation, personalization, and multi-task learning, we demonstrate that our approach achieves the performance level of a full context model with $5\\times$ smaller context memory size. We further demonstrate the applicability of our approach in a streaming setting with an unlimited context length, outperforming the sliding window approach. Codes are available at https://github.com/snu-mllab/context-memory.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 5. GACT: Activation Compressed Training for Generic Network Architectures\n\n*From Search Query: memory management compressed transformers*\n\n*Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael W. Mahoney, Alvin Cheung*\n\n**TL;DR:** GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge is presented, and the convergence of GACT is proved by analyzing a linearized version of ACT's approximate gradient.\n\n**Abstract:** Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT's approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 20  (*Influential: 3*)\n\n#### 6. ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers\n\n*From Search Query: memory management compressed transformers*\n\n*Z. Yao, Reza Yazdani Aminabadi, Minjia Zhang, Xiaoxia Wu, Conglong Li, Yuxiong He*\n\n**TL;DR:** This work is able to show that ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference.\n\n**Abstract:** How to efficiently serve ever-larger trained natural language models in practice has become exceptionally challenging even for powerful cloud servers due to their prohibitive memory/computation requirements. In this work, we present an efficient and affordable post-training quantization approach to compress large Transformer-based models, termed as ZeroQuant. ZeroQuant is an end-to-end quantization and inference pipeline with three main components: (1) a fine-grained hardware-friendly quantization scheme for both weight and activations; (2) a novel affordable layer-by-layer knowledge distillation algorithm (LKD) even without the access to the original training data; (3) a highly-optimized quantization system backend support to remove the quantization/dequantization overhead. As such, we are able to show that: (1) ZeroQuant can reduce the precision for weights and activations to INT8 in a cost-free way for both BERT and GPT3-style models with minimal accuracy impact, which leads to up to 5.19x/4.16x speedup on those models compared to FP16 inference; (2) ZeroQuant plus LKD affordably quantize the weights in the fully-connected module to INT4 along with INT8 weights in the attention module and INT8 activations, resulting in 3x memory footprint reduction compared to the FP16 model; (3) ZeroQuant can be directly applied to two of the largest open-sourced language models, including GPT-J6B and GPT-NeoX20, for which our INT8 model achieves similar accuracy as the FP16 model but achieves up to 5.2x better efficiency.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 311  (*Influential: 37*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Maybe Deep Neural Networks are the Best Choice for Modeling Source Code\n\n*From Search Query: dynamic compression rate adaptation language models*\n\n*Rafael-Michael Karampatsis, Charles Sutton*\n\n**Abstract:** Statistical language modeling techniques have successfully been applied to\nsource code, yielding a variety of new software development tools, such as\ntools for code suggestion and improving readability. A major issue with these\ntechniques is that code introduces new vocabulary at a far higher rate than\nnatural language, as new identifier names proliferate. But traditional language\nmodels limit the vocabulary to a fixed set of common words. For code, this\nstrong assumption has been shown to have a significant negative effect on\npredictive performance. But the open vocabulary version of the neural network\nlanguage models for code have not been introduced in the literature. We present\na new open-vocabulary neural language model for code that is not limited to a\nfixed vocabulary of identifier names. We employ a segmentation into subword\nunits, subsequences of tokens chosen based on a compression criterion,\nfollowing previous work in machine translation. Our network achieves best in\nclass performance, outperforming even the state-of-the-art methods of\nHellendoorn and Devanbu that are designed specifically to model code.\nFurthermore, we present a simple method for dynamically adapting the model to a\nnew test project, resulting in increased performance. We showcase our\nmethodology on code corpora in three different languages of over a billion\ntokens each, hundreds of times larger than in previous work. To our knowledge,\nthis is the largest neural language model for code that has been reported.\n\n**Published:** 2019-03-13\n\n\n\n#### 2. Greedy-layer Pruning: Speeding up Transformer Models for Natural Language Processing\n\n*From Search Query: dynamic compression rate adaptation language models*\n\n*Antonio Rodriguez-Sanchez, Stefan Engl, Sebastian Stabinger, David Peer*\n\n**Abstract:** Fine-tuning transformer models after unsupervised pre-training reaches a very high performance on many different natural language processing tasks. Unfortunately, transformers suffer from long inference times which greatly increases costs in production. One possible solution is to use knowledge distillation, which solves this problem by transferring information from large teacher models to smaller student models. Knowledge distillation maintains high performance and reaches high compression rates, nevertheless, the size of the student model is fixed after pre-training and can not be changed individually for a given downstream task and use-case to reach a desired performance/speedup ratio. Another solution to reduce the size of models in a much more fine-grained and computationally cheaper fashion is to prune layers after the pre-training. The price to pay is that the performance of layer-wise pruning algorithms is not on par with state-of-the-art knowledge distillation methods. In this paper, Greedy-layer pruning is introduced to (1) outperform current state-of-the-art for layer-wise pruning, (2) close the performance gap when compared to knowledge distillation, while (3) providing a method to adapt the model size dynamically to reach a desired performance/speedup tradeoff without the need of additional pre-training phases. Our source code is available on https://github.com/deepopinion/greedy-layer-pruning.\n\n**Published:** 2021-05-31\n\n\n\n#### 3. SparseLLM: Towards Global Pruning for Pre-trained Language Models\n\n*From Search Query: memory management compressed transformers*\n\n*Liang Zhao, Kibaek Kim, Chen Ling, Yijiang Li, Guangji Bai*\n\n**Abstract:** The transformative impact of large language models (LLMs) like LLaMA and GPT on natural language processing is countered by their prohibitive computational demands. Pruning has emerged as a pivotal compression strategy, introducing sparsity to enhance both memory and computational efficiency. Yet, traditional global pruning is impractical for LLMs due to scalability issues, while local pruning, despite its efficiency, leads to suboptimal solutions. Addressing these challenges, we propose SparseLLM, a novel framework that redefines the global pruning process into manageable, coordinated subproblems, allowing for resource-efficient optimization with global optimality. SparseLLM's approach, which conceptualizes LLMs as a chain of modular functions and leverages auxiliary variables for problem decomposition, not only facilitates a pragmatic application on LLMs but also demonstrates significant performance improvements, particularly in high-sparsity regimes where it surpasses current state-of-the-art methods.\n\n**Published:** 2024-02-28\n\n\n\n#### 4. TTHRESH: Tensor Compression for Multidimensional Visual Data\n\n*From Search Query: memory management compressed transformers*\n\n*Rafael Ballester-Ripoll, Peter Lindstrom, Renato Pajarola*\n\n**Abstract:** Memory and network bandwidth are decisive bottlenecks when handling\nhigh-resolution multidimensional data sets in visualization applications, and\nthey increasingly demand suitable data compression strategies. We introduce a\nnovel lossy compression algorithm for $N$-dimensional data over regular grids.\nIt leverages the higher-order singular value decomposition (HOSVD), a\ngeneralization of the SVD to 3 and more dimensions, together with adaptive\nquantization, run-length and arithmetic coding to store the HOSVD transform\ncoefficients' relative positions as sorted by their absolute magnitude. Our\nscheme degrades the data particularly smoothly and outperforms other\nstate-of-the-art volume compressors at low-to-medium bit rates, as required in\ndata archiving and management for visualization purposes. Further advantages of\nthe proposed algorithm include extremely fine bit rate selection granularity,\nbounded resulting $l^2$ error, and the ability to manipulate data at very small\ncost in the compression domain, for example to reconstruct subsampled or\nfiltered-resampled versions of all (or selected parts) of the data set.\n\n**Published:** 2018-06-15\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on dynamic compression rate adaptation, memory management, test-time training, and efficient state management in compressed architectures, here are some key findings and references:\n\n## Dynamic Compression Rate Adaptation Techniques\n\n- The paper on **CASAK-V: Context-Aware adaptive Sparse Attention with Key-Value cache compression** introduces a dynamic approach to adapting compression strategies based on the current context length and a target sparsity ratio. This method ensures that the attention operation remains sparse even for long sequences, which is crucial for efficient long-context processing.\n\n## Memory Management Strategies for Compressed Transformers\n\n- **CASAK-V** also discusses adaptive KV-cache compression techniques that draw inspiration from dynamic caching strategies. This approach dynamically adjusts the compression based on token importance or attention patterns, which helps in reducing the memory footprint during inference.\n\n- The **SpaLLM** method simplifies the compressive adaptation workflow of large language models (LLMs) by unifying model compression and adaptation into a single process. This approach involves sketching pre-trained LLM weights into lookup tables and fine-tuning the values in these tables, which can help in efficient memory management.\n\n## Test-Time Training Approaches with Compressed Models\n\n- While the provided sources do not explicitly discuss test-time training with compressed models, the **CASAK-V** approach implies that dynamic compression mechanisms can be adapted during inference, which could be extended to test-time training scenarios. The dynamic nature of CASAK-V's compression allows for real-time adjustments, potentially making it suitable for test-time training environments.\n\n## Efficient State Management in Compressed Architectures\n\n- **CASAK-V** emphasizes the use of a custom CUDA kernel to efficiently handle sparse matrix multiplication and softmax operations. This optimization is crucial for maintaining efficiency in compressed architectures, especially when dealing with long sequences.\n\n- The **SpaLLM** method eliminates the need for two-tower architectures, which can simplify the state management and reduce the computational overhead associated with compressed models. This unified approach can lead to more efficient state management.\n\n## Adaptive Compression Mechanisms for Attention Models\n\n- **CASAK-V** introduces dynamic, context-aware mechanisms for both sparse attention and KV-cache compression. This approach ensures that the attention operation remains sparse and efficient even for very long sequences, adapting the compression rate based on the current context.\n\n- The **MoA: Mixture of Sparse Attention** paper, mentioned in the analysis note, automatically tailors distinct sparse attention configurations to different heads and layers, which is another example of adaptive compression mechanisms for attention models[Analysis Note].\n\nBy integrating these dynamic compression rate adaptation techniques, efficient memory management strategies, and adaptive compression mechanisms, researchers can significantly enhance the performance and efficiency of autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search reveals several promising directions for enhancing FastTTTLinear with hierarchical compression and adaptive sparse attention. Key findings include:\n- Various approaches to hierarchical compression in transformer architectures\n- Integration methods for sparse attention with compression mechanisms\n- Hardware-aware optimization techniques\n- Performance implications of compressed attention models\n\nHowever, more specific information is needed about:\n- Dynamic compression rate adaptation\n- Memory management strategies\n- Integration with test-time training capabilities\n\n2. Useful References with Excerpts:\n\nFrom Dynamic Context Pruning paper:\n\"...dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference...\"\n\nFrom Leave No Context Behind:\n\"...incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block...\"\n\nFrom B'MOJO:\n\"...seamlessly combine eidetic and fading memory within an elementary composable module...\"\n\nFrom MoA paper:\n\"...automatically tailors distinct sparse attention configurations to different heads and layers...\"\n\n3. Key Insights:\n\na) Compression Strategies:\n- Dynamic pruning shows promise for adaptive compression\n- Multi-tier memory systems can balance efficiency and expressiveness\n- Hierarchical structures can be integrated with attention mechanisms\n\nb) Attention Mechanisms:\n- Sparse patterns can be automatically configured per layer\n- Local and global attention can be combined efficiently\n- Memory compression can be integrated with attention\n\nc) Hardware Considerations:\n- IO-aware designs are crucial for performance\n- Memory access patterns significantly impact efficiency\n- Distributed processing requires careful consideration\n\n4. Future Search Plan:\n\nRound 2 Search Focus:\n- Dynamic compression rate adaptation mechanisms\n- Memory management in compressed transformers\n- Test-time training with compressed models\n\nRound 3 Search Focus:\n- Hardware optimization techniques\n- Implementation details of similar systems\n- Performance benchmarks and metrics\n\n5. References:\n\n[1] Anagnostidis, S., et al. (2023). Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. Neural Information Processing Systems.\n\n[2] Munkhdalai, T., Faruqui, M., & Gopal, S. (2024). Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention. arXiv.org.\n\n[3] Zancato, L., et al. (2024). B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory.\n\n[4] Liu, H., & Abbeel, P. (2023). Blockwise Parallel Transformer for Large Context Models.\n\n[5] Fu, T., et al. (2024). MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression. arXiv.org."
        },
        {
            "ready": false,
            "query": "test-time training compressed transformers, hierarchical attention compression",
            "detail": "Search for papers discussing:\n1. Test-time training approaches for compressed language models\n2. Integration of hierarchical structures in transformer architectures\n3. Performance benchmarks and metrics for compressed attention models\n4. Methods for maintaining model quality with compression\n5. Dynamic adaptation techniques during inference",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Test-time training approaches for compressed language models\n2. Integration of hierarchical structures in transformer architectures\n3. Performance benchmarks and metrics for compressed attention models\n4. Methods for maintaining model quality with compression\n5. Dynamic adaptation techniques during inference\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Training LLMs over Neurally Compressed Text (Avg. Score: 0.80)\n\n*Brian Lester, Jaehoon Lee, A. Alemi, Jeffrey Pennington, Adam Roberts, Jascha Narain Sohl-Dickstein, Noah Constant*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length, is proposed, demonstrating effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks.\n\n**Abstract:** In this paper, we explore the idea of training large language models (LLMs) over highly compressed text. While standard subword tokenizers compress text by a small factor, neural text compressors can achieve much higher rates of compression. If it were possible to train LLMs directly over neurally compressed text, this would confer advantages in training and serving efficiency, as well as easier handling of long text spans. The main obstacle to this goal is that strong compression tends to produce opaque outputs that are not well-suited for learning. In particular, we find that text na\\\"ively compressed via Arithmetic Coding is not readily learnable by LLMs. To overcome this, we propose Equal-Info Windows, a novel compression technique whereby text is segmented into blocks that each compress to the same bit length. Using this method, we demonstrate effective learning over neurally compressed text that improves with scale, and outperforms byte-level baselines by a wide margin on perplexity and inference speed benchmarks. While our method delivers worse perplexity than subword tokenizers for models trained with the same parameter count, it has the benefit of shorter sequence lengths. Shorter sequence lengths require fewer autoregressive generation steps, and reduce latency. Finally, we provide extensive analysis of the properties that contribute to learnability, and offer concrete suggestions for how to further improve the performance of high-compression tokenizers.\n\n##### *Relevant Chunk: No. 3/68 (Score: 0.80)*\n\n```\nWe discuss three advantages in detail below. Efficiency The most straightforward advantage is efficiency. By compressing the same text into a shorter token sequence, the model can process more text for the same computational cost. In particular, a model trained over $C \\times$ compressed text will see $C \\times$ more text during training compared to a model trained over raw text, given an equal compute budget. Increasing the amount of data seen in pretraining is often an effective means of improving performance (Kaplan et al., 2020 Hoffmann et al. 2022). Processing text more efficiently also confers benefits at inference time, reducing the serving cost for handling a request of a given prompt and continuation length. In addition to reducing the raw compute needed for inference, compression can also improve inference latency, since generating better-compressed output requires fewer sequential autoregressive steps. Longer Context A second advantage is that working with compressed text allows modeling longer contextual dependencies. In vanilla transformer-based models, computation for the self-attention layer scales quadratically with the sequence length, $O\\left(n^{2} d\\right)$. This has limited the sequence lengths used by such models\nin practical settings to $\\sim 10 \\mathrm{k}$ tokens $5^{5}$ If, via compression, each token represents (on average) $C$ bytes of raw text, then the resulting LLM can model dependencies across $C \\times$ longer distances compared to a raw text model operating over the same token sequence length. While the benefits of modeling longer context (beyond $\\sim 1,000$ bytes) are modest when viewed merely as perplexity gains (Press et al. 2022), the ability to condition on long context is critical for many applications, such as retrieving content from a document, or answering a coding question provided documentation. Distribution of Compute A third potential advantage of training over compressed text is that information will be spread more uniformly across the sequence. By the nature of compression, a text span that is relatively predictable (e.g., a boilerplate notice) will be more compressible than a span with high perplexity (e.g., a unique product serial number). When an LLM is trained over well-compressed text, each token will represent roughly an equal amount of information. Since the LLM allocates equal compute to each token, this amounts to allocating more compute for \"harder\" text spans. This adaptivity is similar in spirit to \"Adaptive Computation Time\" (ACT) Graves, 2017), which learns to allocate additional compute at some sequence positions in an end-to-end manner, but with the advantage that in our case the computation remains \"dense\"-identical operations are applied at each position ${ }^{6}$\n\n### 2.2 Challenges of Training over Compressed Text\n\nLearnability It is not at all obvious what types of compression are \"transparent\" enough to be learnable through a standard LLM training process.\n```\n\n#### 2. Adapting Language Models to Compress Contexts (Avg. Score: 0.52)\n\n*Alexis Chevalier, Alexander Wettig, Anirudh Ajith, Danqi Chen*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 75  (*Influential: 11*)\n\n**TL;DR:** AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts and the benefits of pre-computing summary vectors for large corpora are explored.\n\n**Abstract:** Transformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents. We propose to adapt pre-trained LMs into AutoCompressors. These models are capable of compressing long contexts into compact summary vectors, which are then accessible to the model as soft prompts. Summary vectors are trained with an unsupervised objective, whereby long documents are processed in segments and summary vectors from all previous segments are used in language modeling. We fine-tune OPT models on sequences of up to 30,720 tokens and show that AutoCompressors can utilize long contexts to improve perplexity. We evaluate AutoCompressors on in-context learning by compressing task demonstrations. We find that summary vectors are good substitutes for plain-text demonstrations, increasing accuracy while reducing inference cost. Finally, we explore the benefits of pre-computing summary vectors for large corpora by applying summary vectors to retrieval-augmented language modeling. Overall, AutoCompressors emerge as a simple and inexpensive solution for extending the context window of LMs while speeding up inference over long contexts.\n\n##### *Relevant Chunk: No. 1/40 (Score: 0.52)*\n\n```\n# Adapting Language Models to Compress Contexts \n\nAlexis Chevalier* Alexander Wettig* Anirudh Ajith Danqi Chen<br>Department of Computer Science \\& Princeton Language and Intelligence<br>Princeton University<br>\\{achevalier, anirudh.ajith\\}@princeton.edu<br>\\{awettig, danqic\\}@cs.princeton.edu\n\n\n#### Abstract\n\nTransformer-based language models (LMs) are powerful and widely-applicable tools, but their usefulness is constrained by a finite context window and the expensive computational cost of processing long text documents.\n```\n\n#### 3. CORM: Cache Optimization with Recent Message for Large Language Model Inference (Avg. Score: 0.47)\n\n*Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi*\n\n**Published in:**  (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper presents CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning, and shows that CORM reduces the inference memory usage of KV cache by up to 70\\% with negligible performance degradation across six tasks in LongBench.\n\n**Abstract:** Large Language Models (LLMs), despite their remarkable performance across a wide range of tasks, necessitate substantial GPU memory and consume significant computational resources. Beyond the memory taken up by model weights, the memory used by the KV cache rises linearly with sequence length, becoming a primary bottleneck for inference. In this paper, we introduce an innovative method for optimizing the KV cache, which considerably minimizes its memory footprint. Upon thorough investigation, we discover that in most Transformer models, (i) there is a striking similarity between adjacent tokens' query vectors, and (ii) the attention calculation of the current query can rely exclusively on the attention information of a small fraction of preceding queries. Based on these observations, we present CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning. Our validation shows that CORM reduces the inference memory usage of KV cache by up to 70\\% with negligible performance degradation across six tasks in LongBench. Furthermore, we demonstrate that CORM is compatible with GQA for further compression rate.\n\n##### *Relevant Chunk: No. 7/18 (Score: 0.47)*\n\n```\narXiv preprint arXiv:2402.06262, 2024. [15] Paul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? Advances in neural information processing systems, 32, 2019. [16] Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: A bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508, 2023. [17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [18] Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aurelien Lucchi, and Thomas Hofmann. Dynamic context pruning for efficient and interpretable autoregressive transformers. Advances in Neural Information Processing Systems, 36, 2024. [19] Piotr Nawrot, Adrian \u0141a\u0144cucki, Marcin Chochowski, David Tarjan, and Edoardo M Ponti. Dynamic memory compression: Retrofitting llms for accelerated inference. arXiv preprint arXiv:2403.09636, 2024. [20] Chi Han, Qifan Wang, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Simple on-the-fly length generalization for large language models. arXiv preprint arXiv:2308.16137, 2023 . [21] Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, Song Han, and Maosong Sun. Infllm: Unveiling the intrinsic capacity of llms for understanding extremely long sequences with training-free memory.\n```\n\n#### 4. TRAMS: Training-free Memory Selection for Long-range Language Modeling (Avg. Score: 0.46)\n\n*Haofei Yu, Cunxiang Wang, Yue Zhang, Wei Bi*\n\n**Published in:** Conference on Empirical Methods in Natural Language Processing (2023)\t**Cited by** 2  (*Influential: 0*)\n\n**TL;DR:** A plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric, and the results indicate an improvement without having additional training or adding additional parameters.\n\n**Abstract:** The Transformer architecture is crucial for numerous AI models, but it still faces challenges in long-range language modeling. Though several specific transformer architectures have been designed to tackle issues of long-range dependencies, existing methods like Transformer-XL are plagued by a high percentage of ineffective memories. In this study, we present a plug-and-play strategy, known as TRAining-free Memory Selection (TRAMS), that selects tokens participating in attention calculation based on one simple metric. This strategy allows us to keep tokens that are likely to have a high attention score with the current queries and ignore the other ones. We have tested our approach on the word-level benchmark (WikiText-103) and the character-level benchmark (enwik8), and the results indicate an improvement without having additional training or adding additional parameters.\n\n##### *Relevant Chunk: No. 9/16 (Score: 0.46)*\n\n```\nMatt Mahoney. 2011. Large text compression benchmark. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer sentinel mixture models. In International Conference on Learning Representations. Hao Peng, Jungo Kasai, Nikolaos Pappas, Dani Yogatama, Zhaofeng Wu, Lingpeng Kong, Roy Schwartz, and Noah A Smith. 2022a. Abc: Attention with bounded-memory control. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7469-7483. Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah Smith, and Lingpeng Kong. 2022b. Random feature attention. In International Conference on Learning Representations. Micha\u0142 Pietruszka, \u0141ukasz Borchmann, and \u0141ukasz Garncarek. 2022. Sparsifying transformer models with trainable representation pooling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages $8616-8633$. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485-5551.\n```\n\n#### 5. Hierarchical Transformers Are More Efficient Language Models (Avg. Score: 0.43)\n\n*Piotr Nawrot, Szymon Tworkowski, Micha\u0142 Tyrolski, Lukasz Kaiser, Yuhuai Wu, Christian Szegedy, H. Michalewski*\n\n**Published in:** NAACL-HLT (2021)\t**Cited by** 40  (*Influential: 4*)\n\n**TL;DR:** Hourglass is created - a hierarchical Transformer language model that improves language modeling efficiency on the widely studied enwik8 benchmark and sets new state-of-the-art for Transformer models on the ImageNet32 generation task.\n\n**Abstract:** Transformer models yield impressive results on many NLP and sequence modeling tasks. Remarkably, Transformers can handle long sequences which allows them to produce long coherent outputs: full paragraphs produced by GPT-3 or well-structured images produced by DALL-E. These large language models are impressive but also very inefficient and costly, which limits their applications and accessibility. We postulate that having an explicit hierarchical architecture is the key to Transformers that efficiently handle long sequences. To verify this claim, we first study different ways to downsample and upsample activations in Transformers so as to make them hierarchical. We use the best performing upsampling and downsampling layers to create Hourglass - a hierarchical Transformer language model. Hourglass improves upon the Transformer baseline given the same amount of computation and can yield the same results as Transformers more efficiently. In particular, Hourglass sets new state-of-the-art for Transformer models on the ImageNet32 generation task and improves language modeling efficiency on the widely studied enwik8 benchmark.\n\n##### *Relevant Chunk: No. 15/25 (Score: 0.43)*\n\n```\nMatt Mahoney. 2011. Large text compression benchmark. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, \u0141ukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. 2018. Image transformer. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. Hongyu Ren, Hanjun Dai, Zihang Dai, Mengjiao Yang, Jure Leskovec, Dale Schuurmans, and Bo Dai. 2021. Combiner: Full attention transformer with sparse computation cost.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: test-time training compressed transformers, hierarchical attention compression\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Test-Time Training Can Close the Natural Distribution Shift Performance Gap in Deep Learning Based Compressed Sensing\n\n*From Search Query: test-time training compressed transformers*\n\n*Mohammad Zalbagi Darestani, Jiayu Liu, Reinhard Heckel*\n\n**TL;DR:** This work proposes a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference that essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Abstract:** Deep learning based image reconstruction methods outperform traditional methods. However, neural networks suffer from a performance drop when applied to images from a different distribution than the training images. For example, a model trained for reconstructing knees in accelerated magnetic resonance imaging (MRI) does not reconstruct brains well, even though the same network trained on brains reconstructs brains perfectly well. Thus there is a distribution shift performance gap for a given neural network, defined as the difference in performance when training on a distribution $P$ and training on another distribution $Q$, and evaluating both models on $Q$. In this work, we propose a domain adaptation method for deep learning based compressive sensing that relies on self-supervision during training paired with test-time training at inference. We show that for four natural distribution shifts, this method essentially closes the distribution shift performance gap for state-of-the-art architectures for accelerated MRI.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 25  (*Influential: 1*)\n\n#### 2. Test-Time Training on Nearest Neighbors for Large Language Models\n\n*From Search Query: test-time training compressed transformers*\n\n*Moritz Hardt, Yu Sun*\n\n**TL;DR:** This work establishes a first baseline of test-time training for language modeling by building a large-scale distributed index based on text embeddings of the Pile dataset and fine-tunes the model on retrieved data at test time.\n\n**Abstract:** Many recent efforts augment language models with retrieval, by adding retrieved data to the input context. For this approach to succeed, the retrieved data must be added at both training and test time. Moreover, as input length grows linearly with the size of retrieved data, cost in computation and memory grows quadratically for modern Transformers. To avoid these complications, we simply fine-tune the model on retrieved data at test time, using its standard training setup. We build a large-scale distributed index based on text embeddings of the Pile dataset. For each test input, our system retrieves its neighbors and fine-tunes the model on their text. Surprisingly, retrieving and training on as few as 20 neighbors, each for only one gradient iteration, drastically improves performance across more than 20 language modeling tasks in the Pile. For example, test-time training with nearest neighbors significantly narrows the performance gap between a small GPT-2 and a GPT-Neo model more than 10 times larger. Sufficient index quality and size, however, are necessary. Our work establishes a first baseline of test-time training for language modeling.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 3. GACT: Activation Compressed Training for Generic Network Architectures\n\n*From Search Query: test-time training compressed transformers*\n\n*Xiaoxuan Liu, Lianmin Zheng, Dequan Wang, Yukuo Cen, Weize Chen, Xu Han, Jianfei Chen, Zhiyuan Liu, Jie Tang, Joey Gonzalez, Michael W. Mahoney, Alvin Cheung*\n\n**TL;DR:** GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge is presented, and the convergence of GACT is proved by analyzing a linearized version of ACT's approximate gradient.\n\n**Abstract:** Training large neural network (NN) models requires extensive memory resources, and Activation Compressed Training (ACT) is a promising approach to reduce training memory footprint. This paper presents GACT, an ACT framework to support a broad range of machine learning tasks for generic NN architectures with limited domain knowledge. By analyzing a linearized version of ACT's approximate gradient, we prove the convergence of GACT without prior knowledge on operator type or model architecture. To make training stable, we propose an algorithm that decides the compression ratio for each tensor by estimating its impact on the gradient at run time. We implement GACT as a PyTorch library that readily applies to any NN architecture. GACT reduces the activation memory for convolutional NNs, transformers, and graph NNs by up to 8.1x, enabling training with a 4.2x to 24.7x larger batch size, with negligible accuracy loss. We implement GACT as a PyTorch library at https://github.com/LiuXiaoxuanPKU/GACT-ICML.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 20  (*Influential: 3*)\n\n#### 4. Weighted Mutual Learning with Diversity-Driven Model Compression\n\n*From Search Query: hierarchical attention compression*\n\n*Miao Zhang, Li Wang, David Campos, Wei Huang, Chenjuan Guo, B. Yang*\n\n**TL;DR:** A framework called Weighted Mutual Learning with Diversity-Driven Model Compression (WML), which outperforms existing online distillation methods on a variety of deep neural networks and produces a series of students with different model sizes in a single run, which also achieves competitive results compared with existing channel pruning methods.\n\n**Abstract:** Online distillation attracts attention from the community as it simplifies the traditional two-stage knowledge distillation process into a single stage. Online distillation collaboratively trains a group of peer models, which are treated as students, and all students gain extra knowledge from each other. However, memory consumption and diversity among students are two key challenges to the scalability and quality of online distillation. To address the two challenges, this paper presents a framework called Weighted Mutual Learning with Diversity-Driven Model Compression ( WML ) for online distillation. First, at the base of a hierarchical structure where students share different parts, we leverage the structured network pruning to generate diversified students with different models sizes, thus also helping reduce the memory requirements. Second, rather than taking the average of students, this paper, for the first time, leverages a bi-level formulation to estimate the relative importance of students with a close-form, to further boost the effectiveness of the distillation from each other. Extensive experiments show the generalization of the proposed framework, which outperforms existing online distillation methods on a variety of deep neural networks. More interesting, as a byproduct, WML produces a series of students with different model sizes in a single run, which also achieves competitive results compared with existing channel pruning methods.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 5. Hierarchical Attention Networks for Document Classification\n\n*From Search Query: hierarchical attention compression*\n\n*Zichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alex Smola, E. Hovy*\n\n**TL;DR:** Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin.\n\n**Abstract:** We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences.\n\n**Venue:** North American Chapter of the Association for Computational Linguistics\n\n**Year:** 2016\n\n**Citations:** 4297  (*Influential: 554*)\n\n#### 6. FasterViT: Fast Vision Transformers with Hierarchical Attention\n\n*From Search Query: hierarchical attention compression*\n\n*Ali Hatamizadeh, Greg Heinrich, Hongxu Yin, Andrew Tao, J. \u00c1lvarez, J. Kautz, Pavlo Molchanov*\n\n**TL;DR:** The newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs and can be used as a plug-and-play module for existing networks and enhance them.\n\n**Abstract:** We design a new family of hybrid CNN-ViT neural networks, named FasterViT, with a focus on high image throughput for computer vision (CV) applications. FasterViT combines the benefits of fast local representation learning in CNNs and global modeling properties in ViT. Our newly introduced Hierarchical Attention (HAT) approach decomposes global self-attention with quadratic complexity into a multi-level attention with reduced computational costs. We benefit from efficient window-based self-attention. Each window has access to dedicated carrier tokens that participate in local and global representation learning. At a high level, global self-attentions enable the efficient cross-window communication at lower costs. FasterViT achieves a SOTA Pareto-front in terms of accuracy and image throughput. We have extensively validated its effectiveness on various CV tasks including classification, object detection and segmentation. We also show that HAT can be used as a plug-and-play module for existing networks and enhance them. We further demonstrate significantly faster and more accurate performance than competitive counterparts for images with high resolution. Code is available at https://github.com/NVlabs/FasterViT.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 37  (*Influential: 3*)\n\n### 4 related papers from Papers with Code\n\n#### 1. High Fidelity Neural Audio Compression\n\n*From Search Query: test-time training compressed transformers*\n\n*Yossi Adi, Gabriel Synnaeve, Jade Copet, Alexandre D\u00e9fossez*\n\n**Abstract:** We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.\n\n**Published:** 2022-10-24\n\n\n\n#### 2. Shield: Fast, Practical Defense and Vaccination for Deep Learning using JPEG Compression\n\n*From Search Query: test-time training compressed transformers*\n\n*Michael E. Kounavis, Shang-Tse Chen, Siwei Li, Li Chen, Fred Hohman, Madhuri Shanbhogue, Nilaksh Das, Duen Horng Chau*\n\n**Abstract:** The rapidly growing body of research in adversarial machine learning has\ndemonstrated that deep neural networks (DNNs) are highly vulnerable to\nadversarially generated images. This underscores the urgent need for practical\ndefense that can be readily deployed to combat attacks in real-time. Observing\nthat many attack strategies aim to perturb image pixels in ways that are\nvisually imperceptible, we place JPEG compression at the core of our proposed\nShield defense framework, utilizing its capability to effectively \"compress\naway\" such pixel manipulation. To immunize a DNN model from artifacts\nintroduced by compression, Shield \"vaccinates\" a model by re-training it with\ncompressed images, where different compression levels are applied to generate\nmultiple vaccinated models that are ultimately used together in an ensemble\ndefense. On top of that, Shield adds an additional layer of protection by\nemploying randomization at test time that compresses different regions of an\nimage using random compression levels, making it harder for an adversary to\nestimate the transformation performed. This novel combination of vaccination,\nensembling, and randomization makes Shield a fortified multi-pronged\nprotection. We conducted extensive, large-scale experiments using the ImageNet\ndataset, and show that our approaches eliminate up to 94% of black-box attacks\nand 98% of gray-box attacks delivered by the recent, strongest attacks, such as\nCarlini-Wagner's L2 and DeepFool. Our approaches are fast and work without\nrequiring knowledge about the model.\n\n**Published:** 2018-02-19\n\n\n\n#### 3. F3SNet: A Four-Step Strategy for QIM Steganalysis of Compressed Speech Based on Hierarchical Attention Network\n\n*From Search Query: hierarchical attention compression*\n\n*Liusheng Huang, Wei Yang, Chuanpeng Guo*\n\n**Abstract:** Traditional machine learning-based steganalysis methods on compressed speech have achieved great success in the field of communication security. However, previous studies lacked mathematical description and modeling of the correlation between codewords, and there is still room for improvement in steganalysis for small-sized and low embedding rates sample. To deal with the challenge, We use Bayesian networks to measure different types of correlations between codewords in linear prediction code and present F3SNet -- a four-step strategy: Embedding, Encoding, Attention and Classification for quantizaition index modulation steganalysis of compressed speech based on Hierarchical Attention Network. Among them, Embedding converts codewords into high-density numerical vectors, Encoding uses the memory characteristics of LSTM to retain more information by distributing it among all its vectors and Attention further determines which vectors have a greater impact on the final classification result. To evaluate the performance of F3SNet, we make comprehensive comparison of F3SNet with existing steganography methods. Experimental results show that F3SNet surpasses the state-of-the-art methods, particularly for small-sized and low embedding rate samples.\n\n**Published:** 2021-01-13\n\n\n\n#### 4. Residual Non-local Attention Networks for Image Restoration\n\n*From Search Query: hierarchical attention compression*\n\n*Yulun Zhang, Bineng Zhong, Yun Fu, Kunpeng Li, Kai Li*\n\n**Abstract:** In this paper, we propose a residual non-local attention network for\nhigh-quality image restoration. Without considering the uneven distribution of\ninformation in the corrupted images, previous methods are restricted by local\nconvolutional operation and equal treatment of spatial- and channel-wise\nfeatures. To address this issue, we design local and non-local attention blocks\nto extract features that capture the long-range dependencies between pixels and\npay more attention to the challenging parts. Specifically, we design trunk\nbranch and (non-)local mask branch in each (non-)local attention block. The\ntrunk branch is used to extract hierarchical features. Local and non-local mask\nbranches aim to adaptively rescale these hierarchical features with mixed\nattentions. The local mask branch concentrates on more local structures with\nconvolutional operations, while non-local attention considers more about\nlong-range dependencies in the whole feature map. Furthermore, we propose\nresidual local and non-local attention learning to train the very deep network,\nwhich further enhance the representation ability of the network. Our proposed\nmethod can be generalized for various image restoration applications, such as\nimage denoising, demosaicing, compression artifacts reduction, and\nsuper-resolution. Experiments demonstrate that our method obtains comparable or\nbetter results compared with recently leading methods quantitatively and\nvisually.\n\n**Conference:** residual-non-local-attention-networks-for\n\n**Published:** 2019-03-24\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using the specified keywords and details, here are some key findings and relevant information from the available sources and additional insights:\n\n## Test-Time Training Approaches for Compressed Language Models\n\n- The concept of test-time training (TTT) is discussed in the context of adapting pre-trained models to out-of-distribution test data. TTT involves fine-tuning the model on the test sample using self-supervised or unsupervised objectives. This approach can be applied to compressed language models to enhance their generalization capability on unseen data.\n\n## Integration of Hierarchical Structures in Transformer Architectures\n\n- Hierarchical attention mechanisms can be integrated into transformer architectures to improve efficiency and performance. For instance, hierarchical compression can be achieved by dynamically pruning contextual information or continually compressing attention key/value pairs, which can be adapted based on context importance[Analysis Note: Dynamic Context Pruning, Compressed Context Memory].\n\n## Performance Benchmarks and Metrics for Compressed Attention Models\n\n- Performance benchmarks for compressed attention models often include metrics such as memory usage, computational efficiency, and accuracy. For example, the block selective reprogramming (BSR) approach for vision transformers demonstrates significant reductions in training memory and computational cost while maintaining similar performance levels.\n\n## Methods for Maintaining Model Quality with Compression\n\n- To maintain model quality with compression, techniques such as fine-grained quantization, layer-by-layer knowledge distillation, and tensor-specific compression ratios are effective. These methods help in optimizing performance while reducing memory and computational requirements. For instance, ZeroQuant and GACT provide hardware-friendly quantization schemes and run-time estimation of compression impact, respectively[Analysis Note: ZeroQuant, GACT].\n\n## Dynamic Adaptation Techniques During Inference\n\n- Dynamic adaptation during inference can be achieved through methods like dynamic context pruning and continuous compression of attention key/value pairs. These techniques adapt the compression ratio based on the context importance and the impact on the gradient at run time, ensuring efficient and accurate inference[Analysis Note: Dynamic Context Pruning, Compressed Context Memory].\n\n### Additional Insights\n\n- **Memory-Efficient Implementations**: Techniques like block selective reprogramming (BSR) for vision transformers can be adapted for language models. BSR involves fine-tuning only a fraction of the pre-trained weight tensors, which reduces activation memory and computational cost.\n\n- **Hardware-Aware Optimization**: Leveraging hardware-aware optimization strategies, such as those using memristor-based in-memory computing, can significantly improve the efficiency of transformer models during both training and inference.\n\n- **Integration with Sparse Attention**: Methods like Loki, which use low-rank keys for efficient sparse attention, can be integrated with compression techniques to further optimize the performance of autoregressive language models[Analysis Note: References].\n\n### Future Search Plan\n\nTo further enhance the autoregressive language model design, additional research should focus on:\n\n1. **Test-Time Training with Compressed Architectures**: Investigate how TTT can be specifically applied to compressed language models to improve their adaptability to new data.\n2. **Integration of Hierarchical Structures with FastTTTLinear**: Explore how hierarchical attention mechanisms and dynamic compression techniques can be integrated into the FastTTTLinear architecture.\n3. **Performance Benchmarks for Similar Approaches**: Compare the performance of different compression and adaptation techniques using standardized benchmarks to identify the most effective methods.\n\nBy combining these approaches, researchers can develop more efficient, scalable, and adaptive autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several promising approaches for enhancing FastTTTLinear with hierarchical compression and adaptive sparse attention. Key findings include:\n- Dynamic compression techniques that adapt based on context importance\n- Memory-efficient implementations of attention mechanisms\n- Hardware-aware optimization strategies\n- Integration methods for sparse attention with compression\n\n2. Useful References with Excerpts:\n\nFrom Dynamic Context Pruning:\n\"...dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference...\"\n\nFrom Compressed Context Memory:\n\"...continually compresses the accumulating attention key/value pairs into a compact memory space, facilitating language model inference in a limited memory space...\"\n\nFrom GACT:\n\"...decides the compression ratio for each tensor by estimating its impact on the gradient at run time...\"\n\nFrom ZeroQuant:\n\"...fine-grained hardware-friendly quantization scheme for both weight and activations...\"\n\n3. Key Insights:\n\na) Compression Strategies:\n- Dynamic pruning shows promise for adaptive compression\n- Hierarchical compression can be integrated with attention mechanisms\n- Run-time estimation of compression impact is feasible\n\nb) Memory Management:\n- Continuous compression of key/value pairs reduces memory footprint\n- Hardware-friendly quantization schemes improve efficiency\n- Gradient impact estimation guides compression decisions\n\nc) Implementation Considerations:\n- Fine-grained quantization improves hardware efficiency\n- Layer-by-layer knowledge distillation can maintain accuracy\n- Tensor-specific compression ratios optimize performance\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Test-time training with compressed architectures\n2. Integration of hierarchical structures with FastTTTLinear\n3. Performance benchmarks for similar approaches\n\n5. References:\n\n[1] Anagnostidis, S., et al. (2023). Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. Neural Information Processing Systems.\n\n[2] Kim, J.H., et al. (2023). Compressed Context Memory For Online Language Model Interaction. International Conference on Learning Representations.\n\n[3] Liu, X., et al. (2022). GACT: Activation Compressed Training for Generic Network Architectures. International Conference on Machine Learning.\n\n[4] Yao, Z., et al. (2022). ZeroQuant: Efficient and Affordable Post-Training Quantization for Large-Scale Transformers. Neural Information Processing Systems.\n\n[5] Singhania, P., et al. (2024). Loki: Low-Rank Keys for Efficient Sparse Attention. arXiv.org."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.868038,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.000359,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "FastTTTLinear",
    "proposal": "\n# HierarchicalCompressedFastTTTLinear: Enhancing FastTTTLinear with Hierarchical Compression and Adaptive Sparse Attention\n\n## 1. Motivation\n\nCurrent language models face significant challenges in processing long sequences efficiently due to:\n- High memory requirements for storing key-value pairs\n- Computational overhead from attention calculations\n- Inefficient handling of varying information importance\n- Limited hardware utilization\n\nHierarchicalCompressedFastTTTLinear addresses these challenges by introducing a hierarchical compression mechanism integrated with adaptive sparse attention, enabling efficient processing of long sequences while maintaining model expressiveness.\n\n## 2. Problem Analysis\n\n### 2.1 Memory Efficiency\nThe standard FastTTTLinear requires significant memory for storing key-value states, limiting its practical application for long sequences.\n\n### 2.2 Computational Overhead\nProcessing long sequences involves substantial computational costs, particularly in attention calculations and memory access.\n\n### 2.3 Information Management\nCurrent approaches don't effectively differentiate between important and less relevant information, leading to inefficient resource utilization.\n\n## 3. Core Idea and Philosophy\n\nHierarchicalCompressedFastTTTLinear introduces three key innovations:\n\n1. **Hierarchical Compression System**:\n   - Multi-tier compression for different types of information\n   - Dynamic compression rates based on content importance\n   - Efficient memory management through structured state representation\n\n2. **Adaptive Sparse Attention**:\n   - Content-based token selection\n   - Hardware-efficient sparse patterns\n   - Dynamic attention routing\n\n3. **Hardware-Aware Implementation**:\n   - IO-efficient memory access patterns\n   - Optimized sparse operations\n   - Efficient distributed processing\n\n## 4. Design Plan\n\n### 4.1 Architecture Overview\n\n```python\nclass HierarchicalCompressedFastTTTLinear(nn.Module):\n    def __init__(self, dim, num_heads=8, compression_tiers=3):\n        super().__init__()\n        self.dim = dim\n        self.num_heads = num_heads\n        \n        # Hierarchical compression components\n        self.compressor = HierarchicalCompressor(\n            dim, compression_tiers=compression_tiers\n        )\n        \n        # Adaptive sparse attention\n        self.sparse_attention = AdaptiveSparseAttention(\n            dim, num_heads=num_heads\n        )\n        \n        # Memory management\n        self.memory_manager = TieredMemoryManager(\n            dim, tiers=compression_tiers\n        )\n\n    def forward(self, x, state=None):\n        # Compress input based on importance\n        compressed_x, importance = self.compressor(x)\n        \n        # Apply sparse attention\n        attention_output = self.sparse_attention(\n            compressed_x, importance\n        )\n        \n        # Update memory state\n        new_state = self.memory_manager.update(\n            attention_output, importance\n        )\n        \n        return attention_output, new_state\n```\n\n### 4.2 Key Components\n\n1. **Hierarchical Compressor**:\n```python\nclass HierarchicalCompressor(nn.Module):\n    def __init__(self, dim, compression_tiers):\n        super().__init__()\n        self.importance_estimator = nn.Linear(dim, compression_tiers)\n        self.compressors = nn.ModuleList([\n            CompressionLayer(dim, ratio=2**i)\n            for i in range(compression_tiers)\n        ])\n        \n    def forward(self, x):\n        # Estimate importance scores\n        importance = self.importance_estimator(x).softmax(-1)\n        \n        # Apply tiered compression\n        compressed = []\n        for tier, compressor in enumerate(self.compressors):\n            mask = importance[:, :, tier].unsqueeze(-1)\n            compressed.append(compressor(x) * mask)\n            \n        return sum(compressed), importance\n```\n\n2. **Adaptive Sparse Attention**:\n```python\nclass AdaptiveSparseAttention(nn.Module):\n    def __init__(self, dim, num_heads):\n        super().__init__()\n        self.token_selector = TokenSelector(dim)\n        self.attention = LinearAttention(dim, num_heads)\n        \n    def forward(self, x, importance):\n        # Select tokens based on importance\n        selected_tokens = self.token_selector(x, importance)\n        \n        # Apply efficient linear attention\n        return self.attention(selected_tokens)\n```\n\n3. **Tiered Memory Manager**:\n```python\nclass TieredMemoryManager(nn.Module):\n    def __init__(self, dim, tiers):\n        super().__init__()\n        self.memory_tiers = nn.ModuleList([\n            MemoryTier(dim, compression_ratio=2**i)\n            for i in range(tiers)\n        ])\n        \n    def update(self, x, importance):\n        states = []\n        for tier, memory in enumerate(self.memory_tiers):\n            mask = importance[:, :, tier].unsqueeze(-1)\n            states.append(memory.update(x * mask))\n        return states\n```\n\n### 4.3 Memory Management\n\n1. **Compression Strategy**:\n```python\ndef compress_state(state, importance):\n    # Apply different compression rates based on importance\n    compressed = []\n    for tier, imp in enumerate(importance):\n        ratio = 2 ** tier\n        compressed.append(\n            adaptive_pooling(state * imp, ratio)\n        )\n    return compressed\n```\n\n2. **State Updates**:\n```python\ndef update_state(old_state, new_info, importance):\n    # Update states with different decay rates\n    updated = []\n    for tier, (state, imp) in enumerate(zip(old_state, importance)):\n        decay = 0.9 ** (tier + 1)\n        updated.append(\n            decay * state + (1 - decay) * new_info * imp\n        )\n    return updated\n```\n\n## 5. Implementation Guidelines\n\n### 5.1 Initialization\n```python\ndef initialize_model(config):\n    model = HierarchicalCompressedFastTTTLinear(\n        dim=config.hidden_size,\n        num_heads=config.num_attention_heads,\n        compression_tiers=config.compression_tiers\n    )\n    return model\n```\n\n### 5.2 Training Process\n```python\ndef train_step(model, batch, optimizer):\n    # Forward pass with compression\n    output, new_state = model(batch.input_ids)\n    \n    # Compute loss with compression penalty\n    loss = compute_loss(output, batch.labels)\n    compression_loss = compute_compression_loss(new_state)\n    total_loss = loss + config.compression_lambda * compression_loss\n    \n    # Backward pass\n    total_loss.backward()\n    optimizer.step()\n    \n    return loss\n```\n\n### 5.3 Inference\n```python\ndef inference_step(model, input_ids, state=None):\n    # Process with compressed states\n    output, new_state = model(input_ids, state)\n    \n    # Update compressed memory\n    state = new_state\n    \n    return output, state\n```\n\n## 6. Theoretical Analysis\n\n### 6.1 Memory Complexity\n- O(N/k) memory usage for sequence length N and average compression ratio k\n- Reduced memory footprint through hierarchical compression\n- Efficient state management through tiered storage\n\n### 6.2 Computational Complexity\n- O(N) complexity for forward pass\n- Reduced computation through sparse attention\n- Efficient memory access patterns\n\n### 6.3 Performance Analysis\n- Improved efficiency through hierarchical compression\n- Enhanced expressiveness via adaptive attention\n- Better hardware utilization through optimized operations\n\n## 7. References\n\n1. Zhu, Q., et al. (2024). Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention.\n\n2. Yu, H., et al. (2023). TRAMS: Training-free Memory Selection for Long-range Language Modeling.\n\n3. Liu, H., et al. (2023). Ring Attention with Blockwise Transformers for Near-Infinite Context.\n\n4. Cai, R., et al. (2024). LoCoCo: Dropping In Convolutions for Long Context Compression.\n\n5. Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training.\n\n## 8. Conclusion\n\nHierarchicalCompressedFastTTTLinear represents a significant advancement in efficient language modeling by combining hierarchical compression with adaptive sparse attention. The design enables practical deployment of large language models in resource-constrained environments while maintaining model performance through intelligent memory management and hardware-aware optimizations.\n",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "A novel enhancement to FastTTTLinear that integrates hierarchical compression with adaptive sparse attention mechanisms, enabling efficient processing of long sequences through dynamic memory management and hardware-aware optimizations. The design combines content-based token selection with multi-tier compression to reduce memory footprint while maintaining model expressiveness.",
    "ideation": null,
    "modelname": "HierarchicalCompressedLM",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of compression tier interactions\n- Include concrete examples of compression rate adaptation\n- Add guidelines for hardware-specific optimizations\n- Specify requirements for different scales of deployment\n\n2. Memory Management:\n- Elaborate on state synchronization mechanisms\n- Include more detailed cache optimization strategies\n- Provide guidelines for distributed settings\n- Add memory footprint analysis for different scenarios\n\n3. Training Considerations:\n- Add specific guidance for maintaining training stability\n- Include strategies for gradient flow optimization\n- Provide more detailed initialization guidelines\n- Specify hyperparameter adaptation strategies\n\n4. Hardware Integration:\n- Include hardware-specific implementation details\n- Add distributed processing guidelines\n- Provide performance optimization strategies\n- Specify requirements for different hardware architectures\n\n5. Empirical Validation:\n- Include proposed benchmarking methodology\n- Add expected performance metrics\n- Provide comparison frameworks\n- Specify evaluation criteria",
    "user_input": ""
}