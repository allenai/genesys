{
    "variantname": "StreamRetNetMLP",
    "review": "### Comprehensive Analysis of StreamRetNetMLP Proposal\n\n**Clarity**:\nThe proposal is clearly articulated, with well-defined objectives focusing on improving memory efficiency and computational performance in streaming scenarios. The design outlines specific mechanisms such as attention sink integration, adaptive tiling, and selective KV caching, providing a clear roadmap for implementation.\n\n**Innovation**:\nThe proposal introduces innovative elements, particularly the attention sink mechanism and adaptive tiling, which are not commonly found in existing RetNetMLP designs. These innovations align with current research trends in optimizing memory usage and computational efficiency in language models. The selective KV caching mechanism, while not entirely novel, is well-integrated into the overall design to enhance efficiency.\n\n**Feasibility**:\nThe proposed design appears feasible within the given framework. The use of attention sinks and adaptive tiling is supported by existing research, indicating that these mechanisms can be successfully implemented. However, the complexity of integrating these components may require careful engineering and validation.\n\n**Scalability**:\nThe design is likely to scale efficiently with larger models or more data, given its focus on memory efficiency and computational optimization. The use of adaptive tiling and selective KV caching can help manage resource usage as model size and data volume increase.\n\n**Accuracy and Robustness**:\nThe proposal has the potential to improve model performance by stabilizing attention computation and reducing memory overhead. The use of attention sinks can enhance the model's ability to handle diverse inputs and long sequences, contributing to robustness.\n\n**Efficiency**:\nThe design offers promising improvements in computational efficiency and memory usage. Adaptive tiling and selective KV caching are expected to reduce computational overhead and memory footprint, making the model more suitable for streaming applications.\n\n### Strengths\n- **Innovative Mechanisms**: The introduction of attention sinks and adaptive tiling is a significant strength, offering potential improvements in efficiency and robustness.\n- **Alignment with Research Trends**: The proposal aligns well with current research trends, leveraging concepts like memory-efficient computation and intelligent caching.\n- **Scalability**: The design is poised to scale effectively with larger models and datasets, addressing key challenges in modern language modeling.\n\n### Concerns\n- **Implementation Complexity**: The integration of multiple novel components may introduce complexity, requiring careful engineering and validation.\n- **Empirical Validation**: The proposal lacks detailed empirical evaluations to demonstrate the impact of the proposed modifications on standard benchmarks.\n\n### Suggestions for Improvement\n1. **Empirical Validation**: Conduct comprehensive experiments on standard language modeling benchmarks to empirically validate the performance and efficiency gains of StreamRetNetMLP.\n2. **Implementation Details**: Provide more detailed implementation guidelines, particularly for integrating attention sinks and adaptive tiling.\n3. **Complexity Management**: Address potential implementation complexities by outlining strategies for seamless integration and validation.\n\n### Final Rating\nBased on the proposal's clarity, innovation, feasibility, scalability, and potential impact on accuracy and efficiency, I would rate it **4.2 out of 5**. The proposal is strong and holds significant promise for advancing streaming language models, but it requires empirical validation and careful implementation to realize its full potential.",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient attention mechanisms memory optimization",
            "detail": "Analyze recent developments in efficient attention mechanisms and memory optimization techniques for transformer architectures, focusing on approaches that maintain or improve model performance while reducing computational and memory requirements.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in efficient attention mechanisms and memory optimization techniques for transformer architectures, focusing on approaches that maintain or improve model performance while reducing computational and memory requirements.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention (Avg. Score: 1.00)\n\n*Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, Franccois Fleuret*\n\n**Published in:** International Conference on Machine Learning (2020)\t**Cited by** 1155  (*Influential: 164*)\n\n**TL;DR:** This work expresses the self-attention as a linear dot-product of kernel feature maps and makes use of the associativity property of matrix products to reduce the complexity from O(N) to N, where N is the sequence length.\n\n**Abstract:** Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^2\\right)$ to $\\mathcal{O}\\left(N\\right)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.\n\n##### *Relevant Chunk: No. 2/28 (Score: 1.00)*\n\n```\nTo address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from $\\mathcal{O}\\left(N^{2}\\right)$ to $\\mathcal{O}(N)$, where $N$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to $4000 x$ faster on autoregressive prediction of very long sequences. ## 1. Introduction\n\nTransformer models were originally introduced by Vaswani et al. (2017) in the context of neural machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) and have demonstrated impressive results on a variety of tasks dealing with natural language (Devlin et al., 2019), audio (Sperber et al., 2018), and images (Parmar et al., 2019). Apart from tasks with ample supervision, transformers are also effective in transferring knowledge to tasks with limited or no supervision when they are pretrained with autoregressive (Radford et al., 2018; 2019) or masked language modeling objectives (Devlin et al., 2019; Yang et al., 2019; Song et al., 2019; Liu et al., 2020). However, these benefits often come with a very high computational and memory cost. The bottleneck is mainly caused\n\n[^0]by the global receptive field of self-attention, which processes contexts of $N$ inputs with a quadratic memory and time complexity $\\mathcal{O}\\left(N^{2}\\right)$. As a result, in practice transformers are slow to train and their context is limited. This disrupts temporal coherence and hinders the capturing of long-term dependencies. Dai et al. (2019) addressed the latter by attending to memories from previous contexts albeit at the expense of computational efficiency. Lately, researchers shifted their attention to approaches that increase the context length without sacrificing efficiency. Towards this end, Child et al. (2019) introduced sparse factorizations of the attention matrix to reduce the selfattention complexity to $\\mathcal{O}(N \\sqrt{N})$. Kitaev et al. (2020) further reduced the complexity to $\\mathcal{O}(N \\log N)$ using localitysensitive hashing. This made scaling to long sequences possible. Even though the aforementioned models can be efficiently trained on large sequences, they do not speed-up autoregressive inference. In this paper, we introduce the linear transformer model that significantly reduces the memory footprint and scales linearly with respect to the context length. We achieve this by using a kernel-based formulation of self-attention and the associative property of matrix products to calculate the self-attention weights (\u00a7 3.2). Using our linear formulation, we also express causal masking with linear complexity and constant memory (\u00a73.3). This reveals the relation between transformers and RNNs, which enables us to perform autoregressive inference orders of magnitude faster (\u00a7 3.4). Our evaluation on image generation and automatic speech recognition demonstrates that linear transformer can reach the performance levels of transformer, while being up to three orders of magnitude faster during inference. ## 2. Related Work\n\nIn this section, we provide an overview of the most relevant works that seek to address the large memory and computational requirements of transformers. Furthermore, we discuss methods that theoretically analyze the core component of the transformer model, namely self-attention. Finally, we present another line of work that seeks to alleviate the softmax bottleneck in the attention computation. ### 2.1. Efficient Transformers\n\nExisting works seek to improve memory efficiency in transformers through weight pruning (Michel et al., 2019), weight factorization (Lan et al., 2020), weight quantization (Zafrir et al., 2019) or knowledge distillation. Clark et al. (2020) proposed a new pretraining objective called replaced token detection that is more sample efficient and reduces the overall computation. Lample et al. (2019) used product-key attention to increase the capacity of any layer with negligible computational overhead. Reducing the memory or computational requirements with these methods leads to training or inference time speedups, but, fundamentally, the time complexity is still quadratic with respect to the sequence length which hinders scaling to long sequences. In contrast, we show that our method reduces both memory and time complexity of transformers both theoretically ( $\\S 3.2$ ) and empirically (\u00a7 4.1). Another line of research aims at increasing the \"context\" of self-attention in transformers. Context refers to the maximum part of the sequence that is used for computing selfattention. Dai et al. (2019) introduced Transformer-XL which achieves state-of-the-art in language modeling by learning dependencies beyond a fixed length context without disrupting the temporal coherence. However, maintaining previous contexts in memory introduces significant additional computational cost. In contrast, Sukhbaatar et al. (2019) extended the context length significantly by learning the optimal attention span per attention head, while maintaining control over the memory footprint and computation time. Note that both approaches have the same asymptotic complexity as the vanilla model. In contrast, we improve the asymptotic complexity of the self-attention, which allows us to use significantly larger context. More related to our model are the works of Child et al.\n```\n\n#### 2. Recurrent Memory Transformer (Avg. Score: 1.00)\n\n*Aydar Bulatov, Yuri Kuratov, M. Burtsev*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 68  (*Influential: 10*)\n\n**TL;DR:** Recurrent Memory Transformer is a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n**Abstract:** Transformer-based models show their effectiveness across multiple domains and tasks. The self-attention allows to combine information from all sequence elements into context-aware representations. However, global and local information has to be stored mostly in the same element-wise representations. Moreover, the length of an input sequence is limited by quadratic computational complexity of self-attention. In this work, we propose and study a memory-augmented segment-level recurrent Transformer (RMT). Memory allows to store and process local and global information as well as to pass information between segments of the long sequence with the help of recurrence. We implement a memory mechanism with no changes to Transformer model by adding special memory tokens to the input or output sequence. Then the model is trained to control both memory operations and sequence representations processing. Results of experiments show that RMT performs on par with the Transformer-XL on language modeling for smaller memory sizes and outperforms it for tasks that require longer sequence processing. We show that adding memory tokens to Tr-XL is able to improve its performance. This makes Recurrent Memory Transformer a promising architecture for applications that require learning of long-term dependencies and general purpose in memory processing, such as algorithmic tasks and reasoning.\n\n##### *Relevant Chunk: No. 5/29 (Score: 1.00)*\n\n```\n[^0]The recent rise of Transformer models also resulted in introduction of a number of new memory architectures. Transformer-XL (Dai et al. 2019) introduces a segment-level recurrence at the level of hidden representations. These representations of a sequence are computed and stored in the cache to be reused as an extended context for the next segment. Compressive Transformer (Rae et al. 2019) adds the second layer of memory to Transformer-XL. This memory compresses and stores information from the cache. $\\infty$-former (Martins et al., 2021) utilizes continuous-space attention and represents input sequence as a continuous signal to make long-term memory unbounded. Memory Layers (Lample et al, 2019) model has a product key memory layer instead of a feed-forward layer within Transformer block to increase model capacity.\n```\n\n#### 3. Reformer: The Efficient Transformer (Avg. Score: 1.00)\n\n*Nikita Kitaev, Lukasz Kaiser, Anselm Levskaya*\n\n**Published in:** International Conference on Learning Representations (2020)\t**Cited by** 1881  (*Influential: 222*)\n\n**TL;DR:** This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences.\n\n**Abstract:** Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.\n\n##### *Relevant Chunk: No. 4/19 (Score: 1.00)*\n\n```\n2017) has been used widely in natural language tasks and further extended to model diverse data such as music scores (Huang et al., 2018), and images (Parmar et al., 2018; Ramachandran et al., 2019). Most notably, this model class has been applied successfully in the self-supervised training of extremely large language models (Devlin et al., 2018, Radford et al. 2019). Given the enormous computational requirements of state of the art sequence models, there has been increasing interest in finding methods to reduce the memory footprint and computational requirements of Transformer models. In addition to standard methods such as precision reduction and gradient checkpointing (Sohoni et al., 2019), more efficient versions of the Transformer model's self-attention mechanism (Sukhbaatar et al. 2019a b) have also recently been explored. In particular, leveraging sparsity in the attention layers has proved fruitful. OpenAI introduced the sparse Transformer (Child et al. 2019) which exploits a factorized sparse representation of attention. Using product-key attention to increase the key space has also been used to reduce memory requirements in the feed-forward layers with no loss in performance (Lample et al, 2019). Locality-sensitive hashing (LSH) has, to our knowledge, not been directly applied to Transformer attention layers before. But previous work using external memory with neural networks has dealt with memories of large sizes. The original implementation of memory networks (Weston et al., 2014) and later work on scaling it (Bordes et al., 2015, Chandar et al., 2016) used memory with size in the millions. The cost of doing so is that the memory must be fixed prior to training. Moreover, since during the beginning of training the model is unlikely to query the memory correctly, strong supervision is used to encourage the model to query memory locations that are useful. These hints are either given as additional supervising information by the task or determined heuristically as in Hill et al. (2015). The requirement that the memory be fixed before has been removed in Santoro et al. (2016) at the cost of memory size and later alleviated by Rae et al. (2016). The last paper considered memory lookups with approximate nearest neighbors including both LSH and random kd-trees, but only for lookups in external memory.\n```\n\n#### 4. Memorizing Transformers (Avg. Score: 1.00)\n\n*Yuhuai Wu, M. Rabe, DeLesley S. Hutchins, Christian Szegedy*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 138  (*Influential: 15*)\n\n**TL;DR:** It is demonstrated that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext, math papers, books, code, as well as formal theorems (Isabelle).\n\n**Abstract:** Language models typically need to be trained or finetuned in order to acquire new knowledge, which involves updating their weights. We instead envision language models that can simply read and memorize new data at inference time, thus acquiring new knowledge immediately. In this work, we extend language models with the ability to memorize the internal representations of past inputs. We demonstrate that an approximate kNN lookup into a non-differentiable memory of recent (key, value) pairs improves language modeling across various benchmarks and tasks, including generic webtext (C4), math papers (arXiv), books (PG-19), code (Github), as well as formal theorems (Isabelle). We show that the performance steadily improves when we increase the size of memory up to 262K tokens. On benchmarks including code and mathematics, we find that the model is capable of making use of newly defined functions and theorems during test time.\n\n##### *Relevant Chunk: No. 7/26 (Score: 1.00)*\n\n```\nIn $A C L, 2019$. Angela Fan, Thibaut Lavril, Edouard Grave, Armand Joulin, and Sainbayar Sukhbaatar. Addressing some limitations of transformers with feedback memory. arXiv preprint arXiv:2002.09402, 2020. Angela Fan, Claire Gardent, Chlo\u00e9 Braud, and Antoine Bordes. Augmenting transformers with KNN-based composite memory for dialog. Transactions of the Association for Computational Linguistics, 9:82-99, 2021. Edouard Grave, Armand Joulin, and Nicolas Usunier. Improving neural language models with a continuous cache. In ICLR, 2017. Ruiqi Guo, Philip Sun, Erik Lindgren, Quan Geng, David Simcha, Felix Chern, and Sanjiv Kumar. Accelerating large-scale inference with anisotropic vector quantization. In ICML, 2020. Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. URL/https://arxiv.org/ $\\mathrm{abs} / 2106.06899$. Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. Retrieval augmented language model pre-training. In ICML, 2020. Christopher Hahn, Frederik Schmitt, Jens U. Kreber, Markus Norman Rabe, and Bernd Finkbeiner. Teaching temporal logics to neural networks.\n```\n\n#### 5. Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers (Avg. Score: 0.99)\n\n*Sotiris Anagnostidis, Dario Pavllo, Luca Biggio, Lorenzo Noci, Aur\u00e9lien Lucchi, Thomas Hofmann*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 22  (*Influential: 1*)\n\n**TL;DR:** A novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference, offering a valuable tool for mitigating inference costs.\n\n**Abstract:** Autoregressive Transformers adopted in Large Language Models (LLMs) are hard to scale to long sequences. Despite several works trying to reduce their computational cost, most of LLMs still adopt attention layers between all pairs of tokens in the sequence, thus incurring a quadratic cost. In this study, we present a novel approach that dynamically prunes contextual information while preserving the model's expressiveness, resulting in reduced memory and computational requirements during inference. Our method employs a learnable mechanism that determines which uninformative tokens can be dropped from the context at any point across the generation process. By doing so, our approach not only addresses performance concerns but also enhances interpretability, providing valuable insight into the model's decision-making process. Our technique can be applied to existing pre-trained models through a straightforward fine-tuning process, and the pruning strength can be specified by a sparsity parameter. Notably, our empirical findings demonstrate that we can effectively prune up to 80\\% of the context without significant performance degradation on downstream tasks, offering a valuable tool for mitigating inference costs. Our reference implementation achieves up to $2\\times$ increase in inference throughput and even greater memory savings.\n\n##### *Relevant Chunk: No. 10/30 (Score: 0.99)*\n\n```\nIn Proceedings of the AAAI conference on artificial intelligence, volume 34, pages $7432-7439,2020$. Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461, 2022. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, David Belanger, Lucy Colwell, and Adrian Weller. Masked language modeling for proteins via linearly scalable long-context transformers, 2020a. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020 b. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311, 2022. Zihang Dai, Guokun Lai, Yiming Yang, and Quoc Le. Funnel-transformer: Filtering out sequential redundancy for efficient language processing. Advances in neural information processing systems, 33:4271-4282, 2020\n\nTri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, $35: 16344-16359,2022$. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022. Elias Frantar and Dan Alistarh. Massive language models can be accurately pruned in one-shot. arXiv preprint arXiv:2301.00774, 2023a. Elias Frantar and Dan Alistarh. Sparsegpt: Massive language models can be accurately pruned in one-shot, 2023b. Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323, 2022. Elias Frantar, Sidak Pal Singh, and Dan Alistarh. Optimal brain compression: A framework for accurate post-training quantization and pruning, 2023. Yaru Hao, Li Dong, Furu Wei, and Ke Xu. Self-attention attribution: Interpreting information interactions inside transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 12963-12971, 2021. Babak Hassibi, David G. Stork, and Gregory J. Wolff. Optimal brain surgeon and general network pruning. IEEE International Conference on Neural Networks, pages 293-299 vol.1, 1993. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In Proceedings of the IEEE international conference on computer vision, pages 1026-1034, 2015. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556, 2022. Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers. Proceedings of Machine Learning and Systems, 3:711-732, 2021. Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver: General perception with iterative attention, 2021. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: efficient attention mechanisms memory optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 2. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 3. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 4. Pit One Against Many: Leveraging Attention-head Embeddings for Parameter-efficient Multi-head Attention\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Huiyin Xue, Nikolaos Aletras*\n\n**TL;DR:** This work proposes an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head, and empirically demonstrates that this MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks.\n\n**Abstract:** Scaling pre-trained language models has resulted in large performance gains in various natural language processing tasks but comes with a large cost in memory requirements. Inspired by the position embeddings in transformers, we aim to simplify and reduce the memory footprint of the multi-head attention (MHA) mechanism. We propose an alternative module that uses only a single shared projection matrix and multiple head embeddings (MHE), i.e. one per head. We empirically demonstrate that our MHE attention is substantially more memory efficient compared to alternative attention mechanisms while achieving high predictive performance retention ratio to vanilla MHA on several downstream tasks. MHE attention only requires a negligible fraction of additional parameters ($3nd$, where $n$ is the number of attention heads and $d$ the size of the head embeddings) compared to a single-head attention, while MHA requires $(3n^2-3n)d^2-3nd$ additional parameters.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Efficient Attention using a Fixed-Size Memory Representation\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*D. Britz, M. Guan, Minh-Thang Luong*\n\n**TL;DR:** This work proposes an alternative attention mechanism based on a fixed size memory representation that is more efficient and predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory.\n\n**Abstract:** The standard content-based attention mechanism typically used in sequence-to-sequence models is computationally expensive as it requires the comparison of large encoder and decoder states at each time step. In this work, we propose an alternative attention mechanism based on a fixed size memory representation that is more efficient. Our technique predicts a compact set of K attention contexts during encoding and lets the decoder compute an efficient lookup that does not need to consult the memory. We show that our approach performs on-par with the standard attention mechanism while yielding inference speedups of 20% for real-world translation tasks and more for tasks with longer sequences. By visualizing attention scores we demonstrate that our models learn distinct, meaningful alignments.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2017\n\n**Citations:** 32  (*Influential: 2*)\n\n### 3 related papers from Papers with Code\n\n#### 1. vTensor: Flexible Virtual Tensor Management for Efficient LLM Serving\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Jingwen Leng, Minyi Guo, Ke Zhang, Junping Zhao, Yuhong Guo, Changxu Shao, Shixuan Sun, Yu Feng, Feiyang Wu, Zihan Liu, Weiming Hu, Cong Guo, Rui Zhang, Jiale Xu*\n\n**Abstract:** Large Language Models (LLMs) are widely used across various domains, processing millions of daily requests. This surge in demand poses significant challenges in optimizing throughput and latency while keeping costs manageable. The Key-Value (KV) cache, a standard method for retaining previous computations, makes LLM inference highly bounded by memory. While batching strategies can enhance performance, they frequently lead to significant memory fragmentation. Even though cutting-edge systems like vLLM mitigate KV cache fragmentation using paged Attention mechanisms, they still suffer from inefficient memory and computational operations due to the tightly coupled page management and computation kernels. This study introduces the vTensor, an innovative tensor structure for LLM inference based on GPU virtual memory management (VMM). vTensor addresses existing limitations by decoupling computation from memory defragmentation and offering dynamic extensibility. Our framework employs a CPU-GPU heterogeneous approach, ensuring efficient, fragmentation-free memory management while accommodating various computation kernels across different LLM architectures. Experimental results indicate that vTensor achieves an average speedup of 1.86x across different models, with up to 2.42x in multi-turn chat scenarios. Additionally, vTensor provides average speedups of 2.12x and 3.15x in kernel evaluation, reaching up to 3.92x and 3.27x compared to SGLang Triton prefix-prefilling kernels and vLLM paged Attention kernel, respectively. Furthermore, it frees approximately 71.25% (57GB) of memory on the NVIDIA A100 GPU compared to vLLM, enabling more memory-intensive workloads.\n\n**Published:** 2024-07-22\n\n\n\n#### 2. DISTFLASHATTN: Distributed Memory-efficient Attention for Long-context LLMs Training\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Joseph E. Gonzalez, Hao Zhang, Xuezhe Ma, Ion Stoica, Eric P. Xing, Anze Xie, Rulin Shao, Dacheng Li*\n\n**Abstract:** FlashAttention (Dao, 2023) effectively reduces the quadratic peak memory usage to linear in training transformer-based large language models (LLMs) on a single GPU. In this paper, we introduce DISTFLASHATTN, a distributed memory-efficient attention mechanism optimized for long-context LLMs training. We propose three key techniques: token-level workload balancing, overlapping key-value communication, and a rematerialization-aware gradient checkpointing algorithm. We evaluate DISTFLASHATTN on Llama-7B and variants with sequence lengths from 32K to 512K. DISTFLASHATTN achieves 8x longer sequences, 4.45 - 5.64x speedup compared to Ring Self-Attention, 2 - 8x longer sequences, 1.24 - 2.01x speedup compared to Megatron-LM with FlashAttention. It achieves 1.67x and 1.26 - 1.88x speedup compared to recent Ring Attention and DeepSpeed-Ulysses. Code is available at https://github.com/RulinShao/LightSeq.\n\n**Published:** 2023-10-05\n\n\n\n#### 3. Learning Graph Convolutional Network for Skeleton-based Human Action Recognition by Neural Searching\n\n*From Search Query: efficient attention mechanisms memory optimization*\n\n*Xiaopeng Hong, Haoyu Chen, Wei Peng, Guoying Zhao*\n\n**Abstract:** Human action recognition from skeleton data, fueled by the Graph Convolutional Network (GCN), has attracted lots of attention, due to its powerful capability of modeling non-Euclidean structure data. However, many existing GCN methods provide a pre-defined graph and fix it through the entire network, which can loss implicit joint correlations. Besides, the mainstream spectral GCN is approximated by one-order hop, thus higher-order connections are not well involved. Therefore, huge efforts are required to explore a better GCN architecture. To address these problems, we turn to Neural Architecture Search (NAS) and propose the first automatically designed GCN for skeleton-based action recognition. Specifically, we enrich the search space by providing multiple dynamic graph modules after fully exploring the spatial-temporal correlations between nodes. Besides, we introduce multiple-hop modules and expect to break the limitation of representational capacity caused by one-order approximation. Moreover, a sampling- and memory-efficient evolution strategy is proposed to search an optimal architecture for this task. The resulted architecture proves the effectiveness of the higher-order approximation and the dynamic graph modeling mechanism with temporal interactions, which is barely discussed before. To evaluate the performance of the searched model, we conduct extensive experiments on two very large scaled datasets and the results show that our model gets the state-of-the-art results.\n\n**Published:** 2019-11-11\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on efficient attention mechanisms and memory optimization, here are some recent developments and techniques that could be highly relevant:\n\n## Efficient Attention Mechanisms\n\n### Dynamic Sparse Attention\nThe CASAK-V approach introduces dynamic sparse attention and adaptive KV-cache compression, which is particularly useful for long-context processing. This method dynamically generates and applies head-specific sparse attention patterns, reducing memory usage by 40% and maintaining near-linear runtime complexity. This technique leverages a meta-learning framework to fine-tune a compact pre-trained transformer, making it efficient for memory-limited environments.\n\n### Neural Attention Memory Models (NAMMs)\nNAMMs evolve on top of pre-trained transformers to manage memory more efficiently. By learning to focus on the most relevant information for individual layers and attention heads, NAMMs reduce the input context size significantly while improving performance across long-context benchmarks. This approach is universally applicable and enables zero-shot transfer across different architectures and modalities.\n\n### Block-Attention\nBlock-Attention is designed for Retrieval-Augmented Generation (RAG) scenarios and divides the context into discrete blocks, each calculating key-value states independently. This mechanism reduces inference latency and computational overhead by reusing KV states of previously seen passages. It maintains comparable or even better performance than traditional self-attention models while significantly reducing Time-To-First-Token (TTFT) and FLOPs.\n\n### FlashAttention\nFlashAttention is a fast and memory-efficient algorithm that computes attention using tiling and recomputation techniques. It reduces memory usage by processing attention calculations in smaller blocks and avoids storing large intermediate matrices, leading to a 2-4x speedup compared to standard attention mechanisms. FlashAttention-2 further optimizes this by parallelizing over the sequence length and optimizing memory access.\n\n## Memory Optimization Techniques\n\n### Adaptive KV-Cache Compression\nCASAK-V implements adaptive chunk-wise KV-cache compression, which adjusts compression policies based on layer-wise sparse attention configurations. This dynamic adjustment helps in retaining context-awareness during token generation, reducing memory usage significantly without compromising performance.\n\n### Memory Management with NAMMs\nNAMMs introduce a learned network for memory management, which conditions exclusively on the values in the produced attention matrices. This approach allows for the efficient management of memory by focusing on the most relevant information, reducing the memory footprint of the model.\n\n### Efficient Architectures\nUsing architectures like Performer, which estimate full-rank attention using linear space and time complexity, can be beneficial. Additionally, techniques such as quantization, pruning, and shared attention mechanisms (as seen in SwiftFormer) can reduce both computational and memory overhead.\n\n## Integration and Future Directions\n\n### Hybrid Attention Mechanisms\nCombining different attention mechanisms, such as retention mechanisms from RetNet and linear attention from Performer, could offer a hybrid approach that leverages the strengths of each. This could include adaptive computation paths for different sequence lengths, enhancing both efficiency and performance.\n\n### Enhanced Position Encodings\nImproving position encodings based on RoPE scaling laws can further enhance the extrapolation performance of the model. This, coupled with efficient attention mechanisms, can help in better handling long-range dependencies and improving overall model performance.\n\n### Adaptive Computation Strategies\nImplementing adaptive computation strategies, such as those suggested by NAMMs and Block-Attention, can help in optimizing the model for different sequence lengths and input modalities. This adaptability is crucial for maintaining performance while reducing computational and memory requirements.\n\nBy integrating these recent advances, researchers can develop more efficient and scalable autoregressive language models that maintain or improve performance while significantly reducing computational and memory requirements.\n",
            "analysis": "Analysis Note - Round 1\n\n1. Summary of Analysis:\nThe seed design RetNet introduces a retention mechanism that enables parallel training, efficient inference, and good performance. Key aspects to analyze:\n\na) Core Innovations:\n- Retention mechanism supporting three computation paradigms (parallel, recurrent, chunkwise)\n- Multi-scale retention for handling different temporal dependencies\n- O(1) inference capability while maintaining parallel training\n\nb) Areas for Potential Improvement:\n- Memory efficiency during training\n- Expressiveness of the retention mechanism\n- Handling of long-range dependencies\n- Integration of recent advances in efficient attention mechanisms\n\n2. Useful References with Excerpts:\n\nFrom RetNet paper:\n\"We propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost O(1) inference...\"\n\nFrom Performer paper:\n\"We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity...\"\n\nFrom MobileLLM:\n\"Leveraging deep and thin architectures, coupled with embedding sharing and grouped-query attention mechanisms, we establish a strong baseline network...\"\n\nFrom RoPE Scaling Laws:\n\"We first observe that fine-tuning a RoPE-based LLM with either a smaller or larger base in pre-training context length could significantly enhance its extrapolation performance.\"\n\n3. Key Insights and Analysis:\n\na) Architectural Considerations:\n- RetNet's retention mechanism provides a strong foundation for efficient inference\n- Performer's linear attention mechanism could inspire optimizations\n- MobileLLM's grouped-query attention shows promise for efficiency\n- RoPE scaling insights could improve position encoding\n\nb) Performance Bottlenecks:\n- Memory usage during parallel training\n- Computational overhead in multi-scale retention\n- Position encoding limitations\n\nc) Opportunities for Innovation:\n- Hybrid attention mechanisms combining retention and linear attention\n- Adaptive computation paths for different sequence lengths\n- Enhanced position encodings based on RoPE scaling laws\n- Memory-efficient training strategies\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Recent advances in efficient attention mechanisms beyond linear attention\n2. Memory optimization techniques for transformer architectures\n3. Adaptive computation strategies for language models\n4. Position encoding improvements for better extrapolation\n\n5. References:\n\n[1] Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., & Wei, F. (2023). Retentive Network: A Successor to Transformer for Large Language Models. arXiv:2307.08621.\n\n[2] Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., ... & Weller, A. (2020). Rethinking Attention with Performers. arXiv:2009.14794.\n\n[3] Liu, Z., Zhao, C., Iandola, F. N., Lai, C., Tian, Y., Fedorov, I., ... & Chandra, V. (2024). MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. arXiv:2402.14905.\n\n[4] Liu, X., Yan, H., Zhang, S., An, C., Qiu, X., & Lin, D. (2023). Scaling Laws of RoPE-based Extrapolation. arXiv:2312.06924."
        },
        {
            "ready": false,
            "query": "gradient computation memory optimization hardware efficient implementation",
            "detail": "Analyze recent developments in gradient computation and memory optimization techniques during training, focusing on hardware-efficient implementations and methods to reduce memory footprint while maintaining model performance.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in gradient computation and memory optimization techniques during training, focusing on hardware-efficient implementations and methods to reduce memory footprint while maintaining model performance.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Blockwise Parallel Transformer for Large Context Models (Avg. Score: 0.97)\n\n*Hao Liu, P. Abbeel*\n\n**Published in:**  (2023)\t**Cited by** 5  (*Influential: 1*)\n\n**TL;DR:** This work presents a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs and enables training sequences 32 times longer than vanilla Transformers and up to 4 times longerthan previous memory-efficient methods.\n\n**Abstract:** Transformers have emerged as the cornerstone of state-of-the-art natural language processing models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands posed by the self-attention mechanism and the large feedforward network in Transformers limit their ability to handle long sequences, thereby creating challenges for tasks involving multiple long sequences or long-term dependencies. We present a distinct approach, Blockwise Parallel Transformer (BPT), that leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. By processing longer input sequences while maintaining memory efficiency, BPT enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of BPT in reducing memory requirements and improving performance.\n\n##### *Relevant Chunk: No. 4/24 (Score: 0.97)*\n\n```\nAT + ME denotes using \"MemoryEfficient\". AT + BPT denotes using Blockwise Parallel. | ExoRL benchmark | BC-10\\% | DT | AT | AT | AT + ME | AT + BPT |\n| :--- | ---: | ---: | ---: | ---: | ---: | ---: |\n| Task |  |  | N Trajs = 4 | N Trajs = 32 | N Trajs = 32 | N Trajs = 32 |\n| Walker Stand | 52.91 | 34.54 | 68.55 | oom | oom | 95.45 |\n| Walker Run | 34.81 | 49.82 | 88.56 | oom | oom | 105.88 |\n| Walker Walk | 13.53 | 34.94 | 64.56 | oom | oom | 78.56 |\n| Cheetah Run | 34.66 | 67.53 | 125.68 | oom | oom | 178.75 |\n| Jaco Reach | 23.95 | 18.64 | 52.98 | oom | oom | 87.56 |\n| Cartpole Swingup | 56.82 | 67.56 | 97.81 | oom | oom | 120.56 |\n| Total Average | 36.11 | 45.51 | 83.02 | oom | oom | 111.13 |\n\n## 6 Related Work\n\nTransformers have garnered significant attention in the field of natural language processing (NLP) and have become the basis for numerous state-of-the-art models. Several works have explored memoryefficient techniques to address the memory limitations of Transformers and enable their application to longer input sequences. One line of research focuses on various approximation techniques or compressing along the sequence dimension [see e.g. 24, 12, 14, 4, 42, 54, 36, 25]. Other works explored replacing attention [19, 20, 41, 23, 3, 57, 40, 53]. Another line of work explores partitioning the large hidden dimension of the feedforward network into parts and retrieving only one part per token [30, 48, 17, 26, 58, 60]. Additionally, extending the context by attending over states from previous sequences has been explored [13, 44], as well as combining local and global contexts [21, 11]. For a comprehensive review of these techniques, we recommend referring to the surveys by Tay et al. [51], Narang et al. [38], Tay et al. [50]. Several studies explored sharding large model on distributed devices tensor, data, or sequence parallelism [49, 16, 55, 27, 59, 31, 46]. Ours shares similarities with the sequence parallelism [27] where sequences are distributed across devices, in contrast, ours implements blockwise computation on sequences for each device. This creates an orthogonal relationship between our method and sequence parallelism, allowing for straightforward combination. In addition, our methodology is compatible with both tensor and data parallelism. Another direction\ninvolves computing exact self-attention in a blockwise manner using the tiling technique [37]. This approach has led to the development of memory efficient attention mechanisms [14,42]. In line with these advancements, our work falls into this category. We propose computing both the feedforward network and self-attention in a blockwise manner, resulting in a significant reduction in memory requirements. ## 7 Conclusion\n\nIn conclusion, we propose a blockwise parallelization approach to reduce the memory requirements of Transformers, the backbone of state-of-the-art NLP models. Our approach enables processing longer input sequences while maintaining or improving performance. Through extensive experiments, we demonstrate its effectiveness, achieving up to 4 x memory reduction than memory-efficient Transformers. Our contributions include a practical method for large context sizes in large Transformer models. With the increasing capability of hardware, larger models and longer context length are widely used in AI research. At the same time, as we are pushing up against physics and fabrication limits, it is more important to design scaling approaches as efficient as possible to scale up large models and large context size. Our approach holds promise for training and evaluating complex models with longer input sequences, potentially driving new breakthroughs in machine learning research. Limitations and Future Work. Although our method achieves state-of-the-art low memory usage for Transformer models, it does have some limitations that need to be addressed:\n\n- Optimal performance. While our implementation prioritizes simplicity with high-level Jax operations, optimizing low-level operations is crucial for achieving optimal performance. In future work, we suggest considering porting our method to CUDA and OpenAI Triton to achieve minimal memory cost and maximum speedup. ## Acknowledgements\n\nThis project is supported in part by ONR under N00014-21-1-2769. We thank the members of the Berkeley Robot Learning Lab and Berkeley AI Lab for their valuable discussions. We thank Tri Dao at Stanford for the valuable discussions on strengthening BPT. We thank Google TPU Research Cloud for granting us access to TPUs. We also express our appreciation to Anselm Levskaya, Markus Rabe, Federico Lebron, and Sharad Vikram at Google for their insightful discussions and suggestions on optimizing large transformers. In particular, we thank Anselm for his discussions on reducing memory cost, XLA, and training large models. We also appreciate the valuable suggestions on optimizing memory efficient transformers provided by Markus and Federico, as well as the valuable discussions with Sharad on implementing BPT with Triton and Jax Pallas. ## References\n\n[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. Flamingo: a visual language model for few-shot learning. Advances in Neural Information Processing Systems, 35: 23716-23736, 2022. [2] Kapathy Andrej. GitHub - karpathy/nanoGPT: The simplest, fastest repository for training/finetuning medium-sized GPTs. - github.com. https://github.com/karpathy/nanoGPT, 2023. [Accessed 16-May-2023]. [3] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv preprint arXiv:2102.08602, 2021. [4] Iz Beltagy, Matthew E Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [6] Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34:15084-15097, 2021. [7] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. [8] Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv preprint arXiv:1604.06174, 2016. [9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: A jointly-scaled multilingual language-image model.\n```\n\n#### 2. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.93)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 4/32 (Score: 0.93)*\n\n```\nA primary line of work addresses the training efficiency problem. Given the attention to computation's quadratic complexity during training, developing a long-context LLM is both a computational and memory challenge. Solutions have ranged from system-focused optimizations like FlashAttention (Dao et al., 2022; Dao, 2023), which accelerates attention computation and reduces memory footprint, to approximate attention methods (Zaheer et al.\n```\n\n#### 3. Ring Attention with Blockwise Transformers for Near-Infinite Context (Avg. Score: 0.89)\n\n*Hao Liu, Matei Zaharia, Pieter Abbeel*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 68  (*Influential: 7*)\n\n**TL;DR:** This work presents a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention.\n\n**Abstract:** Transformers have emerged as the architecture of choice for many state-of-the-art AI models, showcasing exceptional performance across a wide range of AI applications. However, the memory demands imposed by Transformers limit their ability to handle long sequences, thereby posing challenges in utilizing videos, actions, and other long-form sequences and modalities in complex environments. We present a novel approach, Ring Attention with Blockwise Transformers (Ring Attention), which leverages blockwise computation of self-attention and feedforward to distribute long sequences across multiple devices while fully overlapping the communication of key-value blocks with the computation of blockwise attention. Our approach enables training and inference of sequences that are up to device count times longer than those achievable by prior memory-efficient Transformers, without resorting to approximations or incurring additional communication and computation overheads. Extensive experiments on language modeling and reinforcement learning tasks demonstrate the effectiveness of our approach in allowing millions of tokens context size and improving performance.\n\n##### *Relevant Chunk: No. 17/23 (Score: 0.89)*\n\n```\nAdvances in neural information processing systems, 2023. [24] Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. [25] MosaicML. Introducing mpt-7b: A new standard for open-source, commercially usable llms, 2023. URL https://www.mosaicml.com/blog/mpt-7b\n[26] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Fevry, Michael Matena, Karishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, et al. Do transformer modifications transfer across implementations and applications? arXiv preprint arXiv:2102.11972, 2021 . [27] Deepak Narayanan, Aaron Harlap, Amar Phanishayee, Vivek Seshadri, Nikhil R Devanur, Gregory R Ganger, Phillip B Gibbons, and Matei Zaharia. Pipedream: Generalized pipeline parallelism for dnn training. In Proceedings of the 27th ACM Symposium on Operating Systems Principles, pages 1-15, 2019. [28] Deepak Narayanan, Amar Phanishayee, Kaiyu Shi, Xie Chen, and Matei Zaharia. Memoryefficient pipeline-parallel dnn training. In International Conference on Machine Learning, pages 7937-7947. PMLR, 2021. [29] OpenAI. Gpt-4 technical report, 2023. [30] Markus N Rabe and Charles Staats. Self-attention does not need o(n2) memory. arXiv preprint arXiv:2112.05682, 2021. [31] Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models.\n```\n\n#### 4. Linear Attention Sequence Parallelism (Avg. Score: 0.73)\n\n*Weigao Sun, Zhen Qin, Dong Li, Xuyang Shen, Yu Qiao, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper designs an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP and enhances the practical efficiency of LASP by performing kernel fusion and intermediate state caching.\n\n**Abstract:** Sequence Parallel (SP) serves as a prevalent strategy to handle long sequences that exceed the memory limit of a single GPU. However, existing SP methods do not take advantage of linear attention features, resulting in sub-optimal parallelism efficiency and usability for linear attention-based language models. In this paper, we introduce Linear Attention Sequence Parallel (LASP), an efficient SP method tailored to linear attention-based language models. Specifically, we design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead of SP. We also enhance the practical efficiency of LASP by performing kernel fusion and intermediate state caching, making the implementation of LASP hardware-friendly on GPU clusters. Furthermore, we meticulously ensure the compatibility of sequence-level LASP with all types of batch-level data parallel methods, which is vital for distributed training on large clusters with long sequences and large batches. We conduct extensive experiments on two linear attention-based models with varying sequence lengths and GPU cluster sizes. LASP scales sequence length up to 4096K using 128 A100 80G GPUs on 1B models, which is 8 times longer than existing SP methods while being significantly faster. The code is available at https://github.com/OpenNLPLab/LASP.\n\n##### *Relevant Chunk: No. 21/24 (Score: 0.76)*\n\n```\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, \u0141., and Polosukhin, I. Attention is all you need. Advances in neural information processing systems, 30, 2017. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X. V., Mihaylov, T., Ott, M., Shleifer, S., Shuster, K., Simig, D., Koura, P. S., Sridhar, A., Wang, T., and Zettlemoyer, L. OPT: Open pre-trained transformer language models, 2022. Zhao, Y., Gu, A., Varma, R., Luo, L., Huang, C.-C., Xu, M., Wright, L., Shojanazeri, H., Ott, M., Shleifer, S., et al. Pytorch FSDP: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023. Zheng, L., Wang, C., and Kong, L. Linear complexity randomized self-attention mechanism. In International Conference on Machine Learning, pp. 27011-27041. PMLR, 2022. Zheng, L., Yuan, J., Wang, C., and Kong, L. Efficient attention via control variates. In International Conference on Learning Representations, 2023. URL https:// openreview.net/forum?id=G-uNfHKrj46. Zhou, B., Liu, J., Sun, W., Chen, R., Tomlin, C. J., and Yuan, Y. pbSGD: Powered stochastic gradient descent methods for accelerated non-convex optimization. In IJCAI, pp. $3258-3266,2020$. ## A. Appendix\n\n## A.1. Hardware and Software\n\nHardware. Our experimental configuration involves a maximum of 16 DGX-A100 servers, each equipped with 8 A100 GPUs, these GPUs are interconnected through NVSwitch, ensuring an inter-GPU bandwidth of 600GBps. For inter-node communication, we employ RoCE (RDMA over Converged Ethernet) technology, utilizing 8 RoCE RDMA adapters in each server.\n```\n\n##### *Relevant Chunk: No. 18/24 (Score: 0.70)*\n\n```\nCoRR, abs/2112.05682, 2021. URL https://arxiv.org/abs/2112.05682. Rajbhandari, S., Rasley, J., Ruwase, O., and He, Y. Zero: Memory optimizations toward training trillion parameter models, 2020. Shoeybi, M., Patwary, M., Puri, R., LeGresley, P., Casper, J., and Catanzaro, B. Megatron-LM: Training multibillion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Sun, W., Qin, Z., Sun, W., Li, S., Li, D., Shen, X., Qiao, Y., and Zhong, Y. CO2: Efficient distributed training with full communication-computation overlap.\n```\n\n\n\n---\n## Found 8 related papers from 2 external sources\n\n\n\nYour 1 raw search queries input to the search frame: gradient computation memory optimization hardware efficient implementation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 2. Revisiting Zeroth-Order Optimization for Memory-Efficient LLM Fine-Tuning: A Benchmark\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen*\n\n**TL;DR:** This study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance, and introduces novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity.\n\n**Abstract:** In the evolving landscape of natural language processing (NLP), fine-tuning pre-trained Large Language Models (LLMs) with first-order (FO) optimizers like SGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial memory overhead from back-propagation (BP) for FO gradient computation presents a significant challenge. Addressing this issue is crucial, especially for applications like on-device training where memory efficiency is paramount. This paper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a solution for reducing memory costs during LLM fine-tuning, building on the initial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work expands the exploration to a wider array of ZO optimization techniques, through a comprehensive, first-of-its-kind benchmarking study across five LLM families (Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five fine-tuning schemes. Our study unveils previously overlooked optimization principles, highlighting the importance of task alignment, the role of the forward gradient method, and the balance between algorithm complexity and fine-tuning performance. We further introduce novel enhancements to ZO optimization, including block-wise descent, hybrid training, and gradient sparsity. Our study offers a promising direction for achieving further memory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at https://github.com/ZO-Bench/ZO-LLM .\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 21  (*Influential: 3*)\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. CAME: Confidence-guided Adaptive Memory Efficient Optimization\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Yang Luo, Xiaozhe Ren, Zangwei Zheng, Zhuo Jiang, Xin Jiang, Yang You*\n\n**TL;DR:** This paper proposes CAME, a confidence-guided strategy to reduce the instability of existing memory efficient optimizers and proposes it to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods.\n\n**Abstract:** Adaptive gradient methods, such as Adam and LAMB, have demonstrated excellent performance in the training of large language models. Nevertheless, the need for adaptivity requires maintaining second-moment estimates of the per-parameter gradients, which entails a high cost of extra memory overheads. To solve this problem, several memory-efficient optimizers (e.g., Adafactor) have been proposed to obtain a drastic reduction in auxiliary memory usage, but with a performance penalty. In this paper, we first study a confidence-guided strategy to reduce the instability of existing memory efficient optimizers. Based on this strategy, we propose CAME to simultaneously achieve two goals: fast convergence as in traditional adaptive methods, and low memory usage as in memory-efficient methods. Extensive experiments demonstrate the training stability and superior performance of CAME across various NLP tasks such as BERT and GPT-2 training. Notably, for BERT pre-training on the large batch size of 32,768, our proposed optimizer attains faster convergence and higher accuracy compared with the Adam optimizer. The implementation of CAME is publicly available.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 1*)\n\n#### 5. DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Joya Chen, Kai Xu, Yifei Cheng, Angela Yao*\n\n**TL;DR:** Experiments show that DropIT can drop up to 90\\% of the intermediate tensor elements in fully-connected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks.\n\n**Abstract:** A standard hardware bottleneck when training deep neural networks is GPU memory. The bulk of memory is occupied by caching intermediate tensors for gradient computation in the backward pass. We propose a novel method to reduce this footprint - Dropping Intermediate Tensors (DropIT). DropIT drops min-k elements of the intermediate tensors and approximates gradients from the sparsified tensors in the backward pass. Theoretically, DropIT reduces noise on estimated gradients and therefore has a higher rate of convergence than vanilla-SGD. Experiments show that we can drop up to 90\\% of the intermediate tensor elements in fully-connected and convolutional layers while achieving higher testing accuracy for Visual Transformers and Convolutional Neural Networks on various tasks (e.g., classification, object detection, instance segmentation). Our code and models are available at https://github.com/chenjoya/dropit.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 3*)\n\n### 3 related papers from Papers with Code\n\n#### 1. Recall@k Surrogate Loss with Large Batches and Similarity Mixup\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Jiri Matas, Giorgos Tolias, Yash Patel*\n\n**Abstract:** This work focuses on learning deep visual representation models for retrieval by exploring the interplay between a new loss function, the batch size, and a new regularization approach. Direct optimization, by gradient descent, of an evaluation metric, is not possible when it is non-differentiable, which is the case for recall in retrieval. A differentiable surrogate loss for the recall is proposed in this work. Using an implementation that sidesteps the hardware constraints of the GPU memory, the method trains with a very large batch size, which is essential for metrics computed on the entire retrieval database. It is assisted by an efficient mixup regularization approach that operates on pairwise scalar similarities and virtually increases the batch size further. The suggested method achieves state-of-the-art performance in several image retrieval benchmarks when used for deep metric learning. For instance-level recognition, the method outperforms similar approaches that train using an approximation of average precision.\n\n**Proceeding:** cvpr-2022-1\n\n**Published:** 2021-08-25\n\n\n\n#### 2. Effective implementation of the High Performance Conjugate Gradient benchmark on GraphBLAS\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Anonymous*\n\n**Abstract:** Applications in High-Performance Computing (HPC) environments face challenges due to increasing complexity. Among them, the increasing usage of sparse data pushes the limits of data structures and programming models and hampers the efficient usage of existing, highly parallel hardware. The GraphBLAS specification tackles these challenges by proposing a set of data containers and primitives, coupled with a semantics based on abstract algebraic concepts: this allows multiple applications on sparse data to be described with a small set of primitives and benefit from the many optimizations of a compile-time-known algebraic specification. Among HPC applications, the High Performance Conjugate Gradient (HPCG) benchmark is an important representative of a large body of sparse workloads, and its structure poses several programmability and performance challenges. This work tackles them by proposing and evaluating an implementation on GraphBLAS of HPCG, highlighting the main changes to its kernels. The results for shared memory systems outperforms the reference, while results in distributed systems highlight fundamental limitations of GraphBLAS-compliant implementations, which suggests several future directions.\n\n**Published:** 2023-04-17\n\n\n\n#### 3. Improving Neural Network Training in Low Dimensional Random Bases\n\n*From Search Query: gradient computation memory optimization hardware efficient implementation*\n\n*Carlo Luschi, Zach Eaton-Rosen, Frithjof Gressmann*\n\n**Abstract:** Stochastic Gradient Descent (SGD) has proven to be remarkably effective in optimizing deep neural networks that employ ever-larger numbers of parameters. Yet, improving the efficiency of large-scale optimization remains a vital and highly active area of research. Recent work has shown that deep neural networks can be optimized in randomly-projected subspaces of much smaller dimensionality than their native parameter space. While such training is promising for more efficient and scalable optimization schemes, its practical application is limited by inferior optimization performance. Here, we improve on recent random subspace approaches as follows: Firstly, we show that keeping the random projection fixed throughout training is detrimental to optimization. We propose re-drawing the random subspace at each step, which yields significantly better performance. We realize further improvements by applying independent projections to different parts of the network, making the approximation more efficient as network dimensionality grows. To implement these experiments, we leverage hardware-accelerated pseudo-random number generation to construct the random projections on-demand at every optimization step, allowing us to distribute the computation of independent random directions across multiple workers with shared random seeds. This yields significant reductions in memory and is up to 10 times faster for the workloads in question.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-11-09\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on gradient computation, memory optimization, and hardware-efficient implementations, here are some key developments and techniques from the provided sources and additional insights:\n\n## Memory Optimization Techniques\n\n### Low-Rank Gradient Approximation\nThe GaLore algorithm, as described in, is a promising approach for reducing the memory footprint during training. It exploits the low-rank structure of the gradient matrix to project gradients into a lower-dimensional space, significantly reducing memory usage without compromising model performance. This method can be integrated into existing optimization algorithms like Adam, making it a versatile solution.\n\n### Memory-Efficient Optimizers\nThe Addax method, introduced in, combines the benefits of in-place Stochastic Gradient Descent (IP-SGD) and the Memory-Efficient Zeroth-order Optimizer (MeZO). Addax assigns data batches to either IP-SGD or MeZO based on memory needs, combining zeroth- and first-order gradient estimates to achieve faster convergence and better performance while maintaining a comparable memory footprint.\n\n### Gradient Checkpointing and Memory Offloading\nTechniques like gradient checkpointing and memory offloading, as mentioned in, can significantly reduce memory usage. Gradient checkpointing involves selectively storing activations during the forward pass and recomputing them during the backward pass, trading computational overhead for reduced memory consumption. Memory offloading strategies, such as ZeRO-Offload, move optimizer states and gradients to CPU memory when not in use, freeing up GPU memory.\n\n## Hardware-Efficient Implementations\n\n### Memristor-Based Accelerators\nThe use of memristor-based in-memory computing (IMC) for transformer networks, as discussed in, offers significant improvements in efficiency. This approach leverages RRAM-based compare and select logics and lookup tables for exponential and logarithmic functions, reducing power consumption and latency. Such hardware accelerators can be particularly beneficial for the softmax function, a critical component in transformer architectures.\n\n### Linear Attention Mechanisms\nLinear attention mechanisms, such as those described in the analysis note, achieve O(N) complexity while maintaining performance. These mechanisms can be implemented in a hardware-efficient manner, enabling faster training and inference times. For example, the Gated Linear Attention Transformers mentioned in the analysis note are especially effective at length generalization and can be trained with hardware-efficient methods.\n\n## Hybrid Attention Architectures and Adaptive Computation\n\n### Hybrid Attention\nCombining different attention mechanisms, such as retention mechanisms with linear attention, can offer both memory efficiency and improved performance. Hybrid attention architectures can leverage the strengths of various methods to achieve better generalization and scalability.\n\n### Adaptive Computation Paths\nImplementing adaptive computation paths based on sequence characteristics can further optimize memory usage and computational efficiency. This approach involves selectively applying different attention mechanisms or pruning strategies based on the input sequence length and other characteristics, as suggested in the analysis note.\n\n## Conclusion\n\nTo improve autoregressive language model design, researchers can focus on the following:\n\n- **Low-Rank Gradient Approximation**: Use GaLore or similar methods to reduce the memory footprint of gradients.\n- **Memory-Efficient Optimizers**: Implement Addax or other memory-efficient optimizers that combine zeroth- and first-order gradients.\n- **Hardware-Efficient Implementations**: Utilize memristor-based accelerators and linear attention mechanisms to enhance computational efficiency.\n- **Hybrid Attention Architectures**: Combine different attention mechanisms to achieve better memory efficiency and performance.\n- **Adaptive Computation Paths**: Implement adaptive pruning and computation strategies based on input sequence characteristics.\n\nThese approaches can help in reducing memory footprint, improving computational efficiency, and enhancing overall model performance and scalability.\n",
            "analysis": "Analysis Note - Round 2\n\n1. Summary of Analysis:\nFrom the search results, several promising directions for improving RetNet emerge:\n\na) Memory Efficiency:\n- Novel attention mechanisms like FlashAttention and Block-Attention offer significant memory savings\n- Adaptive KV-cache compression techniques show promise for reducing memory footprint\n- Constant memory attention blocks demonstrate potential for scalable architectures\n\nb) Computational Efficiency:\n- Linear attention mechanisms achieve O(N) complexity while maintaining performance\n- Gated mechanisms enable selective computation and improved expressiveness\n- Hardware-efficient implementations show significant speedups\n\n2. Useful References with Excerpts:\n\nFrom Gated Linear Attention:\n\"GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\nFrom Linear Transformers:\n\"We express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from O(N\u00b2) to O(N)\"\n\nFrom Memory Efficient Neural Processes:\n\"Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates\"\n\nFrom Dynamic Context Pruning:\n\"Our empirical findings demonstrate that we can effectively prune up to 80% of the context without significant performance degradation\"\n\n3. Key Insights and Analysis:\n\na) Memory Optimization Opportunities:\n- Combining constant memory attention with RetNet's retention mechanism\n- Implementing adaptive pruning for context management\n- Leveraging gated mechanisms for selective computation\n\nb) Performance Enhancement Strategies:\n- Linear attention formulations for improved scaling\n- Hardware-efficient implementations for faster training/inference\n- Length generalization through improved position encodings\n\nc) Integration Possibilities:\n- Hybrid attention combining retention and linear mechanisms\n- Adaptive computation paths based on sequence characteristics\n- Memory-efficient training through smart gradient handling\n\n4. Future Search Plan:\nNeed to investigate:\n1. Gradient computation and memory optimization during training\n2. Hardware-efficient implementations of attention mechanisms\n3. Hybrid attention architectures combining multiple mechanisms\n4. Position encoding improvements for better generalization\n\n5. References:\n\n[1] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. ICML 2023.\n\n[2] Katharopoulos, A., Vyas, A., Pappas, N., & Fleuret, F. (2020). Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. ICML 2020.\n\n[3] Feng, L., Tung, F., Hajimirsadeghi, H., Bengio, Y., & Ahmed, M. O. (2023). Memory Efficient Neural Processes via Constant Memory Attention Block. ICML 2023.\n\n[4] Anagnostidis, S., Pavllo, D., Biggio, L., Noci, L., Lucchi, A., & Hofmann, T. (2023). Dynamic Context Pruning for Efficient and Interpretable Autoregressive Transformers. NeurIPS 2023.\n\n[5] Deb, M., Deiseroth, B., Weinbach, S., Brack, M., Schramowski, P., & Kersting, K. (2023). AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation. NeurIPS 2023."
        },
        {
            "ready": false,
            "query": "blockwise retention computation streaming adaptation",
            "detail": "Analyze implementations of blockwise computation in retention mechanisms and streaming adaptations, focusing on techniques that maintain model performance while reducing memory and computational requirements.",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze implementations of blockwise computation in retention mechanisms and streaming adaptations, focusing on techniques that maintain model performance while reducing memory and computational requirements.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Retentive network: a successor to transformer for large language models (Avg. Score: 0.38)\n\n*Yutao Sun, Li Dong, Shaohan Huang, Shuming Ma, Yuqing Xia, Jilong Xue, Jianyong Wang, Furu Wei*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 143  (*Influential: 18*)\n\n**TL;DR:** This work proposes Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance, and proposes the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent.\n\n**Abstract:** In this work, we propose Retentive Network (RetNet) as a foundation architecture for large language models, simultaneously achieving training parallelism, low-cost inference, and good performance. We theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RetNet achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RetNet a strong successor to Transformer for large language models. Code will be available at https://aka.ms/retnet.\n\n##### *Relevant Chunk: No. 2/21 (Score: 0.75)*\n\n```\nWe theoretically derive the connection between recurrence and attention. Then we propose the retention mechanism for sequence modeling, which supports three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel representation allows for training parallelism. The recurrent representation enables low-cost $O(1)$ inference, which improves decoding throughput, latency, and GPU memory without sacrificing performance. The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks. Experimental results on language modeling show that RETNET achieves favorable scaling results, parallel training, low-cost deployment, and efficient inference. The intriguing properties make RETNET a strong successor to Transformer for large language models. Code will be available at https: / aka.ms/retnet. ![](https://cdn.mathpix.com/cropped/2024_09_12_fb33982bc7545f33a227g-01.jpg?height=571&width=1289&top_left_y=1639&top_left_x=407)\n\nFigure 1: Retentive network (RetNet) achieves low-cost inference (i.e., GPU memory, throughput, and latency), training parallelism, and favorable scaling curves compared with Transformer. Results of inference cost are reported with 8 k as input length. Figure 6 shows more results on different sequence lengths. [^0]\n## 1 Introduction\n\nTransformer $\\left[\\mathrm{VSP}^{+}\\right.$17] has become the de facto architecture for large language models $\\left[\\mathrm{BMR}^{+} 20\\right]$, which was initially proposed to overcome the sequential training issue of recurrent models [HS97]. However, training parallelism of Transformers is at the cost of inefficient inference, because of the $O(N)$ complexity per step and memory-bound key-value cache [Sha19], which renders Transformers unfriendly to deployment. The growing sequence length increases GPU memory consumption as well as latency and reduces inference speed. Numerous efforts have continued to develop the next-generation architecture, aiming at retaining training parallelism and competitive performance as Transformers while having efficient $O(1)$ inference. It is challenging to achieve the above goals simultaneously, i.e., the so-called \"impossible triangle\" as shown in Figure 2. There have been three main strands of research. First, linearized attention [KVPF20] approximates standard attention scores $\\exp (\\boldsymbol{q} \\cdot \\boldsymbol{k})$ with kernels $\\phi(\\boldsymbol{q}) \\cdot \\phi(\\boldsymbol{k})$, so that autoregressive inference can be rewritten in a recurrent form. However, the modeling capability and performance are worse than Transformers, which hinders the method's popularity. The second strand returns to recurrent models for efficient inference while sacrificing training parallelism. As a remedy, element-wise operators $\\left[\\mathrm{PAA}^{+} 23\\right]$ are used for acceleration, however, representation capacity and performance are harmed. The third line of research explores replacing attention with other mechanisms, such as S 4 [GGR21], and its variants [DFS ${ }^{+}$22, $\\mathrm{PMN}^{+}$23]. None of the previous work can break through the impossible triangle, resulting in no clear winner compared with Transformers. In this work, we propose retentive networks (RetNet), achieving low-cost inference, efficient longsequence modeling, Transformer-comparable performance, and parallel model training simultaneously. Specifically, we introduce a multi-scale retention mechanism to substitute multi-head attention, which has three computation paradigms, i.e., parallel, recurrent, and chunkwise recurrent representations. First, the parallel representation empowers training parallelism to utilize GPU devices fully. Second, the recurrent representation enables efficient $O(1)$ inference in terms of memory and computation. The deployment cost and latency can be significantly reduced. Moreover, the implementation is greatly simplified without key-value cache tricks. Third, the chunkwise recurrent representation can perform efficient long-sequence modeling. We parallelly encode each local block for computation speed while recurrently encoding the global blocks to save GPU memory. We conduct extensive experiments to compare RetNet with Transformer and its variants. Experimental results on language modeling show that RetNet is consistently competitive in terms of both scaling curves and in-context learning. Moreover, the inference cost of RetNet is length-invariant. For a 7B model and 8 k sequence length, RetNet decodes $8.4 \\times$ faster and saves $70 \\%$ of memory than Transformers with key-value caches. During training, RetNet also achieves 25-50\\% memory saving and $7 \\times$ acceleration than standard Transformer and an advantage towards highly-optimized FlashAttention $\\left[\\mathrm{DFE}^{+}\\right.$22]. Besides, RetNet's inference latency is insensitive to batch size, allowing enormous throughput. The intriguing properties make RetNet a strong successor to Transformer for large language models. ## 2 Retentive Networks\n\nRetentive network (RetNet) is stacked with $L$ identical blocks, which follows a similar layout (i.e., residual connection, and pre-LayerNorm) as in Transformer [VSP ${ }^{+}$17]. Each RetNet block contains two modules: a multi-scale retention (MSR) module, and a feed-forward network (FFN) module. We introduce the MSR module in the following sections. Given an input sequence $x=x_{1} \\cdots x_{|x|}$, RetNet encodes the sequence in an autoregressive way. The input vectors $\\left\\{\\boldsymbol{x}_{i}\\right\\}_{i=1}^{|x|}$ is first packed into $X^{0}=\\left[\\boldsymbol{x}_{1}, \\cdots, \\boldsymbol{x}_{|x|}\\right] \\in \\mathbb{R}^{|x| \\times d_{\\text {model }}}$, where $d_{\\text {model }}$ is hidden dimension. Then we compute contextualized vector representations $X^{l}=\\operatorname{RetNet}_{l}\\left(X^{l-1}\\right), l \\in[1, L]$. ### 2.1 Retention\n\nIn this section, we introduce the retention mechanism that has a dual form of recurrence and parallelism. So we can train the models in a parallel way while recurrently conducting inference. Given input $X \\in \\mathbb{R}^{|x| \\times d_{\\text {model }}}$, we project it to one-dimensional function $v(n)=X_{n} \\cdot \\boldsymbol{w}_{V}$. Consider a sequence modeling problem that maps $v(n) \\mapsto o(n)$ through states $s_{n}$. Let $v_{n}, o_{n}$ denote $v(n), o(n)$ for simplicity. We formulate the mapping in a recurrent manner:\n\n$$\n\\begin{array}{lr}\ns_{n}=A s_{n-1}+K_{n}^{\\top} v_{n}, & A \\in \\mathbb{R}^{d \\times d}, K_{n} \\in \\mathbb{R}^{1 \\times d} \\\\\no_{n}=Q_{n} s_{n}=\\sum_{m=1}^{n} Q_{n} A^{n-m} K_{m}^{\\top} v_{m}, & Q_{n} \\in \\mathbb{R}^{1 \\times d}\n\\end{array}\n$$\n\nwhere we map $v_{n}$ to the state vector $s_{n}$, and then implement a linear transform to encode sequence information recurrently. Next, we make the projection $Q_{n}, K_{n}$ content-aware:\n\n$$\nQ=X W_{Q}, \\quad K=X W_{K}\n$$\n\nwhere $W_{Q}, W_{K} \\in \\mathbb{R}^{d \\times d}$ are learnable matrices. We diagonalize the matrix $A=\\Lambda\\left(\\gamma e^{i \\theta}\\right) \\Lambda^{-1}$, where $\\gamma, \\theta \\in \\mathbb{R}^{d}$. Then we obtain $A^{n-m}=$ $\\Lambda\\left(\\gamma e^{i \\theta}\\right)^{n-m} \\Lambda^{-1}$. By absorbing $\\Lambda$ into $W_{Q}$ and $W_{K}$, we can rewrite Equation (1) as:\n\n$$\n\\begin{aligned}\no_{n} & =\\sum_{m=1}^{n} Q_{n}\\left(\\gamma e^{i \\theta}\\right)^{n-m} K_{m}^{\\top} v_{m} \\\\\n& =\\sum_{m=1}^{n}\\left(Q_{n}\\left(\\gamma e^{i \\theta}\\right)^{n}\\right)\\left(K_{m}\\left(\\gamma e^{i \\theta}\\right)^{-m}\\right)^{\\boldsymbol{\\top}} v_{m}\n\\end{aligned}\n$$\n\nwhere $Q_{n}\\left(\\gamma e^{i \\theta}\\right)^{n}, K_{m}\\left(\\gamma e^{i \\theta}\\right)^{-m}$ is known as xPos [SDP ${ }^{+}$22], i.e., a relative position embedding proposed for Transformer. We further simplify $\\gamma$ as a scalar, Equation (3) becomes:\n\n$$\no_{n}=\\sum_{m=1}^{n} \\gamma^{n-m}\\left(Q_{n} e^{i n \\theta}\\right)\\left(K_{m} e^{i m \\theta}\\right)^{\\dagger} v_{m}\n$$\n\nwhere ${ }^{\\dagger}$ is the conjugate transpose. The formulation is easily parallelizable within training instances. In summary, we start with recurrent modeling as shown in Equation (1), and then derive its parallel formulation in Equation (4). We consider the original mapping $v(n) \\mapsto o(n)$ as vectors and obtain the retention mechanism as follows. The Parallel Representation of Retention As shown in Figure 3a, the retention layer is defined as:\n\n$$\n\\begin{array}{r}\nQ=\\left(X W_{Q}\\right) \\odot \\Theta, \\quad K=\\left(X W_{K}\\right) \\odot \\bar{\\Theta}, \\quad V=X W_{V} \\\\\n\\Theta_{n}=e^{i n \\theta}, \\quad D_{n m}= \\begin{cases}\\gamma^{n-m}, & n \\geq m \\\\\n0, & n<m\\end{cases}\n\\end{array}\n$$\n\n$$\n\\operatorname{Retention}(X)=\\left(Q K^{\\boldsymbol{\\top}} \\odot D\\right) V\n$$\n\nwhere $\\bar{\\Theta}$ is the complex conjugate of $\\Theta$, and $D \\in \\mathbb{R}^{|x| \\times|x|}$ combines causal masking and exponential decay along relative distance as one matrix. Similar to self-attention, the parallel representation enables us to train the models with GPUs efficiently. ![](https://cdn.mathpix.com/cropped/2024_09_12_fb33982bc7545f33a227g-04.jpg?height=520&width=1245&top_left_y=235&top_left_x=526)\n\nFigure 3: Dual form of RetNet. \"GN\" is short for GroupNorm. The Recurrent Representation of Retention As shown in Figure 3b, the proposed mechanism can also be written as recurrent neural networks (RNNs), which is favorable for inference. For the $n$-th timestep, we recurrently obtain the output as:\n\n$$\n\\begin{aligned}\n& S_{n}=\\gamma S_{n-1}+K_{n}^{\\top} V_{n} \\\\\n& \\operatorname{Retention}\\left(X_{n}\\right)=Q_{n} S_{n}, \\quad n=1, \\cdots,|x|\n\\end{aligned}\n$$\n\nwhere $Q, K, V, \\gamma$ are the same as in Equation (5). The Chunkwise Recurrent Representation of Retention A hybrid form of parallel representation and recurrent representation is available to accelerate training, especially for long sequences. We divide the input sequences into chunks. Within each chunk, we follow the parallel representation (Equation (5)) to conduct computation. In contrast, cross-chunk information is passed following the recurrent representation (Equation (6)). Specifically, let $B$ denote the chunk length. We compute the retention output of the $i$-th chunk via:\n\n$$\n\\begin{aligned}\n& Q_{[i]}=Q_{B i: B(i+1)}, \\quad K_{[i]}=K_{B i: B(i+1)}, \\quad V_{[i]}=V_{B i: B(i+1)} \\\\\n& R_{i}=K_{[i]}^{\\top}\\left(V_{[i]} \\odot \\zeta\\right)+\\gamma^{B} R_{i-1}, \\quad \\zeta_{i j}=\\gamma^{B-i-1} \\\\\n& \\operatorname{Retention}\\left(X_{[i]}\\right)= \\underbrace{\\left.Q_{[i]} K_{[i]}^{\\top} \\odot D\\right) V_{[i]}}_{\\text {Inner-Chunk }}+\\underbrace{\\left(Q_{[i]} R_{i-1}\\right) \\odot \\xi}_{\\text {Cross-Chunk }}, \\quad \\xi_{i j}=\\gamma^{i+1}\n\\end{aligned}\n$$\n\nwhere $[i]$ indicates the $i$-th chunk, i.e., $x_{[i]}=\\left[x_{(i-1) B+1}, \\cdots, x_{i B}\\right]$. ### 2.2 Gated Multi-Scale Retention\n\nWe use $h=d_{\\text {model }} / d$ retention heads in each layer, where $d$ is the head dimension. The heads use different parameter matrices $W_{Q}, W_{K}, W_{V} \\in \\mathbb{R}^{d \\times d}$. Moreover, multi-scale retention (MSR) assigns different $\\gamma$ for each head. For simplicity, we set $\\gamma$ identical among different layers and keep them fixed. In addition, we add a swish gate [HG16, RZL17] to increase the non-linearity of retention layers. Formally, given input $X$, we define the layer as:\n\n$$\n\\begin{aligned}\n& \\gamma=1-2^{-5-\\operatorname{arange}(0, h)} \\in \\mathbb{R}^{h} \\\\\n& \\operatorname{head}_{i}=\\operatorname{Retention}\\left(X, \\gamma_{i}\\right) \\\\\n& Y=\\operatorname{GroupNorm}_{h}\\left(\\operatorname{Concat}\\left(\\operatorname{head}_{1}, \\cdots, \\text { head }_{h}\\right)\\right) \\\\\n& \\operatorname{MSR}(X)=\\left(\\operatorname{swish}\\left(X W_{G}\\right) \\odot Y\\right) W_{O}\n\\end{aligned}\n$$\n\nwhere $W_{G}, W_{O} \\in \\mathbb{R}^{d_{\\text {model }} \\times d_{\\text {model }}}$ are learnable parameters, and GroupNorm [WH18] normalizes the output of each head, following SubLN proposed in [SPP ${ }^{+}$19].\n```\n\n##### *Relevant Chunk: No. 7/21 (Score: 0.01)*\n\n```\nIn addition, we discuss the training and inference efficiency of the compared methods. Let $d$ denote the hidden dimension, and $n$ the sequence length. For training, RWKV's token-mixing complexity is $O(d n)$ while Hyena's is $O(d n \\log n)$ with Fast Fourier Transform acceleration. The above two methods reduce training FLOPS via employing element-wise operators to trade-off modeling capacity. In comparison with retention, the chunk-wise recurrent representation is $O(d n(b+h))$, where $b$ is the chunk size, $h$ is the head dimension, and we usually set $b=512, h=256$. For either large model size (i.e., larger $d$ ) or sequence length, the additional $b+h$ has negligible effects. So the RetNet training is quite efficient without sacrificing the modeling performance. For inference, among the compared efficient architectures, Hyena has the same complexity (i.e., $O(n)$ per step) as Transformer while the others can perform $O(1)$ decoding. ### 3.6 Ablation Studies\n\nWe ablate various design choices of RetNet and report the language modeling results in Table 6 . The evaluation settings and metrics are the same as in Section 3.5. Architecture We ablate the swish gate and GroupNorm as described in Equation (8). Table 6 shows that the above two components improve the final performance. Firstly, the gating module is essential for enhancing non-linearity and improving model capability. Notice that we use the same parameter allocation as Transformers after removing the gate. Secondly, group normalization in retention balances the variances of multi-head outputs, which improves training stability and language modeling results. Multi-Scale Decay Equation (8) shows that we use different $\\gamma$ as the decay rates for the retention heads. In the ablation studies, we examine removing $\\gamma$ decay (i.e., \" $-\\gamma$ decay\") and applying the same decay rate across heads (i.e., \"- multi-scale decay\"). Specifically, ablating $\\gamma$ decay is equivalent to $\\gamma=1$. In the second setting, we set $\\gamma=127 / 128$ for all heads. Table 6 indicates that both the decay mechanism and using multiple decay rates can improve the language modeling performance. Head Dimension From the recurrent perspective of Equation (1), the head dimension implies the memory capacity of hidden states. In the ablation study, we reduce the default head dimension from\n256 to 64 , i.e., 64 for queries and keys, and 128 for values. We keep the hidden dimension $d_{\\text {model }}$ the same so the number of heads increases. Experimental results in Table 6 show that the larger head dimension achieves better performance. ## 4 Conclusion\n\nIn this work, we propose retentive networks (RetNet) for sequence modeling, which enables various representations, i.e., parallel, recurrent, and chunkwise recurrent. RetNet achieves significantly better inference efficiency (in terms of memory, speed, and latency), favorable training parallelization, and competitive performance compared with Transformers. The above advantages make RetNet an ideal successor to Transformers for large language models, especially considering the deployment benefits brought by the $O(1)$ inference complexity. In the future, we would like to scale up RetNet in terms of model size $\\left[\\mathrm{CDH}^{+}\\right.$22] and training steps. Moreover, retention can efficiently work with structured prompting $\\left[\\mathrm{HSD}^{+} 22 \\mathrm{~b}\\right]$ by compressing long-term memory. We will also use RetNet as the backbone architecture to train multimodal large language models [HSD ${ }^{+} 22 \\mathrm{a}, \\mathrm{HDW}^{+} 23, \\mathrm{PWD}^{+} 23$ ]. In addition, we are interested in deploying RetNet models on various edge devices, such as mobile phones. ## Acknowledgement\n\nWe would like to acknowledge Jiayu Ding, Songlin Yang, and colleagues from MSRA System Group for the helpful discussions. ## References\n\n[BKH16] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877-1901. Curran Associates, Inc., 2020. $\\left[\\mathrm{BZB}^{+}\\right.$20] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.30)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 2/53 (Score: 0.56)*\n\n```\nlength $16 \\mathrm{~K}, 61.4 \\%$ accuracy) and Path- 256 (seq. length $64 \\mathrm{~K}, 63.1 \\%$ accuracy). ## 1 Introduction\n\nTransformer models 82 have emerged as the most widely used architecture in applications such as natural language processing and image classification. Transformers have grown larger [5 and deeper [83, but equipping them with longer context remains difficult [80, since the self-attention module at their heart has time and memory complexity quadratic in sequence length. An important question is whether making attention faster and more memory-efficient can help Transformer models address their runtime and memory challenges for long sequences. Many approximate attention methods have aimed to reduce the compute and memory requirements of attention. These methods range from sparse-approximation [51, 74] to low-rank approximation [12, 50, 84, and their combinations [3, 9, 92]. Although these methods reduce the compute requirements to linear or near-linear in sequence length, many of them do not display wall-clock speedup against standard attention and have not gained wide adoption. One main reason is that they focus on FLOP reduction (which may not correlate with wall-clock speed) and tend to ignore overheads from memory access (IO). In this paper, we argue that a missing principle is making attention algorithms $I O$-aware [1]-that is, carefully accounting for reads and writes to different levels of fast and slow memory (e.g., between fast GPU on-chip SRAM and relatively slow GPU high bandwidth memory, or HBM [45, Figure 1 left). On modern\n\n![](https://cdn.mathpix.com/cropped/2024_09_12_f878cbc7a32833354fa8g-02.jpg?height=565&width=1403&top_left_y=238&top_left_x=361)\n\nFigure 1: Left: FlashAttention uses tiling to prevent materialization of the large $N \\times N$ attention matrix (dotted box) on (relatively) slow GPU HBM. In the outer loop (red arrows), FlashAttention loops through blocks of the $\\mathbf{K}$ and $\\mathbf{V}$ matrices and loads them to fast on-chip SRAM. In each block, FlashAttention loops over blocks of $\\mathbf{Q}$ matrix (blue arrows), loading them to SRAM, and writing the output of the attention computation back to HBM. Right: Speedup over the PyTorch implementation of attention on GPT-2. FlashAttention does not read and write the large $N \\times N$ attention matrix to HBM, resulting in an $7.6 \\times$ speedup on the attention computation. GPUs, compute speed has out-paced memory speed 61, 62, 63, and most operations in Transformers are bottlenecked by memory accesses [43]. IO-aware algorithms have been critical for similar memory-bound operations, when reading and writing data can account for a large portion of the runtime - such as database joins [71, image processing [70, numerical linear algebra [4], and more [40, 85]. However, common Python interfaces to deep learning such as PyTorch and Tensorflow do not allow fine-grained control of memory access. We propose FlashAttention, a new attention algorithm that computes exact attention with far fewer memory accesses. Our main goal is to avoid reading and writing the attention matrix to and from HBM. This requires (i) computing the softmax reduction without access to the whole input (ii) not storing the large intermediate attention matrix for the backward pass. We apply two well-established techniques to address these challenges. (i) We restructure the attention computation to split the input into blocks and make several passes over input blocks, thus incrementally performing the softmax reduction (also known as tiling). (ii) We store the softmax normalization factor from the forward pass to quickly recompute attention on-chip in the backward pass, which is faster than the standard approach of reading the intermediate attention matrix from HBM. We implement FlashAttention in CUDA to achieve fine-grained control over memory access and fuse all the attention operations into one GPU kernel. Even with the increased FLOPs due to recomputation, our algorithm both runs faster (up to 7.6 x on GPT-2 67, Figure 1right) and uses less memory-linear in sequence length - than standard attention, thanks to the massively reduced amount of HBM access. We analyze the IO complexity [1] of FlashAttention, proving that it requires $O\\left(N^{2} d^{2} M^{-1}\\right) \\mathrm{HBM}$ accesses where $d$ is the head dimension and $M$ is the size of SRAM, as compared to $\\Omega\\left(N d+N^{2}\\right)$ of standard attention. For typical values of $d$ and $M$, FlashAttention requires many times fewer HBM accesses compared to standard attention (up to $9 \\times$ fewer, as shown in Fig. 21. Moreover, we provide a lower bound, showing that no exact attention algorithm can asymptotically improve on the number of HBM accesses over all SRAM sizes. We also show that FlashAttention can serve as a useful primitive for realizing the potential of approximate attention algorithms by overcoming their issues with memory access overhead. As a proof of concept, we implement block-sparse FlashAttention, a sparse attention algorithm that is $2-4 \\times$ faster than even FlashAttention, scaling up to sequence length of 64 k . We prove that block-sparse FlashAttention has better IO complexity than FlashAttention by a factor proportional to the sparsity ratio. We discuss further extensions to other operations (attention on multi-GPU, kernel regression, block-sparse matrix\nmultiply) in Section 5 . We open-source FlashAttention to make it easier to build on this primitive 1\nWe empirically validate that FlashAtTEntion speeds up model training and improves model quality by modeling longer context. We also benchmark the runtime and memory footprint of FlashAttention and block-sparse FlashAttention compared to prior attention implementations. - Faster Model Training. FlashAttention trains Transformer models faster in wall-clock time. We train BERT-large (seq. length 512) $15 \\%$ faster than the training speed record in MLPerf 1.1 [58], GPT2 (seq. length 1K) $3 \\times$ faster than baseline implementations from HuggingFace 87] and Megatron-LM 77], and long-range arena (seq. length $1 \\mathrm{~K}-4 \\mathrm{~K}$ ) $2.4 \\times$ faster than baselines. - Higher Quality Models. FlashAttention scales Transformers to longer sequences, which improves their quality and enables new capabilities. We observe a 0.7 improvement in perplexity on GPT-2 and 6.4 points of lift from modeling longer sequences on long-document classification [13]. FlashAttention enables the first Transformer that can achieve better-than-chance performance on the Path-X [80] challenge, solely from using a longer sequence length (16K). Block-sparse FlashAttention enables a Transformer to scale to even longer sequences $(64 \\mathrm{~K})$, resulting in the first model that can achieve better-than-chance performance on Path-256. - Benchmarking Attention. FlashAttention is up to $3 \\times$ faster than the standard attention implementation across common sequence lengths from 128 to 2 K and scales up to 64 K . Up to sequence length of 512 , FlashAttention is both faster and more memory-efficient than any existing attention method, whereas for sequence length beyond 1K, some approximate attention methods (e.g., Linformer) start to become faster. On the other hand, block-sparse FlashAttention is faster than all existing approximate attention methods that we know of. ## 2 Background\n\nWe provide some background on the performance characteristics of common deep learning operations on modern hardware (GPUs). We also describe the standard implementation of attention. ### 2.1 Hardware Performance\n\nWe focus here on GPUs. Performance on other hardware accelerators are similar 46, 48. GPU Memory Hierarchy. The GPU memory hierarchy (Fig. 1 left) comprises multiple forms of memory of different sizes and speeds, with smaller memory being faster. As an example, the A100 GPU has $40-80 \\mathrm{~GB}$ of high bandwidth memory (HBM) with bandwidth $1.5-2.0 \\mathrm{~TB} / \\mathrm{s}$ and 192 KB of on-chip SRAM per each of 108 streaming multiprocessors with bandwidth estimated around 19TB/s 44, 45. The on-chip SRAM is an order of magnitude faster than HBM but many orders of magnitude smaller in size. As compute has gotten faster relative to memory speed 61, 62, 63, operations are increasingly bottlenecked by memory (HBM) accesses. Thus exploiting fast SRAM becomes more important. Execution Model. GPUs have a massive number of threads to execute an operation (called a kernel). Each kernel loads inputs from HBM to registers and SRAM, computes, then writes outputs to HBM. Performance characteristics. Depending on the balance of computation and memory accesses, operations can be classified as either compute-bound or memory-bound. This is commonly measured by the arithmetic intensity [85], which is the number of arithmetic operations per byte of memory access. 1. Compute-bound: the time taken by the operation is determined by how many arithmetic operations there are, while time accessing HBM is much smaller. Typical examples are matrix multiply with large inner dimension, and convolution with large number of channels. 2. Memory-bound: the time taken by the operation is determined by the number of memory accesses, while time spent in computation is much smaller. Examples include most other operations: elementwise (e.g., activation, dropout), and reduction (e.g., sum, softmax, batch norm, layer norm). Kernel fusion. The most common approach to accelerate memory-bound operations is kernel fusion: if there are multiple operations applied to the same input, the input can be loaded once from HBM, instead of multiple times for each operation. Compilers can automatically fuse many elementwise operations 53, 65, 75. [^0]However, in the context of model training, the intermediate values still need to be written to HBM to save for the backward pass, reducing the effectiveness of naive kernel fusion. ### 2.2 Standard Attention Implementation\n\nGiven input sequences $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ where $N$ is the sequence length and $d$ is the head dimension, we want to compute the attention output $\\mathbf{O} \\in \\mathbb{R}^{N \\times d}$ :\n\n$$\n\\mathbf{S}=\\mathbf{Q K}^{\\top} \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{P}=\\operatorname{softmax}(\\mathbf{S}) \\in \\mathbb{R}^{N \\times N}, \\quad \\mathbf{O}=\\mathbf{P V} \\in \\mathbb{R}^{N \\times d}\n$$\n\nwhere softmax is applied row-wise. Standard attention implementations materialize the matrices $\\mathbf{S}$ and $\\mathbf{P}$ to HBM, which takes $O\\left(N^{2}\\right)$ memory. Often $N \\gg d$ (e.g., for GPT2, $N=1024$ and $d=64$ ). We describe the standard attention implementation in Algorithm 0. As some or most of the operations are memory-bound (e.g., softmax), the large number of memory accesses translates to slow wall-clock time. This problem is exacerbated by other elementwise operations applied to the attention matrix, such as masking applied to $\\mathbf{S}$ or dropout applied to $\\mathbf{P}$. As a result, there have been many attempts to fuse several elementwise operations, such as fusing masking with softmax [77. In Section 3.2, we will show that the standard attention implementation performs HBM accesses quadratic in the sequence length $N$. We also compare the number of FLOPs and number of HBM accesses of standard attention and of our method (FlashAttention). ```\nAlgorithm 0 Standard Attention Implementation\nRequire: Matrices \\(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}\\) in HBM. : Load \\(\\mathbf{Q}, \\mathbf{K}\\) by blocks from HBM, compute \\(\\mathbf{S}=\\mathbf{Q K}^{\\top}\\), write \\(\\mathbf{S}\\) to HBM. : Read \\(\\mathbf{S}\\) from HBM, compute \\(\\mathbf{P}=\\operatorname{softmax}(\\mathbf{S})\\), write \\(\\mathbf{P}\\) to HBM. : Load \\(\\mathbf{P}\\) and \\(\\mathbf{V}\\) by blocks from HBM, compute \\(\\mathbf{O}=\\mathbf{P V}\\), write \\(\\mathbf{O}\\) to HBM. Return \\(\\mathbf{O}\\). ```\n\n\n## 3 FlashAttention: Algorithm, Analysis, and Extensions\n\nWe show how to compute exact attention with fewer HBM reads/writes and without storing large intermediate matrices for the backward pass. This yields an attention algorithm that is both memory efficient and faster in wall-clock time. We analyze its IO complexity, showing that our method requires much fewer HBM accesses compared to standard attention. We further show that FlashAtTention can serve as a useful primitive by extending it to handle block-sparse attention. We focus here on the forward pass for ease of exposition; Appendix B contains details for the backward. ### 3.1 An Efficient Attention Algorithm With Tiling and Recomputation\n\nGiven the inputs $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ in HBM, we aim to compute the attention output $\\mathbf{O} \\in \\mathbb{R}^{N \\times d}$ and write it to HBM. Our goal is to reduce the amount of HBM accesses (to sub-quadratic in $N$ ). We apply two established techniques (tiling, recomputation) to overcome the technical challenge of computing exact attention in sub-quadratic HBM accesses. We describe this in Algorithm 1. The main idea is that we split the inputs $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ into blocks, load them from slow HBM to fast SRAM, then compute the attention output with respect to those blocks. By scaling the output of each block by the right normalization factor before adding them up, we get the correct result at the end. Tiling. We compute attention by blocks. Softmax couples columns of $\\mathbf{K}$, so we decompose the large softmax with scaling [51, 60, 66]. For numerical stability, the softmax of vector $x \\in \\mathbb{R}^{B}$ is computed as:\n\n$$\nm(x):=\\max _{i} \\quad x_{i}, \\quad f(x):=\\left[\\begin{array}{lll}\ne^{x_{1}-m(x)} & \\ldots & e^{x_{B}-m(x)}\n\\end{array}\\right], \\quad \\ell(x):=\\sum_{i} f(x)_{i}, \\quad \\operatorname{softmax}(x):=\\frac{f(x)}{\\ell(x)}\n$$\n\nFor vectors $x^{(1)}, x^{(2)} \\in \\mathbb{R}^{B}$, we can decompose the softmax of the concatenated $x=\\left[x^{(1)} x^{(2)}\\right] \\in \\mathbb{R}^{2 B}$ as:\n\n$$\n\\begin{aligned}\n& m(x)=m\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=\\max \\left(m\\left(x^{(1)}\\right), m\\left(x^{(2)}\\right)\\right), \\quad f(x)=\\left[e^{m\\left(x^{(1)}\\right)-m(x)} f\\left(x^{(1)}\\right) \\quad e^{m\\left(x^{(2)}\\right)-m(x)} f\\left(x^{(2)}\\right)\\right] \\\\\n& \\ell(x)=\\ell\\left(\\left[x^{(1)} x^{(2)}\\right]\\right)=e^{m\\left(x^{(1)}\\right)-m(x)} \\ell\\left(x^{(1)}\\right)+e^{m\\left(x^{(2)}\\right)-m(x)} \\ell\\left(x^{(2)}\\right), \\quad \\operatorname{softmax}(x)=\\frac{f(x)}{\\ell(x)}\n\\end{aligned}\n$$\n\nTherefore if we keep track of some extra statistics $(m(x), \\ell(x))$, we can compute softmax one block at a time ${ }^{2}$ We thus split the inputs $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ into blocks (Algorithm 1 line 3), compute the softmax values along with extra statistics (Algorithm 1 line 10, and combine the results (Algorithm 1 line 12).\n```\n\n##### *Relevant Chunk: No. 47/53 (Score: 0.04)*\n\n```\nIn terms of total memory required, both methods offer substantial memory saving. The second difference between the two methods is the way information is summarized from each block to pass to the next block. Rabe and Staats [66] summarizes each block with its temporary output along with the softmax normalization statistics. At the end of the forward pass, the temporary outputs of all the blocks are combined using the statistics to produce the final output. FlashAttention instead incrementally updates the output (Algorithm 1 line 12) after processing each block, so only one copy of the output is needed (instead of $K$ copies for $K$ blocks). This means that FlashAttention has smaller total memory requirement compared to Rabe and Staats 66. The final major difference is the way the backward pass is computed. Rabe and Staats 66] uses gradient checkpointing to recompute the attention matrix and the temporary output of each block. FlashAttention instead simplifies the backward pass analytically (Appendices B.2 and B.4. It only recomputes the attention matrix and does not recompute the temporary output of each block. This reduces the memory requirement for the backward pass and yields speedup. ## C Proofs\n\nProof of Theorem 1. We first count the number of FLOPs and extra memory required. The dominating FLOPs are from matrix multiplication. In the inner loop, (Algorithm 1 line 9), we compute $\\mathbf{Q}_{i} \\mathbf{K}_{j}^{\\top} \\in \\mathbb{R}^{B_{r} \\times B_{c}}$ for $\\mathbf{Q}_{i} \\in \\mathbb{R}^{B_{r} \\times d}$ and $\\mathbf{K}_{j} \\in \\mathbb{R}^{B_{c} \\times d}$, which takes $O\\left(B_{r} B_{c} d\\right)$ FLOPs. We also compute (Algorithm 1 line $12 \\tilde{\\mathbf{P}}_{i j} \\mathbf{V}_{j} \\in \\mathbb{R}^{B_{r} \\times d}$ for $\\tilde{\\mathbf{P}}_{i j} \\in \\mathbb{R}^{B_{r} \\times B_{c}}$ and $\\mathbf{V}_{j} \\in \\mathbb{R}^{B_{c} \\times d}$, which takes $O\\left(B_{r} B_{c} d\\right)$ FLOPs. We execute the inner loops $\\left.T_{c} T_{r}=\\left\\lceil\\frac{N}{B_{c}}\\right\\rceil\\left\\lceil\\frac{N}{B_{r}}\\right\\rceil\\right\\rceil$ times. Therefore the total number of FLOPs is\n\n$$\nO\\left(\\frac{N^{2}}{B_{c} B_{r}} B_{r} B_{c} d\\right)=O\\left(N^{2} d\\right)\n$$\n\nIn terms of extra memory required, we see that we need $O(N)$ memory to store the statistics $(\\ell, m)$. We now prove the algorithm's correctness by induction on $j$ for $0 \\leq j \\leq T_{c}$. Let $\\mathbf{K}_{: j} \\in \\mathbb{R}^{j B_{c} \\times d}$ be the first $j B_{c}$ rows of $\\mathbf{K}$, and similarly $\\mathbf{V}_{: j} \\in \\mathbb{R}^{j B_{c} \\times d}$ the the first $j B_{c}$ rows of $\\mathbf{V}$. Let $\\mathbf{S}_{:,: j}=\\mathbf{Q} \\mathbf{K}_{: j}^{\\top} \\in \\mathbb{R}^{N \\times j B_{c}}$, and $\\mathbf{P}_{i,: j}=\\operatorname{softmax}\\left(\\mathbf{S}_{;,: j}\\right) \\in \\mathbb{R}^{N \\times j B_{c}}$ (softmax applied row-wise). Let $m^{j}, \\ell^{(j)}, \\mathbf{O}^{(j)}$ be the values of $m, \\ell, \\mathbf{O}$ in HBM after the $j$-th iteration of the outer loop (Algorithm 1 line 5). (Note that these values of $m, \\ell, \\mathbf{O}$ are updated after each iteration of the outer loop.) We want to show that after the $j$-th iteration of the outer loop, we have computed in HBM:\n\n$$\nm^{(j)}=\\operatorname{rowmax}\\left(\\mathbf{S}_{: ; j}\\right) \\in \\mathbb{R}^{N}, \\quad \\ell^{(j)}=\\operatorname{rowsum}\\left(\\exp \\left(\\mathbf{S}_{:,: j}-m^{(j)}\\right)\\right) \\in \\mathbb{R}^{N}, \\quad \\mathbf{O}^{(j)}=\\mathbf{P}_{: ; j j} \\mathbf{V}_{: j} \\in \\mathbb{R}^{N \\times d}\n$$\n\nBased on our initialization (Algorithm 1line 2), this claim is true for $j=0$ (i.e., before the any iteration of the outer loop is executed). Suppose that the claim holds for some $j=0, \\ldots, T_{c}-1$. We want to show that the claim also holds for $j+1$. Indeed, when we update the statistics in the inner loop (Algorithm 1 line 10)\non the $(j+1)$-th iteration of the outer loop, we update $m^{(j+1)}=\\max \\left(m^{(j)}, \\tilde{m}\\right)$ where $\\tilde{m} \\in \\mathbb{R}^{N}$ is the row-max of $\\mathbf{S}_{:, j: j+1}$, the slice of $\\mathbf{S}$ from column $j B_{c}$ to column $(j+1) B_{c}-1$. This implies that\n\n$$\nm^{(j+1)}=\\operatorname{rowmax}\\left(\\mathbf{S}_{:, ; j+1}\\right) \\in \\mathbb{R}^{N}\n$$\n\nSimilarly, we update\n\n$$\n\\ell^{(j+1)}=e^{m^{(j)}-m^{(j+1)}} \\ell^{(j)}+e^{\\tilde{m}-m^{(j+1)}} \\tilde{\\ell}\n$$\n\nwhere $\\tilde{\\ell}=\\operatorname{rowsum}\\left(\\exp \\left(\\mathbf{S}_{:, j: j+1}-\\tilde{m}\\right)\\right) \\in \\mathbb{R}^{N}$. By the same algebraic manipulation in Section 3.1, we obtain:\n\n$$\n\\ell^{(j+1)}=\\operatorname{rowsum}\\left(\\exp \\left(\\mathbf{S}_{:,: j+1}-m^{(j+1)}\\right)\\right) \\in \\mathbb{R}^{N}\n$$\n\nLet $\\mathbf{V}_{j: j+1}$ be the slice of $\\mathbf{V}$ from column $j B_{c}$ to column $(j+1) B_{c}-1$, we also update:\n\n$$\n\\begin{aligned}\n& \\mathbf{O}^{(j+1)}=\\operatorname{diag}\\left(\\ell^{(j+1)}\\right)^{-1}\\left(\\operatorname{diag}\\left(\\ell^{(j)}\\right) e^{m^{(j)}-m^{(j+1)}} \\mathbf{O}^{(j)}+e^{\\tilde{m}-m^{(j+1)}} \\exp \\left(\\mathbf{S}_{j: j+1}-\\tilde{m}\\right) \\mathbf{V}_{j: j+1}\\right) \\\\\n& =\\operatorname{diag}\\left(\\ell^{(j+1)}\\right)^{-1}\\left(\\operatorname{diag}\\left(\\ell^{(j)}\\right) e^{m^{(j)}-m^{(j+1)}} \\mathbf{P}_{:,: j} \\mathbf{V}_{: j}+e^{-m^{(j+1)}} \\exp \\left(\\mathbf{S}_{j: j+1}\\right) \\mathbf{V}_{j: j+1}\\right) \\\\\n& =\\operatorname{diag}\\left(\\ell^{(j+1)}\\right)^{-1}\\left(\\operatorname{diag}\\left(\\ell^{(j)}\\right) e^{m^{(j)}-m^{(j+1)}} \\operatorname{diag}\\left(\\ell^{(j)}\\right) \\exp \\left(\\mathbf{S}_{:,: j}-m^{(j)}\\right) \\mathbf{V}_{: j}+e^{-m^{(j+1)}} \\exp \\left(\\mathbf{S}_{j: j+1}\\right) \\mathbf{V}_{j: j+1}\\right) \\\\\n& =\\operatorname{diag}\\left(\\ell^{(j+1)}\\right)^{-1}\\left(e^{-m^{(j+1)}} \\exp \\left(\\mathbf{S}_{:,: j}\\right) \\mathbf{V}_{: j}+e^{-m^{(j+1)}} \\exp \\left(\\mathbf{S}_{j: j+1}\\right) \\mathbf{V}_{j: j+1}\\right) \\\\\n& =\\operatorname{diag}\\left(\\ell^{(j+1)}\\right)^{-1}\\left(\\exp \\left(\\mathbf{S}_{:,: j}-m^{(j+1)}\\right) \\mathbf{V}_{: j}+\\exp \\left(\\mathbf{S}_{j: j+1}-m^{(j+1)}\\right) \\mathbf{V}_{j: j+1}\\right) \\\\\n& =\\operatorname{diag}\\left(\\ell^{(j+1)}\\right)^{-1}\\left(\\exp \\left(\\left[\\begin{array}{ll}\n\\mathbf{S}_{:,: j} & \\mathbf{S}_{j: j+1}\n\\end{array}\\right]-m^{(j+1)}\\right)\\right)\\left[\\begin{array}{c}\n\\mathbf{V}_{: j} \\\\\n\\mathbf{V}_{j: j+1}\n\\end{array}\\right] \\\\\n& =\\operatorname{softmax}\\left(\\mathbf{S}_{: j+1}\\right) \\mathbf{V}_{: j+1} . \\end{aligned}\n$$\n\nWe then see that the claim is also true for $j+1$. By induction, the claim is true for all $j=0, \\ldots, T_{c}$. When $j=T_{c}$, we conclude that the final value of $\\mathbf{O}$ in HBM is $\\operatorname{softmax}(\\mathbf{S}) \\mathbf{V}=\\operatorname{softmax}\\left(\\mathbf{Q} \\mathbf{K}^{\\top}\\right) \\mathbf{V}$. Proof of Theorem 2. We first analyze the IO complexity of standard attention implementation. The inputs $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V} \\in \\mathbb{R}^{N \\times d}$ reside in HBM , and the at the end of the algorithm the output $\\mathbf{O} \\in \\mathbb{R}^{N \\times d}$ is written to HBM. In the first step of computing the matrix multiply $\\mathbf{S}=\\mathbf{Q K}^{\\top}$, the inputs $\\mathbf{Q}, \\mathbf{K}$ are read from HBM and the output $\\mathbf{S} \\in \\mathbb{R}^{N \\times N}$ is written to HBM (Algorithm 0 line 1 ). This incurs $\\Theta\\left(N d+N^{2}\\right) \\mathrm{HBM}$ accesses. In the second step of computing $\\mathbf{P}=\\operatorname{softmax}(\\mathbf{S})$, the input $\\mathbf{S}$ is read from HBM and the output $\\mathbf{P}$ is written to HBM (Algorithm 0 line 2). This incurs $\\Theta\\left(N^{2}\\right)$ HBM accesses. In the last step of computing $\\mathbf{O}=\\mathbf{P V}$, the inputs $\\mathbf{P}, \\mathbf{V}$ are read from global memory and the output $\\mathbf{O}$ is written to HBM (Algorithm 0 line 3). This incurs $\\Theta\\left(N d+N^{2}\\right)$ HBM accesses. Overall, standard attention implementation requires $\\Theta\\left(N d+N^{2}\\right)$ global memory accesses. We now analyze the IO complexity of streaming attention. Following Algorithm 1, we see that each element of $\\mathbf{K}$ and $\\mathbf{V}$ is loaded from HBM once (Algorithm 1 line 6). We make $T_{c}$ passes over $\\mathbf{Q}$ and $\\mathbf{O}$, each pass loading all of $\\mathbf{Q}$ and all of $\\mathbf{O}$ to HBM (Algorithm 1 . line 81. Therefore the number of HBM accesses is $\\Theta\\left(N d+N d T_{c}\\right)=\\Theta\\left(N d T_{c}\\right)$. We derive the conditions on the block sizes $B_{c}$ and $B_{r}$. We need the blocks $\\mathbf{K}_{j}$ and $\\mathbf{V}_{j}$ of size $B_{c} \\times d$ to fit into on-chip memory, which translates to:\n\n$$\nB_{c} d=O(M) \\Leftrightarrow B_{c}=O\\left(\\frac{M}{d}\\right)\n$$\n\nSimilarly, we need the blocks $\\mathbf{Q}_{i}, \\mathbf{O}_{i}$ of size $B_{r} \\times d$ to fit into on-chip memory, which translates to:\n\n$$\nB_{r} d=O(M) \\Leftrightarrow B_{r}=O\\left(\\frac{M}{d}\\right)\n$$\n\nFinally, we need the block $\\mathbf{S}_{i j}$ of size $B_{r} \\times B_{c}$ to fit into on-chip memory, which translates to:\n\n$$\nB_{r} B_{c}=O(M)\n$$\n\nWe therefore set:\n\n$$\nB_{c}=\\Theta\\left(\\frac{M}{d}\\right), \\quad B_{r}=\\Theta\\left(\\min \\left(\\frac{M}{d}, \\frac{M}{B_{c}}\\right)\\right)=\\Theta\\left(\\min \\left(\\frac{M}{d}, d\\right)\\right)\n$$\n\nWe then have:\n\n$$\nT_{c}=\\frac{N}{B_{c}}=\\Theta\\left(\\frac{N d}{M}\\right)\n$$\n\nAs a result, the number of HBM accesses is:\n\n$$\n\\Theta\\left(N d T_{c}\\right)=\\Theta\\left(\\frac{N^{2} d^{2}}{M}\\right)\n$$\n\nProof of Proposition 3. For contradiction, suppose that there exists an algorithm that computes exact attention where the number for HBM access for all $M \\in[d, N d]$ is\n\n$$\no\\left(\\frac{N^{2} d^{2}}{M}\\right)\n$$\n\nIn the regime of $M=\\Theta(N d)$, this results in the number of HBM accesses:\n\n$$\no\\left(\\frac{N^{2} d^{2}}{N d}\\right)=o(N d)\n$$\n\nHowever, the input to attention (matrices $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ ) and the output $\\mathbf{O}$ have size $N d$ and they start out being in HBM, so if the algorithm computes exact attention it must incur at least $\\Omega(N d)$ HBM accesses.\n```\n\n#### 3. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.17)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 3/28 (Score: 0.17)*\n\n```\nThis method stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. - Our SinkLoRA performs favorably against state-of-the-art methods. We evaluate its performance on the PG19, Proof-pile, and LongBench datasets, demonstrating its effectiveness. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-04.jpg?height=375&width=1313&top_left_y=246&top_left_x=428)\n\nFigure 4: Illustration of the Segmentation and Reassembly process in SF-Attn. The process involves three steps: (1) Splitting features along the head dimension into two chunks: one shifted and one unshifted. (2) Splitting tokens, where the tokens belonging to the shifted chunk are shifted by half of the group size, and reassembling them at the tail of the tokens to match the unshifted chunk. (3) Combining the two chunks of tokens together. This figure is adapted from [6]. Specifically, for LLaMA2 7B, SinkLoRA outperforms LongLoRA and is competitive with LongChat-13B. ### 1.1 Motivation for the Research\n\n## Motivation 1: Elevating Attention Scores for Initial Tokens\n\nPrior studies have demonstrated the Attention Sink phenomenon, where certain tokens, typically the initial tokens in a sequence, receive disproportionately high attention scores during the model's computation [39]. This often occurs because these tokens are visible to all subsequent tokens, leading to significant attention even when they lack semantic importance, particularly in autoregressive language models [33]. The Sparse Shifted Attention mechanism implemented in LongLoRA [6] attempts to address this by shifting the high attention scores from these initial tokens to other tokens that previously received lower attention. This shift reduces the overemphasis on initial tokens. To further improve this, we need to develop a method that directly modifies the attention pattern. By applying this technique, we can effectively redistribute attention scores, thereby reducing the undue emphasis on initial tokens across different token groups. ## Motivation 2: Maintaining Initial Tokens During Fine-Tuning\n\nThe concept of attention sinks is also utilized in Streaming LLM [39] to improve the model's handling of long texts. By retaining the Key-Value (KV) pairs of a few initial tokens (attention sinks) along with the most recent tokens, the model ensures stable attention scores and performance even for extended sequences. Inspired by this approach, we aim to carry this mindset from training into inference. Our research aims to modify the fine-tuning process so that initial tokens attend to all other tokens, thereby accumulating more attention scores and enhancing the model's capacity to handle long sequences. ## Motivation 3: Flexible Deployment of Inference Strategy\n\nEfficient deployment of computationally intensive large language models (LLMs) in production environments often relies on Key-Value (KV) caching [16]. KV caching stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. However, LongLoRA [6] retains only the original standard self-attention mechanism during inference. To address this limitation, it is necessary to apply an optional KV cache function. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. ## 2 Related Work\n\n### 2.1 Long-context Transformers\n\nThe primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models. Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to $\\mathrm{O}(\\mathrm{n})$. These models achieve a balance between maintaining sufficient context for understanding while managing computational load. For achieving complexity of $O(n \\log n)$, several approaches have been proposed. Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window. Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity. LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead. Equipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token. This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts. Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited. This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications. ### 2.2 Long-context LLMs\n\nRecent advancements in Large Language Models (LLMs) have significantly extended their capabilities, including handling long-context inputs. Math Word Problems (MWPs) have demonstrated notable performance in solving mathematical questions using LLMs [34]. Moreover, leveraging LLMs for SQL querying has shown promise in optimizing resource allocation, though it remains less efficient than traditional relational databases [42]. LongLoRA [6], employing Position Interpolation [5], has successfully extended the context window of Llama 2 from 4096 to 32768 tokens without requiring substantial GPU or TPU resources. Meta's Llama 3, featuring up to 70 billion parameters, represents a significant advancement in open-source LLMs, offering enhancements in computational efficiency, trust and safety tools, and collaborations with major platforms [38]. Open-source models such as BLOOM [22], OPT [18], and Falcon [28] continue to challenge proprietary models, although models like Vicuna [29] and Alpaca [1] still lag behind their closed-source counterparts in certain aspects. Despite these advancements, effectively managing long-context interactions remains a significant challenge, necessitating ongoing research and development to address the complexities in long-context LLM applications. ### 2.3 KV-Cache Compression\n\nCompressing the size of KV cache is more difficult than reducing the size of weights because they are more sensitive and dependent on model inputs. A cost-effective method for KV cache compression is token dropping [25, 43, 16], which establishes an importance policy to retain significant KVs and remove insignificant ones.\n```\n\n\n\n---\n## Found 0 related papers from 0 external sources\n\n\n\nYour 1 raw search queries input to the search frame: blockwise retention computation streaming adaptation\n\nConsidering refining your search by improving the query keywords input.\n\n\n\n---\n## Web search results\n\n To improve the autoregressive language model design, particularly focusing on blockwise computation in retention mechanisms and streaming adaptations, here are some key insights and techniques from the provided analysis and additional sources:\n\n## Blockwise Computation\n\n- **Blockwise Parallel Transformer**: This approach leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs. It enables training sequences that are significantly longer than those possible with vanilla Transformers or previous memory-efficient methods.\n\n## Memory Optimization and Retention Mechanisms\n\n- **Selective KV Caching**: Implementing window attention where only the most recent key-value (KV) pairs are cached can help in streaming scenarios. This approach can largely recover the performance of full attention mechanisms while reducing memory usage.\n- **Linear Attention Mechanisms**: Linear attention variants, such as those used in the Gated Linear Attention (GLA) Transformer, offer efficient scaling properties and can be integrated with retention mechanisms to maintain model performance while reducing memory requirements.\n\n## Streaming Adaptations\n\n- **Attention Sinks**: Efficient streaming language models can use attention sinks to manage the KV cache effectively. This involves caching the KV of initial tokens to maintain performance in streaming scenarios.\n- **Window Attention**: This method involves only considering the most recent tokens for attention calculations, which is a natural fit for streaming data and can help in reducing computational and memory requirements.\n\n## Hardware Efficiency and Optimization\n\n- **I/O-Aware Implementations**: Optimizing the model for I/O operations is crucial for practical performance. This includes designing efficient point-to-point communication mechanisms and leveraging kernel fusion and caching strategies to enhance hardware utilization.\n- **Hardware-Aware Training**: Implementing hardware-aware training strategies, such as those described in the context of OpenVINO, can significantly improve performance. For example, using sparse and 8-bit quantized models can lead to better performance-accuracy trade-offs and faster inference times.\n\n## Integration Strategies\n\n- **Combining Blockwise Computation with RetNet**: Integrating blockwise computation with RetNet's retention mechanism can help in handling long sequences efficiently while maintaining model performance.\n- **Linear Attention with Retention**: Incorporating linear attention mechanisms into the retention mechanism can provide efficient scaling properties and reduce the computational and memory requirements.\n- **Hardware-Specific Optimizations**: Optimizing the retention computation for specific hardware, such as using the CPU_SPARSE_WEIGHTS_DECOMPRESSION_RATE in OpenVINO for sparse models, can further enhance performance and efficiency.\n\n## Additional Insights\n\n- **Gradient Optimization Techniques**: Techniques like dropping intermediate tensors (DropIT) can reduce the memory footprint during training, which can be beneficial when combined with blockwise computation and streaming adaptations.\n- **Efficient Communication Patterns**: Designing efficient communication patterns for distributed training is essential for scaling the model. This includes leveraging the right-product kernel trick of linear attention to decrease communication overhead.\n\nBy combining these strategies, researchers can develop an autoregressive language model that maintains high performance while reducing memory and computational requirements, making it more scalable and efficient for large datasets and complex tasks.\n",
            "analysis": "Analysis Note - Round 3\n\n1. Summary of Analysis:\n\nThe search results reveal several promising directions for improving RetNet's architecture, particularly in terms of memory efficiency and hardware optimization:\n\na) Memory Optimization Approaches:\n- Blockwise computation strategies for handling long sequences\n- Memory-efficient attention mechanisms with linear complexity\n- Gradient optimization techniques for reduced memory footprint\n\nb) Hardware Efficiency:\n- Hardware-aware implementation strategies\n- Efficient communication patterns for distributed training\n- I/O-aware attention computations\n\n2. Useful References with Excerpts:\n\nFrom Blockwise Parallel Transformer:\n\"...leverages blockwise computation of self-attention and feedforward network fusion to minimize memory costs... enables training sequences 32 times longer than vanilla Transformers and up to 4 times longer than previous memory-efficient methods.\"\n\nFrom Efficient Streaming Language Models:\n\"Window attention, where only the most recent KVs are cached, is a natural approach... keeping the KV of initial tokens will largely recover the performance of window attention.\"\n\nFrom Linear Attention Sequence Parallelism:\n\"...design an efficient point-to-point communication mechanism to leverage the right-product kernel trick of linear attention, which sharply decreases the communication overhead...\"\n\nFrom Gated Linear Attention:\n\"GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations.\"\n\n3. Key Insights and Analysis:\n\na) Memory Optimization Opportunities:\n- Blockwise computation can significantly reduce memory requirements\n- Selective KV caching strategies show promise for streaming scenarios\n- Linear attention mechanisms offer efficient scaling properties\n\nb) Hardware Efficiency Improvements:\n- I/O-aware implementations crucial for practical performance\n- Communication pattern optimization essential for distributed training\n- Kernel fusion and caching strategies can enhance hardware utilization\n\nc) Integration Possibilities:\n- Combine blockwise computation with RetNet's retention mechanism\n- Implement selective KV caching for streaming scenarios\n- Incorporate hardware-efficient linear attention variants\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of blockwise computation in retention mechanisms\n2. Integration strategies for linear attention with retention\n3. Hardware-specific optimizations for retention computation\n4. Streaming adaptations for RetNet architecture\n\n5. References:\n\n[1] Liu, H., & Abbeel, P. (2023). Blockwise Parallel Transformer for Large Context Models. arXiv preprint.\n\n[2] Xiao, G., Tian, Y., Chen, B., Han, S., & Lewis, M. (2023). Efficient Streaming Language Models with Attention Sinks. arXiv preprint.\n\n[3] Sun, W., Qin, Z., Li, D., Shen, X., Qiao, Y., & Zhong, Y. (2024). Linear Attention Sequence Parallelism. arXiv preprint.\n\n[4] Yang, S., Wang, B., Shen, Y., Panda, R., & Kim, Y. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. ICML 2023.\n\n[5] Chen, J., Xu, K., Cheng, Y., & Yao, A. (2022). DropIT: Dropping Intermediate Tensors for Memory-Efficient DNN Training. ICLR 2022."
        },
        {
            "ready": false,
            "query": "attention sink mechanisms memory efficient gradient computation",
            "detail": "Analyze recent developments in attention sink mechanisms and memory-efficient gradient computation techniques, focusing on methods that can be integrated with retention mechanisms for improved streaming performance.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nAnalyze recent developments in attention sink mechanisms and memory-efficient gradient computation techniques, focusing on methods that can be integrated with retention mechanisms for improved streaming performance.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 1.00)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 3/32 (Score: 1.00)*\n\n```\nexisting methods. The language model, pre-trained on texts of length $L$, predicts the $T$ th token $(T \\gg L)$. (a) Dense Attention has $O\\left(T^{2}\\right)$ time complexity and an increasing cache size. Its performance decreases when the text length exceeds the pre-training text length. (b) Window Attention caches the most recent $L$ tokens' KV. While efficient in inference, performance declines sharply once the starting tokens' keys and values are evicted. (c) Sliding Window with Re-computation rebuilds the KV states from the $L$ recent tokens for each new token. While it performs well on long texts, its $O\\left(T L^{2}\\right)$ complexity, stemming from quadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM keeps the attention sink (several initial tokens) for stable attention computation, combined with the recent tokens. It's efficient and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B model on the first book ( 65 K tokens) in the PG-19 test set. When applying LLMs for infinite input streams, two primary challenges arise:\n\n1. During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV) of all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory usage and increasing decoding latency (Pope et al., 2022). 2. Existing models have limited length extrapolation abilities, i.e., their performance degrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the attention window size set during pre-training. An intuitive approach, known as window attention (Beltagy et al. 2020) (Figure 1 b), maintains only a fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant memory usage and decoding speed after the cache is initially filled, the model collapses once the sequence length exceeds the cache size, i.e., even just evicting the $K V$ of the first token, as illustrated in Figure 3 Another strategy is the sliding window with re-computation (shown in Figure 11c), which rebuilds the KV states of recent tokens for each generated token. While it offers strong performance, this approach is significantly slower due to the computation of quadratic attention within its window, making this method impractical for real-world streaming applications. To understand the failure of window attention, we find an interesting phenomenon of autoregressive LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens \"attention sinks\". Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks. Based on the above insights, we propose StreamingLLM, a simple and efficient framework that enables LLMs trained with a finite attention window to work on text of infinite length without finetuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving them can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply keeps the attention sink tokens' KV (with just 4 initial tokens sufficing) together with the sliding window's KV to anchor the attention computation and stabilize the model's performance. With StreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more. Compared with the only viable baseline, sliding window with recomputation, StreamingLLM achieves up to $22.2 \\times$ speedup, realizing the streaming use of LLMs. ![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-03.jpg?height=315&width=1335&top_left_y=271&top_left_x=384)\n\nFigure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1 ) exhibit the \"local\" pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends to the initial token across all layers and heads. Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can be pre-trained to require only a single attention sink token for streaming deployment. Specifically, we suggest that an extra learnable token at the beginning of all training samples can serve as a designated attention sink. By pre-training 160-million parameter language models from scratch, we demonstrate that adding this single sink token preserves the model's performance in streaming cases. This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial tokens as attention sinks to achieve the same performance level. Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within the KV cache without extending the LLMs' context length. It suits continuous operation needs with minimal memory use and past data reliance. Additionally, StreamingLLM can complement context extension methods to increase the attendable recent context. ## 2 RELATEd WORK\n\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus: Length Extrapolation, Context Window Extension, and Improving LLMs' Utilization of Long Text. While seemingly related, it's worth noting that progress in one direction doesn't necessarily lead to progress in the other. For example, extending the context size of LLMs doesn't improve the model's performance beyond the context size, and neither approach ensures effective use of the long context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied to text significantly exceeding the pre-training window size, potentially even of infinite length. We do not expand the attention window size of LLMs or enhance the model's memory and usage on long texts. The last two categories are orthogonal to our focus and could be integrated with our techniques. Length extrapolation aims to enable language models trained on shorter texts to handle longer ones during testing. A predominant avenue of research targets the development of relative position encoding methods for Transformer models, enabling them to function beyond their training window. One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms the queries and keys in every attention layer for relative position integration. Despite its promise, subsequent research (Press et al., 2022, Chen et al., 2023) indicated its underperformance on text that exceeds the training window. Another approach, $\\overline{A L i B i}$ (Press et al., 2022), biases the query-key attention scores based on their distance, thereby introducing relative positional information. While this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the text length was vastly greater than the training length. Current methodologies, however, have yet to achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications. Context Window Extension centers on expanding the LLMs' context window, enabling the processing of more tokens in one forward pass.\n```\n\n#### 2. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.98)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 3/28 (Score: 0.98)*\n\n```\nThis method stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. - Our SinkLoRA performs favorably against state-of-the-art methods. We evaluate its performance on the PG19, Proof-pile, and LongBench datasets, demonstrating its effectiveness. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-04.jpg?height=375&width=1313&top_left_y=246&top_left_x=428)\n\nFigure 4: Illustration of the Segmentation and Reassembly process in SF-Attn. The process involves three steps: (1) Splitting features along the head dimension into two chunks: one shifted and one unshifted. (2) Splitting tokens, where the tokens belonging to the shifted chunk are shifted by half of the group size, and reassembling them at the tail of the tokens to match the unshifted chunk. (3) Combining the two chunks of tokens together. This figure is adapted from [6]. Specifically, for LLaMA2 7B, SinkLoRA outperforms LongLoRA and is competitive with LongChat-13B. ### 1.1 Motivation for the Research\n\n## Motivation 1: Elevating Attention Scores for Initial Tokens\n\nPrior studies have demonstrated the Attention Sink phenomenon, where certain tokens, typically the initial tokens in a sequence, receive disproportionately high attention scores during the model's computation [39]. This often occurs because these tokens are visible to all subsequent tokens, leading to significant attention even when they lack semantic importance, particularly in autoregressive language models [33]. The Sparse Shifted Attention mechanism implemented in LongLoRA [6] attempts to address this by shifting the high attention scores from these initial tokens to other tokens that previously received lower attention. This shift reduces the overemphasis on initial tokens. To further improve this, we need to develop a method that directly modifies the attention pattern. By applying this technique, we can effectively redistribute attention scores, thereby reducing the undue emphasis on initial tokens across different token groups. ## Motivation 2: Maintaining Initial Tokens During Fine-Tuning\n\nThe concept of attention sinks is also utilized in Streaming LLM [39] to improve the model's handling of long texts. By retaining the Key-Value (KV) pairs of a few initial tokens (attention sinks) along with the most recent tokens, the model ensures stable attention scores and performance even for extended sequences. Inspired by this approach, we aim to carry this mindset from training into inference. Our research aims to modify the fine-tuning process so that initial tokens attend to all other tokens, thereby accumulating more attention scores and enhancing the model's capacity to handle long sequences. ## Motivation 3: Flexible Deployment of Inference Strategy\n\nEfficient deployment of computationally intensive large language models (LLMs) in production environments often relies on Key-Value (KV) caching [16]. KV caching stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. However, LongLoRA [6] retains only the original standard self-attention mechanism during inference. To address this limitation, it is necessary to apply an optional KV cache function. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. ## 2 Related Work\n\n### 2.1 Long-context Transformers\n\nThe primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models. Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to $\\mathrm{O}(\\mathrm{n})$. These models achieve a balance between maintaining sufficient context for understanding while managing computational load. For achieving complexity of $O(n \\log n)$, several approaches have been proposed. Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window. Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity. LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead. Equipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token. This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts. Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited. This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications. ### 2.2 Long-context LLMs\n\nRecent advancements in Large Language Models (LLMs) have significantly extended their capabilities, including handling long-context inputs. Math Word Problems (MWPs) have demonstrated notable performance in solving mathematical questions using LLMs [34]. Moreover, leveraging LLMs for SQL querying has shown promise in optimizing resource allocation, though it remains less efficient than traditional relational databases [42]. LongLoRA [6], employing Position Interpolation [5], has successfully extended the context window of Llama 2 from 4096 to 32768 tokens without requiring substantial GPU or TPU resources. Meta's Llama 3, featuring up to 70 billion parameters, represents a significant advancement in open-source LLMs, offering enhancements in computational efficiency, trust and safety tools, and collaborations with major platforms [38]. Open-source models such as BLOOM [22], OPT [18], and Falcon [28] continue to challenge proprietary models, although models like Vicuna [29] and Alpaca [1] still lag behind their closed-source counterparts in certain aspects. Despite these advancements, effectively managing long-context interactions remains a significant challenge, necessitating ongoing research and development to address the complexities in long-context LLM applications. ### 2.3 KV-Cache Compression\n\nCompressing the size of KV cache is more difficult than reducing the size of weights because they are more sensitive and dependent on model inputs. A cost-effective method for KV cache compression is token dropping [25, 43, 16], which establishes an importance policy to retain significant KVs and remove insignificant ones.\n```\n\n#### 3. CORM: Cache Optimization with Recent Message for Large Language Model Inference (Avg. Score: 0.81)\n\n*Jincheng Dai, Zhuowei Huang, Haiyun Jiang, Chen Chen, Deng Cai, Wei Bi, Shuming Shi*\n\n**Published in:**  (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper presents CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning, and shows that CORM reduces the inference memory usage of KV cache by up to 70\\% with negligible performance degradation across six tasks in LongBench.\n\n**Abstract:** Large Language Models (LLMs), despite their remarkable performance across a wide range of tasks, necessitate substantial GPU memory and consume significant computational resources. Beyond the memory taken up by model weights, the memory used by the KV cache rises linearly with sequence length, becoming a primary bottleneck for inference. In this paper, we introduce an innovative method for optimizing the KV cache, which considerably minimizes its memory footprint. Upon thorough investigation, we discover that in most Transformer models, (i) there is a striking similarity between adjacent tokens' query vectors, and (ii) the attention calculation of the current query can rely exclusively on the attention information of a small fraction of preceding queries. Based on these observations, we present CORM, a KV cache eviction policy that dynamically retains essential key-value pairs for inference without the need for model fine-tuning. Our validation shows that CORM reduces the inference memory usage of KV cache by up to 70\\% with negligible performance degradation across six tasks in LongBench. Furthermore, we demonstrate that CORM is compatible with GQA for further compression rate.\n\n##### *Relevant Chunk: No. 3/18 (Score: 0.81)*\n\n```\narXiv preprint arXiv:2305.13245, 2023. [10] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks.\n```\n\n#### 4. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.74)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 21/24 (Score: 0.74)*\n\n```\narXiv preprint arXiv:2402.04617, 2024. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks.\n```\n\n#### 5. Lightning Attention-2: A Free Lunch for Handling Unlimited Sequence Lengths in Large Language Models (Avg. Score: 0.70)\n\n*Zhen Qin, Weigao Sun, Dong Li, Xuyang Shen, Weixuan Sun, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 9  (*Influential: 1*)\n\n**TL;DR:** Lightning Attention-2 is presented, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits and retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms.\n\n**Abstract:** Linear attention is an efficient attention mechanism that has recently emerged as a promising alternative to conventional softmax attention. With its ability to process tokens in linear computational complexities, linear attention, in theory, can handle sequences of unlimited length without sacrificing speed, i.e., maintaining a constant training speed for various sequence lengths with a fixed memory consumption. However, due to the issue with cumulative summation (cumsum), current linear attention algorithms cannot demonstrate their theoretical advantage in a causal setting. In this paper, we present Lightning Attention-2, the first linear attention implementation that enables linear attention to realize its theoretical computational benefits. To achieve this, we leverage the thought of tiling, separately handling the intra-block and inter-block components in linear attention calculation. Specifically, we utilize the conventional attention computation mechanism for the intra-blocks and apply linear attention kernel tricks for the inter-blocks. A tiling technique is adopted through both forward and backward procedures to take full advantage of the GPU hardware. We implement our algorithm in Triton to make it IO-aware and hardware-friendly. Various experiments are conducted on different model sizes and sequence lengths. Lightning Attention-2 retains consistent training and inference speed regardless of input sequence length and is significantly faster than other attention mechanisms. The source code is available at https://github.com/OpenNLPLab/lightning-attention.\n\n##### *Relevant Chunk: No. 3/25 (Score: 0.70)*\n\n```\nMultiple methods have been proposed to replace the softmax operation. For instance, Katharopoulos et al. (2020a) employ the $1+$ elu activation function, Qin et al. (2022b) utilize the cosine function to approximate softmax properties, and Ke et al. (2021); Zheng et al. (2022; 2023) leverage sampling strategies to directly mimic softmax operation. Despite having a theoretical complexity of $O\\left(n d^{2}\\right)$, the practical computational efficiency of linear attention diminishes notably in causal attention scenarios, primarily due to the necessity for cumsum operations (Hua et al., 2022). ### 2.2. IO-aware Attention\n\nThe FlashAttention series (Dao et al., 2022; Dao, 2023) focuses on system-level optimizations for the efficient implementation of the standard attention operator on GPU platforms. Extensive validation has demonstrated its effectiveness. The approach employs tiling strategies to minimize the volume of memory reads/writes between the GPU's high bandwidth memory (HBM) and on-chip SRAM. To address the issue of slow computation for Linear Attention in the causal setting, Lightning Attention 1 (Qin et al., 2023b) employs the approach of FlashAttention-1/2, which involves segmenting the inputs $\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}$ into blocks, transferring them from slow HBM to fast SRAM, and then computing the attention output with respect to these blocks. Subsequently, the final results are accumulated. Although this method is much more efficient than the PyTorch implementation, it does not take advantage of the computational characteristics inherent to Linear Attention, and the theoretical complexity remains $O\\left(n^{2} d\\right)$. ### 2.3. Long Sequence Handling in LLM\n\nA widely adopted strategy to tackle challenges related to length extrapolation involves the integration of Relative Positional Encoding (RPE) techniques (Su et al., 2021; Qin et al., 2023c), strategically directing attention towards neighboring tokens. ALiBi (Press et al., 2022) utilizes linear decay biases in attention mechanisms to mitigate the impact of distant tokens. Roformer (Su et al., 2021) introduces a novel Rotary Position Embedding (RoPE) method, widely embraced in the community, effectively leveraging positional information for transformer-based language model learning. Kerple (Chi et al., 2022) explores shift-invariant conditionally positive definite kernels within RPEs, introducing a suite of kernels aimed at enhancing length extrapolation properties, with ALiBi recognized as one of its instances. Furthermore, Sandwich (Chi et al., 2023) postulates a hypothesis elucidating the mechanism behind ALiBi , empirically validating it by incorporating the hypothesis into sinusoidal positional embeddings. (Qin et al., 2024) explored the sufficient conditions for additive relative position encoding to have extrapolation capabilities. Instead of investigating the length extrapolation capability of transformers, some works also attempt to directly increase the context window sizes. Chen et al. (2023) introduces Position Interpolation (PI), extending context window sizes of RoPE-based pretrained Large Language Models (LLMs) such as LLaMA models to up to 32768 with minimal finetuning (within 1000 steps). StreamingLLM (Xiao et al., 2023) proposes leveraging the attention sink phenomenon, maintaining the Key and Value information of initial tokens to substantially recover the performance of window attention.\n```\n\n\n\n---\n## Found 5 related papers from 1 external sources\n\n\n\nYour 1 raw search queries input to the search frame: attention sink mechanisms memory efficient gradient computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. Memory Efficient Neural Processes via Constant Memory Attention Block\n\n*From Search Query: attention sink mechanisms memory efficient gradient computation*\n\n*Leo Feng, Frederick Tung, Hossein Hajimirsadeghi, Y. Bengio, M. O. Ahmed*\n\n**TL;DR:** This work proposes Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory that achieves state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Abstract:** Neural Processes (NPs) are popular meta-learning methods for efficiently modelling predictive uncertainty. Recent state-of-the-art methods, however, leverage expensive attention mechanisms, limiting their applications, particularly in low-resource settings. In this work, we propose Constant Memory Attentive Neural Processes (CMANPs), an NP variant that only requires constant memory. To do so, we first propose an efficient update operation for Cross Attention. Leveraging the update operation, we propose Constant Memory Attention Block (CMAB), a novel attention block that (i) is permutation invariant, (ii) computes its output in constant memory, and (iii) performs constant computation updates. Finally, building on CMAB, we detail Constant Memory Attentive Neural Processes. Empirically, we show CMANPs achieve state-of-the-art results on popular NP benchmarks while being significantly more memory efficient than prior methods.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 4  (*Influential: 1*)\n\n#### 2. AtMan: Understanding Transformer Predictions Through Memory Efficient Attention Manipulation\n\n*From Search Query: attention sink mechanisms memory efficient gradient computation*\n\n*Mayukh Deb, Bjorn Deiseroth, Samuel Weinbach, Manuel Brack, P. Schramowski, K. Kersting*\n\n**TL;DR:** AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction, and outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient.\n\n**Abstract:** Generative transformer models have become increasingly complex, with large numbers of parameters and the ability to process multiple input modalities. Current methods for explaining their predictions are resource-intensive. Most crucially, they require prohibitively large amounts of extra memory, since they rely on backpropagation which allocates almost twice as much GPU memory as the forward pass. This makes it difficult, if not impossible, to use them in production. We present AtMan that provides explanations of generative transformer models at almost no extra cost. Specifically, AtMan is a modality-agnostic perturbation method that manipulates the attention mechanisms of transformers to produce relevance maps for the input with respect to the output prediction. Instead of using backpropagation, AtMan applies a parallelizable token-based search method based on cosine similarity neighborhood in the embedding space. Our exhaustive experiments on text and image-text benchmarks demonstrate that AtMan outperforms current state-of-the-art gradient-based methods on several metrics while being computationally efficient. As such, AtMan is suitable for use in large model inference deployments.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 15  (*Influential: 1*)\n\n#### 3. Short-Long Convolutions Help Hardware-Efficient Linear Attention to Focus on Long Sequences\n\n*From Search Query: attention sink mechanisms memory efficient gradient computation*\n\n*Zicheng Liu, Siyuan Li, Li Wang, Zedong Wang, Yunfan Liu, Stan Z. Li*\n\n**TL;DR:** CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner and enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity.\n\n**Abstract:** To mitigate the computational complexity in the self-attention mechanism on long sequences, linear attention utilizes computation tricks to achieve linear complexity, while state space models (SSMs) popularize a favorable practice of using non-data-dependent memory pattern, i.e., emphasize the near and neglect the distant, to processing sequences. Recent studies have shown the priorities by combining them as one. However, the efficiency of linear attention remains only at the theoretical level in a causal setting, and SSMs require various designed constraints to operate effectively on specific data. Therefore, in order to unveil the true power of the hybrid design, the following two issues need to be addressed: (1) hardware-efficient implementation for linear attention and (2) stabilization of SSMs. To achieve this, we leverage the thought of tiling and hierarchy to propose CHELA (short-long Convolutions with Hardware-Efficient Linear Attention), which replaces SSMs with short-long convolutions and implements linear attention in a divide-and-conquer manner. This approach enjoys global abstraction and data-dependent selection from stable SSM and linear attention while maintaining real linear complexity. Our comprehensive experiments on the Long Range Arena benchmark and language modeling tasks demonstrate the effectiveness of the proposed method.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 4. MEKER: Memory Efficient Knowledge Embedding Representation for Link Prediction and Question Answering\n\n*From Search Query: attention sink mechanisms memory efficient gradient computation*\n\n*V. Chekalina, Anton Razzhigaev, A. Sayapin, A. Panchenko*\n\n**TL;DR:** A MEKER is proposed, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering and reduces the memory needed in training while providing computational benefits.\n\n**Abstract:** Knowledge Graphs (KGs) are symbolically structured storages of facts. The KG embedding contains concise data used in NLP tasks requiring implicit information about the real world. Furthermore, the size of KGs that may be useful in actual NLP assignments is enormous, and creating embedding over it has memory cost issues. We represent KG as a 3rd-order binary tensor and move beyond the standard CP decomposition (CITATION) by using a data-specific generalized version of it (CITATION). The generalization of the standard CP-ALS algorithm allows obtaining optimization gradients without a backpropagation mechanism. It reduces the memory needed in training while providing computational benefits. We propose a MEKER, a memory-efficient KG embedding model, which yields SOTA-comparable performance on link prediction tasks and KG-based Question Answering.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 6  (*Influential: 0*)\n\n#### 5. Long-range Sequence Modeling with Predictable Sparse Attention\n\n*From Search Query: attention sink mechanisms memory efficient gradient computation*\n\n*Yimeng Zhuang, Jing Zhang, Mei Tu*\n\n**TL;DR:** An efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling, which remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.\n\n**Abstract:** Self-attention mechanism has been shown to be an effective approach for capturing global context dependencies in sequence modeling, but it suffers from quadratic complexity in time and memory usage. Due to the sparsity of the attention matrix, much computation is redundant. Therefore, in this paper, we design an efficient Transformer architecture, named Fourier Sparse Attention for Transformer (FSAT), for fast long-range sequence modeling. We provide a brand-new perspective for constructing sparse attention matrix, i.e. making the sparse attention matrix predictable. Two core sub-modules are: (1) A fast Fourier transform based hidden state cross module, which captures and pools L^2 semantic combinations in \\mathcal{O}(L\\log L) time complexity. (2) A sparse attention matrix estimation module, which predicts dominant elements of an attention matrix based on the output of the previous hidden state cross module. By reparameterization and gradient truncation, FSAT successfully learned the index of dominant elements. The overall complexity about the sequence length is reduced from \\mathcal{O}(L^2) to \\mathcal{O}(L\\log L). Extensive experiments (natural language, vision, and math) show that FSAT remarkably outperforms the standard multi-head attention and its variants in various long-sequence tasks with low computational costs, and achieves new state-of-the-art results on the Long Range Arena benchmark.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 7  (*Influential: 1*)\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on attention sink mechanisms and memory-efficient gradient computation, here are some key findings and techniques from the recent developments:\n\n## Attention Sink Mechanisms\n\n### Pre-Scaling Mechanism\nThe pre-scaling mechanism proposed in the context of continual learning (CL) can be relevant for addressing attention sink issues. This mechanism encourages diverse attention scores on sink tokens, which are typically common tokens that are not semantically significant. By scaling the attention to non-sink tokens, it reduces over-smoothing and interference in task learning, which can be beneficial for maintaining stability in long sequences.\n\n### Dynamic Sparse Attention\nThe CASAK-V method introduces dynamic sparse attention, which can help mitigate the effects of attention sinks. This approach dynamically adjusts the attention distribution, ensuring that the model does not overly focus on specific tokens. The dynamic sparse attention mechanism provides a significant performance boost and helps in maintaining consistent performance even for very long contexts.\n\n## Memory-Efficient Gradient Computation\n\n### Dynamic Sparse Attention and Adaptive KV-Cache Compression\nCASAK-V combines dynamic sparse attention with adaptive KV-cache compression, significantly reducing memory usage and improving speed. This method shows near-linear scaling in memory usage and sub-linear scaling in inference time, making it highly efficient for long-context processing. The adaptive KV-cache compression reduces the need for repetitive computations, lowering latency in autoregressive generation.\n\n### FlashAttention and Tiling\nFlashAttention, an IO-aware exact attention algorithm, uses a tiling approach to reduce memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. This approach can be adapted for retention mechanisms to improve memory efficiency. Tiling and blockwise computation strategies can reduce memory requirements and memory access, which is crucial for practical performance in streaming scenarios[Analysis Note - Round 4].\n\n### Selective KV Caching\nSelective KV caching, as mentioned in the context of SinkLoRA, stores the key-value states of previously generated tokens, reducing the need for repetitive computations and thus lowering latency in autoregressive generation. This technique is particularly beneficial for streaming performance and can be integrated with retention mechanisms to enhance efficiency[Analysis Note - Round 4].\n\n## Integration and Future Directions\n\n### Combining Techniques\nIntegrating the pre-scaling mechanism to address attention sink issues with dynamic sparse attention and adaptive KV-cache compression can enhance the stability and efficiency of autoregressive language models. Combining these with tiling and IO-aware memory management can further optimize memory usage and computational efficiency.\n\n### Hardware-Specific Optimizations\nImplementing hardware-specific optimizations, such as those tailored for GPU architectures, is essential for real-world deployment. These optimizations can ensure that the model leverages the hardware efficiently, reducing memory access and improving overall performance[Analysis Note - Round 4].\n\nBy focusing on these techniques, researchers can develop autoregressive language models that are more memory-efficient, computationally efficient, and robust to long sequences, ultimately improving their overall performance and scalability.\n",
            "analysis": "Analysis Note - Round 4\n\n1. Summary of Analysis:\n\nAfter analyzing the search results, several key opportunities for improving RetNet emerge, particularly around memory efficiency and streaming capabilities:\n\na) Memory Optimization:\n- FlashAttention's tiling approach offers insights for efficient memory management\n- Blockwise computation strategies can significantly reduce memory requirements\n- Selective KV caching shows promise for streaming scenarios\n\nb) Computational Efficiency:\n- IO-aware implementations crucial for practical performance\n- Tiling and recomputation techniques can reduce memory access\n- Hardware-specific optimizations essential for real-world deployment\n\n2. Useful References with Excerpts:\n\nFrom FlashAttention:\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM... requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes.\"\n\nFrom RetNet:\n\"The chunkwise recurrent representation facilitates efficient long-sequence modeling with linear complexity, where each chunk is encoded parallelly while recurrently summarizing the chunks.\"\n\nFrom SinkLoRA:\n\"KV caching stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation.\"\n\n3. Key Insights and Analysis:\n\na) Memory Management Opportunities:\n- Tiling approach from FlashAttention can be adapted for retention mechanism\n- Blockwise computation with selective updates can reduce memory footprint\n- IO-aware implementation crucial for practical efficiency\n\nb) Streaming Adaptations:\n- Selective KV caching can improve streaming performance\n- Attention sink mechanisms help maintain stability in long sequences\n- Efficient state management critical for streaming scenarios\n\nc) Integration Possibilities:\n- Combine tiling with RetNet's retention mechanism\n- Implement IO-aware memory management\n- Incorporate selective KV caching for streaming\n\n4. Future Search Plan:\n\nNeed to investigate:\n1. Specific implementations of attention sink mechanisms\n2. Memory-efficient gradient computation techniques\n3. Hardware-specific optimizations for retention computation\n4. Efficient state management strategies for streaming\n\n5. References:\n\n[1] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[2] Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., & Wei, F. (2023). Retentive Network: A Successor to Transformer for Large Language Models. arXiv:2307.08621.\n\n[3] Zhang, H. (2024). SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models. arXiv.org.\n\n[4] Liu, Z., Zhao, C., Iandola, F. N., Lai, C., Tian, Y., Fedorov, I., ... & Chandra, V. (2024). MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases. arXiv:2402.14905."
        },
        {
            "ready": false,
            "query": "attention sink mechanisms, adaptive tiling, selective KV caching",
            "detail": "attention sink mechanisms in language models, adaptive tiling strategies, selective KV caching techniques",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nattention sink mechanisms in language models, adaptive tiling strategies, selective KV caching techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.97)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 3/28 (Score: 1.00)*\n\n```\nThis method stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. - Our SinkLoRA performs favorably against state-of-the-art methods. We evaluate its performance on the PG19, Proof-pile, and LongBench datasets, demonstrating its effectiveness. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-04.jpg?height=375&width=1313&top_left_y=246&top_left_x=428)\n\nFigure 4: Illustration of the Segmentation and Reassembly process in SF-Attn. The process involves three steps: (1) Splitting features along the head dimension into two chunks: one shifted and one unshifted. (2) Splitting tokens, where the tokens belonging to the shifted chunk are shifted by half of the group size, and reassembling them at the tail of the tokens to match the unshifted chunk. (3) Combining the two chunks of tokens together. This figure is adapted from [6]. Specifically, for LLaMA2 7B, SinkLoRA outperforms LongLoRA and is competitive with LongChat-13B. ### 1.1 Motivation for the Research\n\n## Motivation 1: Elevating Attention Scores for Initial Tokens\n\nPrior studies have demonstrated the Attention Sink phenomenon, where certain tokens, typically the initial tokens in a sequence, receive disproportionately high attention scores during the model's computation [39]. This often occurs because these tokens are visible to all subsequent tokens, leading to significant attention even when they lack semantic importance, particularly in autoregressive language models [33]. The Sparse Shifted Attention mechanism implemented in LongLoRA [6] attempts to address this by shifting the high attention scores from these initial tokens to other tokens that previously received lower attention. This shift reduces the overemphasis on initial tokens. To further improve this, we need to develop a method that directly modifies the attention pattern. By applying this technique, we can effectively redistribute attention scores, thereby reducing the undue emphasis on initial tokens across different token groups. ## Motivation 2: Maintaining Initial Tokens During Fine-Tuning\n\nThe concept of attention sinks is also utilized in Streaming LLM [39] to improve the model's handling of long texts. By retaining the Key-Value (KV) pairs of a few initial tokens (attention sinks) along with the most recent tokens, the model ensures stable attention scores and performance even for extended sequences. Inspired by this approach, we aim to carry this mindset from training into inference. Our research aims to modify the fine-tuning process so that initial tokens attend to all other tokens, thereby accumulating more attention scores and enhancing the model's capacity to handle long sequences. ## Motivation 3: Flexible Deployment of Inference Strategy\n\nEfficient deployment of computationally intensive large language models (LLMs) in production environments often relies on Key-Value (KV) caching [16]. KV caching stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. However, LongLoRA [6] retains only the original standard self-attention mechanism during inference. To address this limitation, it is necessary to apply an optional KV cache function. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. ## 2 Related Work\n\n### 2.1 Long-context Transformers\n\nThe primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models. Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to $\\mathrm{O}(\\mathrm{n})$. These models achieve a balance between maintaining sufficient context for understanding while managing computational load. For achieving complexity of $O(n \\log n)$, several approaches have been proposed. Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window. Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity. LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead. Equipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token. This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts. Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited. This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications. ### 2.2 Long-context LLMs\n\nRecent advancements in Large Language Models (LLMs) have significantly extended their capabilities, including handling long-context inputs. Math Word Problems (MWPs) have demonstrated notable performance in solving mathematical questions using LLMs [34]. Moreover, leveraging LLMs for SQL querying has shown promise in optimizing resource allocation, though it remains less efficient than traditional relational databases [42]. LongLoRA [6], employing Position Interpolation [5], has successfully extended the context window of Llama 2 from 4096 to 32768 tokens without requiring substantial GPU or TPU resources. Meta's Llama 3, featuring up to 70 billion parameters, represents a significant advancement in open-source LLMs, offering enhancements in computational efficiency, trust and safety tools, and collaborations with major platforms [38]. Open-source models such as BLOOM [22], OPT [18], and Falcon [28] continue to challenge proprietary models, although models like Vicuna [29] and Alpaca [1] still lag behind their closed-source counterparts in certain aspects. Despite these advancements, effectively managing long-context interactions remains a significant challenge, necessitating ongoing research and development to address the complexities in long-context LLM applications. ### 2.3 KV-Cache Compression\n\nCompressing the size of KV cache is more difficult than reducing the size of weights because they are more sensitive and dependent on model inputs. A cost-effective method for KV cache compression is token dropping [25, 43, 16], which establishes an importance policy to retain significant KVs and remove insignificant ones.\n```\n\n##### *Relevant Chunk: No. 2/28 (Score: 0.95)*\n\n```\nThis extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention ( $\\left.\\mathrm{S}^{2}-\\mathrm{Attn}\\right)$, effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only $39 \\%$ of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose SinkLoRA, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving $92 \\%$ of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm $\\mathrm{H}_{2} \\mathrm{O}$ to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at https://github.com/Dexter-GT-86/SinkLoRA. ## 1 Introduction\n\nEnhancing the functionality of Transformer models to handle longer sequence lengths has become crucial for numerous applications, including language translation, long-context processing, chatbots, code generation, and multimedia content creation. The primary challenge lies in the self-attention mechanism, which scales quadratically with sequence length, leading to substantial computational time and memory requirements [4, 41, 21]. To address this challenge, several approaches have been proposed. Longformer and BigBird utilize combinations of local, global, and sparse attention mechanisms to manage long contexts, reducing complexity to $\\mathrm{O}(\\mathrm{n})$ [4, 41]. Reformer introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thereby reducing computational complexity [21]. LSG Attention combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead [9]. Despite these advancements, managing long-context interactions in practical applications remains a significant challenge. Recent work, such as LongLoRA, extends the context window of LLaMA2 from 4096 to 32768 tokens using Position Interpolation without substantial GPU or TPU resources\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-02.jpg?height=535&width=1381&top_left_y=404&top_left_x=365)\n\nFigure 1: Evaluation of SinkLoRA in bridging the accuracy gap between sparse shifted attention and full attention during supervised fine-tuning, while maintaining the memory efficiency of LongLoRA, which utilizes 1.8 times less memory compared to full fine-tuning. Furthermore, SinkLoRA retains the training speed of LongLoRA, being 1.8 times faster than full fine-tuning, due to the implementation of Sink Fixed Attention. The Llama2-7B models[37] are fine-tuned to various context lengths using Flash-Attention 2 [10] and DeepSpeed stage 2 [31], and are evaluated on the proof-pile test set [2] in terms of perplexity. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-02.jpg?height=349&width=1214&top_left_y=1644&top_left_x=484)\n\nFigure 2: Overview of the SinkLoRA fine-tuning process, incorporating Sink Fixed Attention (SFAttn). Panels (a), (b), and (c) depict the procedure to convert Sparse Shifted Attention into Short Window Attention and subsequently into Sink Fixed Attention. This conversion is executed in two stages: reassembly and making the initial tokens global. In addition to optimizing the LoRA weights within linear layers, SinkLoRA also enables training of the embedding and normalization layers, consistent with the methodology employed in LongLoRA. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-03.jpg?height=633&width=1264&top_left_y=280&top_left_x=428)\n\nFigure 3: Overview of the SinkLoRA inference process. Unlike LongLoRA, which retains the original standard self-attention during inference, SinkLoRA implements an optional KV cache compression method, $\\mathrm{H}^{2} \\mathrm{O}$ [43]. This extension enhances inference speed without significantly compromising performance. [6]. However, LongLoRA's efficiency is limited, achieving only $39 \\%$ of the perplexity improvement compared to full attention due to chaotic attention head structures and unnecessary information exchange between token groups. To address these issues, we propose SinkLoRA, which offers better work partitioning. This includes the development of Sink Fixed Attention (SF-Attn), a segmentation and reassembly algorithm that, along with the global attention of \"sink attention tokens,\" achieves $92 \\%$ of the perplexity improvement of full attention after fine-tuning. Additionally, we apply a state-of-the-art KV cache compression algorithm, Heavy Hitter Oracle $\\left(\\mathrm{H}_{2} \\mathrm{O}\\right)$, to accelerate inference [43, 16, 25]. We further enhanced SinkLoRA through supervised fine-tuning using our self-collected LongAlpacaPlus dataset, comprising 28,000 entries from various sources, including Natural Questions, RedPajama [8], Book Summarization, and LongQA [6], ensuring a diverse and comprehensive collection for long instruction tuning. In summary, the contributions of our work are as follows:\n\n- We present SinkLoRA, a memory-efficient and effective method to extend the context length of LLaMA2 and LLaMA3, representing a complete update of LongLoRA. This method improves fine-tuning efficiency and offers a flexible deployment inference strategy. - We introduce SF-Attn, a fine-tuning method that combines a segmentation \\& reassembly algorithm and global attention. This method is easy to implement, accurate, and memoryefficient, without increasing computational complexity. By directly modifying the attention pattern, SF-Attn effectively redistributes attention scores, reducing the undue emphasis on initial tokens across different token groups. - We achieve efficient deployment of computationally intensive large language models (LLMs) in production environments by using the Heavy Hitter Oracle $\\left(\\mathrm{H}_{2} \\mathrm{O}\\right) \\mathrm{KV}$ caching method.\n```\n\n#### 2. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.89)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 3/32 (Score: 0.95)*\n\n```\nexisting methods. The language model, pre-trained on texts of length $L$, predicts the $T$ th token $(T \\gg L)$. (a) Dense Attention has $O\\left(T^{2}\\right)$ time complexity and an increasing cache size. Its performance decreases when the text length exceeds the pre-training text length. (b) Window Attention caches the most recent $L$ tokens' KV. While efficient in inference, performance declines sharply once the starting tokens' keys and values are evicted. (c) Sliding Window with Re-computation rebuilds the KV states from the $L$ recent tokens for each new token. While it performs well on long texts, its $O\\left(T L^{2}\\right)$ complexity, stemming from quadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM keeps the attention sink (several initial tokens) for stable attention computation, combined with the recent tokens. It's efficient and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B model on the first book ( 65 K tokens) in the PG-19 test set. When applying LLMs for infinite input streams, two primary challenges arise:\n\n1. During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV) of all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory usage and increasing decoding latency (Pope et al., 2022). 2. Existing models have limited length extrapolation abilities, i.e., their performance degrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the attention window size set during pre-training. An intuitive approach, known as window attention (Beltagy et al. 2020) (Figure 1 b), maintains only a fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant memory usage and decoding speed after the cache is initially filled, the model collapses once the sequence length exceeds the cache size, i.e., even just evicting the $K V$ of the first token, as illustrated in Figure 3 Another strategy is the sliding window with re-computation (shown in Figure 11c), which rebuilds the KV states of recent tokens for each generated token. While it offers strong performance, this approach is significantly slower due to the computation of quadratic attention within its window, making this method impractical for real-world streaming applications. To understand the failure of window attention, we find an interesting phenomenon of autoregressive LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens \"attention sinks\". Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks. Based on the above insights, we propose StreamingLLM, a simple and efficient framework that enables LLMs trained with a finite attention window to work on text of infinite length without finetuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving them can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply keeps the attention sink tokens' KV (with just 4 initial tokens sufficing) together with the sliding window's KV to anchor the attention computation and stabilize the model's performance. With StreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more. Compared with the only viable baseline, sliding window with recomputation, StreamingLLM achieves up to $22.2 \\times$ speedup, realizing the streaming use of LLMs. ![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-03.jpg?height=315&width=1335&top_left_y=271&top_left_x=384)\n\nFigure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1 ) exhibit the \"local\" pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends to the initial token across all layers and heads. Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can be pre-trained to require only a single attention sink token for streaming deployment. Specifically, we suggest that an extra learnable token at the beginning of all training samples can serve as a designated attention sink. By pre-training 160-million parameter language models from scratch, we demonstrate that adding this single sink token preserves the model's performance in streaming cases. This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial tokens as attention sinks to achieve the same performance level. Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within the KV cache without extending the LLMs' context length. It suits continuous operation needs with minimal memory use and past data reliance. Additionally, StreamingLLM can complement context extension methods to increase the attendable recent context. ## 2 RELATEd WORK\n\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus: Length Extrapolation, Context Window Extension, and Improving LLMs' Utilization of Long Text. While seemingly related, it's worth noting that progress in one direction doesn't necessarily lead to progress in the other. For example, extending the context size of LLMs doesn't improve the model's performance beyond the context size, and neither approach ensures effective use of the long context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied to text significantly exceeding the pre-training window size, potentially even of infinite length. We do not expand the attention window size of LLMs or enhance the model's memory and usage on long texts. The last two categories are orthogonal to our focus and could be integrated with our techniques. Length extrapolation aims to enable language models trained on shorter texts to handle longer ones during testing. A predominant avenue of research targets the development of relative position encoding methods for Transformer models, enabling them to function beyond their training window. One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms the queries and keys in every attention layer for relative position integration. Despite its promise, subsequent research (Press et al., 2022, Chen et al., 2023) indicated its underperformance on text that exceeds the training window. Another approach, $\\overline{A L i B i}$ (Press et al., 2022), biases the query-key attention scores based on their distance, thereby introducing relative positional information. While this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the text length was vastly greater than the training length. Current methodologies, however, have yet to achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications. Context Window Extension centers on expanding the LLMs' context window, enabling the processing of more tokens in one forward pass.\n```\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.84)*\n\n```\nFirstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach - but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to $22.2 \\times$ speedup. Code and datasets are provided in the link. ## 1 INTRODUCTION\n\nLarge Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022; OpenAI, 2023, Touvron et al., 2023a b) are becoming ubiquitous, powering many natural language processing applications such as dialog systems (Schulman et al., 2022, Taori et al., 2023, Chiang et al., 2023), document summarization (Goyal \\& Durrett, 2020, Zhang et al. 2023a), code completion (Chen et al. 2021, Rozi\u00e8re et al., 2023) and question answering (Kamalloo et al., 2023). To unleash the full potential of pretrained LLMs, they should be able to efficiently and accurately perform long sequence generation. For example, an ideal ChatBot assistant can stably work over the content of recent day-long conversations. However, it is very challenging for LLM to generalize to longer sequence lengths than they have been pretrained on, e.g., 4 K for Llama-2 Touvron et al. (2023b). The reason is that LLMs are constrained by the attention window during pre-training. Despite substantial efforts to expand this window size (Chen et al., 2023, kaiokendev, 2023; Peng et al., 2023) and improve training (Dao et al., 2022, Dao, 2023) and inference (Pope et al. 2022; Xiao et al. 2023, Anagnostidis et al., 2023;, Wang et al., 2021; Zhang et al. 2023b) efficiency for lengthy inputs, the acceptable sequence length remains intrinsically finite, which doesn't allow persistent deployments. In this paper, we first introduce the concept of LLM streaming applications and ask the question:\nCan we deploy an LLM for infinite-length inputs without sacrificing efficiency and performance? [^0]![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-02.jpg?height=474&width=1409&top_left_y=274&top_left_x=356)\n\nFigure 1: Illustration of StreamingLLM vs.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.52)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.52)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: attention sink mechanisms, adaptive tiling, selective KV caching\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: attention sink mechanisms*\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 336  (*Influential: 59*)\n\n#### 2. Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning\n\n*From Search Query: attention sink mechanisms*\n\n*Mattia Atzeni, Mrinmaya Sachan, Andreas Loukas*\n\n**TL;DR:** This work focuses on geometry priors and introduces LatFormer, a model that incorporates lattice symmetry priors in attention masks that provides preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Abstract:** The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Prototype memory and attention mechanisms for few shot image generation\n\n*From Search Query: attention sink mechanisms*\n\n*Tianqin Li, Zijie Li, Andrew Luo, Harold Rockwell, A. Farimani, T. Lee*\n\n**TL;DR:** The results demonstrate the feasibility of the idea that these super-sparse complex feature detectors of macaque monkeys can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Abstract:** Recent discoveries indicate that the neural codes in the super\ufb01cial layers of the primary visual cortex (V1) of macaque monkeys are complex, diverse and super-sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing during the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized through a memory-based attention operation. Integrating this mechanism, we propose Memory Concept Attention ( MoCA ) to improve few shot image generation quality. We show that having a prototype memory with attention mechanisms can improve image synthesis quality, learn interpretable visual concept clusters, and improve the robustness of the model. Our results demonstrate the feasibility of the idea that these super-sparse complex feature detectors can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 4. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\n\n*From Search Query: adaptive tiling*\n\n*Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao*\n\n**Abstract:** , question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 305  (*Influential: 55*)\n\n#### 5. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n*From Search Query: adaptive tiling*\n\n*Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao*\n\n**TL;DR:** Adaptive KV cache compression is introduced, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs) and demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss.\n\n**Abstract:** In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 102  (*Influential: 12*)\n\n#### 6. AdaPlanner: Adaptive Planning from Feedback with Language Models\n\n*From Search Query: adaptive tiling*\n\n*Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang*\n\n**TL;DR:** A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\n\n**Abstract:** Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 85  (*Influential: 17*)\n\n#### 7. Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time\n\n*From Search Query: selective KV caching*\n\n*Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava*\n\n**TL;DR:** This work hypothesizes the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations, and proposes Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model.\n\n**Abstract:** Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20X compression.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 107  (*Influential: 11*)\n\n#### 8. Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference\n\n*From Search Query: selective KV caching*\n\n*Harry Dong, Xinyu Yang, Zhenyu (Allen) Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen*\n\n**TL;DR:** LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps, shows merit on a variety of tasks where it can help reduce the performance gap from caching everything.\n\n**Abstract:** Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 22  (*Influential: 3*)\n\n### 1 related papers from ArXiv\n\n#### 1. Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms\n\n*From Search Query: attention sink mechanisms*\n\n*Tian Meng, Yang Tao, Wuliang Yin*\n\n**Abstract:** Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.\n\n**Published:** 2024-08-01T02:49:58Z  (*Updated: 2024-08-01T02:49:58Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Flowformer: Linearizing Transformers with Conservation Flows\n\n*From Search Query: attention sink mechanisms*\n\n*Mingsheng Long, Jianmin Wang, Jiehui Xu, Jialong Wu, Haixu Wu*\n\n**Abstract:** Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.\n\n**Published:** 2022-02-13\n\n\n\n#### 2. SEED-Story: Multimodal Long Story Generation with Large Language Model\n\n*From Search Query: attention sink mechanisms*\n\n*Yingcong Chen, Ying Shan, Yixiao Ge, Yukang Chen, Yang Li, Yuying Ge, Shuai Yang*\n\n**Abstract:** With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing narrative texts and vivid images in an interleaved manner, has emerged as a valuable and practical task with broad applications. However, this task poses significant challenges, as it necessitates the comprehension of the complex interplay between texts and images, and the ability to generate long sequences of coherent, contextually relevant texts and visuals. In this work, we propose SEED-Story, a novel method that leverages a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. Our model, built upon the powerful comprehension capability of MLLM, predicts text tokens as well as visual tokens, which are subsequently processed with an adapted visual de-tokenizer to produce images with consistent characters and styles. We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner. Additionally, we present a large-scale and high-resolution dataset named StoryStream for training our model and quantitatively evaluating the task of multimodal story generation in various aspects.\n\n**Published:** 2024-07-11\n\n\n\n#### 3. H2OPUS-TLR: High Performance Tile Low Rank Symmetric Factorizations using Adaptive Randomized Approximation\n\n*From Search Query: adaptive tiling*\n\n*Anonymous*\n\n**Abstract:** Tile low rank representations of dense matrices partition them into blocks of roughly uniform size, where each off-diagonal tile is compressed and stored as its own low rank factorization. They offer an attractive representation for many data-sparse dense operators that appear in practical applications, where substantial compression and a much smaller memory footprint can be achieved. TLR matrices are a compromise between the simplicity of a regular perfectly-strided data structure and the optimal complexity of the unbalanced trees of hierarchically low rank matrices, and provide a convenient performance-tuning parameter through their tile size that can be proportioned to take into account the cache size where the tiles reside in the memory hierarchy. There are currently no high-performance algorithms that can generate Cholesky and $LDL^T$ factorizations, particularly on GPUs. The difficulties in achieving high performance when factoring TLR matrices come from the expensive compression operations that must be performed during the factorization process and the adaptive rank distribution of the tiles that causes an irregular work pattern for the processing cores. In this work, we develop a dynamic batching operation and combine it with batched adaptive randomized approximations to achieve high performance both on GPUs and CPUs. Our implementation attains over 1.2 TFLOP/s in double precision on the V100 GPU, and is limited by the performance of batched GEMM operations. The Cholesky factorization of covariance matrix of size $N = 131K$ arising in spatial statistics can be factored to an accuracy $\\epsilon=10^{-2}$ in just a few seconds. We believe the proposed GEMM-centric algorithm allows it to be readily ported to newer hardware such as the tensor cores that are optimized for small GEMM operations.\n\n**Published:** 2021-08-26\n\n\n\n#### 4. Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection\n\n*From Search Query: adaptive tiling*\n\n*Zhen Lei, Yongqiang Yao, Shifeng Zhang, Stan Z. Li, Cheng Chi*\n\n**Abstract:** Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to $50.7\\%$ AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS\n\n**Conference:** bridging-the-gap-between-anchor-based-and-1\n\n**Published:** 2019-12-05\n\n\n\n#### 5. Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference\n\n*From Search Query: selective KV caching*\n\n*Purushotham Kamath, Ilya Soloveychik, Prashant J. Nair, Gaurav Jain, Akhil Arunkumar, Muhammad Adnan*\n\n**Abstract:** Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.\n\n**Published:** 2024-03-14\n\n\n\n#### 6. CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\n\n*From Search Query: selective KV caching*\n\n*Junchen Jiang, Shan Lu, Kuntai Du, Qizheng Zhang, Yihua Cheng, Siddhant Ray, YuHan Liu, Hanchen Li, Jiayi Yao*\n\n**Abstract:** Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising generation quality or incurring more storage cost.\n\n**Published:** 2024-05-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on attention sink mechanisms, adaptive tiling, and selective KV caching, here are some key insights and references that can guide the researchers:\n\n## Attention Sink Mechanisms\n- Attention sinks refer to the phenomenon where significant attention is assigned to the first token or initial tokens, even if they are not semantically important. This mechanism is crucial for streaming language models as it helps in stabilizing attention computation over long sequences.\n  - Research by Xiao et al. introduces StreamingLLM, which leverages attention sinks to maintain initial tokens, thereby improving performance in long-sequence modeling. This approach shows that keeping the Key and Value (KV) states of initial tokens can largely recover the performance of window attention without fine-tuning.\n  - The paper \"When Attention Sink Emerges in Language Models\" further elucidates that attention sinks emerge during pre-training and are influenced by optimization, data distribution, and loss functions. It also suggests that adding a placeholder token as a dedicated attention sink can improve streaming deployment.\n\n## Adaptive Tiling\n- Adaptive tiling strategies aim to optimize memory access and reduce computational overhead by dynamically adjusting the attention window.\n  - While the specific papers mentioned in the analysis do not directly discuss \"adaptive tiling,\" the concept is related to efficient attention mechanisms. For example, \"FlashAttention\" discusses improvements in attention computation efficiency through IO-awareness, which can inform adaptive tiling implementations to optimize memory access and computational overhead.\n  - The dynamic sparse attention mechanism in CASAK-V, which adapts attention sparsity and KV-cache compression based on the input context and task requirements, can also provide insights into how to implement adaptive tiling effectively. This mechanism learns to focus on local contexts for tasks like language modeling and captures sparse but important global interactions for tasks requiring long-range dependencies.\n\n## Selective KV Caching\n- Selective KV caching techniques are essential for reducing the memory footprint while maintaining generation quality.\n  - The paper \"Adaptive KV Cache Compression for LLMs\" by Ge et al. discusses adaptive KV cache compression methods that reduce the memory footprint without sacrificing performance. This approach aligns with the proposal's selective KV caching mechanism and highlights the importance of intelligent caching strategies.\n  - The CASAK-V framework also includes an adaptive KV-cache compression technique that dynamically adapts both attention sparsity and KV-cache compression based on the input context and task requirements. This approach achieves effective compression ratios and maintains performance across diverse tasks and deployment scenarios.\n\n## Additional Insights\n- **Optimization and Training Dynamics**: The emergence of attention sinks is influenced by optimization techniques, data distribution, and loss functions. Strategies such as replacing softmax with ReLU and Adam with SGD can mitigate extreme-token phenomena during pretraining.\n- **Dynamic Attention Mechanisms**: CASAK-V's dynamic sparse attention mechanism, which builds upon recent works on efficient attention, can serve as a model for implementing adaptive and efficient attention strategies. This mechanism adapts to the task and input, becoming sparser for longer sequences while maintaining important connections.\n\nBy integrating these insights, the researchers can develop a more efficient and scalable autoregressive language model that leverages attention sink mechanisms, adaptive tiling strategies, and selective KV caching techniques to enhance memory efficiency, computational performance, and overall model robustness.\n",
            "analysis": "### Summary of Analysis\nThe proposal for StreamRetNetMLP introduces novel elements such as attention sink mechanisms, adaptive tiling, and selective KV caching to optimize streaming language models. The search results provide insights into similar concepts and techniques that can enhance memory efficiency and computational performance in language models.\n\n### Useful References and Excerpts\n1. **SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models** (Zhang, 2024): This paper discusses the use of attention sinks to stabilize attention computation in streaming scenarios, similar to the proposal's approach. It highlights the importance of maintaining initial tokens as attention sinks to improve performance in long-sequence modeling.\n\n2. **Efficient Streaming Language Models with Attention Sinks** (Xiao et al., 2023): This paper introduces StreamingLLM, which uses attention sinks to maintain initial tokens as attention sinks, improving performance in long-sequence modeling. It emphasizes the need for efficient memory usage and computational optimization in streaming scenarios.\n\n3. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** (Dao et al., 2022): Discusses improvements in attention computation efficiency, which can be related to the proposal's adaptive tiling mechanism to optimize memory access and computational overhead.\n\n4. **Adaptive KV Cache Compression for LLMs** (Ge et al., 2023): This paper discusses adaptive KV cache compression, which aligns with the proposal's selective KV caching mechanism. It emphasizes reducing memory footprint while maintaining generation quality.\n\n### Key Insights\n- **Attention Sink Mechanisms**: The concept of using attention sinks to stabilize attention computation is supported by existing research, indicating its potential effectiveness in streaming scenarios.\n- **Adaptive Tiling**: Techniques like those used in FlashAttention-2 and S2-Attention can inform the implementation of adaptive tiling to improve memory access and reduce computational overhead.\n- **Selective KV Caching**: Adaptive KV cache compression methods highlight the importance of intelligent caching strategies to enhance efficiency without sacrificing performance.\n\n### Future Search Plan\n- Conduct further searches on specific implementations of attention sink mechanisms and adaptive tiling in language models.\n- Explore more detailed technical papers on KV caching strategies to refine the proposal's approach.\n\n### References\n- Zhang, H. (2024). SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models. arXiv.org.\n- Xiao, G., Tian, Y., Chen, B., Han, S., & Lewis, M. (2023). Efficient Streaming Language Models with Attention Sinks. arXiv.org.\n- Dao, T. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. International Conference on Learning Representations.\n- Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., & Gao, J. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "attention sink, adaptive tiling, KV caching",
            "detail": "streaming optimization in language models, memory-efficient gradient computation, attention sink mechanisms",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nstreaming optimization in language models, memory-efficient gradient computation, attention sink mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.99)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 36/64 (Score: 0.99)*\n\n```\narXiv:1803.08494, 2018. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy S Liang, Quoc V Le, Tengyu Ma, and Adams Wei Yu. Doremi: Optimizing data mixtures speeds up language model pretraining.\n```\n\n#### 2. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.98)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 3/32 (Score: 0.99)*\n\n```\nexisting methods. The language model, pre-trained on texts of length $L$, predicts the $T$ th token $(T \\gg L)$. (a) Dense Attention has $O\\left(T^{2}\\right)$ time complexity and an increasing cache size. Its performance decreases when the text length exceeds the pre-training text length. (b) Window Attention caches the most recent $L$ tokens' KV. While efficient in inference, performance declines sharply once the starting tokens' keys and values are evicted. (c) Sliding Window with Re-computation rebuilds the KV states from the $L$ recent tokens for each new token. While it performs well on long texts, its $O\\left(T L^{2}\\right)$ complexity, stemming from quadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM keeps the attention sink (several initial tokens) for stable attention computation, combined with the recent tokens. It's efficient and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B model on the first book ( 65 K tokens) in the PG-19 test set. When applying LLMs for infinite input streams, two primary challenges arise:\n\n1. During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV) of all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory usage and increasing decoding latency (Pope et al., 2022). 2. Existing models have limited length extrapolation abilities, i.e., their performance degrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the attention window size set during pre-training. An intuitive approach, known as window attention (Beltagy et al. 2020) (Figure 1 b), maintains only a fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant memory usage and decoding speed after the cache is initially filled, the model collapses once the sequence length exceeds the cache size, i.e., even just evicting the $K V$ of the first token, as illustrated in Figure 3 Another strategy is the sliding window with re-computation (shown in Figure 11c), which rebuilds the KV states of recent tokens for each generated token. While it offers strong performance, this approach is significantly slower due to the computation of quadratic attention within its window, making this method impractical for real-world streaming applications. To understand the failure of window attention, we find an interesting phenomenon of autoregressive LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens \"attention sinks\". Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks. Based on the above insights, we propose StreamingLLM, a simple and efficient framework that enables LLMs trained with a finite attention window to work on text of infinite length without finetuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving them can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply keeps the attention sink tokens' KV (with just 4 initial tokens sufficing) together with the sliding window's KV to anchor the attention computation and stabilize the model's performance. With StreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more. Compared with the only viable baseline, sliding window with recomputation, StreamingLLM achieves up to $22.2 \\times$ speedup, realizing the streaming use of LLMs. ![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-03.jpg?height=315&width=1335&top_left_y=271&top_left_x=384)\n\nFigure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1 ) exhibit the \"local\" pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends to the initial token across all layers and heads. Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can be pre-trained to require only a single attention sink token for streaming deployment. Specifically, we suggest that an extra learnable token at the beginning of all training samples can serve as a designated attention sink. By pre-training 160-million parameter language models from scratch, we demonstrate that adding this single sink token preserves the model's performance in streaming cases. This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial tokens as attention sinks to achieve the same performance level. Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within the KV cache without extending the LLMs' context length. It suits continuous operation needs with minimal memory use and past data reliance. Additionally, StreamingLLM can complement context extension methods to increase the attendable recent context. ## 2 RELATEd WORK\n\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus: Length Extrapolation, Context Window Extension, and Improving LLMs' Utilization of Long Text. While seemingly related, it's worth noting that progress in one direction doesn't necessarily lead to progress in the other. For example, extending the context size of LLMs doesn't improve the model's performance beyond the context size, and neither approach ensures effective use of the long context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied to text significantly exceeding the pre-training window size, potentially even of infinite length. We do not expand the attention window size of LLMs or enhance the model's memory and usage on long texts. The last two categories are orthogonal to our focus and could be integrated with our techniques. Length extrapolation aims to enable language models trained on shorter texts to handle longer ones during testing. A predominant avenue of research targets the development of relative position encoding methods for Transformer models, enabling them to function beyond their training window. One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms the queries and keys in every attention layer for relative position integration. Despite its promise, subsequent research (Press et al., 2022, Chen et al., 2023) indicated its underperformance on text that exceeds the training window. Another approach, $\\overline{A L i B i}$ (Press et al., 2022), biases the query-key attention scores based on their distance, thereby introducing relative positional information. While this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the text length was vastly greater than the training length. Current methodologies, however, have yet to achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications. Context Window Extension centers on expanding the LLMs' context window, enabling the processing of more tokens in one forward pass.\n```\n\n##### *Relevant Chunk: No. 1/32 (Score: 0.97)*\n\n```\n# EFFICIENT STREAMING LANGUAGE MODELS WITH ATTENTION SinKs \n\nGuangxuan Xiao $^{1 *}$ Yuandong Tian ${ }^{2} \\quad$ Beidi Chen $^{3} \\quad$ Song Han ${ }^{1,4} \\quad$ Mike Lewis $^{2}$<br>${ }^{1}$ Massachusetts Institute of Technology ${ }^{2}$ Meta AI<br>${ }^{3}$ Carnegie Mellon University ${ }^{4}$ NVIDIA<br>https://github.com/mit-han-lab/streaming-llm\n\n\n#### Abstract\n\nDeploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges.\n```\n\n#### 3. Fast Transformers via Sketching Polynomial Kernels (Avg. Score: 0.97)\n\n*Praneeth Kacham, V. Mirrokni, Peilin Zhong*\n\n**Published in:**  (2023)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper demonstrates that polynomial attention with high degree can effectively replace softmax without sacrificing model quality, and develops polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees.\n\n**Abstract:** The quadratic time and memory complexity inherent to self-attention mechanisms, with respect to sequence length, presents a critical computational bottleneck in the training and deployment of large-scale Transformer-based language models. Recent theoretical results indicate the intractability of sub-quadratic softmax attention approximation under reasonable complexity assumptions. This paper addresses this challenge by first demonstrating that polynomial attention with high degree can effectively replace softmax without sacrificing model quality. Next, we develop polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees. Crucially, our approach achieves this speedup without requiring the sparsification of attention matrices. We also present a block-based algorithm to apply causal masking efficiently. Combining these techniques, we provide \\emph{PolySketchFormer}, a practical linear-time Transformer architecture for language modeling that offers provable guarantees. We validate PolySketchFormer empirically by training language models capable of handling long contexts. These experiments utilize both synthetic and real-world datasets (PG19, Wikipedia and C4) on Google Cloud TPUs. For context lengths of 32k and GPT-2 style models, our model achieves a 2.5-4x speedup in training compared to FlashAttention, with no observed degradation in quality across our experiments.\n\n##### *Relevant Chunk: No. 29/40 (Score: 0.97)*\n\n```\nFoundations and Trends\u00ae in Theoretical Computer Science, 10(1-2):1-157, 2014. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. Yang, S., Wang, B., Shen, Y., Panda, R., and Kim, Y. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. Zaheer, M., Guruganesh, G., Dubey, K. A., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:17283-17297, 2020. Zellers, R., Holtzman, A., Bisk, Y., Farhadi, A., and Choi, Y. Hellaswag: Can a machine really finish your sentence?\n```\n\n#### 4. Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention (Avg. Score: 0.96)\n\n*Qianchao Zhu, Jiangfei Duan, Chang Chen, Siran Liu, Xiuhong Li, Guanyu Feng, Xin Lv, Huanqi Cao, Chuanfu Xiao, Xingcheng Zhang, Dahua Lin, Chao Yang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper proposes SampleAttention, an adaptive structured and near-lossless sparse attention, which can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times compared with FlashAttention.\n\n**Abstract:** Large language models (LLMs) now support extremely long context windows, but the quadratic complexity of vanilla attention results in significantly long Time-to-First-Token (TTFT) latency. Existing approaches to address this complexity require additional pretraining or finetuning, and often sacrifice model accuracy. In this paper, we first provide both theoretical and empirical foundations for near-lossless sparse attention. We find dynamically capturing head-specific sparse patterns at runtime with low overhead is crucial. To address this, we propose SampleAttention, an adaptive structured and near-lossless sparse attention. Leveraging observed significant sparse patterns, SampleAttention attends to a fixed percentage of adjacent tokens to capture local window patterns, and employs a two-stage query-guided key-value filtering approach, which adaptively select a minimum set of key-values with low overhead, to capture column stripe patterns. Comprehensive evaluations show that SampleAttention can seamlessly replace vanilla attention in off-the-shelf LLMs with nearly no accuracy loss, and reduces TTFT by up to $2.42\\times$ compared with FlashAttention.\n\n##### *Relevant Chunk: No. 11/25 (Score: 0.96)*\n\n```\narXiv preprint arXiv:2312.00752, 2023. [33] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [34] Aydar Bulatov, Yury Kuratov, and Mikhail Burtsev. Recurrent memory transformer. Advances in Neural Information Processing Systems, 35:11079-11091, 2022. [35] Yuhuai Wu, Markus N Rabe, DeLesley Hutchins, and Christian Szegedy. Memorizing transformers. arXiv preprint arXiv:2203.08913, 2022. [36] Sebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206-2240. PMLR, 2022. [37] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: attention sink, adaptive tiling, KV caching\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\n\n*From Search Query: attention sink*\n\n*Tri Dao*\n\n**TL;DR:** This work tweak the algorithm to reduce the number of non-matmul FLOPs, and parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and distribute the work between warps to reduce communication through shared memory.\n\n**Abstract:** Scaling Transformers to longer sequence lengths has been a major problem in the last several years, promising to improve performance in language modeling and high-resolution image understanding, as well as to unlock new applications in code, audio, and video generation. The attention layer is the main bottleneck in scaling to longer sequences, as its runtime and memory increase quadratically in the sequence length. FlashAttention exploits the asymmetric GPU memory hierarchy to bring significant memory saving (linear instead of quadratic) and runtime speedup (2-4$\\times$ compared to optimized baselines), with no approximation. However, FlashAttention is still not nearly as fast as optimized matrix-multiply (GEMM) operations, reaching only 25-40\\% of the theoretical maximum FLOPs/s. We observe that the inefficiency is due to suboptimal work partitioning between different thread blocks and warps on the GPU, causing either low-occupancy or unnecessary shared memory reads/writes. We propose FlashAttention-2, with better work partitioning to address these issues. In particular, we (1) tweak the algorithm to reduce the number of non-matmul FLOPs (2) parallelize the attention computation, even for a single head, across different thread blocks to increase occupancy, and (3) within each thread block, distribute the work between warps to reduce communication through shared memory. These yield around 2$\\times$ speedup compared to FlashAttention, reaching 50-73\\% of the theoretical maximum FLOPs/s on A100 and getting close to the efficiency of GEMM operations. We empirically validate that when used end-to-end to train GPT-style models, FlashAttention-2 reaches training speed of up to 225 TFLOPs/s per A100 GPU (72\\% model FLOPs utilization).\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 665  (*Influential: 68*)\n\n#### 2. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\n\n*From Search Query: adaptive tiling*\n\n*Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao*\n\n**Abstract:** , question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 305  (*Influential: 55*)\n\n#### 3. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n*From Search Query: adaptive tiling*\n\n*Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao*\n\n**TL;DR:** Adaptive KV cache compression is introduced, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs) and demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss.\n\n**Abstract:** In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 102  (*Influential: 12*)\n\n#### 4. AdaPlanner: Adaptive Planning from Feedback with Language Models\n\n*From Search Query: adaptive tiling*\n\n*Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang*\n\n**TL;DR:** A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\n\n**Abstract:** Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 85  (*Influential: 17*)\n\n#### 5. Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference\n\n*From Search Query: KV caching*\n\n*Harry Dong, Xinyu Yang, Zhenyu (Allen) Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen*\n\n**TL;DR:** LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps, shows merit on a variety of tasks where it can help reduce the performance gap from caching everything.\n\n**Abstract:** Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 22  (*Influential: 3*)\n\n#### 6. Unlocking Data-free Low-bit Quantization with Matrix Decomposition for KV Cache Compression\n\n*From Search Query: KV caching*\n\n*Peiyu Liu, Zeming Gao, Wayne Xin Zhao, Yipeng Ma, Tao Wang, Ji-Rong Wen*\n\n**TL;DR:** This paper introduces DecoQuant, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache, and proposes to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor.\n\n**Abstract:** Key-value~(KV) caching is an important technique to accelerate the inference of large language models~(LLMs), but incurs significant memory overhead. To compress the size of KV cache, existing methods often compromise precision or require extra data for calibration, limiting their practicality in LLM deployment. In this paper, we introduce \\textbf{DecoQuant}, a novel data-free low-bit quantization technique based on tensor decomposition methods, to effectively compress KV cache. Our core idea is to adjust the outlier distribution of the original matrix by performing tensor decomposition, so that the quantization difficulties are migrated from the matrix to decomposed local tensors. Specially, we find that outliers mainly concentrate on small local tensors, while large tensors tend to have a narrower value range. Based on this finding, we propose to apply low-bit quantization to the large tensor, while maintaining high-precision representation for the small tensor. Furthermore, we utilize the proposed quantization method to compress the KV cache of LLMs to accelerate the inference and develop an efficient dequantization kernel tailored specifically for DecoQuant. Through extensive experiments, DecoQuant demonstrates remarkable efficiency gains, showcasing up to a $\\sim$75\\% reduction in memory footprint while maintaining comparable generation quality.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n### 4 related papers from ArXiv\n\n#### 1. When Attention Sink Emerges in Language Models: An Empirical View\n\n*From Search Query: attention sink*\n\n*Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin*\n\n**Abstract:** Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.\n\n**Published:** 2024-10-14T17:50:28Z  (*Updated: 2024-10-14T17:50:28Z*)\n\n\n\n#### 2. Unveiling and Harnessing Hidden Attention Sinks: Enhancing Large\n  Language Models without Training through Attention Calibration\n\n*From Search Query: attention sink*\n\n*Zhongzhi Yu, Zheng Wang, Yonggan Fu, Huihong Shi, Khalid Shaikh, Yingyan Celine Lin*\n\n**Abstract:** Attention is a fundamental component behind the remarkable achievements of\nlarge language models (LLMs). However, our current understanding of the\nattention mechanism, especially regarding how attention distributions are\nestablished, remains limited. Inspired by recent studies that explore the\npresence of attention sink in the initial token, which receives\ndisproportionately large attention scores despite their lack of semantic\nimportance, this work delves deeper into this phenomenon. We aim to provide a\nmore profound understanding of the existence of attention sinks within LLMs and\nto uncover ways to enhance the achievable accuracy of LLMs by directly\noptimizing the attention distributions, without the need for weight finetuning.\nSpecifically, this work begins with comprehensive visualizations of the\nattention distributions in LLMs during inference across various inputs and\ntasks. Based on these visualizations, to the best of our knowledge, we are the\nfirst to discover that (1) attention sinks occur not only at the start of\nsequences but also within later tokens of the input, and (2) not all attention\nsinks have a positive impact on the achievable accuracy of LLMs. Building upon\nour findings, we propose a training-free Attention Calibration Technique (ACT)\nthat automatically optimizes the attention distributions on the fly during\ninference in an input-adaptive manner. Extensive experiments validate that ACT\nconsistently enhances the accuracy of various LLMs across different\napplications. Specifically, ACT achieves an average improvement of up to 7.30%\nin accuracy across different datasets when applied to Llama-30B. Our code is\navailable at https://github.com/GATECH-EIC/ACT.\n\n**Published:** 2024-06-22T07:00:43Z  (*Updated: 2024-06-22T07:00:43Z*)\n\n\n\n#### 3. Cost-Efficient Large Language Model Serving for Multi-turn Conversations\n  with CachedAttention\n\n*From Search Query: KV caching*\n\n*Bin Gao, Zhuomin He, Puru Sharma, Qingxuan Kang, Djordje Jevdjic, Junbo Deng, Xingkun Yang, Zhou Yu, Pengfei Zuo*\n\n**Abstract:** Interacting with humans through multi-turn conversations is a fundamental\nfeature of large language models (LLMs). However, existing LLM serving engines\nexecuting multi-turn conversations are inefficient due to the need to\nrepeatedly compute the key-value (KV) caches of historical tokens, incurring\nhigh serving costs. To address the problem, this paper proposes\nCachedAttention, a new attention mechanism that enables reuse of KV caches\nacross multi-turn conversations, significantly reducing the repetitive\ncomputation overheads. CachedAttention maintains a hierarchical KV caching\nsystem that leverages cost-effective memory/storage mediums to save KV caches\nfor all requests. To reduce KV cache access overheads from slow mediums,\nCachedAttention employs layer-wise pre-loading and asynchronous saving schemes\nto overlap the KV cache access with the GPU computation. To ensure that the KV\ncaches to be accessed are placed in the fastest hierarchy, CachedAttention\nemploys scheduler-aware fetching and eviction schemes to consciously place the\nKV caches in different layers based on the hints from the inference job\nscheduler. To avoid the invalidation of the saved KV caches incurred by context\nwindow overflow, CachedAttention enables the saved KV caches to remain valid\nvia decoupling the positional encoding and effectively truncating the KV\ncaches. Extensive experimental results demonstrate that CachedAttention\nsignificantly decreases the time to the first token (TTFT) by up to 87%,\nimproves the prompt prefilling throughput by up to 7.8$\\times$ for multi-turn\nconversations, and reduces the end-to-end inference cost by up to 70%.\n\n**Published:** 2024-03-23T10:42:49Z  (*Updated: 2024-06-30T23:50:38Z*)\n\n\n\n#### 4. KIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cache\n\n*From Search Query: KV caching*\n\n*Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, Xia Hu*\n\n**Abstract:** Efficiently serving large language models (LLMs) requires batching of many\nrequests to reduce the cost per request. Yet, with larger batch sizes and\nlonger context lengths, the key-value (KV) cache, which stores attention keys\nand values to avoid re-computations, significantly increases memory demands and\nbecomes the new bottleneck in speed and memory usage. Additionally, the loading\nof the KV cache causes the computational core to be idle, which limits the\ninference speed. A straightforward and effective solution to reduce KV cache\nsize is quantization, which decreases the total bytes taken by KV cache.\nHowever, there is a lack of in-depth studies that explore the element\ndistribution of KV cache to understand the hardness and limitation of KV cache\nquantization. To fill the gap, we conducted a comprehensive study on the\nelement distribution in KV cache of popular LLMs. Our findings indicate that\nthe key cache should be quantized per-channel, i.e., group elements along the\nchannel dimension and quantize them together. In contrast, the value cache\nshould be quantized per-token. From this analysis, we developed a tuning-free\n2bit KV cache quantization algorithm named KIVI. With hardware-friendly\nimplementation, KIVI can enable Llama, Falcon, and Mistral models to maintain\nalmost the same quality while using $\\mathbf{2.6\\times}$ less peak memory\n(including model weight). This reduction in memory usage enables up to\n$\\mathbf{4\\times}$ larger batch size, bringing $\\mathbf{2.35\\times \\sim\n3.47\\times}$ throughput on real LLM inference workload. The source code is\navailable at https://github.com/jy-yuan/KIVI.\n\n**Published:** 2024-02-05T06:06:47Z  (*Updated: 2024-07-25T09:16:05Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: attention sink*\n\n*Mike Lewis, Song Han, Beidi Chen, Yuandong Tian, Guangxuan Xiao*\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Published:** 2023-09-29\n\n\n\n#### 2. Prefixing Attention Sinks can Mitigate Activation Outliers for Large Language Model Quantization\n\n*From Search Query: attention sink*\n\n*Jaeho Lee, Kyuyeun Kim, Woohyun Han, Wonpyo Park, Seungwoo Son*\n\n**Abstract:** Despite recent advances in LLM quantization, activation quantization remains to be challenging due to the activation outliers. Conventional remedies, e.g., mixing precisions for different channels, introduce extra overhead and reduce the speedup. In this work, we develop a simple yet effective strategy to facilitate per-tensor activation quantization by preventing the generation of problematic tokens. Precisely, we propose a method to find a set of key-value cache, coined CushionCache, which mitigates outliers in subsequent tokens when inserted as a prefix. CushionCache works in two steps: First, we greedily search for a prompt token sequence that minimizes the maximum activation values in subsequent tokens. Then, we further tune the token cache to regularize the activations of subsequent tokens to be more quantization-friendly. The proposed method successfully addresses activation outliers of LLMs, providing a substantial performance boost for per-tensor activation quantization methods. We thoroughly evaluate our method over a wide range of models and benchmarks and find that it significantly surpasses the established baseline of per-tensor W8A8 quantization and can be seamlessly integrated with the recent activation quantization method.\n\n**Published:** 2024-06-17\n\n\n\n#### 3. H2OPUS-TLR: High Performance Tile Low Rank Symmetric Factorizations using Adaptive Randomized Approximation\n\n*From Search Query: adaptive tiling*\n\n*Anonymous*\n\n**Abstract:** Tile low rank representations of dense matrices partition them into blocks of roughly uniform size, where each off-diagonal tile is compressed and stored as its own low rank factorization. They offer an attractive representation for many data-sparse dense operators that appear in practical applications, where substantial compression and a much smaller memory footprint can be achieved. TLR matrices are a compromise between the simplicity of a regular perfectly-strided data structure and the optimal complexity of the unbalanced trees of hierarchically low rank matrices, and provide a convenient performance-tuning parameter through their tile size that can be proportioned to take into account the cache size where the tiles reside in the memory hierarchy. There are currently no high-performance algorithms that can generate Cholesky and $LDL^T$ factorizations, particularly on GPUs. The difficulties in achieving high performance when factoring TLR matrices come from the expensive compression operations that must be performed during the factorization process and the adaptive rank distribution of the tiles that causes an irregular work pattern for the processing cores. In this work, we develop a dynamic batching operation and combine it with batched adaptive randomized approximations to achieve high performance both on GPUs and CPUs. Our implementation attains over 1.2 TFLOP/s in double precision on the V100 GPU, and is limited by the performance of batched GEMM operations. The Cholesky factorization of covariance matrix of size $N = 131K$ arising in spatial statistics can be factored to an accuracy $\\epsilon=10^{-2}$ in just a few seconds. We believe the proposed GEMM-centric algorithm allows it to be readily ported to newer hardware such as the tensor cores that are optimized for small GEMM operations.\n\n**Published:** 2021-08-26\n\n\n\n#### 4. Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection\n\n*From Search Query: adaptive tiling*\n\n*Zhen Lei, Yongqiang Yao, Shifeng Zhang, Stan Z. Li, Cheng Chi*\n\n**Abstract:** Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to $50.7\\%$ AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS\n\n**Conference:** bridging-the-gap-between-anchor-based-and-1\n\n**Published:** 2019-12-05\n\n\n\n#### 5. NACL: A General and Effective KV Cache Eviction Framework for LLMs at Inference Time\n\n*From Search Query: KV caching*\n\n*Hua Wu, dianhai yu, Yu Sun, Shuohuan Wang, Tingwen Liu, Zhenyu Zhang, Shiyao Cui, Junyuan Shang, Guoxia Wang, Yilong Chen*\n\n**Abstract:** Large Language Models (LLMs) have ignited an innovative surge of AI applications, marking a new era of exciting possibilities equipped with extended context windows. However, hosting these models is cost-prohibitive mainly due to the extensive memory consumption of KV Cache involving long-context modeling. Despite several works proposing to evict unnecessary tokens from the KV Cache, most of them rely on the biased local statistics of accumulated attention scores and report performance using unconvincing metric like perplexity on inadequate short-text evaluation. In this paper, we propose NACL, a general framework for long-context KV cache eviction that achieves more optimal and efficient eviction in a single operation during the encoding phase. Due to NACL's efficiency, we combine more accurate attention score statistics in PROXY TOKENS EVICTION with the diversified random eviction strategy of RANDOM EVICTION, aiming to alleviate the issue of attention bias and enhance the robustness in maintaining pivotal tokens for long-context modeling tasks. Notably, our method significantly improves the performance on short- and long-text tasks by 80% and 76% respectively, reducing KV Cache by up to 50% with over 95% performance maintenance. The code is available at https://github.com/PaddlePaddle/Research/tree/master/NLP/ACL2024-NACL.\n\n**Published:** 2024-08-07\n\n\n\n#### 6. Leveraging Speculative Sampling and KV-Cache Optimizations Together for Generative AI using OpenVINO\n\n*From Search Query: KV caching*\n\n*Yury Gorbachev, Ekaterina Aidova, Haim Barad*\n\n**Abstract:** Inference optimizations are critical for improving user experience and reducing infrastructure costs and power consumption. In this article, we illustrate a form of dynamic execution known as speculative sampling to reduce the overall latency of text generation and compare it with standard autoregressive sampling. This can be used together with model-based optimizations (e.g. quantization) to provide an optimized solution. Both sampling methods make use of KV caching. A Jupyter notebook and some sample executions are provided.\n\n**Published:** 2023-11-08\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on streaming optimization, memory efficiency, and computational performance, here are some key insights and techniques from the provided sources:\n\n## Attention Sink Mechanism\n\n- The concept of an attention sink, as mentioned in the initial analysis, is about maintaining key initial tokens to stabilize attention computation. While the provided sources do not explicitly discuss \"attention sink\" mechanisms, they do address related concepts that can contribute to memory efficiency and stable attention computation.\n  - For instance, the **S2-Attention** mechanism discussed in involves efficiently tracking and merging sparse query blocks that share common key-value pairs. This approach can help in stabilizing attention computation by ensuring that each tile is fully utilized, which might be analogous to the attention sink concept in terms of optimizing memory usage and computational efficiency.\n\n## Adaptive Tiling Mechanism\n\n- **Adaptive tiling** is crucial for efficient memory access and optimized gradient computation. The **S2-Attention** paper introduces a technique called \"MergeQ\" which adaptively merges query segments that share common key-value pairs into the same tile. This approach improves SRAM utilization and minimizes redundant key-value loading, which aligns with the goals of adaptive tiling.\n  - **SageAttention** also uses a tiling strategy inspired by FlashAttention, but with additional quantization techniques to improve efficiency. The use of INT8 quantization and FP16 accumulators in SageAttention can further optimize the computational overhead associated with attention mechanisms.\n\n## Selective KV Caching\n\n- **Selective KV caching** is essential for reducing repetitive computations in autoregressive generation.\n  - The **CASAK-V** approach implements adaptive chunk-wise KV-cache compression based on layer-wise sparse configurations. This method dynamically generates and applies head-specific sparse attention patterns and compresses the KV cache accordingly, which is similar to the concept of selective KV caching.\n  - **S2-Attention** also discusses the reuse of key-value shards across different query shards, which is achieved by merging sparse query blocks into mega shards. This technique reduces the need to reload key-value pairs multiple times, thereby improving efficiency.\n\n## Memory Efficiency and Computational Overhead\n\n- **Memory Efficiency**: The **S2-Attention** mechanism improves memory efficiency by ensuring that each tile is fully utilized, reducing idle threads and improving SRAM usage. This is crucial for streaming scenarios where memory is a limiting factor.\n  - **CASAK-V** also focuses on memory efficiency by implementing dynamic, context-aware sparse attention and adaptive KV-cache compression, which helps in retaining context-awareness while reducing computational complexity.\n\n- **Computational Overhead**: Both **SageAttention** and **S2-Attention** reduce computational overhead through efficient tiling strategies and quantization techniques. **SageAttention** achieves significant speed improvements over existing methods like FlashAttention2 and xformers, while **S2-Attention** optimizes the use of sparse attention patterns.\n\n## Novelty and Scalability\n\n- The integration of **attention sink mechanisms** and **adaptive tiling** in the StreamRetNetMLP proposal appears novel, especially when combined with selective KV caching.\n  - While similar concepts like sparse attention and KV caching have been explored, the specific combination and implementation details in the proposal could offer unique advantages.\n  - The scalability and robustness of these mechanisms, particularly in handling long sequences, would depend on how effectively they can be integrated and optimized. The dynamic and context-aware nature of **CASAK-V** and the efficient tiling strategies in **S2-Attention** and **SageAttention** suggest they could be highly scalable.\n\nIn summary, the techniques discussed in these sources provide valuable insights into how to optimize autoregressive language models for streaming scenarios, focusing on memory efficiency, computational performance, and scalability. The integration of attention sink mechanisms, adaptive tiling, and selective KV caching, along with efficient quantization and sparse attention strategies, can significantly enhance the performance of such models.\n",
            "analysis": "### Initial Analysis of StreamRetNetMLP Proposal\n\nThe proposal for StreamRetNetMLP introduces a streaming-optimized variant of RetNetMLP, focusing on improving memory efficiency and computational performance during streaming inference. The key innovations include:\n\n1. **Attention Sink Integration**: This mechanism is designed to maintain key initial tokens as attention sinks, which can stabilize attention computation in streaming scenarios and reduce memory requirements.\n\n2. **Adaptive Tiling Mechanism**: This involves IO-aware tiling for efficient memory access and optimized gradient computation, which is crucial for reducing computational overhead in processing long sequences.\n\n3. **Selective KV Caching**: This feature aims to intelligently cache key-value pairs, reducing repetitive computations in autoregressive generation, thereby enhancing efficiency.\n\n### Key Aspects for Further Investigation\n\n- **Memory Efficiency**: How does the attention sink mechanism specifically contribute to memory efficiency, and how does it compare to existing methods?\n- **Computational Overhead**: Evaluate the effectiveness of adaptive tiling and selective KV caching in reducing computational overhead.\n- **Novelty**: Compare the proposed mechanisms with existing literature to assess the novelty of the approach.\n- **Scalability and Robustness**: Investigate how these mechanisms affect the scalability and robustness of the model, particularly in handling long sequences.\n\n### Plan for Investigation\n\n1. **External Search**: Conduct a broad search to gather information on similar mechanisms in language models, focusing on memory efficiency and computational optimization.\n2. **Internal Search**: Perform a detailed search in the internal vector store to find specific papers or technical documents that discuss similar techniques or concepts.\n3. **Comparison with Existing Designs**: Compare the proposal with previous designs and existing research to assess its novelty and potential impact.\n\n### Novelty Assessment\n\nThe proposal appears to introduce novel elements, particularly in the integration of attention sink mechanisms and adaptive tiling, which are not commonly found in existing RetNetMLP designs. However, similar concepts like selective caching and memory optimization have been explored in other contexts, so a thorough comparison is needed to confirm the uniqueness of the proposal."
        },
        {
            "ready": false,
            "query": "streaming language models, memory efficiency, computational optimization",
            "detail": "attention sink mechanisms in language models, adaptive tiling strategies, selective KV caching techniques",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nattention sink mechanisms in language models, adaptive tiling strategies, selective KV caching techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.97)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 3/28 (Score: 1.00)*\n\n```\nThis method stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. - Our SinkLoRA performs favorably against state-of-the-art methods. We evaluate its performance on the PG19, Proof-pile, and LongBench datasets, demonstrating its effectiveness. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-04.jpg?height=375&width=1313&top_left_y=246&top_left_x=428)\n\nFigure 4: Illustration of the Segmentation and Reassembly process in SF-Attn. The process involves three steps: (1) Splitting features along the head dimension into two chunks: one shifted and one unshifted. (2) Splitting tokens, where the tokens belonging to the shifted chunk are shifted by half of the group size, and reassembling them at the tail of the tokens to match the unshifted chunk. (3) Combining the two chunks of tokens together. This figure is adapted from [6]. Specifically, for LLaMA2 7B, SinkLoRA outperforms LongLoRA and is competitive with LongChat-13B. ### 1.1 Motivation for the Research\n\n## Motivation 1: Elevating Attention Scores for Initial Tokens\n\nPrior studies have demonstrated the Attention Sink phenomenon, where certain tokens, typically the initial tokens in a sequence, receive disproportionately high attention scores during the model's computation [39]. This often occurs because these tokens are visible to all subsequent tokens, leading to significant attention even when they lack semantic importance, particularly in autoregressive language models [33]. The Sparse Shifted Attention mechanism implemented in LongLoRA [6] attempts to address this by shifting the high attention scores from these initial tokens to other tokens that previously received lower attention. This shift reduces the overemphasis on initial tokens. To further improve this, we need to develop a method that directly modifies the attention pattern. By applying this technique, we can effectively redistribute attention scores, thereby reducing the undue emphasis on initial tokens across different token groups. ## Motivation 2: Maintaining Initial Tokens During Fine-Tuning\n\nThe concept of attention sinks is also utilized in Streaming LLM [39] to improve the model's handling of long texts. By retaining the Key-Value (KV) pairs of a few initial tokens (attention sinks) along with the most recent tokens, the model ensures stable attention scores and performance even for extended sequences. Inspired by this approach, we aim to carry this mindset from training into inference. Our research aims to modify the fine-tuning process so that initial tokens attend to all other tokens, thereby accumulating more attention scores and enhancing the model's capacity to handle long sequences. ## Motivation 3: Flexible Deployment of Inference Strategy\n\nEfficient deployment of computationally intensive large language models (LLMs) in production environments often relies on Key-Value (KV) caching [16]. KV caching stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. However, LongLoRA [6] retains only the original standard self-attention mechanism during inference. To address this limitation, it is necessary to apply an optional KV cache function. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. ## 2 Related Work\n\n### 2.1 Long-context Transformers\n\nThe primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models. Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to $\\mathrm{O}(\\mathrm{n})$. These models achieve a balance between maintaining sufficient context for understanding while managing computational load. For achieving complexity of $O(n \\log n)$, several approaches have been proposed. Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window. Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity. LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead. Equipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token. This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts. Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited. This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications. ### 2.2 Long-context LLMs\n\nRecent advancements in Large Language Models (LLMs) have significantly extended their capabilities, including handling long-context inputs. Math Word Problems (MWPs) have demonstrated notable performance in solving mathematical questions using LLMs [34]. Moreover, leveraging LLMs for SQL querying has shown promise in optimizing resource allocation, though it remains less efficient than traditional relational databases [42]. LongLoRA [6], employing Position Interpolation [5], has successfully extended the context window of Llama 2 from 4096 to 32768 tokens without requiring substantial GPU or TPU resources. Meta's Llama 3, featuring up to 70 billion parameters, represents a significant advancement in open-source LLMs, offering enhancements in computational efficiency, trust and safety tools, and collaborations with major platforms [38]. Open-source models such as BLOOM [22], OPT [18], and Falcon [28] continue to challenge proprietary models, although models like Vicuna [29] and Alpaca [1] still lag behind their closed-source counterparts in certain aspects. Despite these advancements, effectively managing long-context interactions remains a significant challenge, necessitating ongoing research and development to address the complexities in long-context LLM applications. ### 2.3 KV-Cache Compression\n\nCompressing the size of KV cache is more difficult than reducing the size of weights because they are more sensitive and dependent on model inputs. A cost-effective method for KV cache compression is token dropping [25, 43, 16], which establishes an importance policy to retain significant KVs and remove insignificant ones.\n```\n\n##### *Relevant Chunk: No. 2/28 (Score: 0.95)*\n\n```\nThis extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention ( $\\left.\\mathrm{S}^{2}-\\mathrm{Attn}\\right)$, effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only $39 \\%$ of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose SinkLoRA, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving $92 \\%$ of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm $\\mathrm{H}_{2} \\mathrm{O}$ to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at https://github.com/Dexter-GT-86/SinkLoRA. ## 1 Introduction\n\nEnhancing the functionality of Transformer models to handle longer sequence lengths has become crucial for numerous applications, including language translation, long-context processing, chatbots, code generation, and multimedia content creation. The primary challenge lies in the self-attention mechanism, which scales quadratically with sequence length, leading to substantial computational time and memory requirements [4, 41, 21]. To address this challenge, several approaches have been proposed. Longformer and BigBird utilize combinations of local, global, and sparse attention mechanisms to manage long contexts, reducing complexity to $\\mathrm{O}(\\mathrm{n})$ [4, 41]. Reformer introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thereby reducing computational complexity [21]. LSG Attention combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead [9]. Despite these advancements, managing long-context interactions in practical applications remains a significant challenge. Recent work, such as LongLoRA, extends the context window of LLaMA2 from 4096 to 32768 tokens using Position Interpolation without substantial GPU or TPU resources\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-02.jpg?height=535&width=1381&top_left_y=404&top_left_x=365)\n\nFigure 1: Evaluation of SinkLoRA in bridging the accuracy gap between sparse shifted attention and full attention during supervised fine-tuning, while maintaining the memory efficiency of LongLoRA, which utilizes 1.8 times less memory compared to full fine-tuning. Furthermore, SinkLoRA retains the training speed of LongLoRA, being 1.8 times faster than full fine-tuning, due to the implementation of Sink Fixed Attention. The Llama2-7B models[37] are fine-tuned to various context lengths using Flash-Attention 2 [10] and DeepSpeed stage 2 [31], and are evaluated on the proof-pile test set [2] in terms of perplexity. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-02.jpg?height=349&width=1214&top_left_y=1644&top_left_x=484)\n\nFigure 2: Overview of the SinkLoRA fine-tuning process, incorporating Sink Fixed Attention (SFAttn). Panels (a), (b), and (c) depict the procedure to convert Sparse Shifted Attention into Short Window Attention and subsequently into Sink Fixed Attention. This conversion is executed in two stages: reassembly and making the initial tokens global. In addition to optimizing the LoRA weights within linear layers, SinkLoRA also enables training of the embedding and normalization layers, consistent with the methodology employed in LongLoRA. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-03.jpg?height=633&width=1264&top_left_y=280&top_left_x=428)\n\nFigure 3: Overview of the SinkLoRA inference process. Unlike LongLoRA, which retains the original standard self-attention during inference, SinkLoRA implements an optional KV cache compression method, $\\mathrm{H}^{2} \\mathrm{O}$ [43]. This extension enhances inference speed without significantly compromising performance. [6]. However, LongLoRA's efficiency is limited, achieving only $39 \\%$ of the perplexity improvement compared to full attention due to chaotic attention head structures and unnecessary information exchange between token groups. To address these issues, we propose SinkLoRA, which offers better work partitioning. This includes the development of Sink Fixed Attention (SF-Attn), a segmentation and reassembly algorithm that, along with the global attention of \"sink attention tokens,\" achieves $92 \\%$ of the perplexity improvement of full attention after fine-tuning. Additionally, we apply a state-of-the-art KV cache compression algorithm, Heavy Hitter Oracle $\\left(\\mathrm{H}_{2} \\mathrm{O}\\right)$, to accelerate inference [43, 16, 25]. We further enhanced SinkLoRA through supervised fine-tuning using our self-collected LongAlpacaPlus dataset, comprising 28,000 entries from various sources, including Natural Questions, RedPajama [8], Book Summarization, and LongQA [6], ensuring a diverse and comprehensive collection for long instruction tuning. In summary, the contributions of our work are as follows:\n\n- We present SinkLoRA, a memory-efficient and effective method to extend the context length of LLaMA2 and LLaMA3, representing a complete update of LongLoRA. This method improves fine-tuning efficiency and offers a flexible deployment inference strategy. - We introduce SF-Attn, a fine-tuning method that combines a segmentation \\& reassembly algorithm and global attention. This method is easy to implement, accurate, and memoryefficient, without increasing computational complexity. By directly modifying the attention pattern, SF-Attn effectively redistributes attention scores, reducing the undue emphasis on initial tokens across different token groups. - We achieve efficient deployment of computationally intensive large language models (LLMs) in production environments by using the Heavy Hitter Oracle $\\left(\\mathrm{H}_{2} \\mathrm{O}\\right) \\mathrm{KV}$ caching method.\n```\n\n#### 2. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.89)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 3/32 (Score: 0.95)*\n\n```\nexisting methods. The language model, pre-trained on texts of length $L$, predicts the $T$ th token $(T \\gg L)$. (a) Dense Attention has $O\\left(T^{2}\\right)$ time complexity and an increasing cache size. Its performance decreases when the text length exceeds the pre-training text length. (b) Window Attention caches the most recent $L$ tokens' KV. While efficient in inference, performance declines sharply once the starting tokens' keys and values are evicted. (c) Sliding Window with Re-computation rebuilds the KV states from the $L$ recent tokens for each new token. While it performs well on long texts, its $O\\left(T L^{2}\\right)$ complexity, stemming from quadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM keeps the attention sink (several initial tokens) for stable attention computation, combined with the recent tokens. It's efficient and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B model on the first book ( 65 K tokens) in the PG-19 test set. When applying LLMs for infinite input streams, two primary challenges arise:\n\n1. During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV) of all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory usage and increasing decoding latency (Pope et al., 2022). 2. Existing models have limited length extrapolation abilities, i.e., their performance degrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the attention window size set during pre-training. An intuitive approach, known as window attention (Beltagy et al. 2020) (Figure 1 b), maintains only a fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant memory usage and decoding speed after the cache is initially filled, the model collapses once the sequence length exceeds the cache size, i.e., even just evicting the $K V$ of the first token, as illustrated in Figure 3 Another strategy is the sliding window with re-computation (shown in Figure 11c), which rebuilds the KV states of recent tokens for each generated token. While it offers strong performance, this approach is significantly slower due to the computation of quadratic attention within its window, making this method impractical for real-world streaming applications. To understand the failure of window attention, we find an interesting phenomenon of autoregressive LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens \"attention sinks\". Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks. Based on the above insights, we propose StreamingLLM, a simple and efficient framework that enables LLMs trained with a finite attention window to work on text of infinite length without finetuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving them can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply keeps the attention sink tokens' KV (with just 4 initial tokens sufficing) together with the sliding window's KV to anchor the attention computation and stabilize the model's performance. With StreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more. Compared with the only viable baseline, sliding window with recomputation, StreamingLLM achieves up to $22.2 \\times$ speedup, realizing the streaming use of LLMs. ![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-03.jpg?height=315&width=1335&top_left_y=271&top_left_x=384)\n\nFigure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1 ) exhibit the \"local\" pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends to the initial token across all layers and heads. Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can be pre-trained to require only a single attention sink token for streaming deployment. Specifically, we suggest that an extra learnable token at the beginning of all training samples can serve as a designated attention sink. By pre-training 160-million parameter language models from scratch, we demonstrate that adding this single sink token preserves the model's performance in streaming cases. This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial tokens as attention sinks to achieve the same performance level. Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within the KV cache without extending the LLMs' context length. It suits continuous operation needs with minimal memory use and past data reliance. Additionally, StreamingLLM can complement context extension methods to increase the attendable recent context. ## 2 RELATEd WORK\n\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus: Length Extrapolation, Context Window Extension, and Improving LLMs' Utilization of Long Text. While seemingly related, it's worth noting that progress in one direction doesn't necessarily lead to progress in the other. For example, extending the context size of LLMs doesn't improve the model's performance beyond the context size, and neither approach ensures effective use of the long context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied to text significantly exceeding the pre-training window size, potentially even of infinite length. We do not expand the attention window size of LLMs or enhance the model's memory and usage on long texts. The last two categories are orthogonal to our focus and could be integrated with our techniques. Length extrapolation aims to enable language models trained on shorter texts to handle longer ones during testing. A predominant avenue of research targets the development of relative position encoding methods for Transformer models, enabling them to function beyond their training window. One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms the queries and keys in every attention layer for relative position integration. Despite its promise, subsequent research (Press et al., 2022, Chen et al., 2023) indicated its underperformance on text that exceeds the training window. Another approach, $\\overline{A L i B i}$ (Press et al., 2022), biases the query-key attention scores based on their distance, thereby introducing relative positional information. While this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the text length was vastly greater than the training length. Current methodologies, however, have yet to achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications. Context Window Extension centers on expanding the LLMs' context window, enabling the processing of more tokens in one forward pass.\n```\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.84)*\n\n```\nFirstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach - but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to $22.2 \\times$ speedup. Code and datasets are provided in the link. ## 1 INTRODUCTION\n\nLarge Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022; OpenAI, 2023, Touvron et al., 2023a b) are becoming ubiquitous, powering many natural language processing applications such as dialog systems (Schulman et al., 2022, Taori et al., 2023, Chiang et al., 2023), document summarization (Goyal \\& Durrett, 2020, Zhang et al. 2023a), code completion (Chen et al. 2021, Rozi\u00e8re et al., 2023) and question answering (Kamalloo et al., 2023). To unleash the full potential of pretrained LLMs, they should be able to efficiently and accurately perform long sequence generation. For example, an ideal ChatBot assistant can stably work over the content of recent day-long conversations. However, it is very challenging for LLM to generalize to longer sequence lengths than they have been pretrained on, e.g., 4 K for Llama-2 Touvron et al. (2023b). The reason is that LLMs are constrained by the attention window during pre-training. Despite substantial efforts to expand this window size (Chen et al., 2023, kaiokendev, 2023; Peng et al., 2023) and improve training (Dao et al., 2022, Dao, 2023) and inference (Pope et al. 2022; Xiao et al. 2023, Anagnostidis et al., 2023;, Wang et al., 2021; Zhang et al. 2023b) efficiency for lengthy inputs, the acceptable sequence length remains intrinsically finite, which doesn't allow persistent deployments. In this paper, we first introduce the concept of LLM streaming applications and ask the question:\nCan we deploy an LLM for infinite-length inputs without sacrificing efficiency and performance? [^0]![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-02.jpg?height=474&width=1409&top_left_y=274&top_left_x=356)\n\nFigure 1: Illustration of StreamingLLM vs.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.52)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.52)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 19 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: streaming language models, memory efficiency, computational optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Exponential Reservoir Sampling for Streaming Language Models\n\n*From Search Query: streaming language models*\n\n*M. Osborne, Ashwin Lall, Benjamin Van Durme*\n\n**TL;DR:** The experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present.\n\n**Abstract:** We show how rapidly changing textual streams such as Twitter can be modelled in fixed space. Our approach is based upon a randomised algorithm called Exponential Reservoir Sampling, unexplored by this community until now. Using language models over Twitter and Newswire as a testbed, our experimental results based on perplexity support the intuition that recently observed data generally outweighs that seen in the past, but that at times, the past can have valuable signals enabling better modelling of the present.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2014\n\n**Citations:** 27  (*Influential: 1*)\n\n#### 2. Adapting Offline Speech Translation Models for Streaming with Future-Aware Distillation and Inference\n\n*From Search Query: streaming language models*\n\n*Biao Fu, Kai Fan, Minpeng Liao, Zhongqiang Huang, Boxing Chen, Yidong Chen, Xiaodon Shi*\n\n**TL;DR:** A new approach called Future-Aware Streaming Translation (FAST) is proposed that adapts an offline ST model for streaming input and achieves better trade-offs between translation quality and latency than strong baselines.\n\n**Abstract:** A popular approach to streaming speech translation is to employ a single offline model with a wait-k policy to support different latency requirements, which is simpler than training multiple online models with different latency constraints. However, there is a mismatch problem in using a model trained with complete utterances for streaming inference with partial input. We demonstrate that speech representations extracted at the end of a streaming input are significantly different from those extracted from a complete utterance. To address this issue, we propose a new approach called Future-Aware Streaming Translation (FAST) that adapts an offline ST model for streaming input. FAST includes a Future-Aware Inference (FAI) strategy that incorporates future context through a trainable masked embedding, and a Future-Aware Distillation (FAD) framework that transfers future context from an approximation of full speech to streaming input. Our experiments on the MuST-C EnDe, EnEs, and EnFr benchmarks show that FAST achieves better trade-offs between translation quality and latency than strong baselines. Extensive analyses suggest that our methods effectively alleviate the aforementioned mismatch problem between offline training and online inference.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 3. On the Scalability and Memory Efficiency of Semidefinite Programs for Lipschitz Constant Estimation of Neural Networks\n\n*From Search Query: memory efficiency*\n\n*Zi Wang, Bin Hu, Aaron J. Havens, Alexandre Araujo, Yang Zheng, Yudong Chen, Somesh Jha*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 4. Q-Pensieve: Boosting Sample Efficiency of Multi-Objective RL Through Memory Sharing of Q-Snapshots\n\n*From Search Query: memory efficiency*\n\n*Wei-Ting Hung, Bo-Kai Huang, Ping-Chun Hsieh, Xi Liu*\n\n**Abstract:** None\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. Compressing Context to Enhance Inference Efficiency of Large Language Models\n\n*From Search Query: memory efficiency*\n\n*Yucheng Li, Bo Dong, Chenghua Lin, Frank Guerin*\n\n**TL;DR:** Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used, indicating that it strikes a good balance between efficiency and performance.\n\n**Abstract:** Large language models (LLMs) achieved remarkable performance across various tasks. However, they face challenges in managing long documents and extended conversations, due to significantly increased computational requirements, both in memory and inference time, and potential context truncation when the input exceeds the LLM's fixed context length. This paper proposes a method called Selective Context that enhances the inference efficiency of LLMs by identifying and pruning redundancy in the input context to make the input more compact. We test our approach using common data sources requiring long context processing: arXiv papers, news articles, and long conversations, on tasks of summarisation, question answering, and response generation. Experimental results show that Selective Context significantly reduces memory cost and decreases generation latency while maintaining comparable performance compared to that achieved when full context is used. Specifically, we achieve a 50\\% reduction in context cost, resulting in a 36\\% reduction in inference memory usage and a 32\\% reduction in inference time, while observing only a minor drop of .023 in BERTscore and .038 in faithfulness on four downstream applications, indicating that our method strikes a good balance between efficiency and performance.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2023\n\n**Citations:** 34  (*Influential: 5*)\n\n#### 6. A Generalized Weighted Optimization Method for Computational Learning and Inversion\n\n*From Search Query: computational optimization*\n\n*Kui Ren, Yunan Yang, Bjorn Engquist*\n\n**TL;DR:** This paper characterize the impact of the weighting scheme on the generalization error of the learning method, and derive explicit generalization errors for the random Fourier feature model in both the under- and over-parameterized regimes.\n\n**Abstract:** The generalization capacity of various machine learning models exhibits different phenomena in the under- and over-parameterized regimes. In this paper, we focus on regression models such as feature regression and kernel regression and analyze a generalized weighted least-squares optimization method for computational learning and inversion with noisy data. The highlight of the proposed framework is that we allow weighting in both the parameter space and the data space. The weighting scheme encodes both a priori knowledge on the object to be learned and a strategy to weight the contribution of different data points in the loss function. Here, we characterize the impact of the weighting scheme on the generalization error of the learning method, where we derive explicit generalization errors for the random Fourier feature model in both the under- and over-parameterized regimes. For more general feature maps, error bounds are provided based on the singular values of the feature matrix. We demonstrate that appropriate weighting from prior knowledge can improve the generalization capability of the learned model.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 7. Contrastive Decoding: Open-ended Text Generation as Optimization\n\n*From Search Query: computational optimization*\n\n*Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang, Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer, M. Lewis*\n\n**Abstract:** Given a language model (LM), maximum probability is a poor decoding objective for open-ended generation, because it produces short and repetitive text. On the other hand, sampling can often produce incoherent text that drifts from the original topics. We propose contrastive decoding (CD), a reliable decoding approach that optimizes a contrastive objective subject to a plausibility constraint. The contrastive objective returns the difference between the likelihood under a large LM (called the expert, e.g. OPT-13B) and a small LM (called the amateur, e.g. OPT-125M), and the constraint ensures that the outputs are plausible. CD is inspired by the fact that the failures of larger LMs (e.g., repetition, inco- herence) are even more prevalent in smaller LMs, and that this difference signals which texts should be preferred. CD requires zero additional training, and produces higher quality text than decoding from the larger LM alone. It also works across model scales (OPT-13B and GPT2-1.5B) and significantly outperforms four strong decoding algorithms (e.g., nucleus, top-k) in automatic and human evaluations across wikipedia, news and story domains.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 237  (*Influential: 47*)\n\n#### 8. Sample-efficient Multi-objective Molecular Optimization with GFlowNets\n\n*From Search Query: computational optimization*\n\n*Yiheng Zhu, Jialun Wu, Chaowen Hu, Jiahuan Yan, Chang-Yu Hsieh, Tingjun Hou, Jian Wu*\n\n**TL;DR:** This work proposes a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front.\n\n**Abstract:** Many crucial scientific problems involve designing novel molecules with desired properties, which can be formulated as a black-box optimization problem over the discrete chemical space. In practice, multiple conflicting objectives and costly evaluations (e.g., wet-lab experiments) make the diversity of candidates paramount. Computational methods have achieved initial success but still struggle with considering diversity in both objective and search space. To fill this gap, we propose a multi-objective Bayesian optimization (MOBO) algorithm leveraging the hypernetwork-based GFlowNets (HN-GFN) as an acquisition function optimizer, with the purpose of sampling a diverse batch of candidate molecular graphs from an approximate Pareto front. Using a single preference-conditioned hypernetwork, HN-GFN learns to explore various trade-offs between objectives. We further propose a hindsight-like off-policy strategy to share high-performing molecules among different preferences in order to speed up learning for HN-GFN. We empirically illustrate that HN-GFN has adequate capacity to generalize over preferences. Moreover, experiments in various real-world MOBO settings demonstrate that our framework predominantly outperforms existing methods in terms of candidate quality and sample efficiency. The code is available at https://github.com/violet-sto/HN-GFN.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 3*)\n\n### 6 related papers from ArXiv\n\n#### 1. Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms\n\n*From Search Query: streaming language models*\n\n*Tian Meng, Yang Tao, Wuliang Yin*\n\n**Abstract:** Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.\n\n**Published:** 2024-08-01T02:49:58Z  (*Updated: 2024-08-01T02:49:58Z*)\n\n\n\n#### 2. Mind the Gap: Assessing Temporal Generalization in Neural Language\n  Models\n\n*From Search Query: streaming language models*\n\n*Angeliki Lazaridou, Adhiguna Kuncoro, Elena Gribovskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d'Autume, Tomas Kocisky, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, Phil Blunsom*\n\n**Abstract:** Our world is open-ended, non-stationary, and constantly evolving; thus what\nwe talk about and how we talk about it change over time. This inherent dynamic\nnature of language contrasts with the current static language modelling\nparadigm, which trains and evaluates models on utterances from overlapping time\nperiods. Despite impressive recent progress, we demonstrate that Transformer-XL\nlanguage models perform worse in the realistic setup of predicting future\nutterances from beyond their training period, and that model performance\nbecomes increasingly worse with time. We find that, while increasing model size\nalone -- a key driver behind recent progress -- does not solve this problem,\nhaving models that continually update their knowledge with new information can\nindeed mitigate this performance degradation over time. Hence, given the\ncompilation of ever-larger language modelling datasets, combined with the\ngrowing list of language-model-based NLP applications that require up-to-date\nfactual knowledge about the world, we argue that now is the right time to\nrethink the static way in which we currently train and evaluate our language\nmodels, and develop adaptive language models that can remain up-to-date with\nrespect to our ever-changing and non-stationary world. We publicly release our\ndynamic, streaming language modelling benchmarks for WMT and arXiv to\nfacilitate language model evaluation that takes temporal dynamics into account.\n\n**Published:** 2021-02-03T09:01:49Z  (*Updated: 2021-10-26T15:47:43Z*)\n\n\n\n#### 3. Revisiting Zeroth-Order Optimization for Memory-Efficient LLM\n  Fine-Tuning: A Benchmark\n\n*From Search Query: memory efficiency*\n\n*Yihua Zhang, Pingzhi Li, Junyuan Hong, Jiaxiang Li, Yimeng Zhang, Wenqing Zheng, Pin-Yu Chen, Jason D. Lee, Wotao Yin, Mingyi Hong, Zhangyang Wang, Sijia Liu, Tianlong Chen*\n\n**Abstract:** In the evolving landscape of natural language processing (NLP), fine-tuning\npre-trained Large Language Models (LLMs) with first-order (FO) optimizers like\nSGD and Adam has become standard. Yet, as LLMs grow {in size}, the substantial\nmemory overhead from back-propagation (BP) for FO gradient computation presents\na significant challenge. Addressing this issue is crucial, especially for\napplications like on-device training where memory efficiency is paramount. This\npaper proposes a shift towards BP-free, zeroth-order (ZO) optimization as a\nsolution for reducing memory costs during LLM fine-tuning, building on the\ninitial concept introduced by MeZO. Unlike traditional ZO-SGD methods, our work\nexpands the exploration to a wider array of ZO optimization techniques, through\na comprehensive, first-of-its-kind benchmarking study across five LLM families\n(Roberta, OPT, LLaMA, Vicuna, Mistral), three task complexities, and five\nfine-tuning schemes. Our study unveils previously overlooked optimization\nprinciples, highlighting the importance of task alignment, the role of the\nforward gradient method, and the balance between algorithm complexity and\nfine-tuning performance. We further introduce novel enhancements to ZO\noptimization, including block-wise descent, hybrid training, and gradient\nsparsity. Our study offers a promising direction for achieving further\nmemory-efficient LLM fine-tuning. Codes to reproduce all our experiments are at\nhttps://github.com/ZO-Bench/ZO-LLM .\n\n**Published:** 2024-02-18T14:08:48Z  (*Updated: 2024-05-28T03:27:06Z*)\n\n\n\n#### 4. Simultaneous Computation and Memory Efficient Zeroth-Order Optimizer for\n  Fine-Tuning Large Language Models\n\n*From Search Query: memory efficiency*\n\n*Fei Wang, Li Shen, Liang Ding, Chao Xue, Ye Liu, Changxing Ding*\n\n**Abstract:** Fine-tuning is powerful for adapting large language models to downstream\ntasks, but it often results in huge memory usages. A promising approach to\nmitigate this is using Zeroth-Order (ZO) optimization, which estimates\ngradients to replace First-Order (FO) gradient calculations, albeit with longer\ntraining time due to its stochastic nature. By revisiting the Memory-efficient\nZO (MeZO) optimizer, we discover that the full-parameter perturbation and\nupdating processes consume over 50% of its overall fine-tuning time cost. Based\non these observations, we introduce a novel layer-wise sparse computation and\nmemory efficient ZO optimizer, named LeZO. LeZO treats layers as fundamental\nunits for sparsification and dynamically perturbs different parameter subsets\nin each step to achieve full-parameter fine-tuning. LeZO incorporates\nlayer-wise parameter sparsity in the process of simultaneous perturbation\nstochastic approximation (SPSA) and ZO stochastic gradient descent (ZO-SGD). It\nachieves accelerated computation during perturbation and updating processes\nwithout additional memory overhead. We conduct extensive experiments with the\nOPT model family on the SuperGLUE benchmark and two generative tasks. The\nexperiments show that LeZO accelerates training without compromising the\nperformance of ZO optimization. Specifically, it achieves over 3x speedup\ncompared to MeZO on the SST-2, BoolQ, and Copa tasks.\n\n**Published:** 2024-10-13T12:47:37Z  (*Updated: 2024-10-13T12:47:37Z*)\n\n\n\n#### 5. Cerebras-GPT: Open Compute-Optimal Language Models Trained on the\n  Cerebras Wafer-Scale Cluster\n\n*From Search Query: computational optimization*\n\n*Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, Joel Hestness*\n\n**Abstract:** We study recent research advances that improve large language models through\nefficient pre-training and scaling, and open datasets and tools. We combine\nthese advances to introduce Cerebras-GPT, a family of open compute-optimal\nlanguage models scaled from 111M to 13B parameters. We train Cerebras-GPT\nmodels on the Eleuther Pile dataset following DeepMind Chinchilla scaling rules\nfor efficient pre-training (highest accuracy for a given compute budget). We\ncharacterize the predictable power-law scaling and compare Cerebras-GPT with\nother publicly-available models to show all Cerebras-GPT models have\nstate-of-the-art training efficiency on both pre-training and downstream\nobjectives. We describe our learnings including how Maximal Update\nParameterization ($\\mu$P) can further improve large model scaling, improving\naccuracy and hyperparameter predictability at scale. We release our pre-trained\nmodels and code, making this paper the first open and reproducible work\ncomparing compute-optimal model scaling to models trained on fixed dataset\nsizes. Cerebras-GPT models are available on HuggingFace:\nhttps://huggingface.co/cerebras.\n\n**Published:** 2023-04-06T16:43:16Z  (*Updated: 2023-04-06T16:43:16Z*)\n\n\n\n#### 6. Training Compute-Optimal Large Language Models\n\n*From Search Query: computational optimization*\n\n*Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, Laurent Sifre*\n\n**Abstract:** We investigate the optimal model size and number of tokens for training a\ntransformer language model under a given compute budget. We find that current\nlarge language models are significantly undertrained, a consequence of the\nrecent focus on scaling language models whilst keeping the amount of training\ndata constant. By training over 400 language models ranging from 70 million to\nover 16 billion parameters on 5 to 500 billion tokens, we find that for\ncompute-optimal training, the model size and the number of training tokens\nshould be scaled equally: for every doubling of model size the number of\ntraining tokens should also be doubled. We test this hypothesis by training a\npredicted compute-optimal model, Chinchilla, that uses the same compute budget\nas Gopher but with 70B parameters and 4$\\times$ more more data. Chinchilla\nuniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1\n(178B), and Megatron-Turing NLG (530B) on a large range of downstream\nevaluation tasks. This also means that Chinchilla uses substantially less\ncompute for fine-tuning and inference, greatly facilitating downstream usage.\nAs a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%\non the MMLU benchmark, greater than a 7% improvement over Gopher.\n\n**Published:** 2022-03-29T13:38:03Z  (*Updated: 2022-03-29T13:38:03Z*)\n\n\n\n### 5 related papers from Papers with Code\n\n#### 1. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: streaming language models*\n\n*Mike Lewis, Song Han, Beidi Chen, Yuandong Tian, Guangxuan Xiao*\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Published:** 2023-09-29\n\n\n\n#### 2. Mini-Omni: Language Models Can Hear, Talk While Thinking in Streaming\n\n*From Search Query: streaming language models*\n\n*Changqiao Wu, Zhifei Xie*\n\n**Abstract:** Recent advances in language models have achieved significant progress. GPT-4o, as a new milestone, has enabled real-time conversations with humans, demonstrating near-human natural fluency. Such human-computer interaction necessitates models with the capability to perform reasoning directly with the audio modality and generate output in streaming. However, this remains beyond the reach of current academic models, as they typically depend on extra TTS systems for speech synthesis, resulting in undesirable latency. This paper introduces the Mini-Omni, an audio-based end-to-end conversational model, capable of real-time speech interaction. To achieve this capability, we propose a text-instructed speech generation method, along with batch-parallel strategies during inference to further boost the performance. Our method also helps to retain the original model's language capabilities with minimal degradation, enabling other works to establish real-time interaction capabilities. We call this training method \"Any Model Can Talk\". We also introduce the VoiceAssistant-400K dataset to fine-tune models optimized for speech output. To our best knowledge, Mini-Omni is the first fully end-to-end, open-source model for real-time speech interaction, offering valuable potential for future research.\n\n**Published:** 2024-08-29\n\n\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness\n\n*From Search Query: memory efficiency*\n\n*Christopher R\u00e9, Atri Rudra, Stefano Ermon, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n**Published:** 2022-05-27\n\n\n\n#### 4. Memory-Efficient Adaptive Optimization\n\n*From Search Query: memory efficiency*\n\n*Yoram Singer, Tomer Koren, Vineet Gupta, Rohan Anil*\n\n**Abstract:** Adaptive gradient-based optimizers such as Adagrad and Adam are crucial for achieving state-of-the-art performance in machine translation and language modeling. However, these methods maintain second-order statistics for each parameter, thus introducing significant memory overheads that restrict the size of the model being used as well as the number of examples in a mini-batch. We describe an effective and flexible adaptive optimization method with greatly reduced memory overhead. Our method retains the benefits of per-parameter adaptivity while allowing significantly larger models and batch sizes. We give convergence guarantees for our method, and demonstrate its effectiveness in training very large translation and language models with up to 2-fold speedups compared to the state-of-the-art.\n\n**Published:** 2019-01-30\n\n\n\n#### 5. Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design\n\n*From Search Query: computational optimization*\n\n*Lucas Beyer, Alexander Kolesnikov, Xiaohua Zhai, Ibrahim Alabdulmohsin*\n\n**Abstract:** Scaling laws have been recently employed to derive compute-optimal model size (number of parameters) for a given compute duration. We advance and refine such methods to infer compute-optimal model shapes, such as width and depth, and successfully implement this in vision transformers. Our shape-optimized vision transformer, SoViT, achieves results competitive with models that exceed twice its size, despite being pre-trained with an equivalent amount of compute. For example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012, surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical settings, with also less than half the inference cost. We conduct a thorough evaluation across multiple tasks, such as image classification, captioning, VQA and zero-shot transfer, demonstrating the effectiveness of our model across a broad range of domains and identifying limitations. Overall, our findings challenge the prevailing approach of blindly scaling up vision models and pave a path for a more informed scaling.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-05-22\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on streaming language models, memory efficiency, and computational optimization, here are some key insights and techniques from the provided sources and additional relevant information:\n\n## Attention Sink Mechanisms\nAttention sink mechanisms are crucial for stabilizing attention computation in streaming scenarios. Research indicates that attention sinks emerge universally in language models, often focusing on the first token or prefix tokens, even if they are not semantically important.\n\n- **StreamingLLM**: This approach uses attention sinks to maintain initial tokens as attention sinks, which improves performance in long-sequence modeling. This mechanism helps in reducing the computational overhead and stabilizing attention computation.\n\n## Adaptive Tiling Strategies\nAdaptive tiling strategies can significantly optimize memory access and reduce computational overhead.\n\n- **CASAK-V**: This framework employs dynamic sparse attention, which adapts to different tasks and sequence lengths. It learns to focus on local contexts for tasks like language modeling and captures sparse but important global interactions for tasks requiring long-range dependencies. This adaptive sparsity is key to efficient memory usage and computational optimization.\n\n- **FlashAttention-2**: Although not directly mentioned in the sources, similar concepts like FlashAttention-2 involve improvements in attention computation efficiency through better parallelism and work partitioning, which can inform adaptive tiling strategies.\n\n## Selective KV Caching Techniques\nSelective KV caching is essential for reducing the memory footprint while maintaining generation quality.\n\n- **CASAK-V**: This model uses an adaptive KV-cache compression technique that dynamically adapts based on the input context and task requirements. This approach achieves effective compression ratios, making it memory-efficient without compromising performance.\n\n- **Adaptive KV Cache Compression**: This method involves intelligent caching strategies where the model decides what to discard from the cache, optimizing memory usage without sacrificing performance. This is particularly useful in streaming scenarios where long interactions are expected.\n\n## Memory Efficiency and Computational Optimization\nTo enhance memory efficiency and computational performance:\n\n- **Dynamic Sparse Attention**: CASAK-V's dynamic sparse attention mechanism allows for seamless integration with existing pre-trained LLMs without extensive retraining or architectural modifications. This approach maintains efficiency and adaptability across diverse tasks and deployment scenarios.\n\n- **Optimization and Training Dynamics**: Understanding the optimization, data distribution, and loss function during pre-training can help mitigate extreme-token phenomena, which are challenges in LLM inference, quantization, and interpretability. Strategies like replacing softmax with ReLU and Adam with SGD can be beneficial.\n\n## Future Directions\n- **Detailed Implementations**: Conduct further searches on specific implementations of attention sink mechanisms and adaptive tiling in language models to refine the proposal's approach.\n- **Technical Papers on KV Caching**: Explore more detailed technical papers on KV caching strategies to optimize the caching mechanism further.\n- **Integration with Existing Models**: Investigate how these techniques can be integrated with existing state-of-the-art models to enhance their performance in streaming scenarios.\n\nBy incorporating these insights, the researchers can develop an autoregressive language model that is highly efficient in memory usage, computationally optimized, and capable of handling long contexts effectively.\n",
            "analysis": "### Summary of Analysis\nThe proposal for StreamRetNetMLP introduces novel elements such as attention sink mechanisms, adaptive tiling, and selective KV caching to optimize streaming language models. The search results provide insights into similar concepts and techniques that can enhance memory efficiency and computational performance in language models.\n\n### Useful References and Excerpts\n1. **Efficient Streaming Language Models with Attention Sinks** (Xiao et al., 2023): This paper introduces StreamingLLM, which uses attention sinks to stabilize attention computation in streaming scenarios. It highlights the importance of maintaining initial tokens as attention sinks to improve performance in long-sequence modeling.\n\n2. **Adaptive KV Cache Compression for LLMs** (Ge et al., 2023): This paper discusses adaptive KV cache compression, which aligns with the proposal's selective KV caching mechanism. It emphasizes reducing memory footprint while maintaining generation quality.\n\n3. **FlashAttention-2** (Dao, 2023): Discusses improvements in attention computation efficiency, which can be related to the proposal's adaptive tiling mechanism to optimize memory access and computational overhead.\n\n4. **S2-Attention**: Although not directly mentioned, the concepts of efficient memory usage and computational optimization through adaptive tiling and sparse attention patterns are relevant to the proposal's goals.\n\n### Key Insights\n- **Attention Sink Mechanisms**: The concept of using attention sinks to stabilize attention computation is supported by existing research, indicating its potential effectiveness in streaming scenarios.\n- **Adaptive Tiling**: Techniques like those used in FlashAttention-2 and S2-Attention can inform the implementation of adaptive tiling to improve memory access and reduce computational overhead.\n- **Selective KV Caching**: Adaptive KV cache compression methods highlight the importance of intelligent caching strategies to enhance efficiency without sacrificing performance.\n\n### Future Search Plan\n- Conduct further searches on specific implementations of attention sink mechanisms and adaptive tiling in language models.\n- Explore more detailed technical papers on KV caching strategies to refine the proposal's approach.\n\n### References\n- Xiao, G., Tian, Y., Chen, B., Han, S., & Lewis, M. (2023). Efficient Streaming Language Models with Attention Sinks. arXiv preprint arXiv:2309.17453.\n- Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., & Gao, J. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations.\n- Dao, T. (2023). FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. International Conference on Learning Representations."
        },
        {
            "ready": false,
            "query": "attention sink mechanisms, adaptive tiling, selective KV caching",
            "detail": "attention sink mechanisms in language models, adaptive tiling strategies, selective KV caching techniques",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nattention sink mechanisms in language models, adaptive tiling strategies, selective KV caching techniques\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models (Avg. Score: 0.97)\n\n*Hengyu Zhang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention, but is still not as efficient as vanilla attention.\n\n**Abstract:** Extending the functionality of the Transformer model to accommodate longer sequence lengths has become a critical challenge. This extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention (S\\(^2\\)-Attn), effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only 39\\% of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose \\textbf{SinkLoRA}, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of\"sink attention tokens\", achieving 92\\% of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm H$_2$O to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at \\url{https://github.com/Dexter-GT-86/SinkLoRA}.\n\n##### *Relevant Chunk: No. 3/28 (Score: 1.00)*\n\n```\nThis method stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. - Our SinkLoRA performs favorably against state-of-the-art methods. We evaluate its performance on the PG19, Proof-pile, and LongBench datasets, demonstrating its effectiveness. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-04.jpg?height=375&width=1313&top_left_y=246&top_left_x=428)\n\nFigure 4: Illustration of the Segmentation and Reassembly process in SF-Attn. The process involves three steps: (1) Splitting features along the head dimension into two chunks: one shifted and one unshifted. (2) Splitting tokens, where the tokens belonging to the shifted chunk are shifted by half of the group size, and reassembling them at the tail of the tokens to match the unshifted chunk. (3) Combining the two chunks of tokens together. This figure is adapted from [6]. Specifically, for LLaMA2 7B, SinkLoRA outperforms LongLoRA and is competitive with LongChat-13B. ### 1.1 Motivation for the Research\n\n## Motivation 1: Elevating Attention Scores for Initial Tokens\n\nPrior studies have demonstrated the Attention Sink phenomenon, where certain tokens, typically the initial tokens in a sequence, receive disproportionately high attention scores during the model's computation [39]. This often occurs because these tokens are visible to all subsequent tokens, leading to significant attention even when they lack semantic importance, particularly in autoregressive language models [33]. The Sparse Shifted Attention mechanism implemented in LongLoRA [6] attempts to address this by shifting the high attention scores from these initial tokens to other tokens that previously received lower attention. This shift reduces the overemphasis on initial tokens. To further improve this, we need to develop a method that directly modifies the attention pattern. By applying this technique, we can effectively redistribute attention scores, thereby reducing the undue emphasis on initial tokens across different token groups. ## Motivation 2: Maintaining Initial Tokens During Fine-Tuning\n\nThe concept of attention sinks is also utilized in Streaming LLM [39] to improve the model's handling of long texts. By retaining the Key-Value (KV) pairs of a few initial tokens (attention sinks) along with the most recent tokens, the model ensures stable attention scores and performance even for extended sequences. Inspired by this approach, we aim to carry this mindset from training into inference. Our research aims to modify the fine-tuning process so that initial tokens attend to all other tokens, thereby accumulating more attention scores and enhancing the model's capacity to handle long sequences. ## Motivation 3: Flexible Deployment of Inference Strategy\n\nEfficient deployment of computationally intensive large language models (LLMs) in production environments often relies on Key-Value (KV) caching [16]. KV caching stores the key-value states of previously generated tokens, significantly reducing the need for repetitive computations and thus lowering latency in autoregressive generation. However, LongLoRA [6] retains only the original standard self-attention mechanism during inference. To address this limitation, it is necessary to apply an optional KV cache function. This enhancement allows for a more flexible and efficient inference strategy, reducing computational overhead while maintaining model performance. ## 2 Related Work\n\n### 2.1 Long-context Transformers\n\nThe primary obstacle in scaling Transformer models to handle longer sequence lengths lies in the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. This quadratic computational burden has prompted significant research efforts focused on developing more efficient sparse Transformer models. Notable examples include Longformer [4] and BigBird [41], which utilize a combination of local, global, and sparse attention mechanisms to manage long contexts, thereby reducing the complexity to $\\mathrm{O}(\\mathrm{n})$. These models achieve a balance between maintaining sufficient context for understanding while managing computational load. For achieving complexity of $O(n \\log n)$, several approaches have been proposed. Fixed Window Attention [7] employs a fixed-size window for attention, which confines the attention computation to a limited context window. Reformer [21] introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thus reducing the computational complexity. LSG Attention [9], adapted from BigBird, combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead. Equipping Transformer [40] proposes a novel reading strategy termed random access, which enables Transformers to efficiently process long documents without needing to examine every token. This method shows promising results across pretraining, fine-tuning, and inference phases, demonstrating its efficacy in handling extended contexts. Despite these advancements, the ability of these methods to manage long-context conversations, such as those required in chat applications, remains limited. This highlights an ongoing challenge in enhancing the context-handling capabilities of Transformer models for interactive and real-time applications. ### 2.2 Long-context LLMs\n\nRecent advancements in Large Language Models (LLMs) have significantly extended their capabilities, including handling long-context inputs. Math Word Problems (MWPs) have demonstrated notable performance in solving mathematical questions using LLMs [34]. Moreover, leveraging LLMs for SQL querying has shown promise in optimizing resource allocation, though it remains less efficient than traditional relational databases [42]. LongLoRA [6], employing Position Interpolation [5], has successfully extended the context window of Llama 2 from 4096 to 32768 tokens without requiring substantial GPU or TPU resources. Meta's Llama 3, featuring up to 70 billion parameters, represents a significant advancement in open-source LLMs, offering enhancements in computational efficiency, trust and safety tools, and collaborations with major platforms [38]. Open-source models such as BLOOM [22], OPT [18], and Falcon [28] continue to challenge proprietary models, although models like Vicuna [29] and Alpaca [1] still lag behind their closed-source counterparts in certain aspects. Despite these advancements, effectively managing long-context interactions remains a significant challenge, necessitating ongoing research and development to address the complexities in long-context LLM applications. ### 2.3 KV-Cache Compression\n\nCompressing the size of KV cache is more difficult than reducing the size of weights because they are more sensitive and dependent on model inputs. A cost-effective method for KV cache compression is token dropping [25, 43, 16], which establishes an importance policy to retain significant KVs and remove insignificant ones.\n```\n\n##### *Relevant Chunk: No. 2/28 (Score: 0.95)*\n\n```\nThis extension is crucial not only for improving tasks such as language translation and long-context processing but also for enabling novel applications like chatbots, code generation, and multimedia content creation. The primary obstacle is the self-attention mechanism, which scales quadratically with sequence length in terms of computation time and memory requirements. LongLoRA proposed shifted sparse attention ( $\\left.\\mathrm{S}^{2}-\\mathrm{Attn}\\right)$, effectively enabling context extension and leading to non-trivial computation savings with similar performance to fine-tuning with vanilla attention. However, LongLoRA is still not as efficient as vanilla attention, reaching only $39 \\%$ of the perplexity improvement compared to full attention. This inefficiency is due to the cyclic shift applied within different attention head patterns, causing either chaos in the attention head structure or unnecessary information exchange between token groups. To address these issues, We propose SinkLoRA, which features better work partitioning. Specifically, (1) we developed SF-Attn with a segmentation and reassembly algorithm to proportionally return cyclically shifted groups of attention heads to their un-shifted state together with global attention of \"sink attention tokens\", achieving $92 \\%$ of the perplexity improvement compared to full attention after fine tuning, and (2) applied a SOTA KV cache compression algorithm $\\mathrm{H}_{2} \\mathrm{O}$ to accelerate inference. Furthermore, We conducted supervised fine-tuning with SinkLoRA using a self collected LongAlpaca-plus dataset. All our code, models, datasets, and demos are available at https://github.com/Dexter-GT-86/SinkLoRA. ## 1 Introduction\n\nEnhancing the functionality of Transformer models to handle longer sequence lengths has become crucial for numerous applications, including language translation, long-context processing, chatbots, code generation, and multimedia content creation. The primary challenge lies in the self-attention mechanism, which scales quadratically with sequence length, leading to substantial computational time and memory requirements [4, 41, 21]. To address this challenge, several approaches have been proposed. Longformer and BigBird utilize combinations of local, global, and sparse attention mechanisms to manage long contexts, reducing complexity to $\\mathrm{O}(\\mathrm{n})$ [4, 41]. Reformer introduces locality-sensitive hashing (LSH) to approximate attention by hashing similar tokens into the same buckets, thereby reducing computational complexity [21]. LSG Attention combines local, sparse, and global attention to effectively handle long contexts while minimizing computational overhead [9]. Despite these advancements, managing long-context interactions in practical applications remains a significant challenge. Recent work, such as LongLoRA, extends the context window of LLaMA2 from 4096 to 32768 tokens using Position Interpolation without substantial GPU or TPU resources\n\n[^0]![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-02.jpg?height=535&width=1381&top_left_y=404&top_left_x=365)\n\nFigure 1: Evaluation of SinkLoRA in bridging the accuracy gap between sparse shifted attention and full attention during supervised fine-tuning, while maintaining the memory efficiency of LongLoRA, which utilizes 1.8 times less memory compared to full fine-tuning. Furthermore, SinkLoRA retains the training speed of LongLoRA, being 1.8 times faster than full fine-tuning, due to the implementation of Sink Fixed Attention. The Llama2-7B models[37] are fine-tuned to various context lengths using Flash-Attention 2 [10] and DeepSpeed stage 2 [31], and are evaluated on the proof-pile test set [2] in terms of perplexity. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-02.jpg?height=349&width=1214&top_left_y=1644&top_left_x=484)\n\nFigure 2: Overview of the SinkLoRA fine-tuning process, incorporating Sink Fixed Attention (SFAttn). Panels (a), (b), and (c) depict the procedure to convert Sparse Shifted Attention into Short Window Attention and subsequently into Sink Fixed Attention. This conversion is executed in two stages: reassembly and making the initial tokens global. In addition to optimizing the LoRA weights within linear layers, SinkLoRA also enables training of the embedding and normalization layers, consistent with the methodology employed in LongLoRA. ![](https://cdn.mathpix.com/cropped/2024_09_17_6def0ba0294d4b8e8c08g-03.jpg?height=633&width=1264&top_left_y=280&top_left_x=428)\n\nFigure 3: Overview of the SinkLoRA inference process. Unlike LongLoRA, which retains the original standard self-attention during inference, SinkLoRA implements an optional KV cache compression method, $\\mathrm{H}^{2} \\mathrm{O}$ [43]. This extension enhances inference speed without significantly compromising performance. [6]. However, LongLoRA's efficiency is limited, achieving only $39 \\%$ of the perplexity improvement compared to full attention due to chaotic attention head structures and unnecessary information exchange between token groups. To address these issues, we propose SinkLoRA, which offers better work partitioning. This includes the development of Sink Fixed Attention (SF-Attn), a segmentation and reassembly algorithm that, along with the global attention of \"sink attention tokens,\" achieves $92 \\%$ of the perplexity improvement of full attention after fine-tuning. Additionally, we apply a state-of-the-art KV cache compression algorithm, Heavy Hitter Oracle $\\left(\\mathrm{H}_{2} \\mathrm{O}\\right)$, to accelerate inference [43, 16, 25]. We further enhanced SinkLoRA through supervised fine-tuning using our self-collected LongAlpacaPlus dataset, comprising 28,000 entries from various sources, including Natural Questions, RedPajama [8], Book Summarization, and LongQA [6], ensuring a diverse and comprehensive collection for long instruction tuning. In summary, the contributions of our work are as follows:\n\n- We present SinkLoRA, a memory-efficient and effective method to extend the context length of LLaMA2 and LLaMA3, representing a complete update of LongLoRA. This method improves fine-tuning efficiency and offers a flexible deployment inference strategy. - We introduce SF-Attn, a fine-tuning method that combines a segmentation \\& reassembly algorithm and global attention. This method is easy to implement, accurate, and memoryefficient, without increasing computational complexity. By directly modifying the attention pattern, SF-Attn effectively redistributes attention scores, reducing the undue emphasis on initial tokens across different token groups. - We achieve efficient deployment of computationally intensive large language models (LLMs) in production environments by using the Heavy Hitter Oracle $\\left(\\mathrm{H}_{2} \\mathrm{O}\\right) \\mathrm{KV}$ caching method.\n```\n\n#### 2. Efficient Streaming Language Models with Attention Sinks (Avg. Score: 0.89)\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 227  (*Influential: 41*)\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n##### *Relevant Chunk: No. 3/32 (Score: 0.95)*\n\n```\nexisting methods. The language model, pre-trained on texts of length $L$, predicts the $T$ th token $(T \\gg L)$. (a) Dense Attention has $O\\left(T^{2}\\right)$ time complexity and an increasing cache size. Its performance decreases when the text length exceeds the pre-training text length. (b) Window Attention caches the most recent $L$ tokens' KV. While efficient in inference, performance declines sharply once the starting tokens' keys and values are evicted. (c) Sliding Window with Re-computation rebuilds the KV states from the $L$ recent tokens for each new token. While it performs well on long texts, its $O\\left(T L^{2}\\right)$ complexity, stemming from quadratic attention in context re-computation, makes it considerably slow. (d) StreamingLLM keeps the attention sink (several initial tokens) for stable attention computation, combined with the recent tokens. It's efficient and offers stable performance on extended texts. Perplexities are measured using the Llama-2-13B model on the first book ( 65 K tokens) in the PG-19 test set. When applying LLMs for infinite input streams, two primary challenges arise:\n\n1. During the decoding stage, Transformer-based LLMs cache the Key and Value states (KV) of all previous tokens, as illustrated in Figure 1 (a), which can lead to excessive memory usage and increasing decoding latency (Pope et al., 2022). 2. Existing models have limited length extrapolation abilities, i.e., their performance degrades (Press et al., 2022; Chen et al., 2023) when the sequence length goes beyond the attention window size set during pre-training. An intuitive approach, known as window attention (Beltagy et al. 2020) (Figure 1 b), maintains only a fixed-size sliding window on the KV states of most recent tokens. Although it ensures constant memory usage and decoding speed after the cache is initially filled, the model collapses once the sequence length exceeds the cache size, i.e., even just evicting the $K V$ of the first token, as illustrated in Figure 3 Another strategy is the sliding window with re-computation (shown in Figure 11c), which rebuilds the KV states of recent tokens for each generated token. While it offers strong performance, this approach is significantly slower due to the computation of quadratic attention within its window, making this method impractical for real-world streaming applications. To understand the failure of window attention, we find an interesting phenomenon of autoregressive LLMs: a surprisingly large amount of attention score is allocated to the initial tokens, irrespective of their relevance to the language modeling task, as visualized in Figure 2. We term these tokens \"attention sinks\". Despite their lack of semantic significance, they collect significant attention scores. We attribute the reason to the Softmax operation, which requires attention scores to sum up to one for all contextual tokens. Thus, even when the current query does not have a strong match in many previous tokens, the model still needs to allocate these unneeded attention values somewhere so it sums up to one. The reason behind initial tokens as sink tokens is intuitive: initial tokens are visible to almost all subsequent tokens because of the autoregressive language modeling nature, making them more readily trained to serve as attention sinks. Based on the above insights, we propose StreamingLLM, a simple and efficient framework that enables LLMs trained with a finite attention window to work on text of infinite length without finetuning. StreamingLLM exploits the fact that attention sinks have high attention values, and preserving them can maintain the attention score distribution close to normal. Therefore, StreamingLLM simply keeps the attention sink tokens' KV (with just 4 initial tokens sufficing) together with the sliding window's KV to anchor the attention computation and stabilize the model's performance. With StreamingLLM, models including Llama-2-[7, 13, 70]B, MPT-[7, 30]B, Falcon-[7, 40]B, and Pythia[2.9,6.9,12]B can reliably model 4 million tokens, and potentially even more. Compared with the only viable baseline, sliding window with recomputation, StreamingLLM achieves up to $22.2 \\times$ speedup, realizing the streaming use of LLMs. ![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-03.jpg?height=315&width=1335&top_left_y=271&top_left_x=384)\n\nFigure 2: Visualization of the average attention logits in Llama-2-7B over 256 sentences, each with a length of 16. Observations include: (1) The attention maps in the first two layers (layers 0 and 1 ) exhibit the \"local\" pattern, with recent tokens receiving more attention. (2) Beyond the bottom two layers, the model heavily attends to the initial token across all layers and heads. Furthermore, we confirm our attention sink hypothesis and demonstrate that language models can be pre-trained to require only a single attention sink token for streaming deployment. Specifically, we suggest that an extra learnable token at the beginning of all training samples can serve as a designated attention sink. By pre-training 160-million parameter language models from scratch, we demonstrate that adding this single sink token preserves the model's performance in streaming cases. This stands in contrast to vanilla models, which necessitate the reintroduction of multiple initial tokens as attention sinks to achieve the same performance level. Finally, we emphasize that StreamingLLM efficiently generates coherent text from tokens within the KV cache without extending the LLMs' context length. It suits continuous operation needs with minimal memory use and past data reliance. Additionally, StreamingLLM can complement context extension methods to increase the attendable recent context. ## 2 RELATEd WORK\n\nExtensive research has been done on applying LLMs to lengthy texts, with three main areas of focus: Length Extrapolation, Context Window Extension, and Improving LLMs' Utilization of Long Text. While seemingly related, it's worth noting that progress in one direction doesn't necessarily lead to progress in the other. For example, extending the context size of LLMs doesn't improve the model's performance beyond the context size, and neither approach ensures effective use of the long context. Our StreamingLLM framework primarily lies in the first category, where LLMs are applied to text significantly exceeding the pre-training window size, potentially even of infinite length. We do not expand the attention window size of LLMs or enhance the model's memory and usage on long texts. The last two categories are orthogonal to our focus and could be integrated with our techniques. Length extrapolation aims to enable language models trained on shorter texts to handle longer ones during testing. A predominant avenue of research targets the development of relative position encoding methods for Transformer models, enabling them to function beyond their training window. One such initiative is Rotary Position Embeddings (RoPE) (Su et al., 2021), which transforms the queries and keys in every attention layer for relative position integration. Despite its promise, subsequent research (Press et al., 2022, Chen et al., 2023) indicated its underperformance on text that exceeds the training window. Another approach, $\\overline{A L i B i}$ (Press et al., 2022), biases the query-key attention scores based on their distance, thereby introducing relative positional information. While this exhibited improved extrapolation, our tests on MPT models highlighted a breakdown when the text length was vastly greater than the training length. Current methodologies, however, have yet to achieve infinite length extrapolation, causing no existing LLMs to fit for streaming applications. Context Window Extension centers on expanding the LLMs' context window, enabling the processing of more tokens in one forward pass.\n```\n\n##### *Relevant Chunk: No. 2/32 (Score: 0.84)*\n\n```\nFirstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach - but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a \"sink\" even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence length without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to $22.2 \\times$ speedup. Code and datasets are provided in the link. ## 1 INTRODUCTION\n\nLarge Language Models (LLMs) (Radford et al., 2018; Brown et al., 2020; Zhang et al., 2022; OpenAI, 2023, Touvron et al., 2023a b) are becoming ubiquitous, powering many natural language processing applications such as dialog systems (Schulman et al., 2022, Taori et al., 2023, Chiang et al., 2023), document summarization (Goyal \\& Durrett, 2020, Zhang et al. 2023a), code completion (Chen et al. 2021, Rozi\u00e8re et al., 2023) and question answering (Kamalloo et al., 2023). To unleash the full potential of pretrained LLMs, they should be able to efficiently and accurately perform long sequence generation. For example, an ideal ChatBot assistant can stably work over the content of recent day-long conversations. However, it is very challenging for LLM to generalize to longer sequence lengths than they have been pretrained on, e.g., 4 K for Llama-2 Touvron et al. (2023b). The reason is that LLMs are constrained by the attention window during pre-training. Despite substantial efforts to expand this window size (Chen et al., 2023, kaiokendev, 2023; Peng et al., 2023) and improve training (Dao et al., 2022, Dao, 2023) and inference (Pope et al. 2022; Xiao et al. 2023, Anagnostidis et al., 2023;, Wang et al., 2021; Zhang et al. 2023b) efficiency for lengthy inputs, the acceptable sequence length remains intrinsically finite, which doesn't allow persistent deployments. In this paper, we first introduce the concept of LLM streaming applications and ask the question:\nCan we deploy an LLM for infinite-length inputs without sacrificing efficiency and performance? [^0]![](https://cdn.mathpix.com/cropped/2024_09_17_c4d42a88c0df416625cag-02.jpg?height=474&width=1409&top_left_y=274&top_left_x=356)\n\nFigure 1: Illustration of StreamingLLM vs.\n```\n\n#### 3. Loki: Low-Rank Keys for Efficient Sparse Attention (Avg. Score: 0.52)\n\n*Prajwal Singhania, Siddharth Singh, Shwai He, S. Feizi, A. Bhatele*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** Loki is proposed, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space, and is able to maintain the efficacy of the models better than other popular approximation methods.\n\n**Abstract:** Inference on large language models can be expensive in terms of the compute and memory costs involved, especially when long sequence lengths are used. In particular, the self-attention mechanism used in such models contributes significantly to these costs, which has resulted in several recent works that propose sparse attention approximations for inference. In this work, we propose to approximate the self-attention computation by focusing on the dimensionality of key vectors computed in the attention block. Our analysis reveals that the key vectors lie in a significantly lower-dimensional space, consistently across several datasets and models. Exploiting this observation, we propose Loki, a novel sparse attention method that ranks and selects tokens in the KV-cache based on attention scores computed in low-dimensional space. Our evaluations show that Loki is able to maintain the efficacy of the models better than other popular approximation methods, while speeding up the attention computation due to reduced data movement (load/store) and compute costs.\n\n##### *Relevant Chunk: No. 9/24 (Score: 0.52)*\n\n```\narXiv preprint arXiv:1904.10509, 2019. [6] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers, 2022. [7] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noac'h, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. A framework for few-shot language model evaluation, 122023. [8] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [9] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms, 2024. [10] Ankit Gupta, Guy Dar, Shaya Goodman, David Ciprut, and Jonathan Berant. Memory-efficient transformers via top-k attention. CoRR, abs/2106.06899, 2021. [11] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models.\n```\n\n\n\n---\n## Found 15 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: attention sink mechanisms, adaptive tiling, selective KV caching\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Efficient Streaming Language Models with Attention Sinks\n\n*From Search Query: attention sink mechanisms*\n\n*Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, Mike Lewis*\n\n**TL;DR:** StreamingLLM is introduced, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning and can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more.\n\n**Abstract:** Deploying Large Language Models (LLMs) in streaming applications such as multi-round dialogue, where long interactions are expected, is urgently needed but poses two major challenges. Firstly, during the decoding stage, caching previous tokens' Key and Value states (KV) consumes extensive memory. Secondly, popular LLMs cannot generalize to longer texts than the training sequence length. Window attention, where only the most recent KVs are cached, is a natural approach -- but we show that it fails when the text length surpasses the cache size. We observe an interesting phenomenon, namely attention sink, that keeping the KV of initial tokens will largely recover the performance of window attention. In this paper, we first demonstrate that the emergence of attention sink is due to the strong attention scores towards initial tokens as a\"sink\"even if they are not semantically important. Based on the above analysis, we introduce StreamingLLM, an efficient framework that enables LLMs trained with a finite length attention window to generalize to infinite sequence lengths without any fine-tuning. We show that StreamingLLM can enable Llama-2, MPT, Falcon, and Pythia to perform stable and efficient language modeling with up to 4 million tokens and more. In addition, we discover that adding a placeholder token as a dedicated attention sink during pre-training can further improve streaming deployment. In streaming settings, StreamingLLM outperforms the sliding window recomputation baseline by up to 22.2x speedup. Code and datasets are provided at https://github.com/mit-han-lab/streaming-llm.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 336  (*Influential: 59*)\n\n#### 2. Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning\n\n*From Search Query: attention sink mechanisms*\n\n*Mattia Atzeni, Mrinmaya Sachan, Andreas Loukas*\n\n**TL;DR:** This work focuses on geometry priors and introduces LatFormer, a model that incorporates lattice symmetry priors in attention masks that provides preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Abstract:** The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 3. Prototype memory and attention mechanisms for few shot image generation\n\n*From Search Query: attention sink mechanisms*\n\n*Tianqin Li, Zijie Li, Andrew Luo, Harold Rockwell, A. Farimani, T. Lee*\n\n**TL;DR:** The results demonstrate the feasibility of the idea that these super-sparse complex feature detectors of macaque monkeys can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Abstract:** Recent discoveries indicate that the neural codes in the super\ufb01cial layers of the primary visual cortex (V1) of macaque monkeys are complex, diverse and super-sparse. This leads us to ponder the computational advantages and functional role of these \u201cgrandmother cells.\" Here, we propose that such cells can serve as prototype memory priors that bias and shape the distributed feature processing during the image generation process in the brain. These memory prototypes are learned by momentum online clustering and are utilized through a memory-based attention operation. Integrating this mechanism, we propose Memory Concept Attention ( MoCA ) to improve few shot image generation quality. We show that having a prototype memory with attention mechanisms can improve image synthesis quality, learn interpretable visual concept clusters, and improve the robustness of the model. Our results demonstrate the feasibility of the idea that these super-sparse complex feature detectors can serve as prototype memory priors for modulating the image synthesis processes in the visual system.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 4*)\n\n#### 4. Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning\n\n*From Search Query: adaptive tiling*\n\n*Qingru Zhang, Minshuo Chen, Alexander Bukharin, Pengcheng He, Yu Cheng, Weizhu Chen, Tuo Zhao*\n\n**Abstract:** , question answering and natural language generation tasks. Results show that AdaLoRA outperforms existing approaches.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 305  (*Influential: 55*)\n\n#### 5. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs\n\n*From Search Query: adaptive tiling*\n\n*Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, Jianfeng Gao*\n\n**TL;DR:** Adaptive KV cache compression is introduced, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs) and demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss.\n\n**Abstract:** In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 102  (*Influential: 12*)\n\n#### 6. AdaPlanner: Adaptive Planning from Feedback with Language Models\n\n*From Search Query: adaptive tiling*\n\n*Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, Chao Zhang*\n\n**TL;DR:** A closed-loop approach, AdaPlanner, is proposed, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback, and develops a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities.\n\n**Abstract:** Large language models (LLMs) have recently demonstrated the potential in acting as autonomous agents for sequential decision-making tasks. However, most existing methods either take actions greedily without planning or rely on static plans that are not adaptable to environmental feedback. Consequently, the sequential decision-making performance of LLM agents degenerates with problem complexity and plan horizons increase. We propose a closed-loop approach, AdaPlanner, which allows the LLM agent to refine its self-generated plan adaptively in response to environmental feedback. In AdaPlanner, the LLM agent adaptively refines its plan from feedback with both in-plan and out-of-plan refinement strategies. To mitigate hallucination, we develop a code-style LLM prompt structure that facilitates plan generation across a variety of tasks, environments, and agent capabilities. Furthermore, we propose a skill discovery mechanism that leverages successful plans as few-shot exemplars, enabling the agent to plan and refine with fewer task demonstrations. Our experiments in the ALFWorld and MiniWoB++ environments demonstrate that AdaPlanner outperforms state-of-the-art baselines by 3.73% and 4.11% while utilizing 2x and 600x fewer samples, respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 85  (*Influential: 17*)\n\n#### 7. Scissorhands: Exploiting the Persistence of Importance Hypothesis for LLM KV Cache Compression at Test Time\n\n*From Search Query: selective KV caching*\n\n*Zichang Liu, Aditya Desai, Fangshuo Liao, Weitao Wang, Victor Xie, Zhaozhuo Xu, Anastasios Kyrillidis, Anshumali Shrivastava*\n\n**TL;DR:** This work hypothesizes the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations, and proposes Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model.\n\n**Abstract:** Large language models(LLMs) have sparked a new wave of exciting AI applications. Hosting these models at scale requires significant memory resources. One crucial memory bottleneck for the deployment stems from the context window. It is commonly recognized that model weights are memory hungry; however, the size of key-value embedding stored during the generation process (KV cache) can easily surpass the model size. The enormous size of the KV cache puts constraints on the inference batch size, which is crucial for high throughput inference workload. Inspired by an interesting observation of the attention scores, we hypothesize the persistence of importance: only pivotal tokens, which had a substantial influence at one step, will significantly influence future generations. Based on our empirical verification and theoretical analysis around this hypothesis, we propose Scissorhands, a system that maintains the memory usage of the KV cache at a fixed budget without finetuning the model. In essence, Scissorhands manages the KV cache by storing the pivotal tokens with a higher probability. We validate that Scissorhands reduces the inference memory usage of the KV cache by up to 5X without compromising model quality. We further demonstrate that Scissorhands can be combined with 4-bit quantization, traditionally used to compress model weights, to achieve up to 20X compression.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 107  (*Influential: 11*)\n\n#### 8. Get More with LESS: Synthesizing Recurrence with KV Cache Compression for Efficient LLM Inference\n\n*From Search Query: selective KV caching*\n\n*Harry Dong, Xinyu Yang, Zhenyu (Allen) Zhang, Zhangyang Wang, Yuejie Chi, Beidi Chen*\n\n**TL;DR:** LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps, shows merit on a variety of tasks where it can help reduce the performance gap from caching everything.\n\n**Abstract:** Many computational factors limit broader deployment of large language models. In this paper, we focus on a memory bottleneck imposed by the key-value (KV) cache, a computational shortcut that requires storing previous KV pairs during decoding. While existing KV cache methods approach this problem by pruning or evicting large swaths of relatively less important KV pairs to dramatically reduce the memory footprint of the cache, they can have limited success in tasks that require recollecting a majority of previous tokens. To alleviate this issue, we propose LESS, a simple integration of a (nearly free) constant sized cache with eviction-based cache methods, such that all tokens can be queried at later decoding steps. Its ability to retain information throughout time shows merit on a variety of tasks where we demonstrate LESS can help reduce the performance gap from caching everything, sometimes even matching it, all while being efficient. Relevant code can be found at https://github.com/hdong920/LESS.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 22  (*Influential: 3*)\n\n### 1 related papers from ArXiv\n\n#### 1. Enhanced Structured State Space Models via Grouped FIR Filtering and\n  Attention Sink Mechanisms\n\n*From Search Query: attention sink mechanisms*\n\n*Tian Meng, Yang Tao, Wuliang Yin*\n\n**Abstract:** Structured State Space Models (SSMs) have emerged as compelling alternatives\nto Transformer architectures, offering linear-time complexity and superior\nperformance in various sequence modeling tasks. Despite their advantages, SSMs\nlike the original Mamba-2 face training difficulties due to the sensitivities\nintroduced by the extended series of recurrent matrix multiplications. In this\npaper, we propose an advanced architecture that mitigates these challenges by\ndecomposing A-multiplications into multiple groups and optimizing positional\nencoding through Grouped Finite Impulse Response (FIR) filtering. This new\nstructure, denoted as Grouped FIR-enhanced SSM (GFSSM), employs semiseparable\nmatrices for efficient computation. Furthermore, inspired by the \"attention\nsink\" phenomenon identified in streaming language models, we incorporate a\nsimilar mechanism to enhance the stability and performance of our model over\nextended sequences. Our approach further bridges the gap between SSMs and\nTransformer architectures, offering a viable path forward for scalable and\nhigh-performing sequence modeling.\n\n**Published:** 2024-08-01T02:49:58Z  (*Updated: 2024-08-01T02:49:58Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Flowformer: Linearizing Transformers with Conservation Flows\n\n*From Search Query: attention sink mechanisms*\n\n*Mingsheng Long, Jianmin Wang, Jiehui Xu, Jialong Wu, Haixu Wu*\n\n**Abstract:** Transformers based on the attention mechanism have achieved impressive success in various areas. However, the attention mechanism has a quadratic complexity, significantly impeding Transformers from dealing with numerous tokens and scaling up to bigger models. Previous methods mainly utilize the similarity decomposition and the associativity of matrix multiplication to devise linear-time attention mechanisms. They avoid degeneration of attention to a trivial distribution by reintroducing inductive biases such as the locality, thereby at the expense of model generality and expressiveness. In this paper, we linearize Transformers free from specific inductive biases based on the flow network theory. We cast attention as the information flow aggregated from the sources (values) to the sinks (results) through the learned flow capacities (attentions). Within this framework, we apply the property of flow conservation into attention and propose the Flow-Attention mechanism of linear complexity. By respectively conserving the incoming flow of sinks for source competition and the outgoing flow of sources for sink allocation, Flow-Attention inherently generates informative attentions without using specific inductive biases. Empowered by the Flow-Attention, Flowformer yields strong performance in linear time for wide areas, including long sequence, time series, vision, natural language, and reinforcement learning. The code and settings are available at this repository: https://github.com/thuml/Flowformer.\n\n**Published:** 2022-02-13\n\n\n\n#### 2. SEED-Story: Multimodal Long Story Generation with Large Language Model\n\n*From Search Query: attention sink mechanisms*\n\n*Yingcong Chen, Ying Shan, Yixiao Ge, Yukang Chen, Yang Li, Yuying Ge, Shuai Yang*\n\n**Abstract:** With the remarkable advancements in image generation and open-form text generation, the creation of interleaved image-text content has become an increasingly intriguing field. Multimodal story generation, characterized by producing narrative texts and vivid images in an interleaved manner, has emerged as a valuable and practical task with broad applications. However, this task poses significant challenges, as it necessitates the comprehension of the complex interplay between texts and images, and the ability to generate long sequences of coherent, contextually relevant texts and visuals. In this work, we propose SEED-Story, a novel method that leverages a Multimodal Large Language Model (MLLM) to generate extended multimodal stories. Our model, built upon the powerful comprehension capability of MLLM, predicts text tokens as well as visual tokens, which are subsequently processed with an adapted visual de-tokenizer to produce images with consistent characters and styles. We further propose multimodal attention sink mechanism to enable the generation of stories with up to 25 sequences (only 10 for training) in a highly efficient autoregressive manner. Additionally, we present a large-scale and high-resolution dataset named StoryStream for training our model and quantitatively evaluating the task of multimodal story generation in various aspects.\n\n**Published:** 2024-07-11\n\n\n\n#### 3. H2OPUS-TLR: High Performance Tile Low Rank Symmetric Factorizations using Adaptive Randomized Approximation\n\n*From Search Query: adaptive tiling*\n\n*Anonymous*\n\n**Abstract:** Tile low rank representations of dense matrices partition them into blocks of roughly uniform size, where each off-diagonal tile is compressed and stored as its own low rank factorization. They offer an attractive representation for many data-sparse dense operators that appear in practical applications, where substantial compression and a much smaller memory footprint can be achieved. TLR matrices are a compromise between the simplicity of a regular perfectly-strided data structure and the optimal complexity of the unbalanced trees of hierarchically low rank matrices, and provide a convenient performance-tuning parameter through their tile size that can be proportioned to take into account the cache size where the tiles reside in the memory hierarchy. There are currently no high-performance algorithms that can generate Cholesky and $LDL^T$ factorizations, particularly on GPUs. The difficulties in achieving high performance when factoring TLR matrices come from the expensive compression operations that must be performed during the factorization process and the adaptive rank distribution of the tiles that causes an irregular work pattern for the processing cores. In this work, we develop a dynamic batching operation and combine it with batched adaptive randomized approximations to achieve high performance both on GPUs and CPUs. Our implementation attains over 1.2 TFLOP/s in double precision on the V100 GPU, and is limited by the performance of batched GEMM operations. The Cholesky factorization of covariance matrix of size $N = 131K$ arising in spatial statistics can be factored to an accuracy $\\epsilon=10^{-2}$ in just a few seconds. We believe the proposed GEMM-centric algorithm allows it to be readily ported to newer hardware such as the tensor cores that are optimized for small GEMM operations.\n\n**Published:** 2021-08-26\n\n\n\n#### 4. Bridging the Gap Between Anchor-based and Anchor-free Detection via Adaptive Training Sample Selection\n\n*From Search Query: adaptive tiling*\n\n*Zhen Lei, Yongqiang Yao, Shifeng Zhang, Stan Z. Li, Cheng Chi*\n\n**Abstract:** Object detection has been dominated by anchor-based detectors for several years. Recently, anchor-free detectors have become popular due to the proposal of FPN and Focal Loss. In this paper, we first point out that the essential difference between anchor-based and anchor-free detection is actually how to define positive and negative training samples, which leads to the performance gap between them. If they adopt the same definition of positive and negative samples during training, there is no obvious difference in the final performance, no matter regressing from a box or a point. This shows that how to select positive and negative training samples is important for current object detectors. Then, we propose an Adaptive Training Sample Selection (ATSS) to automatically select positive and negative samples according to statistical characteristics of object. It significantly improves the performance of anchor-based and anchor-free detectors and bridges the gap between them. Finally, we discuss the necessity of tiling multiple anchors per location on the image to detect objects. Extensive experiments conducted on MS COCO support our aforementioned analysis and conclusions. With the newly introduced ATSS, we improve state-of-the-art detectors by a large margin to $50.7\\%$ AP without introducing any overhead. The code is available at https://github.com/sfzhang15/ATSS\n\n**Conference:** bridging-the-gap-between-anchor-based-and-1\n\n**Published:** 2019-12-05\n\n\n\n#### 5. Keyformer: KV Cache Reduction through Key Tokens Selection for Efficient Generative Inference\n\n*From Search Query: selective KV caching*\n\n*Purushotham Kamath, Ilya Soloveychik, Prashant J. Nair, Gaurav Jain, Akhil Arunkumar, Muhammad Adnan*\n\n**Abstract:** Transformers have emerged as the underpinning architecture for Large Language Models (LLMs). In generative language models, the inference process involves two primary phases: prompt processing and token generation. Token generation, which constitutes the majority of the computational workload, primarily entails vector-matrix multiplications and interactions with the Key-Value (KV) Cache. This phase is constrained by memory bandwidth due to the overhead of transferring weights and KV cache values from the memory system to the computing units. This memory bottleneck becomes particularly pronounced in applications that require long-context and extensive text generation, both of which are increasingly crucial for LLMs. This paper introduces \"Keyformer\", an innovative inference-time approach, to mitigate the challenges associated with KV cache size and memory bandwidth utilization. Keyformer leverages the observation that approximately 90% of the attention weight in generative inference focuses on a specific subset of tokens, referred to as \"key\" tokens. Keyformer retains only the key tokens in the KV cache by identifying these crucial tokens using a novel score function. This approach effectively reduces both the KV cache size and memory bandwidth usage without compromising model accuracy. We evaluate Keyformer's performance across three foundational models: GPT-J, Cerebras-GPT, and MPT, which employ various positional embedding algorithms. Our assessment encompasses a variety of tasks, with a particular emphasis on summarization and conversation tasks involving extended contexts. Keyformer's reduction of KV cache reduces inference latency by 2.1x and improves token generation throughput by 2.4x, while preserving the model's accuracy.\n\n**Published:** 2024-03-14\n\n\n\n#### 6. CacheBlend: Fast Large Language Model Serving for RAG with Cached Knowledge Fusion\n\n*From Search Query: selective KV caching*\n\n*Junchen Jiang, Shan Lu, Kuntai Du, Qizheng Zhang, Yihua Cheng, Siddhant Ray, YuHan Liu, Hanchen Li, Jiayi Yao*\n\n**Abstract:** Large language models (LLMs) often incorporate multiple text chunks in their inputs to provide the necessary contexts. To speed up the prefill of the long LLM inputs, one can pre-compute the KV cache of a text and re-use the KV cache when the context is reused as the prefix of another LLM input. However, the reused text chunks are not always the input prefix, and when they are not, their precomputed KV caches cannot be directly used since they ignore the text's cross-attention with the preceding text in the LLM input. Thus, the benefits of reusing KV caches remain largely unrealized. This paper tackles just one question: when an LLM input contains multiple text chunks, how to quickly combine their precomputed KV caches in order to achieve the same generation quality as the expensive full prefill (i.e., without reusing KV cache)? We present CacheBlend, a scheme that reuses the pre-computed KV caches, regardless prefix or not, and selectively recomputes the KV values of a small subset of tokens to partially update each reused KV cache. In the meantime,the small extra delay for recomputing some tokens can be pipelined with the retrieval of KV caches within the same job,allowing CacheBlend to store KV caches in slower devices with more storage capacity while retrieving them without increasing the inference delay. By comparing CacheBlend with the state-of-the-art KV cache reusing schemes on three open-source LLMs of various sizes and four popular benchmark datasets of different tasks, we show that CacheBlend reduces time-to-first-token (TTFT) by 2.2-3.3X and increases the inference throughput by 2.8-5X, compared with full KV recompute, without compromising generation quality or incurring more storage cost.\n\n**Published:** 2024-05-26\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on attention sink mechanisms, adaptive tiling, and selective KV caching, here are some key insights and references from the provided sources and additional context:\n\n## Attention Sink Mechanisms\n- The concept of attention sinks is crucial for efficient streaming language models. Research by Xiao et al. introduces StreamingLLM, which utilizes attention sinks by maintaining the key and value states (KV) of initial tokens. This approach helps in generalizing language models to infinite sequence lengths without fine-tuning and improves performance in long-sequence modeling.\n- The phenomenon of attention sinks is further explored in the paper \"When Attention Sink Emerges in Language Models: An Empirical View,\" which demonstrates that attention sinks universally exist in language models, even in small models, and are influenced by optimization, data distribution, and loss functions during pre-training.\n- The active-dormant mechanism identified in \"Active-Dormant Attention Heads: Mechanistically Demystifying Extreme-Token Phenomena in LLMs\" also highlights how attention heads can become sinks for specific input domains, providing insights into mitigating extreme-token phenomena.\n\n## Adaptive Tiling\n- Adaptive tiling strategies can be informed by techniques like those in FlashAttention, which optimize memory access and reduce computational overhead. Although not directly mentioned in the provided sources, the concept of dynamic sparse attention in CASAK-V can be related to adaptive tiling. CASAK-V adapts attention sparsity based on the input context and task requirements, which can be seen as a form of adaptive tiling to focus on relevant contexts.\n- The dynamic sparse attention mechanism in CASAK-V learns to focus on local contexts for tasks like language modeling and captures sparse but important global interactions for tasks requiring long-range dependencies, which aligns with the need for adaptive tiling strategies.\n\n## Selective KV Caching\n- Selective KV caching is a critical component for memory efficiency. The StreamingLLM framework by Xiao et al. emphasizes the use of window attention and caching only the most recent KV states, which can be optimized further by introducing a placeholder token as a dedicated attention sink during pre-training.\n- The paper \"Adaptive KV Cache Compression for LLMs\" (though not directly cited in the sources) is implied in the analysis as a relevant reference. This concept involves compressing KV caches to reduce the memory footprint while maintaining generation quality, which is in line with the proposal's selective KV caching mechanism[Analysis].\n\n## Additional Insights\n- **Dynamic Sparse Attention**: CASAK-V's dynamic sparse attention mechanism effectively captures necessary information for language modeling even in very long contexts, achieving perplexity scores close to full-attention models. This approach can be integrated with existing pre-trained LLMs without extensive retraining, making it a promising method for long-context understanding.\n- **Optimization and Training**: The emergence of attention sinks is influenced by optimization techniques, data distribution, and loss functions during pre-training. Adjusting these factors, such as replacing softmax with ReLU and Adam with SGD, can help mitigate extreme-token phenomena and improve overall model performance.\n\nBy integrating these insights, the researchers can develop an autoregressive language model that leverages attention sink mechanisms to stabilize attention computation, employs adaptive tiling strategies to optimize memory access and computational overhead, and utilizes selective KV caching techniques to enhance memory efficiency while maintaining performance.\n",
            "analysis": "### Summary of Analysis\nThe StreamRetNetMLP proposal introduces innovative elements such as attention sink mechanisms, adaptive tiling, and selective KV caching to optimize streaming language models. The search results provide insights into similar concepts and techniques that can enhance memory efficiency and computational performance in language models.\n\n### Useful References and Excerpts\n1. **SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models** (Zhang, 2024): This paper discusses the use of attention sinks to stabilize attention computation in streaming scenarios, similar to the proposal's approach. It highlights the importance of maintaining initial tokens as attention sinks to improve performance in long-sequence modeling.\n\n2. **Efficient Streaming Language Models with Attention Sinks** (Xiao et al., 2023): This paper introduces StreamingLLM, which uses attention sinks to maintain initial tokens as attention sinks, improving performance in long-sequence modeling. It emphasizes the need for efficient memory usage and computational optimization in streaming scenarios.\n\n3. **FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness** (Dao et al., 2022): Discusses improvements in attention computation efficiency, which can be related to the proposal's adaptive tiling mechanism to optimize memory access and computational overhead.\n\n4. **Adaptive KV Cache Compression for LLMs** (Ge et al., 2023): This paper discusses adaptive KV cache compression, which aligns with the proposal's selective KV caching mechanism. It emphasizes reducing memory footprint while maintaining generation quality.\n\n### Key Insights\n- **Attention Sink Mechanisms**: The concept of using attention sinks to stabilize attention computation is supported by existing research, indicating its potential effectiveness in streaming scenarios.\n- **Adaptive Tiling**: Techniques like those used in FlashAttention-2 and S2-Attention can inform the implementation of adaptive tiling to improve memory access and reduce computational overhead.\n- **Selective KV Caching**: Adaptive KV cache compression methods highlight the importance of intelligent caching strategies to enhance efficiency without sacrificing performance.\n\n### Future Search Plan\n- Conduct further searches on specific implementations of attention sink mechanisms and adaptive tiling in language models.\n- Explore more detailed technical papers on KV caching strategies to refine the proposal's approach.\n\n### References\n- Zhang, H. (2024). SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models. arXiv.org.\n- Xiao, G., Tian, Y., Chen, B., Han, S., & Lewis, M. (2023). Efficient Streaming Language Models with Attention Sinks. arXiv.org.\n- Dao, T. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. International Conference on Learning Representations.\n- Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., & Gao, J. (2023). Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. International Conference on Learning Representations."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.7389600000000001,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.5205200000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "RetNetMLP",
    "proposal": "\n# StreamRetNetMLP: A Streaming-Optimized RetNetMLP with Attention Sink Integration\n\n## 1. Motivation\n\nCurrent RetNetMLP implementations face several challenges:\n- Memory inefficiency during streaming inference\n- Computational overhead in processing long sequences\n- Limited ability to handle attention sink effects\n- Inefficient gradient computation and memory management\n\nStreamRetNetMLP addresses these challenges by integrating attention sink mechanisms with memory-efficient computation strategies, enabling efficient streaming inference while maintaining model performance.\n\n## 2. Problem Analysis\n\n### 2.1 Memory Efficiency\nThe standard RetNetMLP requires significant memory for storing key-value states during inference, limiting its practical application in streaming scenarios.\n\n### 2.2 Attention Distribution\nCurrent implementations don't effectively handle attention sink effects, leading to potential performance degradation in long sequences.\n\n### 2.3 Computational Overhead\nProcessing long sequences involves substantial computational overhead, particularly in gradient computation and memory access.\n\n## 3. Core Idea and Philosophy\n\nStreamRetNetMLP introduces three key innovations:\n\n1. **Attention Sink Integration**:\n   - Maintains key initial tokens as attention sinks\n   - Enables stable attention computation in streaming scenarios\n   - Reduces memory requirements while preserving performance\n\n2. **Adaptive Tiling Mechanism**:\n   - Implements IO-aware tiling for efficient memory access\n   - Optimizes gradient computation through smart memory management\n   - Reduces HBM-SRAM transfers\n\n3. **Selective KV Caching**:\n   - Implements intelligent caching of key-value pairs\n   - Reduces repetitive computations in autoregressive generation\n   - Enables efficient streaming inference\n\n## 4. Design Plan\n\n### 4.1 Architecture Overview\n\n```python\nclass StreamRetNetMLP(nn.Module):\n    def __init__(self, dim, sink_size=4, tile_size=128):\n        super().__init__()\n        self.sink_size = sink_size\n        self.tile_size = tile_size\n        \n        # Core components\n        self.sink_tokens = nn.Parameter(torch.randn(sink_size, dim))\n        self.gate_proj = nn.Linear(dim, dim * 4)\n        self.down_proj = nn.Linear(dim * 2, dim)\n        \n        # Memory management\n        self.kv_cache = AdaptiveKVCache(dim)\n        self.tile_manager = TileManager(tile_size)\n        \n    def forward(self, x, cache_state=None):\n        # Process attention sinks\n        sink_attn = self._process_sinks(x)\n        \n        # Tile-based processing\n        tiled_output = self.tile_manager.process(\n            x, self._compute_block, sink_attn)\n            \n        # Update KV cache\n        new_cache = self.kv_cache.update(tiled_output)\n        \n        return tiled_output, new_cache\n```\n\n### 4.2 Key Components\n\n1. **Attention Sink Manager**:\n```python\ndef _process_sinks(self, x):\n    # Combine sink tokens with input\n    sink_tokens = self.sink_tokens.expand(x.shape[0], -1, -1)\n    sink_attn = torch.cat([sink_tokens, x], dim=1)\n    return self.attention(sink_attn)\n```\n\n2. **Adaptive Tiling**:\n```python\nclass TileManager:\n    def __init__(self, tile_size):\n        self.tile_size = tile_size\n        \n    def process(self, x, compute_fn, sink_attn):\n        tiles = x.chunk(x.shape[1] // self.tile_size, dim=1)\n        outputs = []\n        \n        for tile in tiles:\n            # Process tile with sink attention\n            tile_output = compute_fn(tile, sink_attn)\n            outputs.append(tile_output)\n            \n        return torch.cat(outputs, dim=1)\n```\n\n3. **Selective KV Cache**:\n```python\nclass AdaptiveKVCache:\n    def __init__(self, dim):\n        self.cache = {}\n        self.importance_estimator = nn.Linear(dim, 1)\n        \n    def update(self, x):\n        # Estimate importance scores\n        scores = self.importance_estimator(x)\n        \n        # Selectively update cache based on importance\n        mask = scores > self.threshold\n        self.cache.update({k: v for k, v in zip(\n            range(len(x)), x[mask])})\n        \n        return self.cache\n```\n\n### 4.3 Memory Management\n\n1. **Gradient Computation**:\n```python\ndef backward_pass(self, grad_output):\n    # Tile-based gradient computation\n    grad_tiles = grad_output.chunk(\n        grad_output.shape[1] // self.tile_size, dim=1)\n    \n    grads = []\n    for tile in grad_tiles:\n        # Compute gradients for each tile\n        tile_grad = self._compute_tile_gradient(tile)\n        grads.append(tile_grad)\n    \n    return torch.cat(grads, dim=1)\n```\n\n2. **Memory Access Optimization**:\n```python\ndef _compute_block(self, x, sink_attn):\n    # Efficient block computation with sink attention\n    combined = torch.cat([sink_attn, x], dim=1)\n    \n    # Gate computation\n    gates = self.gate_proj(combined)\n    gates = F.gelu(gates)\n    \n    # Down projection with memory optimization\n    output = self.down_proj(gates)\n    \n    return output\n```\n\n## 5. Implementation Guidelines\n\n### 5.1 Initialization\n```python\ndef initialize_stream_retnet_mlp(config):\n    model = StreamRetNetMLP(\n        dim=config.hidden_size,\n        sink_size=config.sink_size,\n        tile_size=config.tile_size\n    )\n    return model\n```\n\n### 5.2 Training Process\n```python\ndef train_step(model, batch, optimizer):\n    # Forward pass with attention sinks\n    output, cache = model(batch)\n    \n    # Compute loss with gradient checkpointing\n    loss = compute_loss(output, batch)\n    \n    # Backward pass with memory optimization\n    loss.backward()\n    optimizer.step()\n    \n    return loss\n```\n\n### 5.3 Inference\n```python\ndef inference_step(model, input_ids, cache=None):\n    # Process with attention sinks and cached states\n    output, new_cache = model(input_ids, cache)\n    \n    # Update cache for next step\n    cache = new_cache\n    \n    return output, cache\n```\n\n## 6. Theoretical Analysis\n\n### 6.1 Memory Complexity\n- O(N) memory usage for sequence length N\n- Constant memory overhead for attention sinks\n- Reduced memory access through tiling\n\n### 6.2 Computational Complexity\n- O(N) complexity for forward pass\n- O(T) complexity for tile processing, where T is tile size\n- Constant time overhead for sink attention\n\n### 6.3 Performance Analysis\n- Improved stability through attention sink integration\n- Reduced memory bandwidth through efficient tiling\n- Enhanced streaming performance via selective caching\n\n## 7. References\n\n[1] Xiao, G., Tian, Y., Chen, B., Han, S., & Lewis, M. (2023). Efficient Streaming Language Models with Attention Sinks. arXiv preprint.\n\n[2] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.\n\n[3] Zhang, H. (2024). SinkLoRA: Enhanced Efficiency and Chat Capabilities for Long-Context Large Language Models.\n\n[4] Sun, Y., Dong, L., Huang, S., Ma, S., Xia, Y., Xue, J., Wang, J., & Wei, F. (2023). Retentive Network: A Successor to Transformer for Large Language Models.\n\n## 8. Conclusion\n\nStreamRetNetMLP represents a significant advancement in efficient streaming language modeling by combining attention sink mechanisms with memory-efficient computation strategies. The design enables practical deployment of RetNet models in streaming scenarios while maintaining model performance through intelligent memory management and computation optimization.\n",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "gpt4o_0806",
            "IMPLEMENTATION_CODER": "o1_preview",
            "PROPOSAL_REVIEWER": "gpt4o_0806",
            "IMPLEMENTATION_OBSERVER": "o1_preview",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "StreamRetNetMLP introduces a streaming-optimized variant of RetNetMLP that incorporates attention sink mechanisms and memory-efficient gradient computation through adaptive tiling and selective KV caching. The design enables efficient processing of long sequences while maintaining model performance through intelligent memory management and computation optimization.",
    "ideation": null,
    "modelname": "streamretnet",
    "suggestions": "1. Conduct comprehensive empirical evaluations to validate the proposed modifications' impact on performance and efficiency.\n2. Provide detailed implementation guidelines, especially for integrating attention sinks and adaptive tiling.\n3. Address potential implementation complexities by outlining strategies for seamless integration and validation.",
    "user_input": ""
}