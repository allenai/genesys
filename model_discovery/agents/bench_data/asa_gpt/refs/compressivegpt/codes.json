{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 256,\n    'n_block': 9\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 1536,\n    'n_block': 57\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 12\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 2048,\n    'n_block': 57\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 27\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = GPT2(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass GPT2(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.mha = MHA(embed_dim=self.embed_dim, block_loc=self.block_loc,\n            kwarg_all=self.kwarg_all, **self.factory_kwargs, **self.kwarg_all)\n        self.mlp = AdaptiveCompressor(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, Z = self.norm1(X, **Z)\n        X2, Z = self.mha(X1, **Z)\n        X = X + X2\n        X3, Z = self.norm2(X, **Z)\n        X4, Z = self.mlp(X3, **Z)\n        X = X + X4\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass AdaptiveCompressor(GAUBase):\n    \"\"\"\n    AdaptiveCompressor GAU\n\n    This GAU implements content-adaptive compression by estimating the importance\n    of each element in the input sequence and compressing accordingly.\n\n    **Core Idea**:\n\n    - **Importance Estimation**: An importance score is computed for each element\n      in the sequence using a learnable importance estimator.\n\n    - **Adaptive Compression**: The input is compressed by scaling it with the\n      importance scores, ensuring that critical information is preserved\n      while less important information is attenuated.\n\n    **Mathematical Formulation**:\n\n    \\\\[\n    \text{scores} = \\\\sigma(W_{i} X + b_{i})\n    \\\\]\n\n    \\\\[\n    Y = X \\\\odot \text{scores}\n    \\\\]\n\n    where:\n    - \\\\( X \\\\) is the input tensor of shape (batch, seq_len, embed_dim)\n    - \\\\( W_{i} \\\\) and \\\\( b_{i} \\\\) are learnable parameters of the importance estimator\n    - \\\\( \\\\sigma \\\\) is the sigmoid activation function\n    - \\\\( \\\\odot \\\\) denotes element-wise multiplication\n\n    **Args**:\n        embed_dim (int): The embedding dimension of the input.\n        block_loc (tuple): The location of the block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The data type of the module's parameters.\n        compression_ratio (float, optional): The desired compression ratio. Default is 1.0 (no compression).\n\n    **Attributes**:\n        importance_estimator (nn.Linear): Linear layer to estimate importance scores.\n\n    **Example**:\n\n        >>> compressor = AdaptiveCompressor(embed_dim=512, block_loc=(0, 6), kwarg_all={}, compression_ratio=0.5)\n        >>> X = torch.randn(32, 128, 512)\n        >>> Y, Z = compressor(X)\n\n    **Note**:\n\n    - The output tensor \\\\( Y \\\\) has the same shape as the input tensor \\\\( X \\\\).\n    - Intermediate computations are stored in \\\\( Z \\\\) if needed.\n\n    **References**:\n        - Wang, Y., & Xiao, Z. (2024). LoMA: Lossless Compressed Memory Attention.\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, compression_ratio: float=1.0, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.importance_estimator = nn.Linear(embed_dim, 1, **self.\n            factory_kwargs)\n        self.compression_ratio = compression_ratio\n\n    def _forward(self, X, **Z):\n        scores = torch.sigmoid(self.importance_estimator(X))\n        Y = X * scores\n        Z['importance_scores'] = scores\n        return Y, Z\n\n\nimport torch.nn.functional as F\nimport math\nfrom einops import rearrange, repeat\n\n\nclass MHA(GAUBase):\n    \"\"\"Multi-head self-attention and cross-attention\"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        n_heads: int=8, causal: bool=True, num_heads_kv: int=None, head_dim:\n        int=None, mlp_dim: int=0, qkv_proj_bias: bool=True, out_proj_bias:\n        bool=True, softmax_scale: float=None, rotary_emb_base=10000.0,\n        d_conv: int=0, device=None, dtype=None, **kwargs) ->None:\n        \"\"\"\n        num_heads_kv: can be used to toggle MQA / GQA. If None, use num_heads.\n        return_residual: whether to return the input x along with the output. This is for\n            performance reason: for post-norm architecture, returning the input allows us\n            to fuse the backward of nn.Linear with the residual connection.\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.embed_dim = embed_dim\n        self.d_conv = d_conv\n        self.softmax_scale = softmax_scale\n        self.causal = causal\n        self.num_heads = n_heads\n        self.num_heads_kv = (num_heads_kv if num_heads_kv is not None else\n            n_heads)\n        assert self.num_heads % self.num_heads_kv == 0, 'num_heads must be divisible by num_heads_kv'\n        if head_dim is None:\n            assert self.embed_dim % n_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = (head_dim if head_dim is not None else self.\n            embed_dim // n_heads)\n        self.mlp_dim = math.ceil(mlp_dim / 256) * 256\n        qkv_dim = self.head_dim * (self.num_heads + 2 * self.num_heads_kv)\n        out_dim = self.head_dim * self.num_heads\n        kwarg_all['rotary_emb_dim'] = self.head_dim\n        self.rotary_emb = RotaryPositionalEmbeddings(embed_dim=self.\n            embed_dim, block_loc=self.block_loc, kwarg_all=self.kwarg_all,\n            **self.factory_kwargs, **self.kwarg_all)\n        self.in_proj = nn.Linear(embed_dim, qkv_dim + self.mlp_dim, bias=\n            qkv_proj_bias, **self.factory_kwargs)\n        if self.d_conv > 0:\n            self.conv1d = nn.Conv1d(qkv_dim, qkv_dim, kernel_size=self.\n                d_conv, padding=self.d_conv - 1, groups=qkv_dim, **self.\n                factory_kwargs)\n        self.out_proj = nn.Linear(out_dim + self.mlp_dim // 2, embed_dim,\n            bias=out_proj_bias, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        \"\"\"\n        Arguments:\n            x: (batch, seqlen, hidden_dim) (where hidden_dim = num heads * head dim) if\n                cu_seqlens is None and max_seqlen is None, else (total, hidden_dim) where total\n                is the is the sum of the sequence lengths in the batch.\n            inference_params: for generation. Adapted from Megatron-LM (and Apex)\n            https://github.com/NVIDIA/apex/blob/3ff1a10f72ec07067c4e44759442329804ac5162/apex/transformer/testing/standalone_transformer_lm.py#L470\n        \"\"\"\n        qkv = self.in_proj(X)\n        if self.mlp_dim > 0:\n            qkv, x_mlp = qkv.split([qkv.shape[-1] - self.mlp_dim, self.\n                mlp_dim], dim=-1)\n            x_mlp_up, x_mlp_gate = x_mlp.chunk(2, dim=-1)\n            x_mlp = x_mlp_up * F.silu(x_mlp_gate)\n        if self.d_conv > 0:\n            qkv = rearrange(self.conv1d(rearrange(qkv, 'b s d -> b d s'))[\n                ..., :-(self.d_conv - 1)], 'b d s -> b s d').contiguous()\n        q, k, v = qkv.split([self.num_heads * self.head_dim] * 3, dim=-1)\n        q = rearrange(q, '... (h d) -> ... h d', d=self.head_dim)\n        k = rearrange(k, '... (h d) -> ... h d', d=self.head_dim)\n        v = rearrange(v, '... (h d) -> ... h d', d=self.head_dim)\n        Z['input_emb'] = q\n        _, Z = self.rotary_emb(X, **Z)\n        q = Z['output_emb']\n        Z['input_emb'] = k\n        _, Z = self.rotary_emb(X, **Z)\n        k = Z['output_emb']\n        k = torch.repeat_interleave(k, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        v = torch.repeat_interleave(v, dim=2, repeats=self.num_heads //\n            self.num_heads_kv)\n        context = F.scaled_dot_product_attention(q.transpose(1, 2), k.\n            transpose(1, 2), v.transpose(1, 2), is_causal=self.causal,\n            scale=self.softmax_scale).transpose(1, 2)\n        context = rearrange(context, '... h d -> ... (h d)')\n        if self.mlp_dim > 0:\n            context = torch.cat([context, x_mlp], dim=-1)\n        out = self.out_proj(context)\n        return out\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom typing import Optional\n\n\nclass RotaryPositionalEmbeddings(GAUBase):\n    \"\"\"\n    This class implements Rotary Positional Embeddings (RoPE)\n    proposed in https://arxiv.org/abs/2104.09864.\n\n    Reference implementation (used for correctness verfication)\n    can be found here:\n    https://github.com/meta-llama/llama/blob/main/llama/model.py#L80\n\n    In this implementation we cache the embeddings for each position upto\n    ``max_seq_len`` by computing this during init.\n\n    Args:\n        dim (int): Embedding dimension. This is usually set to the dim of each\n            head in the attention module computed as ````embed_dim`` // ``num_heads````\n        max_seq_len (int): Maximum expected sequence length for the\n            model, if exceeded the cached freqs will be recomputed\n        base (int): The base for the geometric progression used to compute\n            the rotation angles\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, rotary_emb_base: int=10000, rotary_emb_dim:\n        int=None, max_seq_len: int=4096, **kwargs) ->None:\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.dim = rotary_emb_dim\n        self.base = rotary_emb_base\n        self.max_seq_len = max_seq_len\n        self._rope_init()\n\n    def reset_parameters(self):\n        self._rope_init()\n\n    def _rope_init(self):\n        theta = 1.0 / self.base ** (torch.arange(0, self.dim, 2, **self.\n            factory_kwargs)[:self.dim // 2].float() / self.dim)\n        self.register_buffer('theta', theta, persistent=False)\n        self.build_rope_cache(self.max_seq_len)\n\n    def build_rope_cache(self, max_seq_len: int=4096) ->None:\n        seq_idx = torch.arange(max_seq_len, dtype=self.theta.dtype, device=\n            self.theta.device)\n        idx_theta = torch.einsum('i, j -> ij', seq_idx, self.theta).float()\n        cache = torch.stack([torch.cos(idx_theta), torch.sin(idx_theta)],\n            dim=-1)\n        self.register_buffer('cache', cache, persistent=False)\n\n    def _forward(self, X: Tensor, input_emb: Tensor, input_pos: Optional[\n        Tensor]=None) ->Tensor:\n        \"\"\"\n        Args:\n            x (Tensor): input tensor with shape\n                [b, s, n_h, h_d]\n            input_pos (Optional[Tensor]): Optional tensor which contains the position ids\n                of each token. During training, this is used to indicate the positions\n                of each token relative to its sample when packed, shape [b, s].\n                During inference, this indicates the position of the current token.\n                If none, assume the index of the token is its position id. Default is None.\n\n        Returns:\n            Tensor: output tensor with RoPE applied\n\n        Notation used for tensor shapes:\n            - b: batch size\n            - s: sequence length\n            - n_h: num heads\n            - h_d: head dim\n\n        TODO: The implementation below can be made more efficient\n        for inference.\n        \"\"\"\n        seq_len = input_emb.size(1)\n        rope_cache = self.cache[:seq_len] if input_pos is None else self.cache[\n            input_pos]\n        xshaped = input_emb.float().reshape(*input_emb.shape[:-1], -1, 2)\n        rope_cache = rope_cache.view(-1, xshaped.size(1), 1, xshaped.size(3), 2\n            )\n        x_out = torch.stack([xshaped[..., 0] * rope_cache[..., 0] - xshaped\n            [..., 1] * rope_cache[..., 1], xshaped[..., 1] * rope_cache[...,\n            0] + xshaped[..., 0] * rope_cache[..., 1]], -1)\n        x_out = x_out.flatten(3)\n        output_emb = x_out.type_as(input_emb)\n        return X, {'output_emb': output_emb}\n\n\nimport torch.nn.functional as F\nfrom torch import Tensor\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    Root Mean Square Layer Normalization (RMSNorm).\n\n    This layer applies a variant of layer normalization that uses only the root mean square\n    statistics, without centering. It's computationally more efficient than standard\n    layer normalization and has been shown to be effective in various NLP tasks.\n\n    Args:\n        embed_dim (int): The size of the input feature dimension.\n        block_loc (tuple): The location of this block in the model architecture.\n        kwarg_all (dict): Additional keyword arguments passed to the parent class.\n        device (torch.device, optional): The device on which to allocate the module's parameters.\n        dtype (torch.dtype, optional): The dtype of the module's parameters.\n        eps (float, optional): A small constant added to the denominator for numerical stability.\n            Default: 1e-5.\n\n    Attributes:\n        weight (nn.Parameter): Learnable scale parameter of shape (embed_dim,).\n        variance_epsilon (float): The epsilon value used in the normalization formula.\n\n    Shape:\n        - Input: (*, embed_dim)\n        - Output: (*, embed_dim) (same shape as input)\n\n    Examples:\n        >>> rmsnorm = RMSNorm(128, (0, 6), {})\n        >>> x = torch.randn(1, 100, 128)\n        >>> output = rmsnorm(x)\n        >>> print(output.shape)\n        torch.Size([1, 100, 128])\n\n    References:\n        - Paper: \"Root Mean Square Layer Normalization\" by Biao Zhang and Rico Sennrich\n          https://arxiv.org/abs/1910.07467\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-05, **kwargs):\n        \"\"\"If group_size is not None, we do GroupNorm with each group having group_size elements.\n        group_size=None is equivalent to group_size=hidden_size (i.e. there's only 1 group).\n        \"\"\"\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.variance_epsilon = eps\n\n    def _forward(self, X, **Z):\n        input_dtype = X.dtype\n        X = X.to(torch.float32)\n        variance = X.pow(2).mean(-1, keepdim=True)\n        X = X * torch.rsqrt(variance + self.variance_epsilon)\n        return self.weight * X.to(input_dtype)\n\n\ngab_config = {'eps': 1e-05, 'rotary_emb_base': 10000.0, 'max_seq_len': 4096,\n    'n_heads': 8, 'causal': True, 'num_heads_kv': None, 'head_dim': None,\n    'mlp_dim': 0, 'qkv_proj_bias': True, 'out_proj_bias': True,\n    'softmax_scale': None, 'd_conv': 0, 'compression_ratio': 1.0}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 56\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}