{
    "variantname": "SpectralHierarchicalMamba",
    "review": "The SpectralHierarchicalMamba proposal presents an innovative approach to enhancing the HierarchicalMambaLayer through the integration of spectral filtering and adaptive memory mechanisms. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Theoretical Foundation\n- Well-grounded in spectral theory and state space modeling\n- Clear mathematical formulation of spectral filtering approach\n- Strong connection to recent advances in SSMs\n- Thoughtful integration with hierarchical processing\n\n2. Innovation\n- Novel combination of spectral filtering with hierarchical SSMs\n- Unique approach to multi-scale temporal processing\n- Distinguished from existing work through fixed spectral filters\n- Original integration of theoretical spectral methods with practical architecture\n\n3. Technical Design\n- Detailed mathematical formulations\n- Clear implementation guidelines\n- Well-structured architecture\n- Careful consideration of memory management\n\n4. Efficiency Considerations\n- Maintains linear computational complexity\n- Efficient memory management through hierarchical structure\n- Parameter-free spectral filters reduce model size\n- Parallel processing capabilities\n\nCONCERNS:\n\n1. Implementation Complexity\n- Complex integration of spectral filtering\n- Challenging FFT implementation requirements\n- Additional computational overhead from multi-scale processing\n- Potential numerical stability issues\n\n2. Memory Management\n- Complex hierarchical memory structure\n- Need for efficient FFT implementation\n- Trade-off between memory and computation\n- Risk of cache inefficiency\n\n3. Training Dynamics\n- Limited discussion of training stability\n- Need for careful initialization strategies\n- Potential challenges in gradient flow\n- Complex interaction between spectral and hierarchical components\n\n4. Hardware Considerations\n- FFT operations may not be hardware-efficient\n- Complex memory access patterns\n- Potential bottlenecks in parallel processing\n- Need for specialized implementations\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More theoretically grounded than Mamba's selective mechanisms\n2. More efficient than traditional spectral approaches\n3. Novel integration of fixed filters with hierarchical SSMs\n4. Unique approach to multi-scale processing\n\nHowever, it shares some concepts with:\n1. Spectral State Space Models (Agarwal et al., 2023)\n2. Hierarchical SSMs (Bhirangi et al., 2024)\n3. DenseMamba's information flow mechanisms",
    "search_stack": [
        {
            "ready": false,
            "query": "polynomial kernel neural networks, hardware efficient memory mechanisms, state space model stability",
            "detail": "Find papers discussing:\n1. Applications of polynomial kernels in neural networks, particularly for attention or state space models\n2. Hardware-efficient memory mechanisms and their implementation details\n3. Stability analysis and training techniques for deep state space models\nFocus on papers from the last 2-3 years with practical implementation details.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Applications of polynomial kernels in neural networks, particularly for attention or state space models\n2. Hardware-efficient memory mechanisms and their implementation details\n3. Stability analysis and training techniques for deep state space models\nFocus on papers from the last 2-3 years with practical implementation details.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks (Avg. Score: 0.91)\n\n*Jerome Sieber, Carmen Amo Alonso, A. Didier, M. Zeilinger, Antonio Orvieto*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** This paper introduces the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation, and facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class.\n\n**Abstract:** Softmax attention is the principle backbone of foundation models for various artificial intelligence applications, yet its quadratic complexity in sequence length can limit its inference throughput in long-context settings. To address this challenge, alternative architectures such as linear attention, State Space Models (SSMs), and Recurrent Neural Networks (RNNs) have been considered as more efficient alternatives. While connections between these approaches exist, such models are commonly developed in isolation and there is a lack of theoretical understanding of the shared principles underpinning these architectures and their subtle differences, greatly influencing performance and scalability. In this paper, we introduce the Dynamical Systems Framework (DSF), which allows a principled investigation of all these architectures in a common representation. Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class. For instance, we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent. We also provide principled comparisons between softmax attention and other model classes, discussing the theoretical conditions under which softmax attention can be approximated. Additionally, we substantiate these new insights with empirical validations and mathematical arguments. This shows the DSF's potential to guide the systematic development of future more efficient and scalable foundation models.\n\n##### *Relevant Chunk: No. 16/29 (Score: 0.97)*\n\n```\narXiv preprint arXiv:2401.10166, 2024b. Daniel L\u00f3pez-S\u00e1nchez, Ang\u00e9lica Gonz\u00e1lez Arrieta, and Juan M Corchado. Data-independent random projections from the feature-space of the homogeneous polynomial kernel. Pattern Recognition, $82: 130-146,2018$. Ilya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id= Bkg6RiCqY7. Chris Lu, Yannick Schroecker, Albert Gu, Emilio Parisotto, Jakob Foerster, Satinder Singh, and Feryal Behbahani. Structured state space models for in-context reinforcement learning. Advances in Neural Information Processing Systems, 2023. William Merrill, Jackson Petty, and Ashish Sabharwal. The illusion of state in state-space models. arXiv preprint arXiv:2404.08819, 2024. Tobias Christian Nauen, Sebastian Palacio, and Andreas Dengel. Taylorshift: Shifting the complexity of self-attention from squared to linear (and back) using taylor-softmax.\n```\n\n##### *Relevant Chunk: No. 14/29 (Score: 0.85)*\n\n```\nURL https://arxiv.org/abs/2402.19427. Daniel Y. Fu, Tri Dao, Khaled K. Saab, Armin W. Thomas, Atri Rudra, and Christopher R\u00e9. Hungry Hungry Hippos: Towards Language Modeling with State Space Models, 2023. URL https: //arxiv.org/abs/2212.14052\nKaran Goel, Albert Gu, Chris Donahue, and Christopher R\u00e9. It's raw! audio generation with state-space models. arXiv preprint arXiv:2202.09729, 2022. Albert Gu and Tri Dao. Mamba: Linear-Time Sequence Modeling with Selective State Spaces, 2023. URL https://arxiv.org/abs/2312.00752\n\nAlbert Gu, Tri Dao, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. HiPPO: Recurrent Memory with Optimal Polynomial Projections. In Advances in Neural Information Processing Systems, volume 33, pages 1474-1487. Curran Associates, Inc., 2020. Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently Modeling Long Sequences with Structured State Spaces. In The International Conference on Learning Representations (ICLR), 2022a. Albert Gu, Ankit Gupta, Karan Goel, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models, 2022b. URL https://arxiv.org/abs/2206.11893. Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. In Advances in Neural Information Processing Systems, volume 35, pages 22982-22994. Curran Associates, Inc., 2022. Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, 9(8): $1735-1780,1997$. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Fran\u00e7ois Fleuret. Transformers are rnns: fast autoregressive transformers with linear attention.\n```\n\n#### 2. Structured state-space models are deep Wiener models (Avg. Score: 0.91)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 16/22 (Score: 0.91)*\n\n```\nForgione, M. and Piga, D. (2021). dynoNet: A neural network architecture for learning dynamical systems. International Journal of Adaptive Control and Signal Processing, 35(4), 612-626. Gu, A., Dao, T., Ermon, S., Rudra, A., and R\u00e9, C. (2020). Hippo: Recurrent memory with optimal polynomial projections. Advances in neural information processing systems, 33, 1474-1487. Gu, A., Goel, K., Gupta, A., and R\u00e9, C. (2022). On the parameterization and initialization of diagonal state space models. Advances in Neural Information Processing Systems, 35, 35971-35983. Gu, A., Goel, K., and R\u00e9, C. (2021). Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396. Gupta, A., Gu, A., and Berant, J. (2022). Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35, $22982-22994$. Kumar, S.K. (2017). On weight initialization in deep neural networks. arXiv preprint arXiv:1704.08863. Lanzetti, N. et al. (2019). Recurrent neural network based MPC for process industries. In 2019 18th European Control Conference (ECC), 1005-1010. IEEE. Ljung, L., Zhang, Q., Lindskog, P., and Juditski, A. (2004). Estimation of grey box and black box models for nonlinear circuit data. IFAC Proceedings Volumes, 37(13), $399-404$. Marconato, A., Sj\u00f6berg, J., Suykens, J.A., and Schoukens, J. (2013). Improved initialization for nonlinear statespace modeling. IEEE Transactions on instrumentation and Measurement, 63(4), 972-980.\n```\n\n#### 3. Random Feature Attention (Avg. Score: 0.84)\n\n*Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A. Smith, Lingpeng Kong*\n\n**Published in:** International Conference on Learning Representations (2021)\t**Cited by** 292  (*Influential: 26*)\n\n**TL;DR:** RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, is proposed and explored, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.\n\n**Abstract:** Transformers are state-of-the-art models for a variety of sequence modeling tasks. At their core is an attention function which models pairwise interactions between the inputs at every timestep. While attention is powerful, it does not scale efficiently to long sequences due to its quadratic time and space complexity in the sequence length. We propose RFA, a linear time and space attention that uses random feature methods to approximate the softmax function, and explore its application in transformers. RFA can be used as a drop-in replacement for conventional softmax attention and offers a straightforward way of learning with recency bias through an optional gating mechanism. Experiments on language modeling and machine translation demonstrate that RFA achieves similar or better performance compared to strong transformer baselines. In the machine translation experiment, RFA decodes twice as fast as a vanilla transformer. Compared to existing efficient transformer variants, RFA is competitive in terms of both accuracy and efficiency on three long text classification datasets. Our analysis shows that RFA's efficiency gains are especially notable on long sequences, suggesting that RFA will be particularly useful in tasks that require working with large inputs, fast decoding speed, or low memory footprints.\n\n##### *Relevant Chunk: No. 13/40 (Score: 0.84)*\n\n```\nIn Proc. of EMNLP, 2014. Youngmin Cho and Lawrence K. Saul. Kernel methods for deep learning. In Proc. of NeurIPS, 2009. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, David Belanger, Lucy Colwell, and Adrian Weller. Rethinking attention with performers. In Proc. of ICLR, 2020. Djork-Arn\u00e9 Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (ELUs).\n```\n\n#### 4. Spectral State Space Models (Avg. Score: 0.71)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.71)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n\n\n---\n## Found 6 related papers from 1 external sources\n\n\n\nYour 3 raw search queries input to the search frame: polynomial kernel neural networks, hardware efficient memory mechanisms, state space model stability\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Papers with Code\n\n#### 1. Neural Tangent Kernel: Convergence and Generalization in Neural Networks\n\n*From Search Query: polynomial kernel neural networks*\n\n*Cl\u00e9ment Hongler, Franck Gabriel, Arthur Jacot*\n\n**Abstract:** At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit, thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function $f_\\theta$ (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We prove the positive-definiteness of the limiting NTK when the data is supported on the sphere and the non-linearity is non-polynomial. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function $f_\\theta$ follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.\n\n**Conference:** neural-tangent-kernel-convergence-and-1\n\n**Published:** 2018-06-20\n\n\n\n#### 2. Multi-scale Location-aware Kernel Representation for Object Detection\n\n*From Search Query: polynomial kernel neural networks*\n\n*WangMeng Zuo, Peihua Li, Hao Wang, Mingqi Gao, Qilong Wang*\n\n**Abstract:** Although Faster R-CNN and its variants have shown promising performance in\nobject detection, they only exploit simple first-order representation of object\nproposals for final classification and regression. Recent classification\nmethods demonstrate that the integration of high-order statistics into deep\nconvolutional neural networks can achieve impressive improvement, but their\ngoal is to model whole images by discarding location information so that they\ncannot be directly adopted to object detection. In this paper, we make an\nattempt to exploit high-order statistics in object detection, aiming at\ngenerating more discriminative representations for proposals to enhance the\nperformance of detectors. To this end, we propose a novel Multi-scale\nLocation-aware Kernel Representation (MLKP) to capture high-order statistics of\ndeep features in proposals. Our MLKP can be efficiently computed on a modified\nmulti-scale feature map using a low-dimensional polynomial kernel\napproximation.Moreover, different from existing orderless global\nrepresentations based on high-order statistics, our proposed MLKP is location\nretentive and sensitive so that it can be flexibly adopted to object detection.\nThrough integrating into Faster R-CNN schema, the proposed MLKP achieves very\ncompetitive performance with state-of-the-art methods, and improves Faster\nR-CNN by 4.9% (mAP), 4.7% (mAP) and 5.0% (AP at IOU=[0.5:0.05:0.95]) on PASCAL\nVOC 2007, VOC 2012 and MS COCO benchmarks, respectively. Code is available at:\nhttps://github.com/Hwang64/MLKP.\n\n**Conference:** multi-scale-location-aware-kernel-1\n\n**Published:** 2018-04-02\n\n\n\n#### 3. HyperConformer: Multi-head HyperMixer for Efficient Speech Recognition\n\n*From Search Query: hardware efficient memory mechanisms*\n\n*Petr Motlicek, Titouan Parcollet, Juan Zuluaga-Gomez, Florian Mai*\n\n**Abstract:** State-of-the-art ASR systems have achieved promising results by modeling local and global interactions separately. While the former can be computed efficiently, global interactions are usually modeled via attention mechanisms, which are expensive for long input sequences. Here, we address this by extending HyperMixer, an efficient alternative to attention exhibiting linear complexity, to the Conformer architecture for speech recognition, leading to HyperConformer. In particular, multi-head HyperConformer achieves comparable or higher recognition performance while being more efficient than Conformer in terms of inference speed, memory, parameter count, and available training data. HyperConformer achieves a word error rate of 2.9% on Librispeech test-clean with less than 8M neural parameters and a peak memory during training of 5.7GB, hence trainable with accessible hardware. Encoder speed is between 38% on mid-length speech and 56% on long speech faster than an equivalent Conformer. (The HyperConformer recipe is publicly available in: https://github.com/speechbrain/speechbrain/tree/develop/recipes/LibriSpeech/ASR/transformer/)\n\n**Published:** 2023-05-29\n\n\n\n#### 4. Gated Slot Attention for Efficient Linear-Time Sequence Modeling\n\n*From Search Query: hardware efficient memory mechanisms*\n\n*Guohong Fu, Peng Zhou, Wei Bi, Bailin Wang, Freda Shi, Bolun Wang, Yiqiao Wang, Leyang Cui, Yue Zhang, Ruijie Zhu, Songlin Yang, Yu Zhang*\n\n**Abstract:** Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.\n\n**Published:** 2024-09-11\n\n\n\n#### 5. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: state space model stability*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n#### 6. Diffusion Policy: Visuomotor Policy Learning via Action Diffusion\n\n*From Search Query: state space model stability*\n\n*Anonymous*\n\n**Abstract:** This paper introduces Diffusion Policy, a new way of generating robot behavior by representing a robot's visuomotor policy as a conditional denoising diffusion process. We benchmark Diffusion Policy across 12 different tasks from 4 different robot manipulation benchmarks and find that it consistently outperforms existing state-of-the-art robot learning methods with an average improvement of 46.9%. Diffusion Policy learns the gradient of the action-distribution score function and iteratively optimizes with respect to this gradient field during inference via a series of stochastic Langevin dynamics steps. We find that the diffusion formulation yields powerful advantages when used for robot policies, including gracefully handling multimodal action distributions, being suitable for high-dimensional action spaces, and exhibiting impressive training stability. To fully unlock the potential of diffusion models for visuomotor policy learning on physical robots, this paper presents a set of key technical contributions including the incorporation of receding horizon control, visual conditioning, and the time-series diffusion transformer. We hope this work will help motivate a new generation of policy learning techniques that are able to leverage the powerful generative modeling capabilities of diffusion models. Code, data, and training details is publicly available diffusion-policy.cs.columbia.edu\n\n**Published:** 2023-03-07\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, here are some key findings and references that align with the specified keywords and details:\n\n## Applications of Polynomial Kernels in Neural Networks\n\n### Polynomial Kernels for Attention Mechanisms\n- The concept of polynomial kernels as an efficient alternative to softmax attention is highlighted in the context of state space models and sequence processing. For instance, the HiPPO (High-dimensional Polynomial Projection) technique, mentioned in and, transforms input data into a higher-dimensional space using polynomial functions. This allows the model to capture complex relationships and long-range dependencies efficiently.\n- The HiPPO framework is particularly relevant as it provides a method for continuous-time memorization and can be described using polynomial projection operators, which is crucial for state space models.\n\n## Hardware-Efficient Memory Mechanisms\n\n### Efficient Memory Handling\n- The Sparse Transformer and Mamba models introduce hardware-efficient memory mechanisms. For example, the Sparse Transformer reduces complexity from \\(O(n^2)\\) to \\(O(n\\sqrt{n})\\) through sparse factorizations, enabling faster processing times and efficient handling of larger input sizes.\n- The Mamba model, especially the Sparse-Mamba (S-Mamba), incorporates a hardware-aware algorithm that streamlines storage and processing through parallel operations, kernel fusion, and recomputation. This significantly boosts efficiency and allows the model to handle larger datasets without compromising performance.\n\n### Memory Augmentation\n- The concept of external dynamic memory, as seen in models like Memformer, can be applied to state space models. This involves encoding and retrieving past information using an external memory, which can improve long-range dependency modeling and reduce memory usage (though not directly cited, the concept is relevant and aligns with the analysis note).\n\n## Stability Analysis and Training Techniques for Deep State Space Models\n\n### Stability and Training\n- The stability of deep state space models is a critical aspect. The S-Mamba model, for instance, reinforces controllability and observability in state-space equations, which reduces complexity and computational costs. This reinforcement improves perplexity by 5% and decreases training time by 3%.\n- The use of polynomial projection operators and structured state spaces (like S4 models) helps in maintaining stability by compressing context into a smaller state. These models address the trade-off between efficiency and effectiveness, particularly in tasks requiring context-aware output generation.\n\n### Normalization and Gradient Flow\n- Normalization techniques are essential for deep architectures. The analysis note highlights the importance of normalization and careful consideration of gradient flow in memory-augmented models. This is consistent with the need for stable training techniques in deep state space models.\n\n## Additional Insights\n\n### Parallel Processing and Efficiency\n- The SPikE-SSM model proposed in integrates dynamic hidden states for handling long-range dependencies and achieves fast inference and parallel training. This model uses a max-min boundary compression strategy to facilitate efficient inference, which is crucial for scalability and hardware efficiency.\n\nBy integrating these insights, researchers can develop novel autoregressive language models that leverage polynomial kernels for efficient attention mechanisms, employ hardware-efficient memory handling, and ensure stability through advanced training techniques and normalization methods. These approaches can significantly enhance the performance, efficiency, and scalability of the models.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe seed design \"hierarchicalmamba\" introduces a hierarchical state space model approach to enhance Mamba2Layer. From analyzing this and the references, several key themes and opportunities emerge:\n\na) State Space Models (SSMs) are showing promise for efficient sequence processing but face challenges in:\n- Capturing dependencies across different temporal scales\n- Maintaining stability in deep architectures\n- Efficient hardware utilization\n\nb) Recent innovations in attention mechanisms and memory handling suggest potential directions:\n- Polynomial kernels as efficient alternatives to softmax\n- Memory augmentation for improved context handling\n- Sketching techniques for reducing computational complexity\n\nc) Key areas for potential innovation:\n- Novel kernel functions for attention computation\n- Efficient memory mechanisms\n- Hardware-aware architectural optimizations\n- Improved normalization and stability techniques\n\n2. Useful References with Excerpts\n\nFrom \"Fast Transformers via Sketching Polynomial Kernels\":\n\"polynomial attention with high degree can effectively replace softmax without sacrificing model quality\"\n\"polynomial sketching techniques from numerical linear algebra to achieve linear-time polynomial attention with approximation guarantees\"\n\nFrom \"Memformer\":\n\"utilizes an external dynamic memory to encode and retrieve past information\"\n\"memory replay back-propagation (MRBP), which promotes long-range back-propagation through time\"\n\"achieved comparable performance compared to the baselines by using 8.1x less memory space and 3.2x faster on inference\"\n\nFrom \"Performers\":\n\"estimate regular (softmax) full-rank-attention Transformers with provable accuracy\"\n\"using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness\"\n\"FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax\"\n\n3. Key Insights and Analysis\n\na) Kernel Functions:\n- Polynomial kernels offer an efficient alternative to softmax attention\n- Sketching techniques can reduce complexity while maintaining accuracy\n- Opportunity to develop novel kernel functions optimized for state space models\n\nb) Memory Mechanisms:\n- External memory can improve long-range dependency modeling\n- Memory replay techniques can enhance gradient flow\n- Potential to combine memory mechanisms with state space models\n\nc) Hardware Efficiency:\n- Block-based processing can improve cache utilization\n- Linear-time attention mechanisms are crucial for scalability\n- Memory access patterns significantly impact performance\n\nd) Stability and Training:\n- Normalization techniques are important for deep architectures\n- Gradient flow needs careful consideration in memory-augmented models\n- Multi-scale processing can improve stability\n\n4. Future Search Plan\n\nNext round of searches should focus on:\n1. Recent advances in polynomial kernel methods and their applications in deep learning\n2. Hardware-efficient implementations of memory mechanisms\n3. Stability analysis of deep state space models\n\n5. References\n\n1. Kacham, P., Mirrokni, V., & Zhong, P. (2024). Fast Transformers via Sketching Polynomial Kernels. arXiv preprint.\n\n2. Wu, Q., Lan, Z., Qian, K., Gu, J., Geramifard, A., & Yu, Z. (2020). Memformer: A Memory-Augmented Transformer for Sequence Modeling. AACL/IJCNLP.\n\n3. Choromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarl\u00f3s, T., ... & Weller, A. (2020). Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy. International Conference on Learning Representations.\n\n4. Yu, R., Yu, W., & Wang, X. (2024). KAN or MLP: A Fairer Comparison. arXiv preprint."
        },
        {
            "ready": false,
            "query": "spectral state space models, gated memory mechanisms, hierarchical neural architectures",
            "detail": "Find papers discussing:\n1. Applications of spectral methods in state space models, particularly for stability and efficiency\n2. Implementations of gated memory mechanisms in neural architectures\n3. Hierarchical approaches to sequence modeling\nFocus on papers from the last 2 years with theoretical analysis and implementation details.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Applications of spectral methods in state space models, particularly for stability and efficiency\n2. Implementations of gated memory mechanisms in neural architectures\n3. Hierarchical approaches to sequence modeling\nFocus on papers from the last 2 years with theoretical analysis and implementation details.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 1/31 (Score: 1.00)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.99)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.99)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.98)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.98)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.97)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.97)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: spectral state space models, gated memory mechanisms, hierarchical neural architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 1 related papers from ArXiv\n\n#### 1. Language Model Pre-training for Hierarchical Document Representations\n\n*From Search Query: hierarchical neural architectures*\n\n*Ming-Wei Chang, Kristina Toutanova, Kenton Lee, Jacob Devlin*\n\n**Abstract:** Hierarchical neural architectures are often used to capture long-distance\ndependencies and have been applied to many document-level tasks such as\nsummarization, document segmentation, and sentiment analysis. However,\neffective usage of such a large context can be difficult to learn, especially\nin the case where there is limited labeled data available. Building on the\nrecent success of language model pretraining methods for learning flat\nrepresentations of text, we propose algorithms for pre-training hierarchical\ndocument representations from unlabeled data. Unlike prior work, which has\nfocused on pre-training contextual token representations or context-independent\n{sentence/paragraph} representations, our hierarchical document representations\ninclude fixed-length sentence/paragraph representations which integrate\ncontextual information from the entire documents. Experiments on document\nsegmentation, document-level question answering, and extractive document\nsummarization demonstrate the effectiveness of the proposed pre-training\nalgorithms.\n\n**Published:** 2019-01-26T00:35:35Z  (*Updated: 2019-01-26T00:35:35Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Spectral State Space Models\n\n*From Search Query: spectral state space models*\n\n*Elad Hazan, Xinyi Chen, Daniel Suo, Naman Agarwal*\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n**Published:** 2023-12-11\n\n\n\n#### 2. SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral Image Denoising\n\n*From Search Query: spectral state space models*\n\n*Jun Zhou, Jianfeng Lu, Fengchao Xiong, Guanyiman Fu*\n\n**Abstract:** Denoising is a crucial preprocessing step for hyperspectral images (HSIs) due to noise arising from intra-imaging mechanisms and environmental factors. Long-range spatial-spectral correlation modeling is beneficial for HSI denoising but often comes with high computational complexity. Based on the state space model (SSM), Mamba is known for its remarkable long-range dependency modeling capabilities and computational efficiency. Building on this, we introduce a memory-efficient spatial-spectral UMamba (SSUMamba) for HSI denoising, with the spatial-spectral continuous scan (SSCS) Mamba being the core component. SSCS Mamba alternates the row, column, and band in six different orders to generate the sequence and uses the bidirectional SSM to exploit long-range spatial-spectral dependencies. In each order, the images are rearranged between adjacent scans to ensure spatial-spectral continuity. Additionally, 3D convolutions are embedded into the SSCS Mamba to enhance local spatial-spectral modeling. Experiments demonstrate that SSUMamba achieves superior denoising results with lower memory consumption per batch compared to transformer-based methods. The source code is available at https://github.com/lronkitty/SSUMamba.\n\n**Published:** 2024-05-02\n\n\n\n#### 3. A Combinatorial Perspective on Transfer Learning\n\n*From Search Query: gated memory mechanisms*\n\n*Joel Veness, Marcus Hutter, David Budden, Eren Sezener, Jianan Wang*\n\n**Abstract:** Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both offline and online methods on standard continual learning benchmarks.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-23\n\n\n\n#### 4. Gated Slot Attention for Efficient Linear-Time Sequence Modeling\n\n*From Search Query: gated memory mechanisms*\n\n*Guohong Fu, Peng Zhou, Wei Bi, Bailin Wang, Freda Shi, Bolun Wang, Yiqiao Wang, Leyang Cui, Yue Zhang, Ruijie Zhu, Songlin Yang, Yu Zhang*\n\n**Abstract:** Linear attention Transformers and their gated variants, celebrated for enabling parallel training and efficient recurrent inference, still fall short in recall-intensive tasks compared to traditional Transformers and demand significant resources for training from scratch. This paper introduces Gated Slot Attention (GSA), which enhances Attention with Bounded-memory-Control (ABC) by incorporating a gating mechanism inspired by Gated Linear Attention (GLA). Essentially, GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size. This design greatly enhances both training and inference efficiency through GLA's hardware-efficient training algorithm and reduced state size. Additionally, retaining the softmax operation is particularly beneficial in \"finetuning pretrained Transformers to RNNs\" (T2R) settings, reducing the need for extensive training from scratch. Extensive experiments confirm GSA's superior performance in scenarios requiring in-context recall and in T2R settings.\n\n**Published:** 2024-09-11\n\n\n\n#### 5. Auto-DeepLab: Hierarchical Neural Architecture Search for Semantic Image Segmentation\n\n*From Search Query: hierarchical neural architectures*\n\n*Li Fei-Fei, Alan Yuille, Florian Schroff, Liang-Chieh Chen, Wei Hua, Hartwig Adam, Chenxi Liu*\n\n**Abstract:** Recently, Neural Architecture Search (NAS) has successfully identified neural\nnetwork architectures that exceed human designed ones on large-scale image\nclassification. In this paper, we study NAS for semantic image segmentation.\nExisting works often focus on searching the repeatable cell structure, while\nhand-designing the outer network structure that controls the spatial resolution\nchanges. This choice simplifies the search space, but becomes increasingly\nproblematic for dense image prediction which exhibits a lot more network level\narchitectural variations. Therefore, we propose to search the network level\nstructure in addition to the cell level structure, which forms a hierarchical\narchitecture search space. We present a network level search space that\nincludes many popular designs, and develop a formulation that allows efficient\ngradient-based architecture search (3 P100 GPU days on Cityscapes images). We\ndemonstrate the effectiveness of the proposed method on the challenging\nCityscapes, PASCAL VOC 2012, and ADE20K datasets. Auto-DeepLab, our\narchitecture searched specifically for semantic image segmentation, attains\nstate-of-the-art performance without any ImageNet pretraining.\n\n**Conference:** auto-deeplab-hierarchical-neural-architecture-1\n\n**Published:** 2019-01-10\n\n\n\n#### 6. Hierarchical Neural Architecture Search for Deep Stereo Matching\n\n*From Search Query: hierarchical neural architectures*\n\n*ZongYuan Ge, Hongdong Li, Tom Drummond, Xiaojun Chang, Yuchao Dai, Mehrtash Harandi, Yiran Zhong, Xuelian Cheng*\n\n**Abstract:** To reduce the human efforts in neural network design, Neural Architecture Search (NAS) has been applied with remarkable success to various high-level vision tasks such as classification and semantic segmentation. The underlying idea for the NAS algorithm is straightforward, namely, to enable the network the ability to choose among a set of operations (e.g., convolution with different filter sizes), one is able to find an optimal architecture that is better adapted to the problem at hand. However, so far the success of NAS has not been enjoyed by low-level geometric vision tasks such as stereo matching. This is partly due to the fact that state-of-the-art deep stereo matching networks, designed by humans, are already sheer in size. Directly applying the NAS to such massive structures is computationally prohibitive based on the currently available mainstream computing resources. In this paper, we propose the first end-to-end hierarchical NAS framework for deep stereo matching by incorporating task-specific human knowledge into the neural architecture search framework. Specifically, following the gold standard pipeline for deep stereo matching (i.e., feature extraction -- feature volume construction and dense matching), we optimize the architectures of the entire pipeline jointly. Extensive experiments show that our searched network outperforms all state-of-the-art deep stereo matching architectures and is ranked at the top 1 accuracy on KITTI stereo 2012, 2015 and Middlebury benchmarks, as well as the top 1 on SceneFlow dataset with a substantial improvement on the size of the network and the speed of inference. The code is available at https://github.com/XuelianCheng/LEAStereo.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-26\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, particularly focusing on spectral state space models, gated memory mechanisms, and hierarchical neural architectures, here are some relevant findings and insights from the provided sources and additional context:\n\n## Applications of Spectral Methods in State Space Models\n\n- The paper on \"Who Wins? Transformers vs. State-Space Models for PDEs\" introduces the concept of the Mamba Neural Operator (MNO), which integrates structured state-space models (SSMs) to handle continuous data and long-range dependencies more effectively than traditional Transformers. While it does not explicitly discuss spectral methods, it highlights the potential of SSMs in improving stability and efficiency, which can be a foundation for incorporating spectral techniques. Spectral methods could theoretically enhance the robustness and stability of these models, aligning with the need for proper initialization and parameterization for stable training.\n\n## Implementations of Gated Memory Mechanisms in Neural Architectures\n\n- The concept of \"Gated Slot Attention\" is relevant here. Although not directly mentioned in the provided sources, the analysis note references it as a useful approach for efficient memory management. Gated Slot Attention (GSA) involves using context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining a compact recurrent state size. This aligns with the need for hardware-efficient memory mechanisms and adaptive resource allocation (Analysis Note).\n\n## Hierarchical Approaches to Sequence Modeling\n\n- The Mamba Neural Operator (MNO) framework discussed in the first source uses a structured state-space model that can be generalized to any architecture, including hierarchical ones. This framework is designed to capture long-range dependencies and continuous dynamics effectively, which is crucial for hierarchical sequence modeling. The hierarchical structure helps in managing long-range dependencies, a key insight from the analysis note.\n\n## Additional Insights\n\n- **Spectral Methods and Stability**: While the provided sources do not directly discuss spectral methods, the theoretical foundations of state-space models and their stability analysis (as seen in the stability analysis of N-D discrete state-space systems) suggest that incorporating spectral methods could enhance the robustness and stability of these models. This is in line with the analysis note's emphasis on spectral methods providing theoretical guarantees for robustness.\n\n- **Hardware-Efficient Implementations**: The paper on \"A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequence Learning\" introduces SPikE-SSM, which achieves high sparsity and efficiency. Although focused on spiking neural networks, the approach highlights the importance of efficient and sparse models, which can be adapted to other neural architectures. This aligns with the need for hardware-efficient implementations and parallel processing strategies.\n\n- **Combining Hierarchical and Spectral Approaches**: The analysis note suggests combining spectral methods with hierarchical processing to improve stability. This approach can leverage the strengths of both methods: the hierarchical structure for managing long-range dependencies and the spectral methods for enhancing stability and robustness.\n\nIn summary, the Mamba Neural Operator and the concepts around state-space models provide a strong foundation for improving autoregressive language model design. Incorporating gated memory mechanisms like Gated Slot Attention and exploring spectral methods for stability can further enhance these models. Hierarchical approaches, as seen in the MNO framework, are crucial for managing long-range dependencies efficiently.\n",
            "analysis": "Analysis Note: Second Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe second round of research reveals several promising directions for enhancing the Mamba2Layer architecture:\n\na) State Space Model Stability and Training:\n- Recent work shows the importance of proper initialization and parameterization for stable training\n- Spectral methods offer theoretical guarantees for robustness\n- Hierarchical structures can help manage long-range dependencies\n\nb) Hardware-Efficient Memory Mechanisms:\n- Gated slot attention provides efficient memory management\n- Adaptive multi-task architectures suggest ways to dynamically allocate computational resources\n- Parallel processing strategies are crucial for performance\n\nc) Polynomial Kernel Applications:\n- Neural Tangent Kernel theory provides insights into convergence and generalization\n- Location-aware kernel representations improve feature extraction\n- Multi-scale approaches enhance model capacity\n\n2. Useful References with Excerpts\n\nFrom \"Understanding the differences in Foundation Models\":\n\"Our framework facilitates rigorous comparisons, providing new insights on the distinctive characteristics of each model class... we compare linear attention and selective SSMs, detailing their differences and conditions under which both are equivalent.\"\n\nFrom \"Structured state-space models are deep Wiener models\":\n\"SSMs appear as an effective way to learn deep Wiener models... owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems.\"\n\nFrom \"Gated Slot Attention\":\n\"GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size.\"\n\n3. Key Insights and Analysis\n\na) Model Architecture:\n- Combining spectral methods with hierarchical processing could improve stability\n- Gated mechanisms can help manage memory more efficiently\n- Location-aware representations are important for capturing spatial dependencies\n\nb) Training Dynamics:\n- Proper initialization is crucial for stable training\n- Adaptive resource allocation can improve efficiency\n- Multi-scale processing helps capture different levels of dependencies\n\nc) Implementation Considerations:\n- Hardware-aware design is essential for practical deployment\n- Parallel processing capabilities should be maximized\n- Memory management needs careful optimization\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in spectral methods for state space models\n2. Hardware-efficient implementations of gated mechanisms\n3. Techniques for combining hierarchical and spectral approaches\n\n5. References\n\n1. Sieber, J., Alonso, C. A., Didier, A., Zeilinger, M., & Orvieto, A. (2024). Understanding the differences in Foundation Models: Attention, State Space Models, and Recurrent Neural Networks. arXiv preprint arXiv:2401.10166.\n\n2. Bonassi, F., Andersson, C. R., Mattsson, P., & Sch\u00f6n, T. B. (2023). Structured state-space models are deep Wiener models. arXiv preprint.\n\n3. Zhou, P., et al. (2024). Gated Slot Attention for Efficient Linear-Time Sequence Modeling. arXiv preprint.\n\n4. Jacot, A., Gabriel, F., & Hongler, C. (2018). Neural Tangent Kernel: Convergence and Generalization in Neural Networks. NeurIPS.\n\n5. Li, Y., et al. (2023). AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts. ICCV 2023."
        },
        {
            "ready": false,
            "query": "spectral filtering neural networks, adaptive memory mechanisms, hierarchical state space models",
            "detail": "Find papers discussing:\n1. Applications of spectral filtering in neural networks, particularly for stability and efficiency\n2. Adaptive memory mechanisms in hierarchical architectures\n3. Techniques for combining hierarchical and spectral approaches in state space models\nFocus on papers from the last 2 years with theoretical analysis and implementation details.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Applications of spectral filtering in neural networks, particularly for stability and efficiency\n2. Adaptive memory mechanisms in hierarchical architectures\n3. Techniques for combining hierarchical and spectral approaches in state space models\nFocus on papers from the last 2 years with theoretical analysis and implementation details.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.95)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 7/31 (Score: 0.89)*\n\n```\nIn our experiments we search over two values of $k_{y}=\\{2,32\\}$. For non-image tasks, ListOps, Text and Retrieval, we find that setting $k_{y}=2$ is sufficient to get optimal results. For the image tasks, CIFAR, Pathfinder and PathX, we found that $k_{y}=32$ led to significant performance gains. A performance ablation over this parameter can be found in the appendix (Table 2]. Overall we find that the STU model provides improvements over baselines such as S4 and LRU on 4 out of the 6 tasks and performs comparably to the best baseline on the others. Remarkably, the STU layers come with provable guarantees and thus performs well out of the box without the need for specific initializations, discretizations or normalizations. We initialize all parameters $M_{i}^{y}, M_{i}^{u}, M_{k}^{\\phi+}, M_{k}^{\\phi-}$ with 0 . We provide details of the experimental setup, including hyperparameter tuning in the appendix (Section E). ## 6 Conclusion\n\nInsprired by the success of SSMs, we present a new theoretically-founded deep neural network architecture, Spectral SSM, for sequence modelling based on the Spectral Filtering algorithm for\nlearning Linear Dynamical Systems. The SSM performs a reparameterization of the LDS and is guaranteed to learn even marginally stable symmetric LDS stably and efficiently. We demonstrate the core advantages of the Spectal SSM, viz. robustness to long memory through experiments on a synthetic LDS and the Long Range Arena benchmark. We find that the Spectral SSM is able to learn even in the presence of large context lengths/memory without the need for designing specific initializations, discretizations or normalizations which were necessary for existing SSMs to learn in such settings. While spectral SSMs only model symmetric A, our presented set of experiments on the LRA benchmark suggest that the gap between symmetric and general A is potentially small in real world tasks. Indeed more recent SSM models like [GD23, $\\mathrm{DSF}^{+}$24] work with real diagonals (i.e. symmetric case) as they do not find evidence that adding complex eigenvalues help. Spectral filtering has been extended in certain settings to asymmetric A [HLS ${ }^{+}$18] and a similar extension to our proposal is straightforward but comes with efficiency losses and we leave it to future work. ## References\n\n[ASB16] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 1120-1128. PMLR, 2016. [Ble89] Guy E Blelloch. Scans as primitive parallel operations. IEEE Transactions on computers, 38(11):1526-1538, 1989. $\\left[\\mathrm{BMR}^{+}\\right.$20] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877-1901, 2020. [BSF94] Yoshua Bengio, Patrice Simard, and Paolo Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE transactions on neural networks, 5(2):157-166, 1994. [CVMG ${ }^{+}$14] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014. $\\left[\\mathrm{DBK}^{+}\\right.$20] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth $16 \\times 16$ words: Transformers for image recognition at scale.\n```\n\n#### 2. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.32)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.32)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n#### 3. Hierarchically Gated Recurrent Neural Network for Sequence Modeling (Avg. Score: 0.20)\n\n*Zhen Qin, Songlin Yang, Yiran Zhong*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 38  (*Influential: 4*)\n\n**TL;DR:** This paper proposes a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value and the lower bound increases monotonically when moving up layers.\n\n**Abstract:** Transformers have surpassed RNNs in popularity due to their superior abilities in parallel training and long-term dependency modeling. Recently, there has been a renewed interest in using linear RNNs for efficient sequence modeling. These linear RNNs often employ gating mechanisms in the output of the linear recurrence layer while ignoring the significance of using forget gates within the recurrence. In this paper, we propose a gated linear RNN model dubbed Hierarchically Gated Recurrent Neural Network (HGRN), which includes forget gates that are lower bounded by a learnable value. The lower bound increases monotonically when moving up layers. This allows the upper layers to model long-term dependencies and the lower layers to model more local, short-term dependencies. Experiments on language modeling, image classification, and long-range arena benchmarks showcase the efficiency and effectiveness of our proposed model. The source code is available at https://github.com/OpenNLPLab/HGRN.\n\n##### *Relevant Chunk: No. 12/30 (Score: 0.20)*\n\n```\nZenodo, Sept. 2021. [17] Felix A. Gers, J\u00fcrgen Schmidhuber, and Fred A. Cummins. Learning to forget: Continual prediction with LSTM. Neural Comput., 12(10):2451-2471, 2000. [18] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio Spectrogram Transformer. In Proc. Interspeech 2021, pages 571-575, 2021. [19] Klaus Greff, Rupesh Kumar Srivastava, Jan Koutn\u00edk, Bas R. Steunebrink, and J\u00fcrgen Schmidhuber. Lstm: A search space odyssey. IEEE Transactions on Neural Networks and Learning Systems, 28:2222-2232, 2015. [20] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. In NeurIPS, 2022. [21] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. [22] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [23] Albert Gu, \u00c7aglar G\u00fcl\u00e7ehre, Thomas Paine, Matt Hoffman, and Razvan Pascanu. Improving the gating mechanism of recurrent neural networks. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 3800-3809. PMLR, 2020. [24] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. In Marc'Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 572-585, 2021. [25] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state-space layers, 2021. [26] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces, 2022. [27] Ankit Gupta, Harsh Mehta, and Jonathan Berant. Simplifying and understanding state space models with diagonal linear rnns. CoRR, abs/2212.00768, 2022. [28] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. In The Eleventh International Conference on Learning Representations, 2023. [29] Hongyu He and Marko Kabic. A unified view of long-sequence models towards modeling million-scale dependencies. CoRR, abs/2302.06218, 2023. [30] Sepp Hochreiter and Yoshua Bengio. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies.\n```\n\n#### 4. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.20)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.20)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n\n\n---\n## Found 7 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: spectral filtering neural networks, adaptive memory mechanisms, hierarchical state space models\n\nConsidering refining your search by improving the query keywords input.\n\n### 1 related papers from ArXiv\n\n#### 1. RecallM: An Adaptable Memory Mechanism with Temporal Understanding for\n  Large Language Models\n\n*From Search Query: adaptive memory mechanisms*\n\n*Brandon Kynoch, Hugo Latapie, Dwane van der Sluis*\n\n**Abstract:** Large Language Models (LLMs) have made extraordinary progress in the field of\nArtificial Intelligence and have demonstrated remarkable capabilities across a\nlarge variety of tasks and domains. However, as we venture closer to creating\nArtificial General Intelligence (AGI) systems, we recognize the need to\nsupplement LLMs with long-term memory to overcome the context window limitation\nand more importantly, to create a foundation for sustained reasoning,\ncumulative learning and long-term user interaction. In this paper we propose\nRecallM, a novel architecture for providing LLMs with an adaptable and\nupdatable long-term memory mechanism. Unlike previous methods, the RecallM\narchitecture is particularly effective at belief updating and maintaining a\ntemporal understanding of the knowledge provided to it. We demonstrate through\nvarious experiments the effectiveness of this architecture. Furthermore,\nthrough our own temporal understanding and belief updating experiments, we show\nthat RecallM is four times more effective than using a vector database for\nupdating knowledge previously stored in long-term memory. We also demonstrate\nthat RecallM shows competitive performance on general question-answering and\nin-context learning tasks.\n\n**Published:** 2023-07-06T02:51:54Z  (*Updated: 2023-10-03T01:16:33Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering\n\n*From Search Query: spectral filtering neural networks*\n\n*Micha\u00ebl Defferrard, Pierre Vandergheynst, Xavier Bresson*\n\n**Abstract:** In this work, we are interested in generalizing convolutional neural networks\n(CNNs) from low-dimensional regular grids, where image, video and speech are\nrepresented, to high-dimensional irregular domains, such as social networks,\nbrain connectomes or words' embedding, represented by graphs. We present a\nformulation of CNNs in the context of spectral graph theory, which provides the\nnecessary mathematical background and efficient numerical schemes to design\nfast localized convolutional filters on graphs. Importantly, the proposed\ntechnique offers the same linear computational complexity and constant learning\ncomplexity as classical CNNs, while being universal to any graph structure.\nExperiments on MNIST and 20NEWS demonstrate the ability of this novel deep\nlearning system to learn local, stationary, and compositional features on\ngraphs.\n\n**Conference:** convolutional-neural-networks-on-graphs-with-1\n\n**Published:** 2016-06-30\n\n\n\n#### 2. Deep Convolutional Networks on Graph-Structured Data\n\n*From Search Query: spectral filtering neural networks*\n\n*Yann Lecun, Joan Bruna, Mikael Henaff*\n\n**Abstract:** Deep Learning's recent successes have mostly relied on Convolutional\nNetworks, which exploit fundamental statistical properties of images, sounds\nand video data: the local stationarity and multi-scale compositional structure,\nthat allows expressing long range interactions in terms of shorter, localized\ninteractions. However, there exist other important examples, such as text\ndocuments or bioinformatic data, that may lack some or all of these strong\nstatistical regularities.\n  In this paper we consider the general question of how to construct deep\narchitectures with small learning complexity on general non-Euclidean domains,\nwhich are typically unknown and need to be estimated from the data. In\nparticular, we develop an extension of Spectral Networks which incorporates a\nGraph Estimation procedure, that we test on large-scale classification\nproblems, matching or improving over Dropout Networks with far less parameters\nto estimate.\n\n**Published:** 2015-06-16\n\n\n\n#### 3. A Combinatorial Perspective on Transfer Learning\n\n*From Search Query: adaptive memory mechanisms*\n\n*Joel Veness, Marcus Hutter, David Budden, Eren Sezener, Jianan Wang*\n\n**Abstract:** Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both offline and online methods on standard continual learning benchmarks.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-10-23\n\n\n\n#### 4. Improving Neural Language Models with a Continuous Cache\n\n*From Search Query: adaptive memory mechanisms*\n\n*Nicolas Usunier, Edouard Grave, Armand Joulin*\n\n**Abstract:** We propose an extension to neural network language models to adapt their\nprediction to the recent history. Our model is a simplified version of memory\naugmented networks, which stores past hidden activations as memory and accesses\nthem through a dot product with the current hidden activation. This mechanism\nis very efficient and scales to very large memory sizes. We also draw a link\nbetween the use of external memory in neural network and cache models used with\ncount based language models. We demonstrate on several language model datasets\nthat our approach performs significantly better than recent memory augmented\nnetworks.\n\n**Published:** 2016-12-13\n\n\n\n#### 5. Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling\n\n*From Search Query: hierarchical state space models*\n\n*Lerrel Pinto, Tess Hellebrekers, Abhinav Gupta, Carmel Majidi, Venkatesh Pattabiraman, Chenyu Wang, Raunaq Bhirangi*\n\n**Abstract:** Reasoning from sequences of raw sensory data is a ubiquitous problem across fields ranging from medical devices to robotics. These problems often involve using long sequences of raw sensor data (e.g. magnetometers, piezoresistors) to predict sequences of desirable physical quantities (e.g. force, inertial measurements). While classical approaches are powerful for locally-linear prediction problems, they often fall short when using real-world sensors. These sensors are typically non-linear, are affected by extraneous variables (e.g. vibration), and exhibit data-dependent drift. For many problems, the prediction task is exacerbated by small labeled datasets since obtaining ground-truth labels requires expensive equipment. In this work, we present Hierarchical State-Space Models (HiSS), a conceptually simple, new technique for continuous sequential prediction. HiSS stacks structured state-space models on top of each other to create a temporal hierarchy. Across six real-world sensor datasets, from tactile-based state prediction to accelerometer-based inertial measurement, HiSS outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE. Our experiments further indicate that HiSS demonstrates efficient scaling to smaller datasets and is compatible with existing data-filtering techniques. Code, datasets and videos can be found on https://hiss-csp.github.io.\n\n**Published:** 2024-02-15\n\n\n\n#### 6. Hieros: Hierarchical Imagination on Structured State Space Sequence World Models\n\n*From Search Query: hierarchical state space models*\n\n*Ralf Herbrich, Rainer Schlosser, Paul Mattes*\n\n**Abstract:** One of the biggest challenges to modern deep reinforcement learning (DRL) algorithms is sample efficiency. Many approaches learn a world model in order to train an agent entirely in imagination, eliminating the need for direct environment interaction during training. However, these methods often suffer from either a lack of imagination accuracy, exploration capabilities, or runtime efficiency. We propose Hieros, a hierarchical policy that learns time abstracted world representations and imagines trajectories at multiple time scales in latent space. Hieros uses an S5 layer-based world model, which predicts next world states in parallel during training and iteratively during environment interaction. Due to the special properties of S5 layers, our method can train in parallel and predict next world states iteratively during imagination. This allows for more efficient training than RNN-based world models and more efficient imagination than Transformer-based world models. We show that our approach outperforms the state of the art in terms of mean and median normalized human score on the Atari 100k benchmark, and that our proposed world model is able to predict complex dynamics very accurately. We also show that Hieros displays superior exploration capabilities compared to existing approaches.\n\n**Published:** 2023-10-08\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design using the specified keywords and details, here are some relevant findings and suggestions:\n\n## Applications of Spectral Filtering in Neural Networks\n\n### Stability and Efficiency\n- The concept of spectral filtering is not directly addressed in the provided sources, but the idea of using fixed convolutional filters in spectral state space models (SSMs) is mentioned in the analysis note. This approach provides provable robustness properties and does not require learning the filters, which can enhance stability and reduce model complexity[5,.\n\nFor a more direct application, the paper on \"Filtered Randomized Smoothing\" discusses the use of spectral filtering to enhance robustness against adversarial attacks. This method combines low-pass filtering with randomized smoothing to remove perturbations while preserving the benign signal components. This approach could be adapted to improve the stability of neural networks by filtering out noise and maintaining the integrity of the signal.\n\n## Adaptive Memory Mechanisms in Hierarchical Architectures\n\n### Hierarchical State Space Models\n- The analysis note highlights the use of \"Gated Slot Attention\" (GSA) for efficient memory management. GSA involves context-aware memory reading and adaptive forgetting, which improves memory capacity while maintaining a compact recurrent state size. This mechanism can be integrated into hierarchical state space models to enhance their performance and efficiency[Analysis Note].\n\n## Techniques for Combining Hierarchical and Spectral Approaches in State Space Models\n\n### Combining Spectral and Hierarchical Methods\n- The \"Spectral State Space Models\" concept mentioned in the analysis note suggests using fixed convolutional filters that do not require learning. To combine this with hierarchical processing, one could integrate these spectral filters within a hierarchical architecture. For example, using dense hidden connections as in \"DenseMamba\" can help retain fine-grained information while maintaining computational efficiency. This combination can leverage the robustness of spectral methods and the efficiency of dense connections[Analysis Note].\n\n### Implementation Details\n- For efficient implementations, the paper on \"Generalization of neural network models for complex dynamical systems\" discusses the separation of self- and neighbor-interaction components in graph neural networks, which can be applied to state space models. This separation improves accuracy and can be adapted to maintain stability in deep SSM architectures. Additionally, the use of statistical hypothesis testing and null models can help gauge the confidence of predictions and ensure robustness.\n\n### Summary\n- **Spectral Filtering**: Adapt the concept of filtered randomized smoothing to enhance robustness and stability in neural networks.\n- **Adaptive Memory Mechanisms**: Implement Gated Slot Attention or similar context-aware memory mechanisms to improve memory efficiency in hierarchical state space models.\n- **Combining Hierarchical and Spectral Approaches**: Integrate fixed convolutional filters from spectral state space models into hierarchical architectures with dense hidden connections to leverage both robustness and efficiency.\n\nBy combining these approaches, researchers can develop more robust, efficient, and scalable autoregressive language models.\n",
            "analysis": "Analysis Note: Second Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe second round of research reveals several promising directions for enhancing the Mamba2Layer architecture:\n\na) Spectral State Space Models (SSMs):\n- Provide provable robustness properties independent of spectrum and dimensionality\n- Use fixed convolutional filters that don't require learning\n- Show strong theoretical benefits for long-range memory tasks\n\nb) Dense Hidden Connections:\n- Enhance information flow between layers in SSMs\n- Retain fine-grained information crucial for output quality\n- Maintain training parallelizability and inference efficiency\n\nc) Hardware-Efficient Memory Mechanisms:\n- Gated Slot Attention (GSA) offers efficient memory management\n- Context-aware memory reading with adaptive forgetting\n- Compact recurrent state size with improved capacity\n\n2. Useful References with Excerpts\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.\"\n\nFrom \"DenseMamba\":\n\"By selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency.\"\n\nFrom \"Gated Slot Attention\":\n\"GSA comprises a two-layer GLA linked via softmax, utilizing context-aware memory reading and adaptive forgetting to improve memory capacity while maintaining compact recurrent state size.\"\n\n3. Key Insights and Analysis\n\na) Spectral Methods for Stability:\n- Fixed spectral filters provide theoretical guarantees for robustness\n- Independence from spectrum allows handling long-range dependencies\n- No learning required for filters reduces model complexity\n\nb) Dense Connectivity:\n- Selective integration of shallow layer information improves performance\n- Maintains computational efficiency despite added connections\n- Helps preserve fine-grained features through deep networks\n\nc) Memory Management:\n- Context-aware memory mechanisms improve efficiency\n- Adaptive forgetting helps manage memory capacity\n- Compact state representations reduce computational overhead\n\nd) Implementation Considerations:\n- Need to balance spectral filtering with computational efficiency\n- Memory mechanisms must be carefully integrated with SSM structure\n- Dense connections require efficient implementation to maintain speed\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Techniques for combining spectral methods with hierarchical processing\n2. Efficient implementations of adaptive memory mechanisms in SSMs\n3. Methods for maintaining stability in deep SSM architectures\n4. Hardware-efficient implementations of spectral filtering\n\n5. References\n\n1. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n2. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n3. Fu, G., Zhou, P., Bi, W., Wang, B., Shi, F., Wang, B., ... & Yang, S. (2024). Gated Slot Attention for Efficient Linear-Time Sequence Modeling. arXiv preprint.\n\n4. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint."
        },
        {
            "ready": false,
            "query": "efficient spectral filtering implementation, hierarchical memory neural networks, spectral hierarchical integration",
            "detail": "Find papers discussing:\n1. Efficient implementations of spectral filtering in neural networks, particularly for sequence modeling\n2. Integration techniques for hierarchical memory in neural architectures\n3. Methods for combining spectral and hierarchical approaches in state space models\nFocus on papers from the last 2 years with implementation details and theoretical analysis.",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Efficient implementations of spectral filtering in neural networks, particularly for sequence modeling\n2. Integration techniques for hierarchical memory in neural architectures\n3. Methods for combining spectral and hierarchical approaches in state space models\nFocus on papers from the last 2 years with implementation details and theoretical analysis.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 0.99)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.99)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.99)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.99)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 3. Liquid Structural State-Space Models (Avg. Score: 0.99)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 49/54 (Score: 0.99)*\n\n```\nW. Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. C. W. Tan, C. Bergmeir, F. Petitjean, and G. I. Webb. Time series extrinsic regression. Data Mining and Knowledge Discovery, 35(3):1032-1060, 2021. Y. Tay, D. Bahri, L. Yang, D. Metzler, and D.-C. Juan. Sparse sinkhorn attention. In International Conference on Machine Learning, pages 9438-9447. PMLR, 2020a. Y. Tay, M. Dehghani, S. Abnar, Y. Shen, D. Bahri, P. Pham, J. Rao, L. Yang, S. Ruder, and D. Metzler. Long range arena: A benchmark for efficient transformers. In International Conference on Learning Representations, 2020b. T. Trinh, A. Dai, T. Luong, and Q. Le. Learning longer-term dependencies in rnns with auxiliary losses. In International Conference on Machine Learning, pages 4965-4974. PMLR, 2018. A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. A. Voelker, I. Kaji\u0107, and C. Eliasmith. Legendre memory units: Continuous-time representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. C. Vorbach, R. Hasani, A. Amini, M. Lechner, and D. Rus. Causal navigation by continuous-time neural networks. Advances in Neural Information Processing Systems, 34, 2021. C. Wang and M. Niepert. State-regularized recurrent neural networks. In International Conference on Machine Learning, pages 6596-6606, 2019. S. Wang, B. Z. Li, M. Khabsa, H.\n```\n\n#### 4. Dual-path Mamba: Short and Long-term Bidirectional Selective Structured State Space Models for Speech Separation (Avg. Score: 0.89)\n\n*Xilin Jiang, Cong Han, N. Mesgarani*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 13  (*Influential: 1*)\n\n**TL;DR:** This work replaces transformers with Mamba, a selective state space model, for speech separation, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces.\n\n**Abstract:** Transformers have been the most successful architecture for various speech modeling tasks, including speech separation. However, the self-attention mechanism in transformers with quadratic complexity is inefficient in computation and memory. Recent models incorporate new layers and modules along with transformers for better performance but also introduce extra model complexity. In this work, we replace transformers with Mamba, a selective state space model, for speech separation. We propose dual-path Mamba, which models short-term and long-term forward and backward dependency of speech signals using selective state spaces. Our experimental results on the WSJ0-2mix data show that our dual-path Mamba models of comparably smaller sizes outperform state-of-the-art RNN model DPRNN, CNN model WaveSplit, and transformer model Sepformer. Code: https://github.com/xi-j/Mamba-TasNet\n\n##### *Relevant Chunk: No. 12/15 (Score: 0.89)*\n\n```\nInterspeech 2022, 2022, pp. 5353-5357. [10] Rudolph Emil Kalman, \"A new approach to linear filtering and prediction problems,\" 1960. [11] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9, \"Combining recurrent, convolutional, and continuous-time models with linear state space layers,\" Advances in neural information processing systems, vol. 34, pp. 572-585, 2021. [12] Albert Gu, Karan Goel, and Christopher R\u00e9, \"Efficiently modeling long sequences with structured state spaces,\" in The International Conference on Learning Representations (ICLR), 2022. [13] Albert Gu and Tri Dao, \"Mamba: Linear-time sequence modeling with selective state spaces,\" arXiv preprint arXiv:2312.00752, 2023. [14] Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, and Xinggang Wang, \"Vision mamba: Efficient visual representation learning with bidirectional state space model,\" arXiv preprint arXiv:2401.09417, 2024. [15] John R. Hershey, Zhuo Chen, Jonathan Le Roux, and Shinji Watanabe, \"Deep clustering: Discriminative embeddings for segmentation and separation,\" in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 31-35. [16] Neil Zeghidour and David Grangier, \"Wavesplit: End-to-end speech separation by speaker clustering,\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2840-2849, 2021. [17] Eliya Nachmani, Yossi Adi, and Lior Wolf, \"Voice separation with an unknown number of multiple speakers,\" in International Conference on Machine Learning. PMLR, 2020, pp. 7164-7175. [18] Jingjing Chen, Qirong Mao, and Dong Liu, \"Dual-Path Transformer Network: Direct Context-Aware Modeling for End-toEnd Monaural Speech Separation,\" in Proc. Interspeech 2020, 2020, pp. 2642-2646. [19] Shrikant Venkataramani, Jonah Casebeer, and Paris Smaragdis, \"End-to-end source separation with adaptive front-ends,\" in 2018 52nd asilomar conference on signals, systems, and computers. IEEE, 2018, pp. 684-688. [20] Yi Luo and Nima Mesgarani, \"Tasnet: time-domain audio separation network for real-time, single-channel speech separation,\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 696700. [21] Efthymios Tzinis, Zhepei Wang, and Paris Smaragdis, \"Sudo rm-rf: Efficient networks for universal audio source separation,\" in 2020 IEEE 30th International Workshop on Machine Learning for Signal Processing (MLSP). IEEE, 2020, pp. 1-6. [22] Efthymios Tzinis, Zhepei Wang, Xilin Jiang, and Paris Smaragdis, \"Compute and memory efficient universal sound source separation,\" Journal of Signal Processing Systems, vol.\n```\n\n\n\n---\n## Found 6 related papers from 1 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient spectral filtering implementation, hierarchical memory neural networks, spectral hierarchical integration\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Papers with Code\n\n#### 1. Graph Neural Networks with convolutional ARMA filters\n\n*From Search Query: efficient spectral filtering implementation*\n\n*Filippo Maria Bianchi, Lorenzo Livi, Daniele Grattarola, Cesare Alippi*\n\n**Abstract:** Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.\n\n**Published:** 2019-01-05\n\n\n\n#### 2. A two-step learning approach for solving full and almost full cold start problems in dyadic prediction\n\n*From Search Query: efficient spectral filtering implementation*\n\n*Antti Airola, Michiel Stock, Tapio Pahikkala, Bernard De Baets, Tero Aittokallio, Willem Waegeman*\n\n**Abstract:** Dyadic prediction methods operate on pairs of objects (dyads), aiming to\ninfer labels for out-of-sample dyads. We consider the full and almost full cold\nstart problem in dyadic prediction, a setting that occurs when both objects in\nan out-of-sample dyad have not been observed during training, or if one of them\nhas been observed, but very few times. A popular approach for addressing this\nproblem is to train a model that makes predictions based on a pairwise feature\nrepresentation of the dyads, or, in case of kernel methods, based on a tensor\nproduct pairwise kernel. As an alternative to such a kernel approach, we\nintroduce a novel two-step learning algorithm that borrows ideas from the\nfields of pairwise learning and spectral filtering. We show theoretically that\nthe two-step method is very closely related to the tensor product kernel\napproach, and experimentally that it yields a slightly better predictive\nperformance. Moreover, unlike existing tensor product kernel methods, the\ntwo-step method allows closed-form solutions for training and parameter\nselection via cross-validation estimates both in the full and almost full cold\nstart settings, making the approach much more efficient and straightforward to\nimplement.\n\n**Published:** 2014-05-17\n\n\n\n#### 3. Hierarchical Neural Memory Network for Low Latency Event Processing\n\n*From Search Query: hierarchical memory neural networks*\n\n*Ken Sakurada, Masaki Onishi, Yasutaka Furukawa, Ryuhei Hamaguchi*\n\n**Abstract:** This paper proposes a low latency neural network architecture for event-based dense prediction tasks. Conventional architectures encode entire scene contents at a fixed rate regardless of their temporal characteristics. Instead, the proposed network encodes contents at a proper temporal scale depending on its movement speed. We achieve this by constructing temporal hierarchy using stacked latent memories that operate at different rates. Given low latency event steams, the multi-level memories gradually extract dynamic to static scene contents by propagating information from the fast to the slow memory modules. The architecture not only reduces the redundancy of conventional architectures but also exploits long-term dependencies. Furthermore, an attention-based event representation efficiently encodes sparse event streams into the memory cells. We conduct extensive evaluations on three event-based dense prediction tasks, where the proposed approach outperforms the existing methods on accuracy and latency, while demonstrating effective event and image fusion capabilities. The code is available at https://hamarh.github.io/hmnet/\n\n**Conference:** hierarchical-neural-memory-network-for-low\n\n**Published:** 2023-05-29\n\n\n\n#### 4. Distributed Hierarchical GPU Parameter Server for Massive Scale Deep Learning Ads Systems\n\n*From Search Query: hierarchical memory neural networks*\n\n*Weijie Zhao, Deping Xie, Yulei Qian, Ruiquan Ding, Mingming Sun, Ronglai Jia, Ping Li*\n\n**Abstract:** Neural networks of ads systems usually take input from multiple resources, e.g., query-ad relevance, ad features and user portraits. These inputs are encoded into one-hot or multi-hot binary features, with typically only a tiny fraction of nonzero feature values per example. Deep learning models in online advertising industries can have terabyte-scale parameters that do not fit in the GPU memory nor the CPU main memory on a computing node. For example, a sponsored online advertising system can contain more than $10^{11}$ sparse features, making the neural network a massive model with around 10 TB parameters. In this paper, we introduce a distributed GPU hierarchical parameter server for massive scale deep learning ads systems. We propose a hierarchical workflow that utilizes GPU High-Bandwidth Memory, CPU main memory and SSD as 3-layer hierarchical storage. All the neural network training computations are contained in GPUs. Extensive experiments on real-world data confirm the effectiveness and the scalability of the proposed system. A 4-node hierarchical GPU parameter server can train a model more than 2X faster than a 150-node in-memory distributed parameter server in an MPI cluster. In addition, the price-performance ratio of our proposed system is 4-9 times better than an MPI-cluster solution.\n\n**Published:** 2020-03-12\n\n\n\n#### 5. Revisiting Spatial-Frequency Information Integration from a Hierarchical Perspective for Panchromatic and Multi-Spectral Image Fusion\n\n*From Search Query: spectral hierarchical integration*\n\n*Feng Zhao, Danfeng Hong, Keyu Yan, Man Zhou, Naishan Zheng, Jie Huang, Jiangtong Tan*\n\n**Abstract:**     Pan-sharpening is a super-resolution problem that essentially relies on spectra fusion of panchromatic (PAN) images and low-resolution multi-spectral (LRMS) images. The previous methods have validated the effectiveness of information fusion in the Fourier space of the whole image. However they haven't fully explored the Fourier relationships at different hierarchies between PAN and LRMS images. To this end we propose a Hierarchical Frequency Integration Network (HFIN) to facilitate hierarchical Fourier information integration for pan-sharpening. Specifically our network consists of two designs: information stratification and information integration. For information stratification we hierarchically decompose PAN and LRMS information into spatial global Fourier and local Fourier information and fuse them independently. For information integration the above hierarchical fused information is processed to further enhance their relationships and undergo comprehensive integration. Our method extend a new space for exploring the relationships of PAN and LRMS images enhancing the integration of spatial-frequency information. Extensive experiments robustly validate the effectiveness of the proposed network showcasing its superior performance compared to other state-of-the-art methods and generalization in real-world scenes and other fusion tasks as a general image fusion framework. Code is available at https://github.com/JosephTiTan/HFIN.    \n\n**Proceeding:** cvpr-2024-1\n\n**Published:** 2024-01-01\n\n\n\n#### 6. Materials Representation and Transfer Learning for Multi-Property Prediction\n\n*From Search Query: spectral hierarchical integration*\n\n*John M. Gregoire, Carla P. Gomes, Dan Guevarra, Shufeng Kong*\n\n**Abstract:** The adoption of machine learning in materials science has rapidly transformed materials property prediction. Hurdles limiting full capitalization of recent advancements in machine learning include the limited development of methods to learn the underlying interactions of multiple elements, as well as the relationships among multiple properties, to facilitate property prediction in new composition spaces. To address these issues, we introduce the Hierarchical Correlation Learning for Multi-property Prediction (H-CLMP) framework that seamlessly integrates (i) prediction using only a material's composition, (ii) learning and exploitation of correlations among target properties in multi-target regression, and (iii) leveraging training data from tangential domains via generative transfer learning. The model is demonstrated for prediction of spectral optical absorption of complex metal oxides spanning 69 3-cation metal oxide composition spaces. H-CLMP accurately predicts non-linear composition-property relationships in composition spaces for which no training data is available, which broadens the purview of machine learning to the discovery of materials with exceptional properties. This achievement results from the principled integration of latent embedding learning, property correlation learning, generative transfer learning, and attention models. The best performance is obtained using H-CLMP with Transfer learning (H-CLMP(T)) wherein a generative adversarial network is trained on computational density of states data and deployed in the target domain to augment prediction of optical absorption from composition. H-CLMP(T) aggregates multiple knowledge sources with a framework that is well-suited for multi-target regression across the physical sciences.\n\n**Published:** 2021-06-04\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral filtering, hierarchical memory, and their integration, here are some relevant findings and references from the provided sources and additional insights:\n\n## Efficient Implementations of Spectral Filtering in Neural Networks\n\n- The concept of **Spectral Attention** introduced in a recent paper could be beneficial. Spectral Attention preserves long-period trends through a low-pass filter and facilitates gradient flow between samples, which can be particularly useful for sequence modeling. This method ensures that the model captures long-range dependencies efficiently without the need for learning complex filters.\n\n## Integration Techniques for Hierarchical Memory in Neural Architectures\n\n- **Hierarchical State Space Models (HiSS)** are a promising approach. HiSS stacks structured state-space models to create a temporal hierarchy, which improves the modeling of complex temporal patterns and enhances the handling of long-range dependencies. This hierarchical processing can be integrated with memory mechanisms to improve temporal understanding and information organization.\n\n## Methods for Combining Spectral and Hierarchical Approaches\n\n- **Spatio-Spectral Graph Neural Networks (S\u00b2GNNs)**, although primarily discussed in the context of graph neural networks, offer insights into combining spatial and spectral parametrizations. This synergistic approach can be adapted to sequence modeling by integrating spectral filters with hierarchical state space models. S\u00b2GNNs overcome limitations such as over-squashing and provide tighter approximation-theoretic error bounds, which could be beneficial for autoregressive language models.\n\n## Additional Insights\n\n- **Fixed Spectral Filters**: Using fixed convolutional filters in spectral state space models can provide robustness properties and stability guarantees without requiring complex parameterizations. This can be particularly useful in sequence modeling to handle long-range dependencies efficiently[Analysis Note].\n\n- **Hierarchical Memory Structures**: Integrating adaptable memory mechanisms like those in RecallM with hierarchical state space models can enhance temporal understanding and long-term context retention. This combination can improve the model's ability to update beliefs and manage information effectively[Analysis Note].\n\n- **Efficient Implementation Considerations**: For hardware-efficient implementations, techniques such as neural network sparsification and the use of Bayesian marginal likelihood with sparsity-inducing priors can help reduce computational time and memory costs. These methods can be applied to both spectral filtering and hierarchical memory integration to balance efficiency and performance.\n\nBy combining these approaches, researchers can develop an autoregressive language model that leverages the strengths of spectral filtering for robustness and stability, hierarchical processing for effective temporal hierarchies, and efficient memory mechanisms for long-term context retention.\n",
            "analysis": "Analysis Note: Third Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe third round of research reveals several promising directions for enhancing the Mamba2Layer architecture, particularly around spectral methods and hierarchical processing:\n\na) Spectral State Space Models (SSMs):\n- Provide provable robustness properties independent of spectrum\n- Use fixed convolutional filters that don't require learning\n- Show strong theoretical benefits for long-range memory tasks\n- Offer stability guarantees without complex parameterizations\n\nb) Hierarchical Processing:\n- HiSS (Hierarchical State Space Models) create effective temporal hierarchies\n- Stacking structured SSMs improves modeling of complex temporal patterns\n- Multi-scale processing enhances both local and global feature capture\n- Temporal hierarchies improve handling of long-range dependencies\n\nc) Memory Mechanisms:\n- RecallM demonstrates effective belief updating and temporal understanding\n- Adaptable memory mechanisms show promise for long-term context retention\n- Memory-based architectures can effectively handle temporal dependencies\n- Hierarchical memory structures improve information organization\n\n2. Useful References with Excerpts\n\nFrom \"Spectral State Space Models\":\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice.\"\n\nFrom \"Hierarchical State Space Models\":\n\"HiSS stacks structured state-space models on top of each other to create a temporal hierarchy... outperforms state-of-the-art sequence models such as causal Transformers, LSTMs, S4, and Mamba by at least 23% on MSE.\"\n\nFrom \"RecallM\":\n\"RecallM architecture is particularly effective at belief updating and maintaining a temporal understanding of the knowledge provided to it... four times more effective than using a vector database for updating knowledge previously stored in long-term memory.\"\n\n3. Key Insights and Analysis\n\na) Spectral Methods for Stability:\n- Fixed spectral filters provide theoretical guarantees for robustness\n- Independence from spectrum allows better handling of long-range dependencies\n- No learning required for filters reduces model complexity\n- Spectral methods can improve stability without sacrificing performance\n\nb) Hierarchical Processing:\n- Stacking SSMs creates effective temporal hierarchies\n- Multi-scale processing improves both local and global feature capture\n- Hierarchical structures help manage long-range dependencies\n- Temporal hierarchies enhance information organization\n\nc) Memory Integration:\n- Adaptable memory mechanisms improve temporal understanding\n- Hierarchical memory structures enhance information organization\n- Memory-based architectures help with long-term context retention\n- Belief updating mechanisms improve model adaptability\n\nd) Implementation Considerations:\n- Need to balance spectral filtering with computational efficiency\n- Memory mechanisms must be carefully integrated with SSM structure\n- Hierarchical processing requires efficient implementation\n- Trade-offs between memory capacity and computational overhead\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Efficient implementations of spectral filtering in neural networks\n2. Integration techniques for hierarchical memory in SSMs\n3. Methods for combining spectral and hierarchical approaches\n4. Hardware-efficient implementations of memory mechanisms\n\n5. References\n\n1. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n2. Bhirangi, R., Wang, C., Pattabiraman, V., Majidi, C., Gupta, A., Hellebrekers, T., & Pinto, L. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. arXiv preprint.\n\n3. Kynoch, B., Latapie, H., & van der Sluis, D. (2023). RecallM: An Adaptable Memory Mechanism with Temporal Understanding for Large Language Models. arXiv preprint.\n\n4. Defferrard, M., Vandergheynst, P., & Bresson, X. (2016). Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering. Neural Information Processing Systems.\n\n5. Herbrich, R., Schlosser, R., & Mattes, P. (2023). Hieros: Hierarchical Imagination on Structured State Space Sequence World Models. arXiv preprint."
        },
        {
            "ready": false,
            "query": "spectral hierarchical implementation, training dynamics deep SSM, memory optimization multi-scale",
            "detail": "Find papers discussing:\n1. Practical implementations of combined spectral and hierarchical architectures in SSMs\n2. Training dynamics and stability analysis in deep state space models\n3. Memory optimization techniques for multi-scale processing\n4. Hardware-efficient implementations of hierarchical models\n5. Empirical comparisons of different SSM architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Practical implementations of combined spectral and hierarchical architectures in SSMs\n2. Training dynamics and stability analysis in deep state space models\n3. Memory optimization techniques for multi-scale processing\n4. Hardware-efficient implementations of hierarchical models\n5. Empirical comparisons of different SSM architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.99)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.99)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 2. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.99)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.99)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 3. Spectral State Space Models (Avg. Score: 0.99)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.99)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.99)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 4. Structured state-space models are deep Wiener models (Avg. Score: 0.99)\n\n*Fabio Bonassi, Carl R. Andersson, Per Mattsson, Thomas B. Sch\u00f6n*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** This paper provides a system identification-friendly introduction to the Structured State-space Models (SSMs), and highlights future research directions for which this community could provide impactful contributions.\n\n**Abstract:** The goal of this paper is to provide a system identification-friendly introduction to the Structured State-space Models (SSMs). These models have become recently popular in the machine learning community since, owing to their parallelizability, they can be efficiently and scalably trained to tackle extremely-long sequence classification and regression problems. Interestingly, SSMs appear as an effective way to learn deep Wiener models, which allows to reframe SSMs as an extension of a model class commonly used in system identification. In order to stimulate a fruitful exchange of ideas between the machine learning and system identification communities, we deem it useful to summarize the recent contributions on the topic in a structured and accessible form. At last, we highlight future research directions for which this community could provide impactful contributions.\n\n##### *Relevant Chunk: No. 3/22 (Score: 0.99)*\n\n```\n(2022), and mainly involves stability promotion via regularization. To address these problems, Gu et al. (2021) proposed a Structured State-space Model (SSM) architecture named S4, which consists of multiple layers composed by LTI discrete-time systems followed by a nonlinear function. The term \"structured\" stems from the fact that this LTI system is given a specific structure to improve the architecture's modeling performances while also reducing the computational cost at training (Yu et al., 2018). Nonlinear state-space models are not new, see Marconato et al. (2013), yet their adoption has been hampered by their crucial reliance on the model structure and on the initialization method of learnable parameters. The contribution of the S4 approach towards SSMs has therefore been that of providing ( $i$ ) a novel, intrinsically stable, parametrization of the LTI system obtained by discretizing a continuoustime Diagonal Plus-Low Rank (DPLR) system, (ii) a new strategy towards the parameters' initialization problem, (iii) a computationally efficient approach to simulate (and train) these models over extremely long sequences, and (iv) an empirical proof of the state-of-the-art performances of these models in long-term sequence learning problems. Motivated by these appealing features, many works have continued to build on the S4 architecture. For example, Gupta et al. (2022) and Gu et al. (2022) have explored the benefits entailed by stricter SSM structures, namely the parametrization via diagonal continuous-time systems (S4D), and by simpler initialization strategies. Smith et al. (2022) have explored a novel, and somewhat more computationally efficient, simulation method for diagonal continuous-time parametrizations, named S5. Orvieto et al. (2023) recently investigated the parametrization of the LTI subsystems directly in the discrete time domain, resulting in the Linear Recurrent Unit (LRU) architecture. Contribution Despite the appealing results achieved by SSMs in the long-range arena benchmarks sequence classification problems, their use for nonlinear system identification is still unexplored. With this paper, we want to change that by making the following contributions. First of all we show that it is possible to interpret SSMs as deep Wiener models, i.e. model structures where several Wiener models are interconnected in series. An interesting note here is that even though the Wiener models have been extremely popular within system identification - see e.g. Schoukens and Tiels (2017) and references therein - their structure has been limited to \"single-layer\" or parallel architectures (Wills and Ninness, 2012). Our second contribution is to dissect the recent developments on SSMs and explain them in terms of their structure and parameterization, and to clearly separate this from their initialization, simulation, and training strategies. The presentation in the paper is also done using the language commonly used in the system identification community in order to speed up the use of these tools within this area. Notation The imaginary unit is denoted by $i=\\sqrt{-1}$. Given a vector $v$, we denote by $v^{\\prime}$ its real transpose. For a time-dependent vector, the discrete-time index $k$ is reported as a subscript, e.g., $v_{k}$. Moreover, we denote by $v_{a: b}$ (where $a \\leqslant b$ ) the sequence $v_{a: b}=\\left(v_{a}, v_{a+1}, \\ldots, v_{b}\\right)$. For this sequence, we indicate by $\\operatorname{cat}\\left(v_{a: b}\\right)$ the concatenation of its elements, i.e. $\\operatorname{cat}\\left(v_{a: b}\\right)=\\left[v_{a}^{\\prime}, \\ldots, v_{b}^{\\prime}\\right]^{\\prime}$, and by uncat $(\\cdot)$ its inverse operation returning a sequence of vectors given their concatenation. Given a complex matrix $A$, we let $\\operatorname{conj}(A)$ be its element-wise complex conjugate and $A^{*}$ be its Hermitian transpose. Diagonal matrices may be defined via the $\\operatorname{diag}$ operator, as $A=\\operatorname{diag}\\left(a_{1}, \\ldots, a_{n}\\right)$. ## 2. STRUCTURED STATE-SPACE MODELS\n\nConsider the model depicted in Figure 1, which consists of $L$ Wiener systems interconnected in series. Each of these layers is here referred to as Structured Statespace Layer (SSL). Their interconnection results in an SSM, which can be interpreted as a specific configuration of a deep Wiener system. We let the generic $\\ell$-th SSL $(\\ell \\in\\{1, \\ldots, L\\})$ be represented by a discrete-time statespace model\n\n$$\n\\mathrm{SSL}_{\\ell}:\\left\\{\\begin{array}{l}\nx_{k+1}=A x_{k}+B u_{k} \\\\\n\\eta_{k}=C x_{k}+D u_{k} \\\\\ny_{k}=\\sigma\\left(\\eta_{k}\\right)+F u_{k}\n\\end{array}\\right. $$\n\nwhere, for compactness, the layer index is omitted. System (1) is characterized by the input vector $u \\in \\mathbb{R}^{n_{u}}$, the intermediate vector $\\eta \\in \\mathbb{R}^{n_{y}}$, the output vector $y \\in \\mathbb{R}^{n_{y}}$, and the complex-valued state vector $x \\in \\mathbb{C}^{n_{x}}$. The SSL is parametrized by the matrices $\\{A, B, C, D, F\\}$. The output transformation $\\sigma(\\cdot)$ can be any nonlinear, Lipschitzcontinuous activation function, such as the tanh, ELU, or Swish, see Ramachandran et al. (2017). In what follows, we aim to provide an overview of the possible structure, parametrization, initialization, and simulation strategies for this SSL. Remark 1. When a deep SSM is considered $(L>1)$, each layer is parametrized and initialized independently from the others. The simulation is carried out iteratively over the set of layers, meaning that the output sequence of the $\\ell$-th layer is used as input of the layer $\\ell+1$.\n```\n\n\n\n---\n## Found 6 related papers from 1 external sources\n\n\n\nYour 3 raw search queries input to the search frame: spectral hierarchical implementation, training dynamics deep SSM, memory optimization multi-scale\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Papers with Code\n\n#### 1. Multiresolution Graph Transformers and Wavelet Positional Encoding for Learning Hierarchical Structures\n\n*From Search Query: spectral hierarchical implementation*\n\n*Risi Kondor, Truong Son Hy, Nhat Khang Ngo*\n\n**Abstract:** Contemporary graph learning algorithms are not well-defined for large molecules since they do not consider the hierarchical interactions among the atoms, which are essential to determine the molecular properties of macromolecules. In this work, we propose Multiresolution Graph Transformers (MGT), the first graph transformer architecture that can learn to represent large molecules at multiple scales. MGT can learn to produce representations for the atoms and group them into meaningful functional groups or repeating units. We also introduce Wavelet Positional Encoding (WavePE), a new positional encoding method that can guarantee localization in both spectral and spatial domains. Our proposed model achieves competitive results on two macromolecule datasets consisting of polymers and peptides, and one drug-like molecule dataset. Importantly, our model outperforms other state-of-the-art methods and achieves chemical accuracy in estimating molecular properties (e.g., GAP, HOMO and LUMO) calculated by Density Functional Theory (DFT) in the polymers dataset. Furthermore, the visualizations, including clustering results on macromolecules and low-dimensional spaces of their representations, demonstrate the capability of our methodology in learning to represent long-range and hierarchical structures. Our PyTorch implementation is publicly available at https://github.com/HySonLab/Multires-Graph-Transformer\n\n**Published:** 2023-02-17\n\n\n\n#### 2. Constructing A Flexible Likelihood Function For Spectroscopic Inference\n\n*From Search Query: spectral hierarchical implementation*\n\n*Anonymous*\n\n**Abstract:** We present a modular, extensible likelihood framework for spectroscopic\ninference based on synthetic model spectra. The subtraction of an imperfect\nmodel from a continuously sampled spectrum introduces covariance between\nadjacent datapoints (pixels) into the residual spectrum. For the high\nsignal-to-noise data with large spectral range that is commonly employed in\nstellar astrophysics, that covariant structure can lead to dramatically\nunderestimated parameter uncertainties (and, in some cases, biases). We\nconstruct a likelihood function that accounts for the structure of the\ncovariance matrix, utilizing the machinery of Gaussian process kernels. This\nframework specifically address the common problem of mismatches in model\nspectral line strengths (with respect to data) due to intrinsic model\nimperfections (e.g., in the atomic/molecular databases or opacity\nprescriptions) by developing a novel local covariance kernel formalism that\nidentifies and self-consistently downweights pathological spectral line\n\"outliers.\" By fitting many spectra in a hierarchical manner, these local\nkernels provide a mechanism to learn about and build data-driven corrections to\nsynthetic spectral libraries. An open-source software implementation of this\napproach is available at http://iancze.github.io/Starfish, including a\nsophisticated probabilistic scheme for spectral interpolation when using model\nlibraries that are sparsely sampled in the stellar parameters. We demonstrate\nsome salient features of the framework by fitting the high resolution $V$-band\nspectrum of WASP-14, an F5 dwarf with a transiting exoplanet, and the moderate\nresolution $K$-band spectrum of Gliese 51, an M5 field dwarf.\n\n**Published:** 2015-09-15\n\n\n\n#### 3. Vision Mamba: A Comprehensive Survey and Taxonomy\n\n*From Search Query: training dynamics deep SSM*\n\n*Lei Zhang, Chenxu Zhang, Xiao Liu*\n\n**Abstract:** State Space Model (SSM) is a mathematical model used to describe and analyze the behavior of dynamic systems. This model has witnessed numerous applications in several fields, including control theory, signal processing, economics and machine learning. In the field of deep learning, state space models are used to process sequence data, such as time series analysis, natural language processing (NLP) and video understanding. By mapping sequence data to state space, long-term dependencies in the data can be better captured. In particular, modern SSMs have shown strong representational capabilities in NLP, especially in long sequence modeling, while maintaining linear time complexity. Notably, based on the latest state-space models, Mamba merges time-varying parameters into SSMs and formulates a hardware-aware algorithm for efficient training and inference. Given its impressive efficiency and strong long-range dependency modeling capability, Mamba is expected to become a new AI architecture that may outperform Transformer. Recently, a number of works have attempted to study the potential of Mamba in various fields, such as general vision, multi-modal, medical image analysis and remote sensing image analysis, by extending Mamba from natural language domain to visual domain. To fully understand Mamba in the visual domain, we conduct a comprehensive survey and present a taxonomy study. This survey focuses on Mamba's application to a variety of visual tasks and data types, and discusses its predecessors, recent advances and far-reaching impact on a wide range of domains. Since Mamba is now on an upward trend, please actively notice us if you have new findings, and new progress on Mamba will be included in this survey in a timely manner and updated on the Mamba project at https://github.com/lx6c78/Vision-Mamba-A-Comprehensive-Survey-and-Taxonomy.\n\n**Published:** 2024-05-07\n\n\n\n#### 4. Model order reduction of deep structured state-space models: A system-theoretic approach\n\n*From Search Query: training dynamics deep SSM*\n\n*Dario Piga, Manas Mejari, Marco Forgione*\n\n**Abstract:** With a specific emphasis on control design objectives, achieving accurate system modeling with limited complexity is crucial in parametric system identification. The recently introduced deep structured state-space models (SSM), which feature linear dynamical blocks as key constituent components, offer high predictive performance. However, the learned representations often suffer from excessively large model orders, which render them unsuitable for control design purposes. The current paper addresses this challenge by means of system-theoretic model order reduction techniques that target the linear dynamical blocks of SSMs. We introduce two regularization terms which can be incorporated into the training loss for improved model order reduction. In particular, we consider modal $\\ell_1$ and Hankel nuclear norm regularization to promote sparsity, allowing one to retain only the relevant states without sacrificing accuracy. The presented regularizers lead to advantages in terms of parsimonious representations and faster inference resulting from the reduced order models. The effectiveness of the proposed methodology is demonstrated using real-world ground vibration data from an aircraft.\n\n**Published:** 2024-03-21\n\n\n\n#### 5. AWQ: Activation-aware Weight Quantization for LLM Compression and Acceleration\n\n*From Search Query: memory optimization multi-scale*\n\n*Song Han, Xingyu Dang, Guangxuan Xiao, Wei-Chen Wang, Wei-Ming Chen, Chuang Gan, Shang Yang, Haotian Tang, Jiaming Tang, Ji Lin*\n\n**Abstract:** Large language models (LLMs) have transformed numerous AI applications. On-device LLM is becoming increasingly important: running LLMs locally on edge devices can reduce the cloud computing cost and protect users' privacy. However, the astronomical model size and the limited hardware resource pose significant deployment challenges. We propose Activation-aware Weight Quantization (AWQ), a hardware-friendly approach for LLM low-bit weight-only quantization. AWQ finds that not all weights in an LLM are equally important. Protecting only 1% salient weights can greatly reduce quantization error. To identify salient weight channels, we should refer to the activation distribution, not weights. To avoid the hardware-inefficient mix-precision quantization, we mathematically derive that scaling up the salient channels can reduce the quantization error. AWQ employs an equivalent transformation to scale the salient weight channels to protect them. The scale is determined by collecting the activation statistics offline. AWQ does not rely on any backpropagation or reconstruction, so it generalizes to different domains and modalities without overfitting the calibration set. AWQ outperforms existing work on various language modeling and domain-specific benchmarks (coding and math). Thanks to better generalization, it achieves excellent quantization performance for instruction-tuned LMs and, for the first time, multi-modal LMs. Alongside AWQ, we implement TinyChat, an efficient and flexible inference framework tailored for 4-bit on-device LLM/VLMs. With kernel fusion and platform-aware weight packing, TinyChat offers more than 3x speedup over the Huggingface FP16 implementation on both desktop and mobile GPUs. It also democratizes the deployment of the 70B Llama-2 model on mobile GPUs.\n\n**Published:** 2023-06-01\n\n\n\n#### 6. BigDL 2.0: Seamless Scaling of AI Pipelines from Laptops to Distributed Cluster\n\n*From Search Query: memory optimization multi-scale*\n\n*Ge Song, Junwei Deng, Yina Chen, Le Zheng, Shan Yu, Jiaming Song, Qiyuan Gong, Yang Wang, Guoqiong Song, Kai Huang, Xin Qiu, Jiao Wang, Shengsheng Huang, Dongjie Shi, Ding Ding, Jason Dai*\n\n**Abstract:** Most AI projects start with a Python notebook running on a single laptop; however, one usually needs to go through a mountain of pains to scale it to handle larger dataset (for both experimentation and production deployment). These usually entail many manual and error-prone steps for the data scientists to fully take advantage of the available hardware resources (e.g., SIMD instructions, multi-processing, quantization, memory allocation optimization, data partitioning, distributed computing, etc.). To address this challenge, we have open sourced BigDL 2.0 at https://github.com/intel-analytics/BigDL/ under Apache 2.0 license (combining the original BigDL and Analytics Zoo projects); using BigDL 2.0, users can simply build conventional Python notebooks on their laptops (with possible AutoML support), which can then be transparently accelerated on a single node (with up-to 9.6x speedup in our experiments), and seamlessly scaled out to a large cluster (across several hundreds servers in real-world use cases). BigDL 2.0 has already been adopted by many real-world users (such as Mastercard, Burger King, Inspur, etc.) in production.\n\n**Proceeding:** cvpr-2022-1\n\n**Published:** 2022-04-03\n\n\n\n\n\n---\n## Web search results\n\n To help the researcher improve the autoregressive language model design, particularly focusing on state space models (SSMs) with spectral and hierarchical implementations, here are some key findings and references that align with the specified details:\n\n## Practical Implementations of Combined Spectral and Hierarchical Architectures in SSMs\n\n- The Mamba model, discussed in the context of sequence modeling, incorporates a modular structure with spectral and hierarchical elements. It uses a layer-stacking approach with residual connections and includes local convolution operations and state space model (SSM) states to capture both local and long-term dependencies. This architecture is described as having a selective state mechanism, which is efficient and scalable.\n\n## Training Dynamics and Stability Analysis in Deep State Space Models\n\n- The Mamba architecture paper touches on the training dynamics by introducing a hardware-aware parallel algorithm. This algorithm ensures fast inference and linear scaling in sequence length, which is crucial for stability and efficiency in deep models.\n- For stability analysis, the concept of fixed convolutional filters in spectral SSMs is important. These filters reduce the parameter count and provide stability guarantees, which is a key aspect of maintaining robustness in deep state space models.\n\n## Memory Optimization Techniques for Multi-Scale Processing\n\n- Memory hierarchy optimization is highlighted as crucial in the context of SSMs. The Mamba model, for instance, enjoys fast inference and efficient memory usage due to its hardware-aware design. This includes optimizing the memory hierarchy to handle parallel processing effectively.\n- The paper \"Mamba: Linear-Time Sequence Modeling\" by Dao and Gu (2023) discusses how Mamba's design principles, including its selective state mechanism, optimize memory usage while maintaining computational efficiency.\n\n## Hardware-Efficient Implementations of Hierarchical Models\n\n- The Mamba model is designed with hardware-aware parallel algorithms, which significantly improve its efficiency. It achieves a 5\u00d7 higher throughput than Transformers and linear scaling in sequence length, making it a hardware-efficient implementation of hierarchical models.\n\n## Empirical Comparisons of Different SSM Architectures\n\n- The comparison between Mamba and other architectures, such as Transformers, is discussed in terms of performance and efficiency. Mamba's ability to model bounded hierarchical structures with optimal memory and its faster inference times are highlighted as advantages over traditional Transformer models.\n- The paper \"State Space Models as Foundation Models\" by Amo Alonso et al. (2024) provides a broader comparison of SSMs with other sequence modeling approaches, emphasizing the synergies and unique benefits of SSMs in handling long-range dependencies and hierarchical structures[Analysis Note].\n\n### Additional Insights\n\n- **Spectral Properties**: Fixed filters in spectral SSMs provide stability guarantees and reduce the parameter count, which is beneficial for long-range memory modeling.\n- **Hardware Considerations**: IO-aware design principles and memory hierarchy optimization are critical for efficient parallel processing in hierarchical models.\n- **Integration Challenges**: Balancing stability and expressiveness, managing memory-computation trade-offs, and ensuring efficient hardware utilization are key challenges in implementing these models[Analysis Note].\n\nBy focusing on these aspects, the researcher can leverage the strengths of spectral and hierarchical SSM architectures to improve the design of autoregressive language models, particularly in terms of efficiency, stability, and scalability.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about spectral filtering, hierarchical architectures, and hardware efficiency in SSMs:\n\na) Theoretical Foundations:\n- Spectral SSMs provide provable robustness properties independent of spectrum\n- Fixed convolutional filters reduce parameter count and improve stability\n- Hierarchical structures enable multi-scale temporal processing\n\nb) Hardware and Implementation:\n- IO-aware designs crucial for efficiency\n- Memory hierarchy optimization important for parallel processing\n- Trade-offs between memory usage and computational complexity\n\nc) Recent Advances:\n- Mamba's selective state mechanism shows promise\n- Hierarchical state space models demonstrate effectiveness\n- Hardware-aware parallel algorithms improve efficiency\n\n2. Useful References with Excerpts:\n\nFrom \"State Space Models as Foundation Models\" (Amo Alonso et al., 2024):\n\"SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas\"\n\nFrom \"The Expressive Capacity of State Space Models\" (Sarrof et al., 2024):\n\"SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack.\"\n\nFrom \"Mamba: Linear-Time Sequence Modeling\" (Dao & Gu, 2023):\n\"We design a hardware-aware parallel algorithm in recurrent mode [...] Mamba enjoys fast inference (5\u00d7 higher throughput than Transformers) and linear scaling in sequence length\"\n\n3. Key Insights:\n\na) Spectral Properties:\n- Fixed filters provide stability guarantees\n- Spectral filtering reduces parameter count\n- Theoretical benefits for long-range memory\n\nb) Hardware Considerations:\n- Memory hierarchy optimization crucial\n- IO-aware design principles important\n- Parallel processing opportunities\n\nc) Integration Challenges:\n- Balancing stability and expressiveness\n- Managing memory-computation trade-offs\n- Ensuring efficient hardware utilization\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Specific spectral filtering implementations in hierarchical architectures\n2. Memory optimization techniques for multi-scale processing\n3. Training stability analysis for deep hierarchical models\n4. Hardware efficiency comparisons across architectures\n\nNext search queries should focus on:\n1. Practical implementations of spectral-hierarchical models\n2. Training dynamics in deep SSMs\n3. Memory optimization in multi-scale architectures\n\n5. References:\n\n[1] Amo Alonso, C., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint.\n\n[2] Sarrof, Y., Veitsman, Y., & Hahn, M. (2024). The Expressive Capacity of State Space Models: A Formal Language Perspective. arXiv preprint.\n\n[3] Dao, T., & Gu, A. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[4] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "spectral state space models, hierarchical mamba, efficient spectral filtering",
            "detail": "Find papers discussing:\n1. Integration of spectral filtering with state space models\n2. Hierarchical architectures in SSMs and their efficiency\n3. Hardware-efficient implementations of spectral methods\n4. Theoretical analysis of spectral filtering in sequence models\n5. Memory-efficient implementations of hierarchical models",
            "search_ret": "\n---\n## Found 3 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Integration of spectral filtering with state space models\n2. Hierarchical architectures in SSMs and their efficiency\n3. Hardware-efficient implementations of spectral methods\n4. Theoretical analysis of spectral filtering in sequence models\n5. Memory-efficient implementations of hierarchical models\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 3 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.98)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.98)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.99)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 1/31 (Score: 0.93)*\n\n```\n# Spectral State Space Models \n\nNaman Agarwal<br>Google Deepmind<br>namanagarwal@google.com\n\nDaniel Suo<br>Google Deepmind\n\nXinyi Chen<br>Princeton University<br>Google Deepmind\n\nElad Hazan<br>Princeton University<br>Google Deepmind\n\n\n#### Abstract\n\nThis paper studies sequence modeling for prediction tasks with long range dependencies.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.92)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 1/27 (Score: 0.92)*\n\n```\n# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.\n```\n\n\n\n---\n## Found 6 related papers from 1 external sources\n\n\n\nYour 3 raw search queries input to the search frame: spectral state space models, hierarchical mamba, efficient spectral filtering\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Papers with Code\n\n#### 1. Spectral State Space Models\n\n*From Search Query: spectral state space models*\n\n*Elad Hazan, Xinyi Chen, Daniel Suo, Naman Agarwal*\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n**Published:** 2023-12-11\n\n\n\n#### 2. SSUMamba: Spatial-Spectral Selective State Space Model for Hyperspectral Image Denoising\n\n*From Search Query: spectral state space models*\n\n*Jun Zhou, Jianfeng Lu, Fengchao Xiong, Guanyiman Fu*\n\n**Abstract:** Denoising is a crucial preprocessing step for hyperspectral images (HSIs) due to noise arising from intra-imaging mechanisms and environmental factors. Long-range spatial-spectral correlation modeling is beneficial for HSI denoising but often comes with high computational complexity. Based on the state space model (SSM), Mamba is known for its remarkable long-range dependency modeling capabilities and computational efficiency. Building on this, we introduce a memory-efficient spatial-spectral UMamba (SSUMamba) for HSI denoising, with the spatial-spectral continuous scan (SSCS) Mamba being the core component. SSCS Mamba alternates the row, column, and band in six different orders to generate the sequence and uses the bidirectional SSM to exploit long-range spatial-spectral dependencies. In each order, the images are rearranged between adjacent scans to ensure spatial-spectral continuity. Additionally, 3D convolutions are embedded into the SSCS Mamba to enhance local spatial-spectral modeling. Experiments demonstrate that SSUMamba achieves superior denoising results with lower memory consumption per batch compared to transformer-based methods. The source code is available at https://github.com/lronkitty/SSUMamba.\n\n**Published:** 2024-05-02\n\n\n\n#### 3. PlainMamba: Improving Non-Hierarchical Mamba in Visual Recognition\n\n*From Search Query: hierarchical mamba*\n\n*Elliot J. Crowley, Jiaming Liu, Zhenyu Wang, Linus Ericsson, Miguel Espinosa, Zehui Chen, Chenhongyi Yang*\n\n**Abstract:** We present PlainMamba: a simple non-hierarchical state space model (SSM) designed for general visual recognition. The recent Mamba model has shown how SSMs can be highly competitive with other architectures on sequential data and initial attempts have been made to apply it to images. In this paper, we further adapt the selective scanning process of Mamba to the visual domain, enhancing its ability to learn features from two-dimensional images by (i) a continuous 2D scanning process that improves spatial continuity by ensuring adjacency of tokens in the scanning sequence, and (ii) direction-aware updating which enables the model to discern the spatial relations of tokens by encoding directional information. Our architecture is designed to be easy to use and easy to scale, formed by stacking identical PlainMamba blocks, resulting in a model with constant width throughout all layers. The architecture is further simplified by removing the need for special tokens. We evaluate PlainMamba on a variety of visual recognition tasks, achieving performance gains over previous non-hierarchical models and is competitive with hierarchical alternatives. For tasks requiring high-resolution inputs, in particular, PlainMamba requires much less computing while maintaining high performance. Code and models are available at: https://github.com/ChenhongyiYang/PlainMamba .\n\n**Published:** 2024-03-26\n\n\n\n#### 4. MambaClinix: Hierarchical Gated Convolution and Mamba-Based U-Net for Enhanced 3D Medical Image Segmentation\n\n*From Search Query: hierarchical mamba*\n\n*Qian Dong, Bin Wei, Fengjiao Wang, Feifei Wang, Xia Yang, Nan Xia, Chenyuan Bian*\n\n**Abstract:** Deep learning, particularly convolutional neural networks (CNNs) and Transformers, has significantly advanced 3D medical image segmentation. While CNNs are highly effective at capturing local features, their limited receptive fields may hinder performance in complex clinical scenarios. In contrast, Transformers excel at modeling long-range dependencies but are computationally intensive, making them expensive to train and deploy. Recently, the Mamba architecture, based on the State Space Model (SSM), has been proposed to efficiently model long-range dependencies while maintaining linear computational complexity. However, its application in medical image segmentation reveals shortcomings, particularly in capturing critical local features essential for accurate delineation of clinical regions. In this study, we propose MambaClinix, a novel U-shaped architecture for medical image segmentation that integrates a hierarchical gated convolutional network(HGCN) with Mamba in an adaptive stage-wise framework. This design significantly enhances computational efficiency and high-order spatial interactions, enabling the model to effectively capture both proximal and distal relationships in medical images. Specifically, our HGCN is designed to mimic the attention mechanism of Transformers by a purely convolutional structure, facilitating high-order spatial interactions in feature maps while avoiding the computational complexity typically associated with Transformer-based methods. Additionally, we introduce a region-specific Tversky loss, which emphasizes specific pixel regions to improve auto-segmentation performance, thereby optimizing the model's decision-making process. Experimental results on five benchmark datasets demonstrate that the proposed MambaClinix achieves high segmentation accuracy while maintaining low model complexity.\n\n**Published:** 2024-09-19\n\n\n\n#### 5. Graph Neural Networks with convolutional ARMA filters\n\n*From Search Query: efficient spectral filtering*\n\n*Filippo Maria Bianchi, Lorenzo Livi, Daniele Grattarola, Cesare Alippi*\n\n**Abstract:** Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.\n\n**Published:** 2019-01-05\n\n\n\n#### 6. StyleMelGAN: An Efficient High-Fidelity Adversarial Vocoder with Temporal Adaptive Normalization\n\n*From Search Query: efficient spectral filtering*\n\n*Guillaume Fuchs, Nicola Pia, Ahmed Mustafa*\n\n**Abstract:** In recent years, neural vocoders have surpassed classical speech generation approaches in naturalness and perceptual quality of the synthesized speech. Computationally heavy models like WaveNet and WaveGlow achieve best results, while lightweight GAN models, e.g. MelGAN and Parallel WaveGAN, remain inferior in terms of perceptual quality. We therefore propose StyleMelGAN, a lightweight neural vocoder allowing synthesis of high-fidelity speech with low computational complexity. StyleMelGAN employs temporal adaptive normalization to style a low-dimensional noise vector with the acoustic features of the target speech. For efficient training, multiple random-window discriminators adversarially evaluate the speech signal analyzed by a filter bank, with regularization provided by a multi-scale spectral reconstruction loss. The highly parallelizable speech generation is several times faster than real-time on CPUs and GPUs. MUSHRA and P.800 listening tests show that StyleMelGAN outperforms prior neural vocoders in copy-synthesis and Text-to-Speech scenarios.\n\n**Published:** 2020-11-03\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models using spectral state space models, hierarchical architectures, and efficient spectral filtering, here are some key findings and references that align with the researcher's goals:\n\n## Integration of Spectral Filtering with State Space Models\n\n- The paper on \"What Can We Learn from State Space Models for Machine Learning\" discusses how state space models can be integrated with other machine learning techniques, including spectral methods. It introduces the concept of Graph State Space Convolution (GSSC) and Graph State Space Network (GrassNet), which use structured state space models to design and learn arbitrary graph spectral filters. This approach shows greater expressive power compared to polynomial filters and could be adapted for sequence models.\n\n## Hierarchical Architectures in SSMs and Their Efficiency\n\n- The \"Mamba Neural Operator\" paper presents a hierarchical approach by integrating state-space models into neural operators. Mamba generalizes the SSM framework, making it compatible with any architecture, including Transformers. This hierarchical structure is shown to capture long-range dependencies efficiently and reduce memory consumption, which is crucial for sequence-to-sequence problems.\n\n- The \"What Can We Learn from State Space Models for Machine Learning\" paper also touches on the hierarchical processing aspect by proposing the use of state space models in graph neural networks. It highlights how these models can capture long-range dependencies and offer efficient computation, which are key aspects for hierarchical SSM architectures.\n\n## Hardware-Efficient Implementations of Spectral Methods\n\n- The discussion on Graph State Space Network (GrassNet) in the \"What Can We Learn from State Space Models for Machine Learning\" paper provides insights into designing spectral filters that are efficient in terms of computation. The use of structured state space models to model graph signals at different frequencies offers a simple yet effective scheme, which can be optimized for hardware efficiency.\n\n## Theoretical Analysis of Spectral Filtering in Sequence Models\n\n- The \"Mamba Neural Operator\" paper delves into the theoretical foundations of integrating state-space models with neural operators. It shows that the neural operator layers share a comparable structural framework with time-varying SSMs, demonstrating alignment between hidden space updates and the iterative process in neural operator layers. This theoretical analysis can be extended to understand the stability and efficiency of spectral filtering in sequence models.\n\n## Memory-Efficient Implementations of Hierarchical Models\n\n- The \"Mamba Neural Operator\" paper highlights that Mamba reduces memory consumption by efficiently handling continuous data and long-range dependencies. This is achieved through a discretized form of the state update, which can be replaced with a zero-order hold method, showing that Mamba is memory-efficient compared to traditional Transformers.\n\n- The \"What Can We Learn from State Space Models for Machine Learning\" paper also emphasizes the memory efficiency of state space models, particularly in the context of graph neural networks. It suggests that adopting a state space viewpoint can lead to new architectural designs that are more memory-efficient and computationally viable.\n\n### Summary of Key Findings\n\n- **Spectral Filtering Integration**: The use of spectral filtering in state space models, as seen in Graph State Space Convolution (GSSC) and Graph State Space Network (GrassNet), offers greater expressive power and efficiency.\n- **Hierarchical Processing**: Mamba Neural Operator and other hierarchical SSM architectures are efficient in capturing long-range dependencies and reducing memory consumption.\n- **Implementation Efficiency**: The designs proposed in these papers focus on efficient computation and memory access patterns, which are crucial for hardware-efficient implementations.\n- **Theoretical Analysis**: The theoretical foundations of integrating state-space models with neural operators provide insights into stability and efficiency, which can be applied to spectral filtering in sequence models.\n\nThese findings and references provide a solid foundation for improving autoregressive language model design by incorporating spectral state space models, hierarchical architectures, and efficient spectral filtering methods.\n",
            "analysis": "Initial Analysis of SpectralHierarchicalMamba Proposal:\n\nKey Aspects Requiring Investigation:\n\n1. Spectral Filtering Integration\n- How does spectral filtering compare to existing approaches in Mamba and other SSMs?\n- What are the theoretical guarantees and stability properties?\n- Impact on computational efficiency and hardware utilization\n\n2. Hierarchical Processing\n- Comparison with existing hierarchical SSM approaches\n- Effectiveness of multi-scale temporal processing\n- Memory-computation trade-offs in hierarchical structures\n\n3. Implementation Efficiency\n- Hardware considerations for spectral operations\n- Memory access patterns and cache efficiency\n- Parallelization potential\n\nInvestigation Plan:\n\n1. Search for recent advances in:\n- Spectral filtering in neural networks\n- Hierarchical state space models\n- Efficient implementations of spectral methods\n\n2. Compare with:\n- Existing Mamba variants\n- Other hierarchical SSM architectures\n- Spectral-based sequence models\n\n3. Analyze:\n- Theoretical foundations\n- Implementation challenges\n- Hardware considerations\n\nInitial findings suggest this proposal combines spectral filtering with hierarchical SSMs in a novel way, but requires careful investigation of implementation efficiency and practical benefits.\n\nReferences to track:\n1. Agarwal et al. (2023) - Spectral State Space Models\n2. Bhirangi et al. (2024) - Hierarchical SSMs\n3. He et al. (2024) - DenseMamba\n\nNext steps:\n1. Investigate spectral filtering implementations\n2. Analyze hierarchical SSM architectures\n3. Evaluate hardware considerations\n4. Compare with existing approaches"
        },
        {
            "ready": false,
            "query": "efficient spectral neural networks, hardware optimization SSM, memory hierarchical models",
            "detail": "Find papers discussing:\n1. Practical implementations of spectral filtering in deep neural networks\n2. Hardware optimization techniques for state space models\n3. Memory management strategies in hierarchical architectures\n4. Stability analysis of combined spectral-hierarchical systems\n5. Efficient parallel processing of spectral operations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Practical implementations of spectral filtering in deep neural networks\n2. Hardware optimization techniques for state space models\n3. Memory management strategies in hierarchical architectures\n4. Stability analysis of combined spectral-hierarchical systems\n5. Efficient parallel processing of spectral operations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Spectral State Space Models (Avg. Score: 1.00)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 1.00)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.53)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.53)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory (Avg. Score: 0.05)\n\n*Shida Wang, Beichen Xue*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 14  (*Influential: 2*)\n\n**TL;DR:** It is proved that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship.\n\n**Abstract:** State-space models have gained popularity in sequence modelling due to their simple and efficient network structures. However, the absence of nonlinear activation along the temporal direction limits the model's capacity. In this paper, we prove that stacking state-space models with layer-wise nonlinear activation is sufficient to approximate any continuous sequence-to-sequence relationship. Our findings demonstrate that the addition of layer-wise nonlinear activation enhances the model's capacity to learn complex sequence patterns. Meanwhile, it can be seen both theoretically and empirically that the state-space models do not fundamentally resolve the issue of exponential decaying memory. Theoretical results are justified by numerical verifications.\n\n##### *Relevant Chunk: No. 9/20 (Score: 0.05)*\n\n```\nIn International Conference on Learning Representations, January 2021. [7] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the Parameterization and Initialization of Diagonal State Space Models. Advances in Neural Information Processing Systems, 35:35971-35983, December 2022. [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is All you Need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017. [9] Albert Gu, Isys Johnson, Aman Timalsina, Atri Rudra, and Christopher Re. How to Train your HIPPO: State Space Models with Generalized Orthogonal Basis Projections. In International Conference on Learning Representations, February 2023. [10] Shaojie Bai, J Zico Kolter, and Vladlen Koltun. An empirical evaluation of generic convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271, 2018. [11] Eric Martin and Chris Cundy. Parallelizing Linear Recurrent Neural Nets Over Sequence Length. In International Conference on Learning Representations, February 2018. [12] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y. Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher Re. Hyena Hierarchy: Towards Larger Convolutional Language Models. In International Conference on Machine Learning, June 2023. [13] Joshua Hanson, Maxim Raginsky, and Eduardo Sontag. Learning Recurrent Neural Net Models of Nonlinear Systems. In Proceedings of the 3rd Conference on Learning for Dynamics and Control, pages 425-435. PMLR, May 2021. [14] Zhong Li, Jiequn Han, Weinan E, and Qianxiao Li. Approximation and Optimization Theory for Linear Continuous-Time Recurrent Neural Networks. Journal of Machine Learning Research, 23(42):1-85, 2022. ISSN 1533-7928. [15] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157-166, March 1994.\n```\n\n#### 4. Robustifying State-space Models for Long Sequences via Approximate Diagonalization (Avg. Score: 0.04)\n\n*Annan Yu, Arnur Nigmetov, Dmitriy Morozov, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 4  (*Influential: 0*)\n\n**TL;DR:** A generic, backward-stable \"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non- normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs, is introduced, which shows resilience to Fourier-mode noise-perturbed inputs.\n\n**Abstract:** State-space models (SSMs) have recently emerged as a framework for learning long-range sequence tasks. An example is the structured state-space sequence (S4) layer, which uses the diagonal-plus-low-rank structure of the HiPPO initialization framework. However, the complicated structure of the S4 layer poses challenges; and, in an effort to address these challenges, models such as S4D and S5 have considered a purely diagonal structure. This choice simplifies the implementation, improves computational efficiency, and allows channel communication. However, diagonalizing the HiPPO framework is itself an ill-posed problem. In this paper, we propose a general solution for this and related ill-posed diagonalization problems in machine learning. We introduce a generic, backward-stable\"perturb-then-diagonalize\"(PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs. Based on this, we introduce the S4-PTD and S5-PTD models. Through theoretical analysis of the transfer functions of different initialization schemes, we demonstrate that the S4-PTD/S5-PTD initialization strongly converges to the HiPPO framework, while the S4D/S5 initialization only achieves weak convergences. As a result, our new models show resilience to Fourier-mode noise-perturbed inputs, a crucial property not achieved by the S4D/S5 models. In addition to improved robustness, our S5-PTD model averages 87.6% accuracy on the Long-Range Arena benchmark, demonstrating that the PTD methodology helps to improve the accuracy of deep learning models.\n\n##### *Relevant Chunk: No. 19/37 (Score: 0.04)*\n\n```\nIn International Conference on Machine Learning, pages 9168-9178. PMLR, 2021. [31] Biswa Sengupta and Karl J Friston. How robust are deep neural networks? arXiv preprint arXiv:1804.11313, 2018. [32] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [33] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [34] Lloyd N Trefethen and Mark Embree. Spectra and Pseudospectra: The Behaviour of Nonnormal Matrices and Operators. Springer, 2005. [35] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, $32,2019$.\n```\n\n#### 5. Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions (Avg. Score: 0.02)\n\n*Stefano Massaroli, Michael Poli, Daniel Y. Fu, Hermann Kumbong, Rom N. Parnichkun, Aman Timalsina, David W. Romero, Quinn McIntyre, Beidi Chen, A. Rudra, Ce Zhang, Christopher R\u00e9, Stefano Ermon, Y. Bengio*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 10  (*Influential: 2*)\n\n**TL;DR:** This paper seeks to enable compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation, and introduces architectural improvements to convolution-based layers such as Hyena.\n\n**Abstract:** Recent advances in attention-free sequence models rely on convolutions as alternatives to the attention operator at the core of Transformers. In particular, long convolution sequence models have achieved state-of-the-art performance in many domains, but incur a significant cost during auto-regressive inference workloads -- naively requiring a full pass (or caching of activations) over the input sequence for each generated token -- similarly to attention-based models. In this paper, we seek to enable $\\mathcal O(1)$ compute and memory cost per token in any pre-trained long convolution architecture to reduce memory footprint and increase throughput during generation. Concretely, our methods consist in extracting low-dimensional linear state-space models from each convolution layer, building upon rational interpolation and model-order reduction techniques. We further introduce architectural improvements to convolution-based layers such as Hyena: by weight-tying the filters across channels into heads, we achieve higher pre-training quality and reduce the number of filters to be distilled. The resulting model achieves 10x higher throughput than Transformers and 1.5x higher than Hyena at 1.3B parameters, without any loss in quality after distillation.\n\n##### *Relevant Chunk: No. 15/64 (Score: 0.02)*\n\n```\n7462-7473 (cit. on p. 3). [16] Rizal Fathony et al. \"Multiplicative filter networks\". In: International Conference on Learning Representations. 2020 (cit. on p. 3). [17] Daniel Y. Fu et al. \"Simple Hardware-Efficient Long Convolutions for Sequence Modeling\". In: International Conference on Machine Learning (2023) (cit.\n```\n\n\n\n---\n## Found 4 related papers from 1 external sources\n\n\n\nYour 3 raw search queries input to the search frame: efficient spectral neural networks, hardware optimization SSM, memory hierarchical models\n\nConsidering refining your search by improving the query keywords input.\n\n### 4 related papers from Papers with Code\n\n#### 1. Benchmarking Spectral Graph Neural Networks: A Comprehensive Study on Effectiveness and Efficiency\n\n*From Search Query: efficient spectral neural networks*\n\n*Laks V. S. Lakshmanan, Siqiang Luo, Zulun Zhu, Haoyu Liu, Ningyi Liao*\n\n**Abstract:** With the recent advancements in graph neural networks (GNNs), spectral GNNs have received increasing popularity by virtue of their specialty in capturing graph signals in the frequency domain, demonstrating promising capability in specific tasks. However, few systematic studies have been conducted on assessing their spectral characteristics. This emerging family of models also varies in terms of designs and settings, leading to difficulties in comparing their performance and deciding on the suitable model for specific scenarios, especially for large-scale tasks. In this work, we extensively benchmark spectral GNNs with a focus on the frequency perspective. We analyze and categorize over 30 GNNs with 27 corresponding filters. Then, we implement these spectral models under a unified framework with dedicated graph computations and efficient training schemes. Thorough experiments are conducted on the spectral models with inclusive metrics on effectiveness and efficiency, offering practical guidelines on evaluating and selecting spectral GNNs with desirable performance. Our implementation enables application on larger graphs with comparable performance and less overhead, which is available at: https://github.com/gdmnl/Spectral-GNN-Benchmark.\n\n**Published:** 2024-06-14\n\n\n\n#### 2. Graph Neural Networks with convolutional ARMA filters\n\n*From Search Query: efficient spectral neural networks*\n\n*Filippo Maria Bianchi, Lorenzo Livi, Daniele Grattarola, Cesare Alippi*\n\n**Abstract:** Popular graph neural networks implement convolution operations on graphs based on polynomial spectral filters. In this paper, we propose a novel graph convolutional layer inspired by the auto-regressive moving average (ARMA) filter that, compared to polynomial ones, provides a more flexible frequency response, is more robust to noise, and better captures the global graph structure. We propose a graph neural network implementation of the ARMA filter with a recursive and distributed formulation, obtaining a convolutional layer that is efficient to train, localized in the node space, and can be transferred to new graphs at test time. We perform a spectral analysis to study the filtering effect of the proposed ARMA layer and report experiments on four downstream tasks: semi-supervised node classification, graph signal classification, graph classification, and graph regression. Results show that the proposed ARMA layer brings significant improvements over graph neural networks based on polynomial filters.\n\n**Published:** 2019-01-05\n\n\n\n#### 3. NiuTrans: An Open Source Toolkit for Phrase-based and Syntax-based Machine Translation\n\n*From Search Query: memory hierarchical models*\n\n*Qiang Li, Hao Zhang, Tong Xiao, Jingbo Zhu*\n\n**Abstract:** \n\n**Proceeding:** acl-2012-7\n\n**Published:** 2012-07-01\n\n\n\n#### 4. Neighborhood Attention Transformer\n\n*From Search Query: memory hierarchical models*\n\n*Humphrey Shi, Shen Li, Jiachen Li, Steven Walton, Ali Hassani*\n\n**Abstract:** We present Neighborhood Attention (NA), the first efficient and scalable sliding-window attention mechanism for vision. NA is a pixel-wise operation, localizing self attention (SA) to the nearest neighboring pixels, and therefore enjoys a linear time and space complexity compared to the quadratic complexity of SA. The sliding-window pattern allows NA's receptive field to grow without needing extra pixel shifts, and preserves translational equivariance, unlike Swin Transformer's Window Self Attention (WSA). We develop NATTEN (Neighborhood Attention Extension), a Python package with efficient C++ and CUDA kernels, which allows NA to run up to 40% faster than Swin's WSA while using up to 25% less memory. We further present Neighborhood Attention Transformer (NAT), a new hierarchical transformer design based on NA that boosts image classification and downstream vision performance. Experimental results on NAT are competitive; NAT-Tiny reaches 83.2% top-1 accuracy on ImageNet, 51.4% mAP on MS-COCO and 48.4% mIoU on ADE20K, which is 1.9% ImageNet accuracy, 1.0% COCO mAP, and 2.6% ADE20K mIoU improvement over a Swin model with similar size. To support more research based on sliding-window attention, we open source our project and release our checkpoints at: https://github.com/SHI-Labs/Neighborhood-Attention-Transformer .\n\n**Proceeding:** cvpr-2023-1\n\n**Published:** 2022-04-14\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design, particularly focusing on efficient spectral neural networks, hardware optimization for state space models, memory management in hierarchical architectures, stability analysis of combined systems, and efficient parallel processing, here are some relevant insights and pointers from the provided sources and additional context:\n\n## Practical Implementations of Spectral Filtering in Deep Neural Networks\n\n- The concept of spectral filtering is well-demonstrated in the context of modulation classification and signal processing. For instance, the \"Filtered Randomized Smoothing\" (FRS) technique described in combines spectral filtering with randomized smoothing to enhance robustness. This approach involves filtering the input signal to attenuate high-frequency components that are often associated with adversarial attacks, which can be adapted to other domains like language models to improve robustness.\n\n- The frequency principle/spectral bias discussed in highlights how deep neural networks tend to fit target functions from low to high frequencies. This principle can guide the design of spectral filtering mechanisms in language models, ensuring that the model learns low-frequency components first and gradually captures high-frequency details.\n\n## Hardware Optimization Techniques for State Space Models\n\n- While the provided sources do not directly address hardware optimization for state space models, the general principle of using fixed convolutional filters in spectral state space models, as mentioned in the analysis note, can be hardware-optimized. Fixed filters reduce the parameter count and computational complexity, which can be beneficial for hardware implementation. Research on efficient hardware architectures for convolutional neural networks (CNNs) can be applied here, focusing on optimizing the fixed convolutional filters used in state space models.\n\n## Memory Management Strategies in Hierarchical Architectures\n\n- Hierarchical models, such as those discussed in the analysis note, benefit from efficient memory management. The use of hierarchical state management, as mentioned, can reduce memory usage. For example, the \"Deep Unfolding Network\" approach in involves unfolding an optimization problem into a deep network, where each module corresponds to a specific operation. This can be adapted to manage memory efficiently in hierarchical language models by breaking down complex operations into manageable, memory-efficient modules.\n\n## Stability Analysis of Combined Spectral-Hierarchical Systems\n\n- Stability analysis in combined spectral-hierarchical systems is crucial. The frequency principle provides insights into how deep neural networks learn functions from low to high frequencies, which can help in analyzing the stability of such systems. By ensuring that the spectral filtering mechanisms align with the hierarchical structure's learning process, the stability of the combined system can be enhanced.\n\n## Efficient Parallel Processing of Spectral Operations\n\n- Efficient parallel processing is essential for scaling language models. The use of spectral filtering and hierarchical architectures can be optimized for parallel processing by leveraging techniques such as those described in the FRS method, where filtering and smoothing are combined. Additionally, the symmetric 'U'-shaped layout of U-Net architectures can be parallelized efficiently, which might be applicable to spectral operations in language models.\n\n### Additional Considerations\n\n- **Spectral State Space Models**: These models offer provable robustness properties and fixed convolutional filters that do not require learning, which can be particularly beneficial for autoregressive language models needing long-range memory capabilities[Analysis Note].\n\n- **Hierarchical Processing**: Implementing hierarchical structures with cross-layer connections and efficient memory management can enhance the performance and scalability of language models[Analysis Note].\n\n- **Implementation Considerations**: Efficient spectral filtering, trade-offs between memory usage and computational complexity, and hardware optimization are key considerations. Adapting techniques from other domains, such as image processing and signal processing, can provide valuable insights for language model design.\n\nBy integrating these concepts, researchers can develop more robust, efficient, and scalable autoregressive language models that leverage the benefits of spectral filtering, hierarchical processing, and optimized hardware and memory management.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several key aspects relevant to the SpectralHierarchicalMamba proposal:\n\na) Spectral State Space Models:\n- Provide provable robustness properties independent of spectrum and dimensionality\n- Use fixed convolutional filters that don't require learning\n- Show theoretical benefits for long-range memory tasks\n\nb) Hierarchical Processing:\n- Current Mamba implementations show promise in hierarchical structures\n- Memory efficiency through hierarchical state management\n- Importance of cross-layer connections\n\nc) Implementation Considerations:\n- Need for efficient spectral filtering implementations\n- Trade-offs between memory usage and computational complexity\n- Hardware optimization requirements\n\n2. Useful References with Excerpts:\n\nFrom \"Spectral State Space Models\" (Agarwal et al., 2023):\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning\"\n\nFrom \"State Space Models as Foundation Models\" (Amo Alonso et al., 2024):\n\"SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas\"\n\nFrom \"Mamba: Linear-Time Sequence Modeling\" (Gu & Dao, 2023):\n\"SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures\"\n\n3. Key Insights:\n\na) Spectral Filtering Benefits:\n- Fixed filters reduce parameter count\n- Provable robustness properties\n- Theoretical guarantees for long-range memory\n\nb) Implementation Challenges:\n- Need efficient spectral operations\n- Memory-computation trade-offs\n- Hardware considerations for parallel processing\n\nc) Integration Opportunities:\n- Combining spectral and hierarchical approaches\n- Cross-layer information flow\n- Adaptive state management\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Efficient implementations of spectral filtering in neural networks\n2. Hardware optimization techniques for SSMs\n3. Memory management strategies in hierarchical models\n4. Stability analysis of spectral-hierarchical architectures\n\nNext search queries should focus on:\n1. Practical implementations of spectral filtering in deep learning\n2. Memory optimization in hierarchical neural networks\n3. Hardware-efficient SSM implementations\n\n5. References:\n\n[1] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n[2] Amo Alonso, C., Sieber, J., & Zeilinger, M. (2024). State Space Models as Foundation Models: A Control Theoretic Overview. arXiv preprint.\n\n[3] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint."
        },
        {
            "ready": false,
            "query": "spectral SSM training dynamics, hardware efficiency SSM, hierarchical model evaluation",
            "detail": "Find papers discussing:\n1. Training dynamics and stability of spectral state space models\n2. Hardware efficiency comparisons between different SSM architectures\n3. Empirical evaluations of hierarchical SSM models\n4. Practical implementations of combined spectral-hierarchical systems\n5. Memory optimization techniques in hierarchical architectures",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Training dynamics and stability of spectral state space models\n2. Hardware efficiency comparisons between different SSM architectures\n3. Empirical evaluations of hierarchical SSM models\n4. Practical implementations of combined spectral-hierarchical systems\n5. Memory optimization techniques in hierarchical architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. The Expressive Capacity of State Space Models: A Formal Language Perspective  (Avg. Score: 0.99)\n\n*Yash Sarrof, Yana Veitsman, Michael Hahn*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that SSMs and transformers have overlapping but distinct strengths, and a design choice in current SSMs that limits their expressive power is identified.\n\n**Abstract:** Recently, recurrent models based on linear state space models (SSMs) have shown promising performance in language modeling (LM), competititve with transformers. However, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba.\n\n##### *Relevant Chunk: No. 2/63 (Score: 0.99)*\n\n```\nHowever, there is little understanding of the in-principle abilities of such models, which could provide useful guidance to the search for better LM architectures. We present a comprehensive theoretical study of the capacity of such SSMs as it compares to that of transformers and traditional RNNs. We find that SSMs and transformers have overlapping but distinct strengths. In star-free state tracking, SSMs implement straightforward and exact solutions to problems that transformers struggle to represent exactly. They can also model bounded hierarchical structure with optimal memory even without simulating a stack. On the other hand, we identify a design choice in current SSMs that limits their expressive power. We discuss implications for SSM and LM research, and verify results empirically on a recent SSM, Mamba. ## 1 Introduction\n\nAfter their introduction [69], transformers rapidly became the primary workhorse of NLP, powering most of today's large language models (LLMs). Compared to previously-dominant recurrent architectures [RNNs 17, 29], transformers offered a key advantage: parallelized training by avoiding recurrence. However, building on a long history of continuous dynamical models [e.g. 34, 35] and early work on faster RNNs [8, 41], a recent line of work has developed state space models (SSMs) rivaling the performance of transformers [e.g. 24, 23, 67, 14, 72, 56]. These SSMs are recurrent models that-while formulated in terms of iterative state updates-allow efficient parallelization. The impressive empirical performance of such SSMs raises the question of whether they might have capabilities that the transformer architecture might lack in principle. Simultaneously, to understand whether SSMs may plausibly overtake the dominant role of transformers, it is an important question whether SSMs may lack abilities present in transformers. A better understanding of these questions may also point the way to future architectures that unite the strengths of both architectures. One common approach to understanding the capabilities of computational architectures is through their expressive capacity in simulating automata and modeling language classes; indeed, a sizeable literature has studied transformers [e.g. 54, 25, 6, 73, 44, 45, 15, 66, 10, 59, 53] and RNNs [e.g. 62, 31, 32, 70, 28] through this lens. As the difficulty of many computational problems is wellunderstood in terms of such language classes, results about expressive capacity directly yield results about the ability to model specific computational problems. While a substantial number of results have been obtained for transformers and traditional RNNs, understanding remains largely open for SSMs. In an initial step, Merrill et al. [49] showed that all problems computable by SSMs are contained in $\\mathrm{TC}^{0}$, a circuit complexity class that is known to\nalso cover transformers [48,65]. Under standard conjectures, this suggests that certain types of state tracking are hard for both models. Jelassi et al. [33] provided evidence for differences between the architectures, showing that transformers are better than SSMs at the specific problem of copying strings - a problem well within $\\mathrm{TC}^{0}$. However, beyond these results, broader detailed understanding of the power of SSMs and how they compare to RNNs and transformers remains open. Our contribution in this paper is to provide rigorous understanding of SSMs' abilities in different classes of languages. We show that transformers and SSMs cover overlapping but distinct fragments of $\\mathrm{TC}^{0}$. For instance, SSMs can model bounded hierarchical structure in ways similar to transformers and traditional RNNs, even without embedding a stack-like structure (Theorem 6). For regular languages involving modular counting, such as the PARITY function (Theorem 2), we identify a design choice that makes extant SSMs struggle in ways similar to transformers. In other cases, we show that SSMs resolve a failure case of transformers: they effortlessly model Flip Flop state tracking (Theorem 1). We discuss take-aways for SSM and LLM research in Section 5; among others, our results suggest future LM architectures might need to combine both attention and state spaces. ## 2 Background: State Space Models\n\nSSM Layers We define a single layer of a state space model as a map, at input length $T$,\n\n$$\n\\mathbb{R}^{T \\times d} \\rightarrow \\mathbb{R}^{T \\times d} \\quad\\left(x_{t}\\right)_{t=1, \\ldots, T} \\mapsto\\left(z_{t}\\right)_{t=1, \\ldots, T}\n$$\n\ngiven by the recurrence\n\n$$\nh_{t}=A\\left(x_{t}\\right) \\circ h_{t-1}+B\\left(x_{t}\\right) \\quad z_{t}=\\phi\\left(h_{t}, x_{t}\\right)\n$$\n\nwhere $\\circ$ denotes elementwise product, and, for each $x_{t} \\in \\mathbb{R}^{d}$,\n\n$$\n\\begin{array}{cl}\nh_{0} \\in \\mathbb{R}^{d} & B\\left(x_{t}\\right) \\in \\mathbb{R}^{d} \\text { (increment) } \\\\\nA\\left(x_{t}\\right) \\in \\mathbb{R}^{d}(\\text { gate }) & \\phi: \\mathbb{R}^{2 d} \\rightarrow \\mathbb{R}^{d} \\text { (transform) }\n\\end{array}\n$$\n\nWe allow $A, B$ to be arbitrary smooth maps.\n```\n\n#### 2. Spectral State Space Models (Avg. Score: 0.97)\n\n*Naman Agarwal, Daniel Suo, Xinyi Chen, Elad Hazan*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** A new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017) gives rise to a novel sequence prediction architecture the authors call a spectral state space model.\n\n**Abstract:** This paper studies sequence modeling for prediction tasks with long range dependencies. We propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm (Hazan et al. (2017)). This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory.\n\n##### *Relevant Chunk: No. 2/31 (Score: 0.99)*\n\n```\nWe propose a new formulation for state space models (SSMs) based on learning linear dynamical systems with the spectral filtering algorithm [HSZ17]. This gives rise to a novel sequence prediction architecture we call a spectral state space model. Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning while still outperforming SSMs in both theory and practice. The resulting models are evaluated on synthetic dynamical systems and long-range prediction tasks of various modalities. These evaluations support the theoretical benefits of spectral filtering for tasks requiring very long range memory. ## 1 Introduction\n\nHandling long-range dependencies efficiently remains a core problem in sequence prediction/modelling. Recurrent Neural Networks (RNN) [Hop82, RHW ${ }^{+}$85, Elm90] are a natural choice, but are notoriously hard to train; they often suffer from vanishing and exploding gradients [BSF94, PMB13] and despite techniques to mitigate the issue [HS97, $\\mathrm{CVMG}^{+}$14, ASB16], they are also hard to scale given the inherently sequential nature of their computation. In recent years, transformer models $\\mathrm{VSP}^{+}$17 have become the staple of sequence modelling, achieving remarkable success across multiple domains $\\left[\\mathrm{BMR}^{+}\\right.$20, $\\mathrm{DBK}^{+}$20, $\\mathrm{JEP}^{+}$21]. Transformer models are naturally parallelizable and hence scale significantly better than RNNs. However, attention layers have memory/computation requirements that scale quadratically with context length. Many approximations have been proposed (see [TDBM22] for a recent survey). RNNs have seen a recent resurgence in the form of state space models (SSM) which have shown promise in modelling long sequences across varied modalities GGR21, $\\mathrm{DFS}^{+}$22, GGB22, $\\mathrm{OSG}^{+} 23$, $\\mathrm{PMN}^{+}$23, GD23]. SSMs use linear dynamical systems (LDS) to model the sequence-to sequence transform by evolving the internal state of a dynamical system according to the dynamics equations\n\n$$\nx_{t}=A x_{t-1}+B u_{t} \\quad y_{t}=C x_{t}+D u_{t}\n$$\n\nHere $x_{t} \\in \\mathbb{R}^{d}$ is the hidden state of the dynamical system, $u_{t}$ is the input to the system, and $y_{t}$ are observations. The matrices $A, B, C, D$ govern the evolution of the system and are called system matrices. Despite its simplicity, this linear model can capture a rich set of natural dynamical systems\nin engineering and the physical sciences due to the potentially large number of hidden dimensions. Linear dynamical systems are also attractive as a sequence model because their structure is amenable to both fast inference and fast training via parallel scans [Ble89, SWL23] or convolutions [GGR21]. A rich literature stemming from control theory and recent machine learning interest has given rise to efficient techniques for system identification, filtering, and prediction for linear dynamical systems. For a survey of recent literature see [HS22]. These techniques make SSMs attractive for sequence tasks which inherently depend on long contexts that scale poorly for transformers. Examples include large language models [DFS ${ }^{+}$22], modelling time series [ZSP ${ }^{+}$23], and audio generation [GGDR22]. To understand the factors affecting the memory in an SSM or simply a linear dynamical system, we now proceed to delineate how past states and inputs affect the future. Geometric decay in LDS. The linear equations governing the dynamics are recursive in nature, and imply that in a noiseless environment, the $t$ 'th output can be written as\n\n$$\ny_{t}=C x_{t}+D u_{t}=C\\left(A x_{t-1}+B u_{t}\\right)+D u_{t}=\\ldots=\\sum_{i=0}^{t-1} C A^{i} B u_{t-i}+D u_{t}\n$$\n\nThe matrix $A$ is asymmetric in general, and can have complex eigenvalues. If the amplitude of these eigenvalues is $>1$, then the output $y_{t}$ can grow without bounds. This is called an \"explosive\" system. In a well-behaved system, the eigenvalues of $A$ have magnitude $<1$. If the magnitudes are bounded away from 1 , say $\\left|\\lambda_{i}(A)\\right|<1-\\delta$, for some $\\delta>0$ (referred to as spectral gap), then we can write\n\n$$\ny_{t}=\\sum_{i=0}^{k} C A^{i} B u_{t-i}+\\omega_{k},\\left\\|\\omega_{k}\\right\\| \\leq \\varepsilon\n$$\n\nfor $k=O\\left(\\frac{1}{\\delta} \\log \\frac{1}{\\varepsilon}\\right)$. This mathematical fact implies that the effective memory of the system is on the order of $\\frac{1}{\\delta}$. In general, the parameter $\\delta$ is unknown apriori and can get arbitrarily small as we approach systems with have long range dependencies leading to instability in training linear dynamical systems with a long context. This issue is specifically highlighted in the work of [ $\\mathrm{OSG}^{+}$23] who observe that on long range tasks learning an LDS directly does not succeed and requires interventions such as stable exponential parameterizations and specific normalization which have been repeatedly used either implicitly or explicitly in the SSM literature [GGR21]. Unfortunately these reparametrizations and normalizations come with no theoretical guarantees. In fact this limitation is generally known to be fundamental to the use of linear dynamical systems, and can only be circumvented via a significant increase in sample complexity $\\left[\\mathrm{GLS}^{+}\\right.$20] or via control over the input sequence [SMT ${ }^{+}$18]. Spectral filtering for linear dynamical systems. A notable deviation from the standard theory of linear dynamical systems that allows efficient learning in the presence of arbitrarily long memory is the technique of spectral filtering [HSZ17]. The idea is to project the sequence of inputs to a small subspace that is constructed using special structure of discrete LDS where successive powers of the system matrix appear in the impulse response function. The basic idea is to represent the output as\n\n$$\ny_{t}=\\sum_{j=1}^{k} M_{j}\\left(\\sum_{i} \\phi_{j}(i) \\cdot u_{t-i}\\right)\n$$\n\nwhere $\\phi_{j}$ are spectral filters which are sequence-length sized vectors that given the target sequence length can be computed offline, and $M_{j}$ are matrices parameterizing the model. These spectral-filters are the eigenvectors of the matrix constructed as the average of outer products of the discrete impulseresponse functions, viz $Z=\\int_{0}^{1}\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]^{\\top} d \\alpha$. It is shown that this matrix is inherently low-dimensional and for all $\\alpha \\in[0,1]$, vectors of the form $\\left[1, \\alpha, \\alpha^{2} \\ldots\\right]$ are well approximated by the top-eigenspace of Z. Figure 1 depicts these filters. For the details of how these filters are derived and their computation, see Section 2\n\nWhy is spectral filtering important? The main advantage of spectral filtering is that for certain types of linear dynamical systems, in particular those with symmetric matrices $A$, the effective memory(measured by the number of filters) required to represent an observation at any point in the sequence in the spectral basis is independent of the spectral gap parameter $\\delta!$. This guarantee indicates that if we featurize the input into the spectral basis, we can potentially design models that\nare capable of efficiently and stably representing systems with extremely long memory even with $\\delta \\rightarrow 0$. This striking fact motivates our derivation of the recurrent spectral architecture, and is the underlying justification for the performance and training stability gains we see in experiments. ![](https://cdn.mathpix.com/cropped/2024_09_17_28085b3c06af8ebfb6a7g-03.jpg?height=524&width=816&top_left_y=429&top_left_x=641)\n\nFigure 1: Spectral Filters used by the Spectral Filtering Algorithm. The x-axis is the time domain. ### 1.1 Our Contributions\n\nWe start by proposing state space models with learned components that apply spectral filtering for their featurization. We consider two types of spectral filters, which augment the original spectral filters proposed in HSZ17] with negative eigenvalues in two different ways. Our main contribution is a neural architecture that is based on these spectral state space models. This neural architecture can be applied recursively in layers, resulting in an expressive architecture for modeling sequential data. Finally we implement this neural architecture and apply it towards synthetically generated data as well as the Long Range Arena benchmark [TDA ${ }^{+21]}$. We demonstrate that spectral state space models can stably and more efficiently learn on sequence modelling tasks with long range dependencies without the need for exponential parameterizations, particular initializations and normalizations. Main Advantages of Spectral SSM. Previously proposed convolutional models for sequence modeling, surveyed in the related work section, learn the kernels from the data. The kernels used in Spectral SSM are theoretically-founded and fixed and thus parameter-free. In addition, our models are provably as expressive as an LDS. In particular, their expressiveness neither depends on the spectra gap nor on the dimension of the system, which are necessary in all other methods. ### 1.2 Related work\n\nDue to limited space, we provide a short overview of the most related work to us below and provide a detailed report on the related work in the appendix (Section A). State space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years starting with the works [GDE $\\left.{ }^{+} 20\\right],\\left[\\mathrm{GJG}^{+} 21\\right]$ which propose and develop the HiPPO theory. [GGR21] develop the S4 parameterization to address the bottlenecks of training efficiency, performance and numberical stability. The $S 4$ parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization. The S 4 model was further streamlined in later works, viz. using diagonal system matrices without a loss in performance [GGB22] and the S5 model [SWL23] which uses a MIMO diagonal system and associative scans for computational efficiency. [OSG $\\left.{ }^{+} 23\\right]$ investigate whether simpler deep Linear Recurrent Units (LRU) can recover the performance of deep SSMs, and provide an affirmative answer under the crucial caveat that specific modifications on linear RNNs, namely the stable exponential parameterization, $\\gamma$ - normalization and ring initialization, are necessary to learn on certain challenging long-context modeling tasks.\n```\n\n##### *Relevant Chunk: No. 13/31 (Score: 0.96)*\n\n```\nNature, 596(7873):583-589, 2021. $\\left[\\mathrm{LCZ}^{+} 22\\right]$ Yuhong Li, Tianle Cai, Yi Zhang, Deming Chen, and Debadeepta Dey. What makes convolutional models great on long sequence modeling? arXiv preprint arXiv:2210.09298, 2022. [OSG ${ }^{+}$23] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349, 2023. [PMB13] Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio. On the difficulty of training recurrent neural networks. In International conference on machine learning, pages 1310-1318. Pmlr, 2013. $\\left[\\mathrm{PMN}^{+} 23\\right]$ Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. $\\left[\\mathrm{RHW}^{+}\\right.$85] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning internal representations by error propagation, 1985. [SMT ${ }^{+}$18] Max Simchowitz, Horia Mania, Stephen Tu, Michael I Jordan, and Benjamin Recht. Learning without mixing: Towards a sharp analysis of linear system identification. In Conference On Learning Theory, pages 439-473. PMLR, 2018. [SWF23] Jiaxin Shi, Ke Alexander Wang, and Emily Fox. Sequence modeling with multiresolution convolutional memory. In International Conference on Machine Learning, pages 31312-31327. PMLR, 2023. [SWL23] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [TDA ${ }^{+}$21] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena : A benchmark for efficient transformers. In International Conference on Learning Representations, 2021. [TDBM22] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: A survey. ACM Comput. Surv., 55(6), dec 2022. $\\left[\\mathrm{VSP}^{+}\\right.$17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [ZSP ${ }^{+}$23] Michael Zhang, Khaled K Saab, Michael Poli, Tri Dao, Karan Goel, and Christopher R\u00e9. Effectively modeling time series with simple discrete state spaces. arXiv preprint arXiv:2303.09489, 2023. ## A Detailed Related work\n\nState space models. SSMs for learning long range phenomenon have received much attention in the deep learning community in recent years. $\\mathrm{GDE}^{+}$20] propose the HiPPO framework for continuous-time memorization, and shows that with a special class of system matrices $A$ (HiPPO matrices), SSMs have the capacity for long-range memory. Subsequently, $\\left[\\mathrm{GJG}^{+} 21\\right]$ propose the Linear State-Space Layer (LSSL), where the system matrix is learnable. The LSSL can be viewed as a recurrence in the state domain and a convolution in the time domain, and generalizes particular RNN and CNN architectures. For efficient learning of the system matrices, authors propose learning within a class of structured matrices that contain the HiPPO dynamics, and have efficient convolution schemes. However, the proposed method is numerically unstable in practice as well as memoryintensive. As a result, [GGR21] develop the S 4 parameterization to address these bottlenecks. The S4 parameterization restricts the system matrices $A$ to be normal plus low-rank, allowing for stable diagonalization of the dynamics. Under this parameterization, authors design memory and computationally efficient methods that are also numerically stable. The S4 model has been further streamlined in later works. [GGB22] simplify the S 4 parameterization to diagonal system matrices, and shows that the diagonal state-space model (DSS) is competitive with S4 on several benchmarks. [SWL23] propose the S5 architecture, which improves upon S4 in two directions: 1) instead of having independent SISO SSMs in the feature dimension, S5 has one MIMO DSS that produces vector-valued outputs; 2) S5 uses efficient parallel scans in place of convolutions, bypassing custom-designed algorithms for computing the convolutional filters. To improve the performance of SSMs on language modeling tasks, [DFS ${ }^{+}$22] develops the H3 layer by stacking two SSMs together. They identify two areas where SSMs underperform compared to the transformer: remembering earlier tokens and comparing tokens across the input sequence. The H3 layer includes a shift SSM, where the dynamics matrix is a shifting operator, and a DSS, with multiplicative interactions. The shift SSM enables the layer to store earlier tokens, while the multiplicative interaction allows for comparison (inner product) between tokens in a sequence. They also develop FFT algorithms with better hardware utilization, to close the speed gap between SSMs and Transformers. Motivated by the similarities between SSMs and RNNs, [OSG ${ }^{+}$23] investigate whether deep RNNs can recover the performance of deep SSMs, and provide an affirmative answer. The proposed RNN architecture is a deep model with stacked Linear Recurrent Unit (LRU) layers. Each LRU has linear recurrence specified by a complex diagonal matrix, learned with exponential parameterization and proper normalization techniques. The deep LRU architecture has comparable computational efficiency as SSMs and matches their performance on benchmarks that require long-term memory. However, the paper also shows that without the specific modifications on linear RNNS, namely the stable exponential parameterization, gamma normalization and ring initialization, LRU fails to learn on certain challenging long-context modeling tasks.\n```\n\n#### 3. State Space Models as Foundation Models: A Control Theoretic Overview (Avg. Score: 0.95)\n\n*Carmen Amo Alonso, Jerome Sieber, M. Zeilinger*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 5  (*Influential: 0*)\n\n**TL;DR:** A systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective is provided, and a comparative analysis of these models is presented, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n**Abstract:** In recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences.\n\n##### *Relevant Chunk: No. 1/27 (Score: 0.95)*\n\n```\n# State Space Models as Foundation Models: A Control Theoretic Overview \n\nCarmen Amo Alonso*, Jerome Sieber*, and Melanie N. Zeilinger\n\n\n#### Abstract\n\nIn recent years, there has been a growing interest in integrating linear state-space models (SSM) in deep neural network architectures of foundation models. This is exemplified by the recent success of Mamba, showing better performance than the state-of-the-art Transformer architectures in language tasks. Foundation models, like e.g. GPT-4, aim to encode sequential data into a latent space in order to learn a compressed representation of the data. The same goal has been pursued by control theorists using SSMs to efficiently model dynamical systems. Therefore, SSMs can be naturally connected to deep sequence modeling, offering the opportunity to create synergies between the corresponding research areas. This paper is intended as a gentle introduction to SSM-based architectures for control theorists and summarizes the latest research developments. It provides a systematic review of the most successful SSM proposals and highlights their main features from a control theoretic perspective. Additionally, we present a comparative analysis of these models, evaluating their performance on a standardized benchmark designed for assessing a model's efficiency at learning long sequences. Index Terms-Machine learning, Linear systems, Timevarying systems. ## I. INTRODUCTION\n\nRecently, foundation models have become central to the field of artificial intelligence. These models are large-scale learning models that are initially pretrained on extensive datasets, and subsequently fine-tuned for specific tasks. The term foundation models highlights these models' capability to learn and effectively generalize across a wide array of modalities, encompassing language, audio, images, video, genomics, and more. At their core, the predominant architecture for foundation models is the Transformer [1]. This architecture, based on the attention mechanism, allows to efficiently process information and model global dependencies in complex data; but it suffers from two main limitations. One is computational complexity: it requires the complete sequence to be fed into the model every time an output is generated, which results in poor scalability with the time horizon window ${ }^{1}$ and therefore poor performance in long context tasks [2]. The other limitation is explainability: despite its simple mathematical representation, it is currently not possible to interpret or understand the choice of outputs made by the Transformer [3]. Efforts to address the scalability challenges of Transformers have led to various architectural variants that still leverage the merits of the\n\n[^0]attention mechanism. Examples of such variants are the Longformer [4], BigBird [5], the Reformer [6], the Performer [7], and approaches leveraging Axial Attention [8]. However, despite extensive research on these fronts, the proposed solutions often degrade the inherent merits of the architecture or fail to perform well in practice [2]. A recent and promising research avenue proposes to fully replace the attention mechanism with a different representation based on State Space Models (SSM). The advantage of the SSM representation lies in its recurrent nature, where only the latest input has to be passed to the model since the state is able to capture information about past inputs. Moreover, due to their mathematical structure, they are amenable to computationally efficient training and inferencein contrast to their predecessors, recurrent neural networks (RNNs) [9]. This new family of SSM-based architectures has been shown to beat Transformers in long-context tasks such as the Long Range Arena (LRA) benchmark [2], and recent proposals such as Mamba [10] exhibit performance and computational efficiency superior to state-of-the-art Transformers on long-context tasks. These results highlight the potential of SSMs to overcome many of the current limitations of Transformers. Although SSMs show great promise to serve as foundation models, most of the existing literature on SSMs focuses on providing performant architectures and efficient implementations. Despite the clear connection with control theory, in particular linear systems theory, to date a principled understanding of these models is lacking, and most design choices are motivated from an empirical performance rather than a systematic system theoretical viewpoint. There is large potential in leveraging existing system theoretic results and analysis to complement current implementations and enhance explainability, design and performance. Towards this goal, the aim of this paper is to provide an overview of state-of-the-art SSMs from a control theoretical perspective. In Section $\\Pi$, we provide an overview of the essential components and considerations in SSMs. In Section III we review the most relevant SSM proposals to date. Since these models were primarily motivated by their ability to handle long contexts, we present the first performance comparison to date on the LRA benchmark in Section IV. Lastly, we end in Section V with concluding remarks and open research questions that could help advance SSMs and cross-pollinate the fields of foundation models and systems and control theory. ## II. State Space Models\n\nWe first present a generic language modelling task to define the learning goal of a foundation model. Then, we give an overview of the state space model architecture, mathematical structure, and computational considerations that guide the SSMs introduced in the literature. ## A. Learning setup\n\nA foundation model, such as those used in language modeling, can be seen as a map between input and output signals, i.e.,\n\n$$\ny(k)=f(u(k), \\ldots, u(k-T) ; \\theta)\n$$\n\nwhere at each time $k$, the output $y(k)$ is produced after evaluating an input signal of length $k-T$, i.e., $u(k), \\ldots, u(k-$ $T)$, and a set of parameters $\\theta$. The parameters $\\theta$ are task dependent, and can be fine-tuned accordingly. Since the search space of general $f(\\cdot ; \\theta)$ is too broad, different parameterizations of $f(\\cdot ; \\theta)$ can be used to render the problem tractable. For instance, the model $f(\\cdot ; \\theta)$ can consist of multiple stacked models like e.g. the Transformer or more recently SSMs. The architectural choice of $f(\\cdot ; \\theta)$ is a fundamental factor in determining the success of the model at effectively learning structure from data. The goal of a foundation model used as large language model is to learn a compressed representation of structure present in language in order to perform tasks like machine translation or human-level conversations (e.g. ChatGPT). To learn such a representation the parameterized model $f(\\cdot ; \\theta)$ is presented with input-output pairs $(u(k), y(k)) \\forall k$, where $\\theta$ represents the parameters. The parameters $\\theta$ are then iteratively updated to minimize a loss function $\\mathscr{L}(\\cdot)$, i.e., iteratively solving the following optimization problem\n\n$$\n\\min _{\\theta} \\mathscr{L}(y-f(u ; \\theta))\n$$\n\nFor a language model the inputs $u$ are tokenized ${ }^{2}$ sentences and the outputs $y$ are a shifted version of the same inputs, i.e., an auto-regressive setup. ## B. Parametrization\n\nLet us consider the following continuous-time linear system with dynamics\n\n$$\n\\begin{aligned}\n& \\dot{x}(t)=A x(t)+B u(t) \\\\\n& y(t)=C x(t)+D u(t)\n\\end{aligned}\n$$\n\nwhere $x \\in \\mathbb{C}^{p}$ represents the complex-valued state, $u, y \\in \\mathbb{R}^{q}$ are the input and the output, respectively, and $t$ denotes the continuous-time index. We note that the input fed into the system denoted as $u$ is not a control input; it is seen as an exogenous input exciting the system (3). This choice of notation is made to maintain consistency with the corresponding literature. $A, B, C, D$ are complex-valued matrices of appropriate dimensions and in representation (3), these matrices\n\n[^1]are assumed to be time-invariant. When considering their time-varying version, a time sub-index would be appended, i.e., $A_{t}, B_{t}, C_{t}, D_{t}$. In the SSM literature, system (3) is used as a black-box representation in a foundation model. Here, the exogenous input $u(t)$ represents a signal or input token fed into the model at a given time $t$. The state $x(t)$ represents the hidden state that stores the relevant information about the current and previous inputs up to time $t$, and $y(t)$ is the output of the model at time $t$. In a learning setup, the matrices $A, B, C, D$ are parameters, which are commonly learned via stochastic gradient descent. Since computational efficiency and initialization are essential aspects in this framework, the dynamic matrix $A$ is often assumed to have a particular structure. As such, SSMs are often referred to as Structured SSMs. Assumption 2.1: The dynamic matrix in dynamics (3) has a diagonal structure, i.e., $A=\\operatorname{diag}\\left(\\lambda_{1}, \\ldots, \\lambda_{p}\\right)$ with $\\lambda_{i} \\in \\mathbb{C} \\forall i$. Although initial proposals [11], [12] deviate slightly from Assumption 2.1, most of the Structured SSMs literature assumes a diagonal $A$ matrix. Specific choices will be discussed in Section III\n\n## C. Discretization\n\nIn order to implement a SSM, a discrete-time version of system (3) is used. Hence, the implementation of system (3) in discrete-time is\n\n$$\n\\begin{aligned}\nx(k+1) & =\\bar{A} x(k)+\\bar{B} u(k) \\\\\ny(k) & =\\bar{C} x(k)+\\bar{D} u(k)\n\\end{aligned}\n$$\n\nwhere $\\bar{A}, \\bar{B}, \\bar{C}, \\bar{D}$ are the discrete-time dynamic matrices discretized with time-step $\\Delta \\in \\mathbb{R}$, possibly with complexvalued components, and $k$ denotes the discrete-time index. The choice of discretization scheme chosen varies widely among the proposed models in the SSM literature, and an overview is presented in Section III\nWe note that it is also possible to directly start from a discrete-time model as in equation (4), oblivious to its continuous-time representation (3). However, in most of the SSM literature, a continuous-time view of the dynamics is preferred in order to better motivate the choice of initialization for the dynamical matrices[13]. ## D. Structure and Initialization\n\nSince the dynamics (3) are being learned via gradient descent, initialization of the parameters was found to be of crucial importance. In particular, the initial values of matrix $A$ have a significant impact on the performance after training: on a simple classification task, performance increases from $67 \\%$ when $A$ is randomly initialized, to $80 \\%$ when $A$ is initialized using a principled strategy [12, Section 4.4]. Different strategies and parametrizations have been proposed in order to achieve a successful initialization, i.e. an initialization that results in the state $x(k)$ being able to capture the recent history of the inputs $u(k), \\ldots, u(k-T)$ for some time horizon $T$. This property is referred to as memory in the standard SSM literature. As is well-known in control\ntheory, the memory of system (4) is directly linked to the eigenvalues of matrix $A$. Lemma 2.2: (Informal) A dynamical system with dynamics (4) has long-range memory, i.e., captures information from past inputs, if the eigenvalues of $A$ are inside the unit circle and very close to the unit circumference, i.e. $|\\operatorname{eig}(A)| \\leq 1$ and $|\\operatorname{eig}(A)| \\approx 1 \\forall \\operatorname{eig}(A)$. Hence, the various initialization schemes presented in the SSM literature aim to ensure that the modulo of the eigenvalues of the learned $A$ matrix is approximately equal to (but not bigger than) 1 . For the initialization of the other matrices, i.e., $B, C$, and $D$, standard initialization methods are used, e.g., Glorot [14] or LeCun [15], which essentially draw the initial values from a transformed uniform or normal distribution. Therefore, we omit the initialization details of $B, C$, and $D$ in the following and refer the reader to the original papers [14], [15]. ## E. Implementation\n\nOne of the major challenges addressed in the SSM literature is how to efficiently learn (training time) and deploy (inference time) the recurrence (4). At inference time, a causal representation is needed since the model does not have access to excitation inputs beyond the current time step. For this reason, the recurrent representation (4) is directly used starting with an initial excitation $u(1)$ and zero initial state $x(1)=0$. In order to speed up this process, parallel scans algorithms [16] are used that efficiently compute the recurrence by computing each output component in parallel and caching intermediate results. During training, it is possible (and desirable) to use a noncausal representation since input-output pairs $(u(k), y(k))$ are available for all $k$. Different techniques have been proposed in the literature. Some of the architectures can take advantage of parallel scan algorithms and use the recurrent representation from equation (4). Some other architectures rely on the convolutional representation of system (4), i.e.,\n\n$$\ny(k)=\\sum_{\\tau=0}^{k} \\bar{C} \\bar{A}^{k-\\tau} \\bar{B} u(\\tau)\n$$\n\nThis convolutional representation allows for faster learning because the complete input sequence $u(k) \\forall k$ can be passed through the model in one step. In terms of learning algorithms, SSM models are commonly trained using a standard stochastic gradient descent variation, i.e. Adam [17], and backpropagation [9]. Additionally, they can utilize the same heuristic methods to improve training as other deep-learning models, e.g., dropout or normalization [9]. ## F. Scaffolding and Layers\n\nAlthough learning the dynamics in equation (3) is a major focus of SSMs, these dynamics are not simply implemented in isolation. In fact, pre-processing of the input $u$ and postprocessing of the output $y$ is necessary to ensure good performance. In this paper, we refer to the algebraic operations of pre- and post-processing as the scaffolding surrounding\nA\n\n![](https://cdn.mathpix.com/cropped/2024_09_17_0018655aa9d43ff9f8d3g-3.jpg?height=776&width=874&top_left_y=211&top_left_x=1078)\n\nFig. 1: A. General scaffolding of a SSM. The dynamical model (4) is represented in green. The input to the SSM is pre-processed and forked off in a skip connection (lower signal). The nature of the pre-processing map (linear or nonlinear) depends on the specific scaffolding. The output of the recursion is then post-processed with a nonlinear gate. B. Overall architecture of a SSM. Each of the SSMs including its scaffolding (Fig. 1.A.) is structured in a layered fashion, where the output from one layer is the input to the next. the SSM computation in dynamics (4). A general overview of the architecture used in SSMs is provided in Figure 1. A collection of different scaffolding choices have been proposed in the literature, ranging from standard multilayer perceptron (MLP) choices to gating operations, as defined in Definition 2.3. In general, a linear or nonlinear map is performed on the input $\\bar{u}$ before it is fed into system (4). Once the output $y$ has been computed, a gating operation is generally performed to control the flow of information from the input $\\tilde{u}$ to the output $\\tilde{y}$. Intuitively, the gate $g(\\tilde{y}, \\tilde{u})$ controls which outputs $\\tilde{y}$ are set to zero based on the inputs $\\tilde{u}$ via the softmax operation. Definition 2.3: Given two vectors $x_{1}, x_{2} \\in \\mathbb{R}^{p}$, a gating operation is defined as $g\\left(x_{1}, x_{2}\\right):=x_{1} \\odot \\sigma\\left(W x_{2}\\right)$, where $W \\in \\mathbb{R}^{p \\times p}$, $\\odot$ is the element-wise multiplication, and $\\sigma$ is the softmax operation ${ }^{3}$\n\nAs is common practice in deep learning, several layers of SSMs (dynamics (3) and accompanying scaffolding) are stacked together, where each of them processes the output of the previous layer as its input, which is then fed into the next layer. This is possible since input $y$ and output $u$ are of the same dimension $\\mathbb{R}^{q}$. For example on smaller tasks like e.g. the LRA benchmark [2], a SSM is composed of 6 structurally-identical layers (with different dynamic matri-\n\n[^2]ces), and the size of the systems ranges in $p \\in[64,512], q \\in$ [32, 1024]. For language modelling the number of layers and system size can be significantly larger.\n```\n\n#### 4. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.92)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.92)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n\n\n---\n## Found 6 related papers from 1 external sources\n\n\n\nYour 3 raw search queries input to the search frame: spectral SSM training dynamics, hardware efficiency SSM, hierarchical model evaluation\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Papers with Code\n\n#### 1. Dynamics-based machine learning of transitions in Couette flow\n\n*From Search Query: spectral SSM training dynamics*\n\n*Anonymous*\n\n**Abstract:** We derive low-dimensional, data-driven models for transitions among exact coherent states (ECSs) in one of the most studied canonical shear flows, the plane Couette flow. These one- or two-dimensional nonlinear models represent the leading-order reduced dynamics on attracting spectral submanifolds (SSMs), which we construct using the recently developed SSMLearn algorithm from a small number of simulated transitions. We find that the energy input and output rates provide efficient parametrizations for the most important SSMs. By restricting the dynamics to these SSMs, we obtain reduced-order models that also reliably predict nearby, off-SSM transitions that were not used in their training.\n\n**Published:** 2022-03-24\n\n\n\n#### 2. Data-Driven Modeling and Prediction of Non-Linearizable Dynamics via Spectral Submanifolds\n\n*From Search Query: spectral SSM training dynamics*\n\n*George Haller, Kerstin Avila, Bastian B\u00e4uerlein, Joar Ax\u00e5s, Mattia Cenedese*\n\n**Abstract:** We develop a methodology to construct low-dimensional predictive models from data sets representing essentially nonlinear (or non-linearizable) dynamical systems with a hyperbolic linear part that are subject to external forcing with finitely many frequencies. Our data-driven, sparse, nonlinear models are obtained as extended normal forms of the reduced dynamics on low-dimensional, attracting spectral submanifolds (SSMs) of the dynamical system. We illustrate the power of data-driven SSM reduction on high-dimensional numerical data sets and experimental measurements involving beam oscillations, vortex shedding and sloshing in a water tank. We find that SSM reduction trained on unforced data also predicts nonlinear response accurately under additional external forcing.\n\n**Published:** 2022-01-13\n\n\n\n#### 3. Mamba: Linear-Time Sequence Modeling with Selective State Spaces\n\n*From Search Query: hardware efficiency SSM*\n\n*Tri Dao, Albert Gu*\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n**Published:** 2023-12-01\n\n\n\n#### 4. Hungry Hungry Hippos: Towards Language Modeling with State Space Models\n\n*From Search Query: hardware efficiency SSM*\n\n*Christopher R\u00e9, Atri Rudra, Armin W. Thomas, Khaled K. Saab, Daniel Y. Fu, Tri Dao*\n\n**Abstract:** State space models (SSMs) have demonstrated state-of-the-art sequence modeling performance in some modalities, but underperform attention in language modeling. Moreover, despite scaling nearly linearly in sequence length instead of quadratically, SSMs are still slower than Transformers due to poor hardware utilization. In this paper, we make progress on understanding the expressivity gap between SSMs and attention in language modeling, and on reducing the hardware barrier between SSMs and attention. First, we use synthetic language modeling tasks to understand the gap between SSMs and attention. We find that existing SSMs struggle with two capabilities: recalling earlier tokens in the sequence and comparing tokens across the sequence. To understand the impact on language modeling, we propose a new SSM layer, H3, that is explicitly designed for these abilities. H3 matches attention on the synthetic languages and comes within 0.4 PPL of Transformers on OpenWebText. Furthermore, a hybrid 125M-parameter H3-attention model that retains two attention layers surprisingly outperforms Transformers on OpenWebText by 1.0 PPL. Next, to improve the efficiency of training SSMs on modern hardware, we propose FlashConv. FlashConv uses a fused block FFT algorithm to improve efficiency on sequences up to 8K, and introduces a novel state passing algorithm that exploits the recurrent properties of SSMs to scale to longer sequences. FlashConv yields 2$\\times$ speedup on the long-range arena benchmark and allows hybrid language models to generate text 2.4$\\times$ faster than Transformers. Using FlashConv, we scale hybrid H3-attention language models up to 2.7B parameters on the Pile and find promising initial results, achieving lower perplexity than Transformers and outperforming Transformers in zero- and few-shot learning on a majority of tasks in the SuperGLUE benchmark.\n\n**Published:** 2022-12-28\n\n\n\n#### 5. Construction and Quality Evaluation of Heterogeneous Hierarchical Topic Models\n\n*From Search Query: hierarchical model evaluation*\n\n*Anton Belyy*\n\n**Abstract:** In our work, we propose to represent HTM as a set of flat models, or layers,\nand a set of topical hierarchies, or edges. We suggest several quality measures\nfor edges of hierarchical models, resembling those proposed for flat models. We\nconduct an assessment experimentation and show strong correlation between the\nproposed measures and human judgement on topical edge quality. We also\nintroduce heterogeneous algorithm to build hierarchical topic models for\nheterogeneous data sources. We show how making certain adjustments to learning\nprocess helps to retain original structure of customized models while allowing\nfor slight coherent modifications for new documents. We evaluate this approach\nusing the proposed measures and show that the proposed heterogeneous algorithm\nsignificantly outperforms the baseline concat approach. Finally, we implement\nour own ESE called Rysearch, which demonstrates the potential of ARTM approach\nfor visualizing large heterogeneous document collections.\n\n**Published:** 2018-11-07\n\n\n\n#### 6. Hierarchical Prompting Taxonomy: A Universal Evaluation Framework for Large Language Models\n\n*From Search Query: hierarchical model evaluation*\n\n*Aman Chadha, Vinija Jain, Ashutosh Kumar, Sankalp KJ, Devichand Budagam*\n\n**Abstract:** Assessing the effectiveness of large language models (LLMs) in addressing diverse tasks is essential for comprehending their strengths and weaknesses. Conventional evaluation techniques typically apply a single prompting strategy uniformly across datasets, not considering the varying degrees of task complexity. We introduce the Hierarchical Prompting Taxonomy (HPT), a taxonomy that employs a Hierarchical Prompt Framework (HPF) composed of five unique prompting strategies, arranged from the simplest to the most complex, to assess LLMs more precisely and to offer a clearer perspective. This taxonomy assigns a score, called the Hierarchical Prompting Score (HP-Score), to datasets as well as LLMs based on the rules of the taxonomy, providing a nuanced understanding of their ability to solve diverse tasks and offering a universal measure of task complexity. Additionally, we introduce the Adaptive Hierarchical Prompt framework, which automates the selection of appropriate prompting strategies for each task. This study compares manual and adaptive hierarchical prompt frameworks using four instruction-tuned LLMs, namely Llama 3 8B, Phi 3 3.8B, Mistral 7B, and Gemma 7B, across four datasets: BoolQ, CommonSenseQA (CSQA), IWSLT-2017 en-fr (IWSLT), and SamSum. Experiments demonstrate the effectiveness of HPT, providing a reliable way to compare different tasks and LLM capabilities. This paper leads to the development of a universal evaluation metric that can be used to evaluate both the complexity of the datasets and the capabilities of LLMs. The implementation of both manual HPF and adaptive HPF is publicly available.\n\n**Published:** 2024-06-18\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models using spectral state space models (SSMs) and hierarchical architectures, here are some key points and references that align with the specified details:\n\n## Training Dynamics and Stability of Spectral SSMs\n\n- The paper by Yu et al. (2023) discusses the robustification of state-space models for long sequences via approximate diagonalization, which is relevant to the stability and training dynamics of spectral SSMs. They propose a \"perturb-then-diagonalize\" (PTD) methodology based on pseudospectral theory, which helps in stabilizing the models and ensuring robust performance.\n\n## Hardware Efficiency Comparisons Between Different SSM Architectures\n\n- The work by Dao et al. (2022) on \"FlashAttention\" provides insights into hardware-efficient implementations, particularly focusing on IO-aware designs. This paper discusses how tiling can reduce memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, which is crucial for efficient hardware utilization.\n\n## Empirical Evaluations of Hierarchical SSM Models\n\n- The abstract from the BYU Statistics Department seminar mentions latent trajectory models for spatio-temporal ecosystem dynamics, which includes hierarchical state space models. This work proposes an alternative class of latent trajectory models that represent ecological processes using dynamic state probabilities, demonstrating empirical evaluations of hierarchical models in a different context. However, for language models, similar hierarchical structures can be evaluated for their performance and efficiency.\n\n## Practical Implementations of Combined Spectral-Hierarchical Systems\n\n- While the provided sources do not directly discuss the combination of spectral and hierarchical SSMs in the context of language models, the references from Agarwal et al. (2023) and Yu et al. (2023) suggest that combining spectral filtering with hierarchical architectures could leverage the benefits of both approaches. Spectral SSMs offer provable robustness and stability, while hierarchical models provide multi-scale temporal processing and efficient state management.\n\n## Memory Optimization Techniques in Hierarchical Architectures\n\n- The \"FlashAttention\" paper by Dao et al. (2022) is particularly relevant here, as it discusses memory-efficient exact attention algorithms. The use of tiling to reduce memory reads/writes is a key technique for optimizing memory usage in hierarchical architectures.\n\n### Additional Insights and Future Search Directions\n\n- For practical implementations, future searches should focus on papers that integrate spectral and hierarchical approaches, such as combining fixed convolutional filters with multi-scale temporal processing.\n- Performance comparisons with existing models, such as transformer-based architectures, would be crucial to evaluate the efficacy of these new designs.\n- Training stability analysis, as discussed in Yu et al. (2023), is essential to ensure that the models generalize well and are robust to different input sequences.\n- Hardware optimization techniques, such as IO-aware designs and parallel processing optimizations, should be explored further to ensure efficient implementation on various hardware platforms.\n\nBy focusing on these areas, researchers can develop more efficient, robust, and scalable autoregressive language models that leverage the strengths of both spectral and hierarchical state space models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights about spectral filtering and hierarchical state space models:\n\na) Spectral State Space Models:\n- Provide provable robustness properties independent of spectrum\n- Use fixed convolutional filters that don't require learning\n- Show theoretical benefits for long-range memory tasks\n- Offer stability guarantees through spectral filtering\n\nb) Hardware and Implementation Considerations:\n- Need for efficient memory management in hierarchical structures\n- Importance of IO-aware implementations\n- Trade-offs between memory usage and computational complexity\n- Potential for parallel processing optimizations\n\nc) Hierarchical Processing:\n- Benefits of multi-scale temporal processing\n- Memory efficiency through hierarchical state management\n- Importance of cross-layer connections\n- Stability considerations in deep hierarchies\n\n2. Useful References with Excerpts:\n\nFrom Agarwal et al. (2023):\n\"Spectral state space models have two primary advantages. First, they have provable robustness properties as their performance depends on neither the spectrum of the underlying dynamics nor the dimensionality of the problem. Second, these models are constructed with fixed convolutional filters that do not require learning\"\n\nFrom Yu et al. (2023):\n\"We propose a generic, backward-stable 'perturb-then-diagonalize' (PTD) methodology, which is based on the pseudospectral theory of non-normal operators, and which may be interpreted as the approximate diagonalization of the non-normal matrices defining SSMs\"\n\nFrom Dao et al. (2022):\n\"IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM\"\n\n3. Key Insights:\n\na) Spectral Filtering Benefits:\n- Fixed filters reduce parameter count\n- Provable robustness properties\n- Theoretical guarantees for long-range memory\n- Stability independent of spectrum\n\nb) Implementation Challenges:\n- Need for efficient spectral operations\n- Memory-computation trade-offs\n- Hardware considerations for parallel processing\n- IO-aware design importance\n\nc) Integration Opportunities:\n- Combining spectral and hierarchical approaches\n- Cross-layer information flow\n- Adaptive state management\n- Hardware-efficient implementations\n\n4. Future Search Plan:\n\nNeed additional information on:\n1. Practical implementations of spectral-hierarchical architectures\n2. Performance comparisons with existing models\n3. Training stability analysis\n4. Hardware optimization techniques\n\nNext search queries should focus on:\n1. Training dynamics of spectral SSMs\n2. Hardware efficiency comparisons\n3. Empirical evaluations of hierarchical models\n\n5. References:\n\n[1] Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n[2] Yu, A., Nigmetov, A., Morozov, D., Mahoney, M. W., & Erichson, N. B. (2023). Robustifying State-space Models for Long Sequences via Approximate Diagonalization. arXiv preprint.\n\n[3] Dao, T., Fu, D. Y., Ermon, S., Rudra, A., & R\u00e9, C. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[4] Wang, S., & Xue, B. (2023). State-space models with layer-wise nonlinearity are universal approximators with exponential decaying memory. Neural Information Processing Systems.\n\n[5] Massaroli, S., et al. (2023). Laughing Hyena Distillery: Extracting Compact Recurrences From Convolutions. Neural Information Processing Systems."
        }
    ],
    "rating": 4.1,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.974568,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 1.0003799999999998,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "HierarchicalMambaLayer",
    "proposal": "# SpectralHierarchicalMamba: Enhancing HierarchicalMambaLayer with Spectral Filtering and Adaptive Memory\n\n## Motivation\n\nModern language models face significant challenges in efficiently modeling long-range dependencies while maintaining computational efficiency and training stability. While the HierarchicalMambaLayer has shown promise in capturing multi-scale temporal patterns, there remain opportunities to enhance its robustness and efficiency through spectral methods and adaptive memory mechanisms.\n\nThe motivation behind SpectralHierarchicalMamba is to integrate spectral filtering techniques with hierarchical state space modeling to achieve:\n- Improved stability through theoretically-grounded fixed spectral filters\n- Enhanced long-range dependency modeling via hierarchical processing\n- Efficient computation through structured state representations\n- Robust performance independent of sequence spectral properties\n\n## Related Work\n\n### Spectral State Space Models\nAgarwal et al. (2023) demonstrated that spectral filtering in state space models provides:\n- Provable robustness properties independent of underlying dynamics\n- Fixed convolutional filters that don't require learning\n- Strong theoretical benefits for long-range memory tasks\n\n### Hierarchical State Space Models\nBhirangi et al. (2024) showed that:\n- Stacking structured SSMs creates effective temporal hierarchies\n- Multi-scale processing improves both local and global feature capture\n- Hierarchical structures help manage long-range dependencies\n\n### Dense Hidden Connections\nHe et al. (2024) introduced:\n- Enhanced information flow between layers in SSMs\n- Retention of fine-grained information crucial for output quality\n- Maintained training parallelizability and inference efficiency\n\n## Problem Analysis\n\n### Key Challenges\n\n1. **Stability and Robustness**:\n   - Traditional SSMs can be sensitive to spectral properties\n   - Training stability often requires complex parameterizations\n   - Performance can degrade with long sequences\n\n2. **Long-Range Dependencies**:\n   - Capturing dependencies across different temporal scales\n   - Maintaining efficient computation with increasing sequence length\n   - Balancing local and global information processing\n\n3. **Computational Efficiency**:\n   - Managing memory requirements for long sequences\n   - Maintaining linear complexity while improving expressiveness\n   - Efficient hardware utilization\n\n### Mathematical Framework\n\nLet X \u2208 \u211d^(B\u00d7L\u00d7D) be the input sequence, where:\n- B is batch size\n- L is sequence length\n- D is embedding dimension\n\nThe spectral filtering operation can be defined as:\n\n\\[\nY_s = \\sum_{j=1}^k M_j(\\sum_i \\phi_j(i) \\cdot X_{t-i})\n\\]\n\nwhere:\n- \u03c6_j are fixed spectral filters\n- M_j are learnable matrices\n- k is the number of filters\n\nThe hierarchical state update becomes:\n\n\\[\nh_t^{(l)} = A^{(l)}h_{t-1}^{(l)} + B^{(l)}(Y_s)_t + C^{(l)}h_t^{(l-1)}\n\\]\n\nwhere:\n- l denotes the hierarchical layer\n- A^(l), B^(l), C^(l) are layer-specific parameters\n- h_t^(l) is the state at layer l and time t\n\n## Design Plan\n\n### Modifications to HierarchicalMambaLayer\n\n1. **Spectral Filtering Integration**:\n   - Add spectral decomposition layer before state processing\n   - Implement fixed convolutional filters based on spectral theory\n   - Maintain linear complexity through efficient filter design\n\n2. **Hierarchical Processing Enhancement**:\n   - Introduce cross-layer state connections\n   - Implement adaptive state updates based on spectral components\n   - Optimize information flow between hierarchical levels\n\n3. **Memory Management**:\n   - Design efficient state compression mechanism\n   - Implement selective state updates based on spectral properties\n   - Optimize memory access patterns for hardware efficiency\n\n### Mathematical Formulation\n\n1. **Spectral Decomposition**:\n\\[\nX_{spec} = \\sum_{j=1}^k \\phi_j \\otimes X\n\\]\n\n2. **State Update**:\n\\[\nh_t^{(l)} = \\sigma(W_h^{(l)}h_{t-1}^{(l)} + W_x^{(l)}X_{spec} + W_c^{(l)}h_t^{(l-1)})\n\\]\n\n3. **Output Generation**:\n\\[\nY = \\sum_{l=1}^L W_o^{(l)}h_t^{(l)}\n\\]\n\n## Implementation Guidelines\n\n### Pseudo-code\n\n```python\nclass SpectralHierarchicalMamba(nn.Module):\n    def __init__(self, dim, num_layers, num_filters):\n        super().__init__()\n        self.spectral_filters = self._init_spectral_filters(num_filters)\n        self.layers = nn.ModuleList([\n            HierarchicalLayer(dim, num_filters)\n            for _ in range(num_layers)\n        ])\n        \n    def _init_spectral_filters(self, num_filters):\n        # Initialize fixed spectral filters based on theory\n        filters = compute_spectral_basis(num_filters)\n        return nn.Parameter(filters, requires_grad=False)\n        \n    def forward(self, x):\n        # Spectral decomposition\n        x_spec = self._apply_spectral_filters(x)\n        \n        # Hierarchical processing\n        states = []\n        for layer in self.layers:\n            x_spec, state = layer(x_spec, states[-1] if states else None)\n            states.append(state)\n            \n        return x_spec\n\nclass HierarchicalLayer(nn.Module):\n    def __init__(self, dim, num_filters):\n        super().__init__()\n        self.state_update = nn.Linear(dim * 2, dim)\n        self.output_proj = nn.Linear(dim, dim)\n        \n    def forward(self, x, prev_state=None):\n        # State update with spectral components\n        if prev_state is not None:\n            state = self.state_update(\n                torch.cat([x, prev_state], dim=-1)\n            )\n        else:\n            state = self.state_update(\n                torch.cat([x, torch.zeros_like(x)], dim=-1)\n            )\n            \n        # Output projection\n        out = self.output_proj(state)\n        return out, state\n```\n\n### Integration Steps\n\n1. **Spectral Filter Initialization**:\n   ```python\n   def compute_spectral_basis(num_filters):\n       # Compute orthogonal spectral basis\n       basis = torch.zeros(num_filters, num_filters)\n       for i in range(num_filters):\n           for j in range(num_filters):\n               basis[i,j] = compute_basis_function(i, j)\n       return basis\n   ```\n\n2. **State Management**:\n   ```python\n   def manage_states(states, max_states):\n       # Compress states if needed\n       if len(states) > max_states:\n           states = compress_states(states)\n       return states\n   ```\n\n3. **Hardware Optimization**:\n   ```python\n   def optimize_memory_access(x, block_size):\n       # Reorganize tensor for efficient memory access\n       return rearrange(x, 'b (n d) c -> b n d c', d=block_size)\n   ```\n\n## Theoretical Analysis\n\n### Stability Guarantees\n\nThe use of fixed spectral filters provides theoretical guarantees:\n1. Independence from spectral properties of input\n2. Bounded state magnitudes\n3. Stable gradient flow during training\n\n### Complexity Analysis\n\n- Time Complexity: O(L) for sequence length L\n- Memory Complexity: O(L) with efficient state compression\n- Computation: Parallelizable across batch and filter dimensions\n\n## Expected Benefits\n\n1. **Improved Stability**:\n   - Robust performance across different input distributions\n   - Stable training without complex parameterizations\n   - Better gradient flow through spectral decomposition\n\n2. **Enhanced Modeling Capability**:\n   - Better capture of long-range dependencies\n   - Improved multi-scale feature extraction\n   - More efficient information flow between layers\n\n3. **Computational Efficiency**:\n   - Linear complexity in sequence length\n   - Efficient memory utilization\n   - Hardware-friendly implementation\n\n## References\n\n1. Agarwal, N., Suo, D., Chen, X., & Hazan, E. (2023). Spectral State Space Models. arXiv preprint.\n\n2. Bhirangi, R., Wang, C., Pattabiraman, V., Majidi, C., Gupta, A., Hellebrekers, T., & Pinto, L. (2024). Hierarchical State Space Models for Continuous Sequence-to-Sequence Modeling. arXiv preprint.\n\n3. He, W., Han, K., Tang, Y., Wang, C., Yang, Y., Guo, T., & Wang, Y. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n4. Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n5. Hasani, R. M., Lechner, M., Wang, T. H., Chahine, M., Amini, A., & Rus, D. (2022). Liquid Structural State-Space Models. International Conference on Learning Representations.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_preview",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "Enhancing HierarchicalMambaLayer with spectral filtering and adaptive memory mechanisms to improve stability, long-range dependency modeling, and computational efficiency. The design integrates fixed spectral filters with hierarchical state processing to achieve robust sequence modeling while maintaining linear complexity.",
    "ideation": null,
    "modelname": "spectralhmamba",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of FFT implementation efficiency\n- Include specific strategies for hardware optimization\n- Add concrete examples of spectral filter design\n- Elaborate on memory management techniques\n\n2. Training Stability:\n- Add detailed analysis of potential training instabilities\n- Provide specific initialization strategies\n- Include gradient flow analysis\n- Consider adding regularization techniques\n\n3. Memory Optimization:\n- Develop more specific memory management strategies\n- Provide detailed analysis of cache efficiency\n- Include memory-computation trade-off analysis\n- Consider adding state compression techniques\n\n4. Hardware Efficiency:\n- Add hardware-specific optimization strategies\n- Include parallel processing guidelines\n- Provide performance analysis across different hardware\n- Consider specialized implementations for different platforms\n\n5. Empirical Validation:\n- Add theoretical performance bounds\n- Include complexity analysis for different sequence lengths\n- Provide comparative analysis with existing approaches\n- Consider adding ablation study guidelines\n\n6. Documentation:\n- Expand on implementation challenges\n- Add more detailed examples\n- Include failure mode analysis\n- Provide debugging guidelines",
    "user_input": ""
}