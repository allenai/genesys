{
    "variantname": "EventDDLerpLinear",
    "review": "The EventDDLerpLinear proposal presents an innovative approach to enhancing DDLerpLinear through event-driven updates and hierarchical state compression. After thorough analysis of the proposal and comparison with existing research, here is a comprehensive review:\n\nSTRENGTHS:\n\n1. Innovation and Theoretical Foundation:\n- Novel combination of event-driven updates with hierarchical compression\n- Well-grounded theoretical framework from spiking neural networks\n- Unique approach to selective computation\n- Clear differentiation from previous proposals (rwkv_lightning, rwkv6_sc)\n\n2. Design Clarity:\n- Well-structured mathematical formulation\n- Clear architectural components and interactions\n- Detailed implementation guidelines\n- Thoughtful consideration of hardware efficiency\n\n3. Efficiency Considerations:\n- Event-driven updates reduce unnecessary computation\n- Hierarchical compression improves memory efficiency\n- Selective computation enables better resource utilization\n- Hardware-aware implementation strategies\n\n4. Adaptability:\n- Dynamic threshold adaptation\n- Content-dependent state updates\n- Flexible compression levels\n- Hierarchical state representation\n\nCONCERNS:\n\n1. Training Stability:\n- Complex interaction between event detection and backpropagation\n- Potential challenges with gradient flow through selective updates\n- Stability of threshold adaptation\n- Impact of compression on training dynamics\n\n2. Memory Management:\n- Overhead from event detection mechanism\n- Memory requirements for hierarchical states\n- Cache pressure from state transitions\n- Storage requirements for multiple compression levels\n\n3. Implementation Complexity:\n- Complex coordination of multiple components\n- Threshold tuning requirements\n- Integration challenges with existing architecture\n- Potential debugging difficulties\n\n4. Scalability Considerations:\n- Impact on very large models unclear\n- Potential bottlenecks in distributed training\n- Memory scaling with sequence length\n- Communication overhead in parallel implementations\n\nCOMPARISON WITH EXISTING RESEARCH:\n\nThe proposal shows significant novelty compared to existing work:\n1. More sophisticated than SpikeGPT's basic event-driven approach\n2. More efficient than Sparse Modular Activation's fixed sparsity\n3. Novel integration of hierarchical compression with event-driven updates\n4. Unique approach to state management compared to SMR\n\nHowever, it shares some concepts with:\n1. Mamba's selective state propagation\n2. NISPA's stability-plasticity mechanisms\n3. Vision Mamba's efficient state representation",
    "search_stack": [
        {
            "ready": false,
            "query": "efficient state compression neural networks, hardware efficient attention mechanisms",
            "detail": "Find papers discussing efficient state compression techniques and hardware-efficient attention mechanisms, particularly focusing on:\n1. Methods for compressing and managing neural network states\n2. Hardware-optimized attention implementations\n3. Cache-friendly neural network architectures",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing efficient state compression techniques and hardware-efficient attention mechanisms, particularly focusing on:\n1. Methods for compressing and managing neural network states\n2. Hardware-optimized attention implementations\n3. Cache-friendly neural network architectures\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.97)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.97)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.91)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.91)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.84)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.84)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.75)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 27/46 (Score: 0.75)*\n\n```\nIn Advances in Neural Information Processing Systems, 2022. [50] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015. [51] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. Advances in neural information processing systems, 28, 2015. [52] Ramin Hasani, Mathias Lechner, Tsun-Huang Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [53] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770-778, 2016. [54] John L Hennessy and David A Patterson. Computer architecture: a quantitative approach.\n```\n\n#### 5. B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory (Avg. Score: 0.58)\n\n*L. Zancato, Arjun Seshadri, Yonatan Dukler, Aditya Golatkar, Yantao Shen, Benjamin Bowman, Matthew Trager, A. Achille, S. Soatto*\n\n**Published in:**  (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** N/A\n\n**Abstract:** We describe a family of architectures to support transductive inference by allowing memory to grow to a finite but a-priori unknown bound while making efficient use of finite resources for inference. Current architectures use such resources to represent data either eidetically over a finite span (\"context\"in Transformers), or fading over an infinite span (in State Space Models, or SSMs). Recent hybrid architectures have combined eidetic and fading memory, but with limitations that do not allow the designer or the learning process to seamlessly modulate the two, nor to extend the eidetic memory span. We leverage ideas from Stochastic Realization Theory to develop a class of models called B'MOJO to seamlessly combine eidetic and fading memory within an elementary composable module. The overall architecture can be used to implement models that can access short-term eidetic memory\"in-context,\"permanent structural memory\"in-weights,\"fading memory\"in-state,\"and long-term eidetic memory\"in-storage\"by natively incorporating retrieval from an asynchronously updated memory. We show that Transformers, existing SSMs such as Mamba, and hybrid architectures such as Jamba are special cases of B'MOJO and describe a basic implementation, to be open sourced, that can be stacked and scaled efficiently in hardware. We test B'MOJO on transductive inference tasks, such as associative recall, where it outperforms existing SSMs and Hybrid models; as a baseline, we test ordinary language modeling where B'MOJO achieves perplexity comparable to similarly-sized Transformers and SSMs up to 1.4B parameters, while being up to 10% faster to train. Finally, we show that B'MOJO's ability to modulate eidetic and fading memory results in better inference on longer sequences tested up to 32K tokens, four-fold the length of the longest sequences seen during training.\n\n##### *Relevant Chunk: No. 26/30 (Score: 0.58)*\n\n```\n[42] Terry A. Welch. A technique for high-performance data compression. Computer, 17(06):8-19, 1984 . [43] Norbert Wiener. Extrapolation, interpolation, and smoothing of stationary time series: with engineering applications. The MIT press, 1949. [44] William M Wonham. On a matrix riccati equation of stochastic control. SIAM Journal on Control, 6(4):681-697, 1968. [45] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2023. [46] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [47] Lili Yu, D\u00e1niel Simig, Colin Flaherty, Armen Aghajanyan, Luke Zettlemoyer, and Mike Lewis. Megabyte: Predicting million-byte sequences with multiscale transformers.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: efficient state compression neural networks, hardware efficient attention mechanisms\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Remember the Past: Distilling Datasets into Addressable Memories for Neural Networks\n\n*From Search Query: efficient state compression neural networks*\n\n*Zhiwei Deng, Olga Russakovsky*\n\n**Abstract:** We propose an algorithm that compresses the critical information of a large dataset into compact addressable memories. These memories can then be recalled to quickly re-train a neural network and recover the performance (instead of storing and re-training on the full original dataset). Building upon the dataset distillation framework, we make a key observation that a shared common representation allows for more efficient and effective distillation. Concretely, we learn a set of bases (aka ``memories'') which are shared between classes and combined through learned flexible addressing functions to generate a diverse set of training examples. This leads to several benefits: 1) the size of compressed data does not necessarily grow linearly with the number of classes; 2) an overall higher compression rate with more effective distillation is achieved; and 3) more generalized queries are allowed beyond recalling the original classes. We demonstrate state-of-the-art results on the dataset distillation task across six benchmarks, including up to 16.5% and 9.7% in retained accuracy improvement when distilling CIFAR10 and CIFAR100 respectively. We then leverage our framework to perform continual learning, achieving state-of-the-art results on four benchmarks, with 23.2% accuracy improvement on MANY. The code is released on our project webpage https://github.com/princetonvisualai/RememberThePast-DatasetDistillation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 78  (*Influential: 17*)\n\n#### 2. LegoNet: Efficient Convolutional Neural Networks with Lego Filters\n\n*From Search Query: efficient state compression neural networks*\n\n*Zhaohui Yang, Yunhe Wang, Chuanjian Liu, Hanting Chen, Chunjing Xu, Boxin Shi, Chao Xu, Chang Xu*\n\n**TL;DR:** A split-transform-merge strategy for an ef\ufb01cient convolution by exploiting intermediate Lego feature maps is developed and Inspired by network engineering, it is suggested that an ordinary \ufb01lter in the neural network can be upgraded to a sophisticated module as well.\n\n**Abstract:** This paper aims to build ef\ufb01cient convolutional neural networks using a set of Lego \ufb01lters. Many successful building blocks, e.g. inception and residual modules, have been designed to refresh state-of-the-art records of CNNs on visual recognition tasks. Beyond these high-level modules, we suggest that an ordinary \ufb01lter in the neural network can be upgraded to a sophisticated module as well. Filter modules are established by assem-bling a shared set of Lego \ufb01lters that are often of much lower dimensions. Weights in Lego \ufb01lters and binary masks to stack Lego \ufb01lters for these \ufb01lter modules can be simultaneously optimized in an end-to-end manner as usual. Inspired by network engineering, we develop a split-transform-merge strategy for an ef\ufb01cient convolution by exploiting intermediate Lego feature maps. The compression and acceleration achieved by Lego Networks using the proposed Lego \ufb01lters have been theoretically discussed. Experimental results on benchmark datasets and deep models demonstrate the advantages of the proposed Lego \ufb01lters and their potential real-world applications on mobile devices.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2019\n\n**Citations:** 33  (*Influential: 5*)\n\n#### 3. WoodFisher: Efficient Second-Order Approximation for Neural Network Compression\n\n*From Search Query: efficient state compression neural networks*\n\n*Sidak Pal Singh, Dan Alistarh*\n\n**TL;DR:** It is demonstrated that WoodFisher significantly outperforms popular state-of-the-art methods for one-shot pruning and can be extended to take into account first-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime.\n\n**Abstract:** Second-order information, in the form of Hessian- or Inverse-Hessian-vector products, is a fundamental tool for solving optimization problems. Recently, there has been significant interest in utilizing this information in the context of deep neural networks; however, relatively little is known about the quality of existing approximations in this context. Our work examines this question, identifies issues with existing approaches, and proposes a method called WoodFisher to compute a faithful and efficient estimate of the inverse Hessian. \nOur main application is to neural network compression, where we build on the classic Optimal Brain Damage/Surgeon framework. We demonstrate that WoodFisher significantly outperforms popular state-of-the-art methods for one-shot pruning. Further, even when iterative, gradual pruning is considered, our method results in a gain in test accuracy over the state-of-the-art approaches, for pruning popular neural networks (like ResNet-50, MobileNetV1) trained on standard image classification datasets such as ImageNet ILSVRC. We examine how our method can be extended to take into account first-order information, as well as illustrate its ability to automatically set layer-wise pruning thresholds and perform compression in the limited-data regime.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 137  (*Influential: 14*)\n\n#### 4. Gated Linear Attention Transformers with Hardware-Efficient Training\n\n*From Search Query: hardware efficient attention mechanisms*\n\n*Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, Yoon Kim*\n\n**TL;DR:** The resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer as well recent linear-time-inference baselines such as RetNet and Mamba on moderate-scale language modeling experiments.\n\n**Abstract:** Transformers with linear attention allow for efficient parallel training but can simultaneously be formulated as an RNN with 2D (matrix-valued) hidden states, thus enjoying linear-time inference complexity. However, linear attention generally underperforms ordinary softmax attention. Moreover, current implementations of linear attention lack I/O-awareness and are thus slower than highly optimized implementations of softmax attention. This work describes a hardware-efficient algorithm for linear attention that trades off memory movement against parallelizability. The resulting implementation, dubbed FLASHLINEARATTENTION, is faster than FLASHATTENTION-2 (Dao, 2023) as a standalone layer even on short sequence lengths (e.g., 1K). We then generalize this algorithm to a more expressive variant of linear attention with data-dependent gates. When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively against the LLaMA-architecture Transformer (Touvron et al., 2023) as well recent linear-time-inference baselines such as RetNet (Sun et al., 2023a) and Mamba (Gu&Dao, 2023) on moderate-scale language modeling experiments. GLA Transformer is especially effective at length generalization, enabling a model trained on 2K to generalize to sequences longer than 20K without significant perplexity degradations. For training speed, the GLA Transformer has higher throughput than a similarly-sized Mamba model.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 69  (*Influential: 12*)\n\n#### 5. Infusing Lattice Symmetry Priors in Attention Mechanisms for Sample-Efficient Abstract Geometric Reasoning\n\n*From Search Query: hardware efficient attention mechanisms*\n\n*Mattia Atzeni, Mrinmaya Sachan, Andreas Loukas*\n\n**TL;DR:** This work focuses on geometry priors and introduces LatFormer, a model that incorporates lattice symmetry priors in attention masks that provides preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Abstract:** The Abstraction and Reasoning Corpus (ARC) (Chollet, 2019) and its most recent language-complete instantiation (LARC) has been postulated as an important step towards general AI. Yet, even state-of-the-art machine learning models struggle to achieve meaningful performance on these problems, falling behind non-learning based approaches. We argue that solving these tasks requires extreme generalization that can only be achieved by proper accounting for core knowledge priors. As a step towards this goal, we focus on geometry priors and introduce LatFormer, a model that incorporates lattice symmetry priors in attention masks. We show that, for any transformation of the hypercubic lattice, there exists a binary attention mask that implements that group action. Hence, our study motivates a modification to the standard attention mechanism, where attention weights are scaled using soft masks generated by a convolutional network. Experiments on synthetic geometric reasoning show that LatFormer requires 2 orders of magnitude fewer data than standard attention and transformers. Moreover, our results on ARC and LARC tasks that incorporate geometric priors provide preliminary evidence that these complex datasets do not lie out of the reach of deep learning models.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 0*)\n\n#### 6. Efficient Attention via Control Variates\n\n*From Search Query: hardware efficient attention mechanisms*\n\n*Lin Zheng, Jianbo Yuan, Chong Wang, Lingpeng Kong*\n\n**TL;DR:** This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity.\n\n**Abstract:** Random-feature-based attention (RFA) is an efficient approximation of softmax attention with linear runtime and space complexity. However, the approximation gap between RFA and conventional softmax attention is not well studied. Built upon previous progress of RFA, we characterize this gap through the lens of control variates and show that RFA can be decomposed into a sum of multiple control variate estimators for each element in the sequence. This new framework reveals that exact softmax attention can be recovered from RFA by manipulating each control variate. Besides, it allows us to develop a more flexible form of control variates, resulting in a novel attention mechanism that significantly reduces the approximation gap while maintaining linear complexity. Extensive experiments demonstrate that our model outperforms state-of-the-art efficient attention mechanisms on both vision and language tasks.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 1*)\n\n### 4 related papers from Papers with Code\n\n#### 1. EPTQ: Enhanced Post-Training Quantization via Hessian-guided Network-wise Optimization\n\n*From Search Query: efficient state compression neural networks*\n\n*Hai Victor Habi, Elad Cohen, Arnon Netzer, Ofir Gordon*\n\n**Abstract:** Quantization is a key method for deploying deep neural networks on edge devices with limited memory and computation resources. Recent improvements in Post-Training Quantization (PTQ) methods were achieved by an additional local optimization process for learning the weight quantization rounding policy. However, a gap exists when employing network-wise optimization with small representative datasets. In this paper, we propose a new method for enhanced PTQ (EPTQ) that employs a network-wise quantization optimization process, which benefits from considering cross-layer dependencies during optimization. EPTQ enables network-wise optimization with a small representative dataset using a novel sample-layer attention score based on a label-free Hessian matrix upper bound. The label-free approach makes our method suitable for the PTQ scheme. We give a theoretical analysis for the said bound and use it to construct a knowledge distillation loss that guides the optimization to focus on the more sensitive layers and samples. In addition, we leverage the Hessian upper bound to improve the weight quantization parameters selection by focusing on the more sensitive elements in the weight tensors. Empirically, by employing EPTQ we achieve state-of-the-art results on various models, tasks, and datasets, including ImageNet classification, COCO object detection, and Pascal-VOC for semantic segmentation.\n\n**Published:** 2023-09-20\n\n\n\n#### 2. HPTQ: Hardware-Friendly Post Training Quantization\n\n*From Search Query: efficient state compression neural networks*\n\n*Arnon Netzer, Roy H. Jennings, Idit Diamant, Oranit Dror, Lior Dikstein, Elad Cohen, Reuven Peretz, Hai Victor Habi*\n\n**Abstract:** Neural network quantization enables the deployment of models on edge devices. An essential requirement for their hardware efficiency is that the quantizers are hardware-friendly: uniform, symmetric, and with power-of-two thresholds. To the best of our knowledge, current post-training quantization methods do not support all of these constraints simultaneously. In this work, we introduce a hardware-friendly post training quantization (HPTQ) framework, which addresses this problem by synergistically combining several known quantization methods. We perform a large-scale study on four tasks: classification, object detection, semantic segmentation and pose estimation over a wide variety of network architectures. Our extensive experiments show that competitive results can be obtained under hardware-friendly constraints.\n\n**Published:** 2021-09-19\n\n\n\n#### 3. GhostNetV2: Enhance Cheap Operation with Long-Range Attention\n\n*From Search Query: hardware efficient attention mechanisms*\n\n*Yunhe Wang, Chao Xu, Chang Xu, Jianyuan Guo, Kai Han, Yehui Tang*\n\n**Abstract:** Light-weight convolutional neural networks (CNNs) are specially designed for applications on mobile devices with faster inference speed. The convolutional operation can only capture local information in a window region, which prevents performance from being further improved. Introducing self-attention into convolution can capture global information well, but it will largely encumber the actual speed. In this paper, we propose a hardware-friendly attention mechanism (dubbed DFC attention) and then present a new GhostNetV2 architecture for mobile applications. The proposed DFC attention is constructed based on fully-connected layers, which can not only execute fast on common hardware but also capture the dependence between long-range pixels. We further revisit the expressiveness bottleneck in previous GhostNet and propose to enhance expanded features produced by cheap operations with DFC attention, so that a GhostNetV2 block can aggregate local and long-range information simultaneously. Extensive experiments demonstrate the superiority of GhostNetV2 over existing architectures. For example, it achieves 75.3% top-1 accuracy on ImageNet with 167M FLOPs, significantly suppressing GhostNetV1 (74.5%) with a similar computational cost. The source code will be available at https://github.com/huawei-noah/Efficient-AI-Backbones/tree/master/ghostnetv2_pytorch and https://gitee.com/mindspore/models/tree/master/research/cv/ghostnetv2.\n\n**Published:** 2022-11-23\n\n\n\n#### 4. Rethinking Vision Transformers for MobileNet Size and Speed\n\n*From Search Query: hardware efficient attention mechanisms*\n\n*Jian Ren, Sergey Tulyakov, Yanzhi Wang, Kamyar Salahi, Georgios Evangelidis, Yang Wen, Ju Hu, Yanyu Li*\n\n**Abstract:** With the success of Vision Transformers (ViTs) in computer vision tasks, recent arts try to optimize the performance and complexity of ViTs to enable efficient deployment on mobile devices. Multiple approaches are proposed to accelerate attention mechanism, improve inefficient designs, or incorporate mobile-friendly lightweight convolutions to form hybrid architectures. However, ViT and its variants still have higher latency or considerably more parameters than lightweight CNNs, even true for the years-old MobileNet. In practice, latency and size are both crucial for efficient deployment on resource-constraint hardware. In this work, we investigate a central question, can transformer models run as fast as MobileNet and maintain a similar size? We revisit the design choices of ViTs and propose a novel supernet with low latency and high parameter efficiency. We further introduce a novel fine-grained joint search strategy for transformer models that can find efficient architectures by optimizing latency and number of parameters simultaneously. The proposed models, EfficientFormerV2, achieve 3.5% higher top-1 accuracy than MobileNetV2 on ImageNet-1K with similar latency and parameters. This work demonstrate that properly designed and optimized vision transformers can achieve high performance even with MobileNet-level size and speed.\n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2022-12-15\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on efficient state compression techniques, hardware-efficient attention mechanisms, and cache-friendly neural network architectures, here are some relevant findings and suggestions based on the provided sources and analysis:\n\n## Efficient State Compression Techniques\n\n1. **Neural Architecture Search (NAS) for Compression**:\n   NAS can be used to compress large language models by pruning structural components such as attention heads, neurons, and layers. This approach can help in achieving a balance between performance and efficiency, which is crucial for managing neural network states efficiently.\n\n2. **Spiking State Space Models (SSMs)**:\n   The SPikE-SSM framework proposes a sparse, precise, and efficient spiking SSM that incorporates boundary compression strategies (PMBC) to accelerate inference and parallel processing for long sequence learning. This method can be adapted to compress and manage neural network states efficiently by leveraging the sparse and parallel nature of spiking neurons.\n\n3. **Compute-In-Memory (CIM) Techniques**:\n   CIM-based methods can be used for efficient compression of intermediate data layers in neural networks. These techniques optimize data compression and processing within the memory, reducing the need for extensive data transfer and thus improving efficiency.\n\n## Hardware-Efficient Attention Mechanisms\n\n1. **Weighted Grouped Query Attention**:\n   Introducing learnable weights for each key and value head in attention mechanisms can enhance efficiency and performance. This approach allows the model to take a weighted average during fine-tuning, which can be optimized for hardware efficiency (as mentioned in the analysis).\n\n2. **FPGA-Based Implementations**:\n   Using Field-Programmable Gate Arrays (FPGAs) can significantly improve the efficiency of attention mechanisms. For example, the C-LSTM framework optimizes LSTM variants on FPGAs using structured compression techniques, which can be extended to attention mechanisms in language models.\n\n## Cache-Friendly Neural Network Architectures\n\n1. **Neurocache**:\n   Neurocache improves inference speed by storing compressed states, performing single retrieval operations per token, and extending the retrieval window to neighboring states. This approach is highly cache-friendly and can be integrated into the model to enhance state management and retrieval efficiency (as mentioned in the analysis).\n\n2. **Dynamic Context Adaptation**:\n   FineZip's use of dynamic context adaptation, where the context size for each token matches its position in a sentence, facilitates more efficient batching during compression and decompression. This approach can be adapted to optimize memory access patterns in neural networks, making them more cache-friendly.\n\n## Additional Considerations\n\n- **Predictive State Caching**:\n  Techniques like self-reasoning tokens, which generate additional tokens to pre-cache information useful for future predictions, can be integrated to improve predictive state caching. This approach encourages the model to plan and cache information ahead, enhancing efficiency (as mentioned in the analysis).\n\n- **Dynamic Routing and Pruning**:\n  Combining compressed state storage with dynamic routing and selective state updates based on importance can further optimize the model. This involves implementing learnable weights for state routing and hierarchical attention patterns to balance compression and computational overhead (as mentioned in the analysis).\n\nBy incorporating these techniques, the researcher can develop an autoregressive language model that is more efficient in state management, attention mechanisms, and overall computational efficiency, while also being optimized for hardware and cache utilization.\n",
            "analysis": "Analysis Note: Initial Research on LM Block Design\n\n1. Summary of Analysis\n\nThe seed design RWKV6 presents an interesting architecture with matrix-valued states and dynamic recurrence. From analyzing the seed design and initial references, several key areas emerge for potential innovation:\n\na) Memory and State Management:\n- RWKV6 uses matrix-valued states but could potentially benefit from more sophisticated memory mechanisms\n- Neurocache's external vector cache approach shows promise for extending context\n- Self-reasoning tokens demonstrate benefits of planning/caching future predictions\n\nb) Attention Mechanisms:\n- Current RWKV6 uses DDLerpLinear for dynamic depth\n- Weighted Grouped Query Attention shows benefits of learnable group weights\n- Opportunity to improve efficiency while maintaining expressiveness\n\nc) Computational Efficiency:\n- Need to balance model capacity with inference speed\n- Hardware-efficient implementations are crucial\n- Memory access patterns and cache utilization important\n\n2. Useful References with Excerpts\n\nFrom Neurocache paper:\n\"Neurocache improves upon previous methods by (1) storing compressed states, which reduces cache size; (2) performing a single retrieval operation per token which increases inference speed; and (3) extending the retrieval window to neighboring states\"\n\nThis suggests potential for incorporating efficient state compression and retrieval mechanisms.\n\nFrom Self Reasoning Tokens:\n\"for each token predicted, an additional token is generated that duplicates the input and doesn't receive a gradient from the next token but from future tokens. This approach encourages the model to pre-cache information useful for future predictions.\"\n\nThis indicates benefits of explicit future planning in the architecture.\n\nFrom Weighted Grouped Query Attention:\n\"introduced new learnable parameters for each key and value head [...] enabling the model to take a weighted average during finetuning. Our model achieves an average of 0.53% improvement over GQA\"\n\nShows potential for learnable routing/weighting mechanisms.\n\n3. Key Insights and Analysis\n\na) State Management Innovation Opportunities:\n- Combine compressed state storage with dynamic routing\n- Implement selective state updates based on importance\n- Use predictive caching for future tokens\n\nb) Attention Enhancement Possibilities:\n- Introduce learnable weights for state routing\n- Implement hierarchical attention patterns\n- Optimize memory access patterns\n\nc) Efficiency Considerations:\n- Focus on hardware-efficient implementations\n- Balance compression vs computational overhead\n- Consider cache-friendly access patterns\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Recent advances in efficient state compression techniques\n2. Hardware-efficient attention mechanisms\n3. Methods for predictive state caching\n4. Techniques for dynamic routing and pruning\n\n5. References\n\n[1] Safaya, A., & Yuret, D. (2024). Neurocache: Efficient Vector Retrieval for Long-range Language Modeling. North American Chapter of the Association for Computational Linguistics.\n\n[2] Bonetto, F.S. (2024). Self Reasoning Tokens. arXiv.org.\n\n[3] Chinnakonduru, S.S., & Mohapatra, A. (2024). Weighted Grouped Query Attention in Transformers. arXiv.org.\n\n[4] Qi, B., et al. (2024). SMR: State Memory Replay for Long Sequence Modeling. arXiv.org.\n\n[5] Peng, B., et al. (2024). Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence. arXiv.org."
        },
        {
            "ready": false,
            "query": "efficient state compression recurrent networks, selective computation neural networks",
            "detail": "Find papers discussing:\n1. State compression techniques specifically for recurrent architectures\n2. Methods for implementing selective computation in neural networks\n3. Approaches for combining selective computation with recurrent models\nFocus on practical implementations and efficiency considerations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. State compression techniques specifically for recurrent architectures\n2. Methods for implementing selective computation in neural networks\n3. Approaches for combining selective computation with recurrent models\nFocus on practical implementations and efficiency considerations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 1.00)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 1.00)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence (Avg. Score: 0.99)\n\n*Bo Peng, Daniel Goldstein, Quentin Anthony, Alon Albalak, Eric Alcaide, Stella Biderman, Eugene Cheah, Teddy Ferdinan, Haowen Hou, P. Kazienko, G. Kranthikiran, Jan Koco'n, Bartlomiej Koptyra, Satyapriya Krishna, Ronald McClelland, Niklas Muennighoff, Fares Obeid, Atsushi Saito, Guangyu Song, Haoqin Tu, Stanislaw Wo'zniak, Ruichong Zhang, Bingchen Zhao, Qihang Zhao, Peng Zhou, Jian Zhu, Ruijie Zhu*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 16  (*Influential: 1*)\n\n**TL;DR:** This work presents Eagle and Finch, sequence models improving upon the RWKV (RWKV-4) architecture, which introduces a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality.\n\n**Abstract:** We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: https://huggingface.co/RWKV Training code at: https://github.com/RWKV/RWKV-LM Inference code at: https://github.com/RWKV/ChatRWKV Time-parallel training code at: https://github.com/RWKV/RWKV-infctx-trainer\n\n##### *Relevant Chunk: No. 45/64 (Score: 0.99)*\n\n```\nPlease refer to Tay et al. (2022) and Wan et al. (2023) for a comprehensive and in-depth survey of efficient transformers. Recurrent architectures Before the advent of transformers, recurrent neural networks, especially Long Short-Term Memory (LSTM) (Hochreiter \\& Schmidhuber, 1997) and Gated Recurrent Unit (GRU) (Cho et al., 2014), were the dominant architectures in NLP for sequence processing. However, traditional RNNs are hard, if not impossible, to parallelize across the time dimension, susceptible to gradient vanishing and explosion, and ineffective in capturing long-range dependencies, which are ubiquitous in natural language. These shortcomings contributed to the rapid decline of traditional RNNs in NLP. There has been a revival of RNNs in NLP research (Tiezzi et al., 2024) in recent years. Compared to transformers with quadratic complexity, RNNs are highly efficient in autoregressive inference with $\\mathrm{O}(1)$ time complexity per step, making them an attractive architecture for large language models. Many efforts have been devoted to parallelized recurrent models and improving their capability to capture long-range dependency, while maintaining the low inference complexity. The Legendre Memory Unit (LMU) (Voelker et al., 2019) was designed to efficiently handle longrange dependencies with a new type of memory cell for recurrent neural networks. Unlike LSTM units, which struggle with remembering information over very long sequences, LMU use Legendre polynomials to create a memory system that can maintain and process information over extended time periods more effectively. High-order polynomial projection operators (HiPPO) (Gu et al., 2020) generalizes LMU by providing a flexible framework for online compression of signals through polynomial projections, accommodating various polynomial bases beyond Legendre polynomials. It optimizes function approximation over time, adapting to different data timescales without needing predefined hyperparameters. SSMs have inspired a range of follow-up research to incorporate SSMs, or modified SSMs into end-to-end architectures for language modeling, including MEGA (Ma et al., 2022), DSS (Gupta et al., 2022), H3 (Fu et al., 2022), and Linear Recurrent Unit (LRU) (Orvieto et al., 2023). Mamba (Gu \\& Dao, 2023) is a selective SSM that introduces time-dependent selective mechanism to enhance the long-range modeling ability of SSMs. The selectivity removes the linear time-variance property of the SSM, making it no longer possible to parallelize Mamba as a long convolution kernel. Yet Mamba can still be effectively parallelized using parallel associative scan\n(Blelloch, 1990; Martin \\& Cundy, 2018; Smith et al., 2023) with a hardware-aware implementation. Recently proposed GateLoop (Katsch, 2023) also adopts a similar data-dependent state transitions. The data-dependent states, also concurrently proposed in GLA (Yang et al., 2023), are similar to the Weighted Key-Value State in Finch. A contemporary but independent work also proposes recurrent models named as Hawk and Griffin (De et al., 2024). Hawk is a recurrent model with the Real-Gated Linear Recurrent Unit (RG-LRU), whereas Griffin mixes the RG-LRU with local multi-query attention, thereby achieving long-context extrapolation efficiently. Please see Tiezzi et al.\n```\n\n#### 3. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.97)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.97)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.92)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.92)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 5. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.75)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.75)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: efficient state compression recurrent networks, selective computation neural networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks\n\n*From Search Query: efficient state compression recurrent networks*\n\n*Rainer Engelken*\n\n**TL;DR:** This work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.\n\n**Abstract:** Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we introduce SparseProp, a novel event-based algorithm for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both the forward and backward pass operations from O(N) to O(log(N)) per network spike, thereby enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By leveraging the sparsity of the network, SparseProp eliminates the need to iterate through all neurons at each spike, employing efficient state updates instead. We demonstrate the efficacy of SparseProp across several classical integrate-and-fire neuron models, including a simulation of a sparse SNN with one million LIF neurons. This results in a speed-up exceeding four orders of magnitude relative to previous event-based implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. Efficient Graph Generation with Graph Recurrent Attention Networks\n\n*From Search Query: efficient state compression recurrent networks*\n\n*Renjie Liao, Yujia Li, Yang Song, Shenlong Wang, C. Nash, William L. Hamilton, D. Duvenaud, R. Urtasun, R. Zemel*\n\n**TL;DR:** A new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs), which better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention.\n\n**Abstract:** We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality. Our code is released at: \\url{https://github.com/lrjconan/GRAN}.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 290  (*Influential: 70*)\n\n#### 3. Neural Wave Machines: Learning Spatiotemporally Structured Representations with Locally Coupled Oscillatory Recurrent Neural Networks\n\n*From Search Query: efficient state compression recurrent networks*\n\n*Thomas Anderson Keller, M. Welling*\n\n**TL;DR:** This work introduces the Neural Wave Machine (NWM) \u2013 a locally coupled oscillatory recurrent neural network capable of exhibiting traveling waves in its hidden state and shows that this model indeed learns static spatial structure such as topographic organization, and further uses complex spatiotemporal structuresuch as traveling waves to encode observed transformations.\n\n**Abstract:** Traveling waves have been measured at a diversity of regions and scales in the brain, however a consensus as to their computational purpose has yet to be reached. An intriguing hypothesis is that traveling waves serve to structure neural representations both in space and time, thereby acting as an inductive bias towards natural data. In this work, we investigate this hypothesis by introducing the Neural Wave Machine (NWM) \u2013 a locally coupled oscillatory recurrent neural network capable of exhibiting traveling waves in its hidden state. After training on simple dynamic sequences, we show that this model indeed learns static spatial structure such as topographic organization, and further uses complex spatiotemporal structure such as traveling waves to encode observed transformations. To measure the computational implications of this structure, we use a suite of sequence classification and physical dynamics modeling tasks to show that the NWM is both more parameter efficient, and is able to forecast future trajectories of simple physical dynamical systems more accurately than existing state of the art counterparts. We conclude with a discussion of how this model may allow for novel investigations of the computational hypotheses surrounding traveling waves which were previously challenging or impossible.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 7  (*Influential: 1*)\n\n#### 4. Understanding the Representation and Computation of Multilayer Perceptrons: A Case Study in Speech Recognition\n\n*From Search Query: selective computation neural networks*\n\n*Tasha Nagamine, N. Mesgarani*\n\n**TL;DR:** This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform.\n\n**Abstract:** Despite the recent success of deep learning, the nature of the transformations they apply to the input features remains poorly understood. This study provides an empirical framework to study the encoding properties of node activations in various layers of the network, and to construct the exact function applied to each data point in the form of a linear transform. These methods are used to discern and quantify properties of feedforward neural networks trained to map acoustic features to phoneme labels. We show a selective and nonlinear warping of the feature space, achieved by forming prototypical functions to account for the possible variation of each class. This study provides a joint framework where the properties of node activations and the functions implemented by the network can be linked together.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2017\n\n**Citations:** 29  (*Influential: 1*)\n\n#### 5. MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition\n\n*From Search Query: selective computation neural networks*\n\n*Nicolas Menet, Michael Hersche, G. Karunaratne, Luca Benini, Abu Sebastian, Abbas Rahimi*\n\n**TL;DR:** This work proposes Multiple-Input-Multiple-Output Neural Networks capable of handling many inputs at once, and applies the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMoFormer, respectively.\n\n**Abstract:** With the advent of deep learning, progressively larger neural networks have been designed to solve complex tasks. We take advantage of these capacity-rich models to lower the cost of inference by exploiting computation in superposition. To reduce the computational burden per input, we propose Multiple-Input-Multiple-Output Neural Networks (MIMONets) capable of handling many inputs at once. MIMONets augment various deep neural network architectures with variable binding mechanisms to represent an arbitrary number of inputs in a compositional data structure via fixed-width distributed representations. Accordingly, MIMONets adapt nonlinear neural transformations to process the data structure holistically, leading to a speedup nearly proportional to the number of superposed input items in the data structure. After processing in superposition, an unbinding mechanism recovers each transformed input of interest. MIMONets also provide a dynamic trade-off between accuracy and throughput by an instantaneous on-demand switching between a set of accuracy-throughput operating points, yet within a single set of fixed parameters. We apply the concept of MIMONets to both CNN and Transformer architectures resulting in MIMOConv and MIMOFormer, respectively. Empirical evaluations show that MIMOConv achieves about 2-4 x speedup at an accuracy delta within [+0.68, -3.18]% compared to WideResNet CNNs on CIFAR10 and CIFAR100. Similarly, MIMOFormer can handle 2-4 inputs at once while maintaining a high average accuracy within a [-1.07, -3.43]% delta on the long range arena benchmark. Finally, we provide mathematical bounds on the interference between superposition channels in MIMOFormer. Our code is available at https://github.com/IBM/multiple-input-multiple-output-nets.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 6  (*Influential: 1*)\n\n#### 6. Beyond Geometry: Comparing the Temporal Structure of Computation in Neural Circuits with Dynamical Similarity Analysis\n\n*From Search Query: selective computation neural networks*\n\n*Mitchell Ostrow, Adam Eisen, L. Kozachkov, I. Fiete*\n\n**TL;DR:** This work introduces a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA), and opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.\n\n**Abstract:** How can we tell whether two neural networks utilize the same internal processes for a particular computation? This question is pertinent for multiple subfields of neuroscience and machine learning, including neuroAI, mechanistic interpretability, and brain-machine interfaces. Standard approaches for comparing neural networks focus on the spatial geometry of latent states. Yet in recurrent networks, computations are implemented at the level of dynamics, and two networks performing the same computation with equivalent dynamics need not exhibit the same geometry. To bridge this gap, we introduce a novel similarity metric that compares two systems at the level of their dynamics, called Dynamical Similarity Analysis (DSA). Our method incorporates two components: Using recent advances in data-driven dynamical systems theory, we learn a high-dimensional linear system that accurately captures core features of the original nonlinear dynamics. Next, we compare different systems passed through this embedding using a novel extension of Procrustes Analysis that accounts for how vector fields change under orthogonal transformation. In four case studies, we demonstrate that our method disentangles conjugate and non-conjugate recurrent neural networks (RNNs), while geometric methods fall short. We additionally show that our method can distinguish learning rules in an unsupervised manner. Our method opens the door to comparative analyses of the essential temporal structure of computation in neural circuits.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 17  (*Influential: 4*)\n\n### 4 related papers from Papers with Code\n\n#### 1. AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos\n\n*From Search Query: efficient state compression recurrent networks*\n\n*Ying Shan, Gen Li, Xintao Wang, Yanze Wu*\n\n**Abstract:** This paper studies the problem of real-world video super-resolution (VSR) for animation videos, and reveals three key improvements for practical animation VSR. First, recent real-world super-resolution approaches typically rely on degradation simulation using basic operators without any learning capability, such as blur, noise, and compression. In this work, we propose to learn such basic operators from real low-quality animation videos, and incorporate the learned ones into the degradation generation pipeline. Such neural-network-based basic operators could help to better capture the distribution of real degradations. Second, a large-scale high-quality animation video dataset, AVC, is built to facilitate comprehensive training and evaluations for animation VSR. Third, we further investigate an efficient multi-scale network structure. It takes advantage of the efficiency of unidirectional recurrent networks and the effectiveness of sliding-window-based methods. Thanks to the above delicate designs, our method, AnimeSR, is capable of restoring real-world low-quality animation videos effectively and efficiently, achieving superior performance to previous state-of-the-art methods. Codes and models are available at https://github.com/TencentARC/AnimeSR.\n\n**Published:** 2022-06-14\n\n\n\n#### 2. Learning for Video Compression with Hierarchical Quality and Recurrent Enhancement\n\n*From Search Query: efficient state compression recurrent networks*\n\n*Luc van Gool, Radu Timofte, Ren Yang, Fabian Mentzer*\n\n**Abstract:** In this paper, we propose a Hierarchical Learned Video Compression (HLVC) method with three hierarchical quality layers and a recurrent enhancement network. The frames in the first layer are compressed by an image compression method with the highest quality. Using these frames as references, we propose the Bi-Directional Deep Compression (BDDC) network to compress the second layer with relatively high quality. Then, the third layer frames are compressed with the lowest quality, by the proposed Single Motion Deep Compression (SMDC) network, which adopts a single motion map to estimate the motions of multiple frames, thus saving bits for motion information. In our deep decoder, we develop the Weighted Recurrent Quality Enhancement (WRQE) network, which takes both compressed frames and the bit stream as inputs. In the recurrent cell of WRQE, the memory and update signal are weighted by quality features to reasonably leverage multi-frame information for enhancement. In our HLVC approach, the hierarchical quality benefits the coding efficiency, since the high quality information facilitates the compression and enhancement of low quality frames at encoder and decoder sides, respectively. Finally, the experiments validate that our HLVC approach advances the state-of-the-art of deep video compression methods, and outperforms the \"Low-Delay P (LDP) very fast\" mode of x265 in terms of both PSNR and MS-SSIM. The project page is at https://github.com/RenYang-home/HLVC.\n\n**Conference:** learning-for-video-compression-with-1\n\n**Published:** 2020-03-04\n\n\n\n#### 3. Loss-aware automatic selection of structured pruning criteria for deep neural network acceleration\n\n*From Search Query: selective computation neural networks*\n\n*Seong-heum Kim, Kilho Lee, Deepak Ghimire*\n\n**Abstract:** Structured pruning is a well-established technique for compressing neural networks, making them suitable for deployment in resource-limited edge devices. This study presents an efficient loss-aware automatic selection of structured pruning (LAASP) criteria for slimming and accelerating deep neural networks. The majority of pruning methods employ a sequential process consisting of three stages, 1) training, 2) pruning, and 3) fine-tuning, whereas the proposed pruning technique adopts a pruning-while-training approach that eliminates the first stage and integrates the second and third stages into a single cycle. The automatic selection of magnitude or similarity-based filter pruning criteria from a specified pool of criteria and the specific pruning layer at each pruning iteration is guided by the network's overall loss on a small subset of training data. To mitigate the abrupt accuracy drop due to pruning, the network is retrained briefly after each reduction of a predefined number of floating-point operations (FLOPs). The optimal pruning rates for each layer in the network are automatically determined, eliminating the need for manual allocation of fixed or variable pruning rates for each layer. Experiments on the VGGNet, ResNet, and MobileNet models on the CIFAR-10 and ImageNet benchmark datasets demonstrate the effectiveness of the proposed method. In particular, the ResNet56 and ResNet110 models on the CIFAR-10 dataset significantly improve the top-1 accuracy compared to state-of-the-art methods while reducing the network FLOPs by 52%. Furthermore, pruning the ResNet50 model on the ImageNet dataset reduces FLOPs by more than 42% with a negligible 0.33% drop in the top-5 accuracy. The source code of this study is publicly available on GitHub: https://github.com/ghimiredhikura/laasp.\n\n**Proceeding:** image-and-vision-computing-2023-7\n\n**Published:** 2023-07-06\n\n\n\n#### 4. Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth\n\n*From Search Query: selective computation neural networks*\n\n*Woonghyun Ka, Junmo Kim, Sehwan Chun, Donggyu Joo, Pyungwhan Ahn, Doyeon Kim*\n\n**Abstract:** Depth estimation from a single image is an important task that can be applied to various fields in computer vision, and has grown rapidly with the development of convolutional neural networks. In this paper, we propose a novel structure and training strategy for monocular depth estimation to further improve the prediction accuracy of the network. We deploy a hierarchical transformer encoder to capture and convey the global context, and design a lightweight yet powerful decoder to generate an estimated depth map while considering local connectivity. By constructing connected paths between multi-scale local features and the global decoding stream with our proposed selective feature fusion module, the network can integrate both representations and recover fine details. In addition, the proposed decoder shows better performance than the previously proposed decoders, with considerably less computational complexity. Furthermore, we improve the depth-specific augmentation method by utilizing an important observation in depth estimation to enhance the model. Our network achieves state-of-the-art performance over the challenging depth dataset NYU Depth V2. Extensive experiments have been conducted to validate and show the effectiveness of the proposed approach. Finally, our model shows better generalisation ability and robustness than other comparative models.\n\n**Published:** 2022-01-19\n\n\n\n\n\n---\n## Web search results\n\n To improve autoregressive language model design, particularly focusing on efficient state compression in recurrent architectures, selective computation, and their combination, here are some key findings and references:\n\n## State Compression Techniques for Recurrent Architectures\n\n1. **State Space Models (SSMs):**\n   SSMs have been shown to be effective in compressing long inputs into dynamic representations. For example, HiPPO and S4 models use orthogonal polynomials and low-rank corrections to achieve stable and efficient state compression. These models can be integrated into recurrent architectures to enhance their state management capabilities.\n\n2. **Spiking State Space Models (SPikE-SSM):**\n   The SPikE-SSM framework proposes a boundary compression strategy (PMBC) to accelerate the inference of spiking neuron models, enabling efficient parallel processing for long sequence learning. This approach integrates a novel neuron model with reset-refractory mechanisms, which can be hierarchically integrated into SSM blocks to enhance dynamics and efficiency.\n\n## Methods for Implementing Selective Computation in Neural Networks\n\n1. **Mamba and DenseMamba:**\n   These models introduce selective state spaces where the SSM parameters are functions of the input, allowing the model to selectively propagate or forget information along the sequence length dimension. This content-based state update mechanism can be particularly beneficial for enhancing the state management in recurrent models like RWKV6 (Analysis Note).\n\n2. **FlashAttention and Gated Linear Attention:**\n   While primarily focused on attention mechanisms, these models demonstrate the principle of selective computation. FlashAttention uses tiling to reduce memory reads/writes, and Gated Linear Attention (GLA) selectively computes attention weights, both of which can be adapted to improve the efficiency of recurrent models (Analysis Note).\n\n## Approaches for Combining Selective Computation with Recurrent Models\n\n1. **Integrating Selective State Updates:**\n   By incorporating content-based state updates from models like Mamba and DenseMamba into recurrent architectures, it is possible to enhance the selective propagation of information. This can be combined with the efficient state compression techniques from SSMs to improve the overall efficiency and expressivity of the model (Analysis Note).\n\n2. **Hybrid Memory Approaches:**\n   Models like B'MOJO, which combine eidetic and fading memory, offer insights into how different timescales can be managed efficiently. Integrating such hybrid approaches with selective computation can help in optimizing memory access patterns and state compression in recurrent models (Analysis Note).\n\n## Practical Implementations and Efficiency Considerations\n\n1. **Parallel Max-min Boundary Compression (PMBC):**\n   The PMBC strategy in SPikE-SSM enables parallel processing for long sequence learning by efficiently determining spiking signals, significantly reducing computational time. This can be a valuable technique for improving the efficiency of recurrent models in handling long sequences.\n\n2. **IO-Aware Algorithms:**\n   FlashAttention's IO-aware approach to attention computation can be adapted to optimize memory access patterns in recurrent models, reducing the number of memory reads/writes and improving overall efficiency (Analysis Note).\n\nBy integrating these techniques, researchers can develop more efficient and scalable autoregressive language models that leverage selective computation, efficient state compression, and optimized memory management.\n",
            "analysis": "Analysis Note: Second Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe search results reveal several promising directions for improving RWKV6:\n\na) State Space Models (SSMs) and Selective Computation:\n- Mamba's selective SSM approach shows benefits of content-based state updates\n- DenseMamba demonstrates advantages of enhanced information flow between layers\n- Opportunity to incorporate selective computation into RWKV6's state management\n\nb) Hardware-Efficient Implementations:\n- FlashAttention principles for IO-aware computation\n- GLA (Gated Linear Attention) shows promise for hardware-efficient linear attention\n- Opportunities to optimize memory access patterns\n\nc) Memory and Cache Optimization:\n- FlashFFTConv demonstrates benefits of tiling and kernel fusion\n- B'MOJO's hybrid approach combining eidetic and fading memory\n- Potential for improved state compression and retrieval\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n\nFrom FlashAttention:\n\"We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM.\"\n\nFrom Gated Linear Attention:\n\"When used as a replacement for the standard attention layer in Transformers, the resulting gated linear attention (GLA) Transformer is found to perform competitively [...] GLA Transformer is especially effective at length generalization\"\n\n3. Key Insights and Analysis\n\na) Selective State Processing:\n- Content-based state updates could enhance RWKV6's DDLerpLinear\n- Potential for selective propagation of information based on token relevance\n- Opportunity to combine with hardware-efficient implementation\n\nb) Memory Hierarchy Optimization:\n- IO-aware algorithms crucial for performance\n- Tiling strategies can reduce memory bandwidth requirements\n- Cache-friendly access patterns important for efficiency\n\nc) Architecture Integration:\n- Need to maintain RWKV6's recurrent structure while adding selective computation\n- Opportunity to enhance state compression while preserving expressivity\n- Potential for hybrid memory approaches combining different timescales\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for efficient state compression in recurrent architectures\n2. Methods for combining selective computation with linear attention\n3. Hardware-efficient implementations of state-space models\n4. Approaches for dynamic memory management in neural networks\n\n5. References\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv preprint.\n\n[2] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint.\n\n[3] Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[4] Yang, S., et al. (2023). Gated Linear Attention Transformers with Hardware-Efficient Training. International Conference on Machine Learning.\n\n[5] Fu, D.Y., et al. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[6] Zancato, L., et al. (2024). B'MOJO: Hybrid State Space Realizations of Foundation Models with Eidetic and Fading Memory."
        },
        {
            "ready": false,
            "query": "event-based computation neural networks, hierarchical state management",
            "detail": "Find papers discussing:\n1. Event-based computation in neural networks, particularly for sequence modeling\n2. Hierarchical state management techniques in deep learning\n3. Methods for combining event-based and hierarchical approaches\nFocus on practical implementations and efficiency considerations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Event-based computation in neural networks, particularly for sequence modeling\n2. Hierarchical state management techniques in deep learning\n3. Methods for combining event-based and hierarchical approaches\nFocus on practical implementations and efficiency considerations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet (Avg. Score: 0.99)\n\n*Zhen Qin, Yuxin Mao, Xuyang Shen, Dong Li, Jing Zhang, Yuchao Dai, Yiran Zhong*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 1*)\n\n**TL;DR:** This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan.\n\n**Abstract:** Linear attention mechanisms have gained prominence in causal language models due to their linear computational complexity and enhanced speed. However, the inherent decay mechanism in linear attention presents challenges when applied to multi-dimensional sequence modeling tasks, such as image processing and multi-modal learning. In these scenarios, the utilization of sequential scanning to establish a global receptive field necessitates multiple scans for multi-dimensional data, thereby leading to inefficiencies. This paper identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence to avoid the issue, as it can handle multi-dimensional data within a single scan. We further develop an efficient multi-dimensional sequential modeling framework called LightNet based on the new recurrence. Moreover, we present two new multi-dimensional linear relative positional encoding methods, MD-TPE and MD-LRPE to enhance the model's ability to discern positional information in multi-dimensional scenarios. Our empirical evaluations across various tasks, including image classification, image generation, bidirectional language modeling, and autoregressive language modeling, demonstrate the efficacy of LightNet, showcasing its potential as a versatile and efficient solution for multi-dimensional sequential modeling.\n\n##### *Relevant Chunk: No. 15/20 (Score: 0.99)*\n\n```\nIn Proceedings of the International Conference on Learning Representations (ICLR), 2021. [11] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [12] Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training. arXiv preprint arXiv:2312.06635, 2023. [13] Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. [14] Albert Gu, Karan Goel, Ankit Gupta, and Christopher R\u00e9. On the parameterization and initialization of diagonal state space models. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 35:35971-35983, 2022. [15] Harsh Mehta, Ankit Gupta, Ashok Cutkosky, and Behnam Neyshabur. Long range language modeling via gated state spaces. In Proceedings of the International Conference on Learning Representations (ICLR), 2023. [16] Jimmy TH Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In Proceedings of the International Conference on Learning Representations (ICLR), 2022. [17] Eric Martin and Chris Cundy. Parallelizing linear recurrent neural nets over sequence length. In Proceedings of the International Conference on Learning Representations (ICLR). OpenReview.net, 2018. [18] Antonio Orvieto, Samuel L. Smith, Albert Gu, Anushan Fernando, \u00c7aglar G\u00fcl\u00e7ehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. CoRR, abs/2303.06349, 2023. [19] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Proceedings of the Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [20] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [21] Weixuan Sun, Zhen Qin, Hui Deng, Jianyuan Wang, Yi Zhang, Kaihao Zhang, Nick Barnes, Stan Birchfield, Lingpeng Kong, and Yiran Zhong. Vicinity vision transformer. IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), 2023. [22] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [23] Bo Peng, Eric Alcaide, Quentin Gregory Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Nguyen Chung, Leon Derczynski, et al. Rwkv: Reinventing rnns for the transformer era. In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 2023. [24] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 4195-4205, 2023. [25] Zhengcong Fei, Mingyuan Fan, Changqian Yu, and Junshi Huang. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. [26] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Diffusion-rwkv: Scaling rwkv-like architectures for diffusion models. arXiv preprint arXiv:2404.04478, 2024. [27] Jing Nathan Yan, Jiatao Gu, and Alexander M. Rush. Diffusion models without attention. arXiv preprint arXiv:2311.18257, 2023. [28] Vincent Tao Hu, Stefan Andreas Baumann, Ming Gui, Olga Grebenkova, Pingchuan Ma, Johannes Fischer, and Bjorn Ommer. Zigma: Zigzag mamba diffusion model.\n```\n\n#### 2. Efficient Beam Tree Recursion (Avg. Score: 0.97)\n\n*Jishnu Ray Chowdhury, Cornelia Caragea*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** These proposals standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n**Abstract:** Beam Tree Recursive Neural Network (BT-RvNN) was recently proposed as a simple extension of Gumbel Tree RvNN and it was shown to achieve state-of-the-art length generalization performance in ListOps while maintaining comparable performance on other tasks. However, although not the worst in its kind, BT-RvNN can be still exorbitantly expensive in memory usage. In this paper, we identify the main bottleneck in BT-RvNN's memory usage to be the entanglement of the scorer function and the recursive cell function. We propose strategies to remove this bottleneck and further simplify its memory usage. Overall, our strategies not only reduce the memory usage of BT-RvNN by $10$-$16$ times but also create a new state-of-the-art in ListOps while maintaining similar performance in other tasks. In addition, we also propose a strategy to utilize the induced latent-tree node representations produced by BT-RvNN to turn BT-RvNN from a sentence encoder of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{d}$ into a sequence contextualizer of the form $f:\\mathbb{R}^{n \\times d} \\rightarrow \\mathbb{R}^{n \\times d}$. Thus, our proposals not only open up a path for further scalability of RvNNs but also standardize a way to use BT-RvNNs as another building block in the deep learning toolkit that can be easily stacked or interfaced with other popular models such as Transformers and Structured State Space models.\n\n##### *Relevant Chunk: No. 19/50 (Score: 0.97)*\n\n```\nIn Proceedings of International Conference on Neural Networks (ICNN'96), volume 1, pages 347-352 vol.1, 1996. doi: 10.1109/ICNN.1996.548916. [25] Alex Graves. Adaptive computation time for recurrent neural networks. ArXiv, abs/1603.08983, 2016. URL http://arxiv.org/abs/1603.08983\n[26] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. [27] Ankit Gupta, Albert Gu, and Jonathan Berant. Diagonal state spaces are as effective as structured state spaces. Advances in Neural Information Processing Systems, 35:22982-22994, 2022. [28] Suchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107-112, New Orleans, Louisiana, June 2018. Association for Computational Linguistics. doi: 10.18653/v1/ N18-2017. URL https://aclanthology.org/N18-2017. [29] Michael Hahn. Theoretical limitations of self-attention in neural sequence models. Transactions of the Association for Computational Linguistics, 8:156-171, 2020. doi: 10.1162/tacl_a_00306. URL https://aclanthology.org/2020.tacl-1.11\n[30] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908-15919, 2021. [31] Serhii Havrylov, Germ\u00e1n Kruszewski, and Armand Joulin. Cooperative learning of disjoint syntax and semantics. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1118-1128, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1115. URLhttps://aclanthology org/N19-1115\n[32] Jonathan Herzig and Jonathan Berant. Span-based semantic parsing for compositional generalization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 908-921, Online, August 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.acl-long.74. URL https://aclanthology.org/2021 acl-long. 74\n[33] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9 (8):1735-1780, November 1997. ISSN 0899-7667. doi: 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735\n[34] Xiang Hu, Haitao Mi, Zujie Wen, Yafang Wang, Yi Su, Jing Zheng, and Gerard de Melo. R2D2: Recursive transformer based on differentiable tree for interpretable hierarchical language modeling.\n```\n\n#### 3. Ordered Memory  (Avg. Score: 0.97)\n\n*Daniel Borisov, Matthew D\u2019Iorio, Jeffrey Hyacinthe*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** It is found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\n\n**Abstract:** Natural language semantics can be modeled using the phrase-structured model, which can be represented using a tree-type architecture. As a result, recent advances in natural language processing have been made utilising recursive neural networks using memory models that allow them to infer tree-type representations of the input sentence sequence. These new tree models have allowed for improvements in sentiment analysis and semantic recognition. Here we review the Ordered Memory model proposed by Shen et al. (2019) at the NeurIPS 2019 conference, and try to either create baselines that can perform better or create simpler models that can perform equally as well. We found that the Ordered Memory model performs on par with the state-of-the-art models used in tree-type modelling, and performs better than simplified baselines that require fewer parameters.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.97)*\n\n```\nIn Proc. of NAACL-HLT, 2019. Athul Paul Jacob, Zhouhan Lin, Alessandro Sordoni, and Yoshua Bengio. Learning hierarchical structures on-the-fly with a recurrent-recursive model for sequences. In Proceedings of The Third Workshop on Representation Learning for NLP, pages 154-158, 2018. Yacine Jernite, Edouard Grave, Armand Joulin, and Tomas Mikolov. Variable computation in recurrent neural networks. arXiv preprint arXiv:1611.06188, 2016. Armand Joulin and Tomas Mikolov. Inferring algorithmic patterns with stack-augmented recurrent nets. In Advances in neural information processing systems, pages 190-198, 2015. Donald E Knuth. On the translation of languages from left to right. Information and control, 8(6): $607-639,1965$. Adhiguna Kuncoro, Chris Dyer, John Hale, Dani Yogatama, Stephen Clark, and Phil Blunsom. Lstms can learn syntax-sensitive dependencies well, but modeling structure makes them better. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), volume 1, pages 1426-1436, 2018. Brenden M Lake and Marco Baroni. Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks. arXiv preprint arXiv:1711.00350, 2017. Xiaodong Liu, Pengcheng He, Weizhu Chen, and Jianfeng Gao. Multi-task deep neural networks for natural language understanding. CoRR, abs/1901.11504, 2019. URL http://arxiv.org/ $\\mathrm{abs} / 1901.11504$\n\nMoshe Looks, Marcello Herreshoff, DeLesley Hutchins, and Peter Norvig. Deep learning with dynamic computation graphs. arXiv preprint arXiv:1702.02181, 2017. Joao Loula, Marco Baroni, and Brenden M Lake. Rearranging the familiar: Testing compositional generalization in recurrent networks.\n```\n\n#### 4. A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models  (Avg. Score: 0.95)\n\n*Itamar Zimerman, Ameen Ali, Lior Wolf*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A unified view of attention-free layers of Mamba, RWKV, and various gated RNNs is presented, formulating such layers as implicit causal self-attention layers and providing a direct means for applying explainability methods.\n\n**Abstract:** Recent advances in efficient sequence modeling have led to attention-free layers, such as Mamba, RWKV, and various gated RNNs, all featuring sub-quadratic complexity in sequence length and excellent scaling properties, enabling the construction of a new type of foundation models. In this paper, we present a unified view of these models, formulating such layers as implicit causal self-attention layers. The formulation includes most of their sub-components and is not limited to a specific part of the architecture. The framework compares the underlying mechanisms on similar grounds for different layers and provides a direct means for applying explainability methods. Our experiments show that our attention matrices and attribution method outperform an alternative and a more limited formulation that was recently proposed for Mamba. For the other architectures for which our method is the first to provide such a view, our method is effective and competitive in the relevant metrics compared to the results obtained by state-of-the-art transformer explainability methods. Our code is publicly available.\n\n##### *Relevant Chunk: No. 19/24 (Score: 0.95)*\n\n```\narXiv preprint arXiv:2401.04081, 2024. [44] Michael Poli, Stefano Massaroli, Eric Nguyen, Daniel Y Fu, Tri Dao, Stephen Baccus, Yoshua Bengio, Stefano Ermon, and Christopher R\u00e9. Hyena hierarchy: Towards larger convolutional language models. arXiv preprint arXiv:2302.10866, 2023. [45] Michael Poli, Armin W Thomas, Eric Nguyen, Pragaash Ponnusamy, Bj\u00f6rn Deiseroth, Kristian Kersting, Taiji Suzuki, Brian Hie, Stefano Ermon, Christopher R\u00e9, et al. Mechanistic design and scaling of hybrid architectures. arXiv preprint arXiv:2403.17844, 2024. [46] Zhen Qin, Songlin Yang, Weixuan Sun, Xuyang Shen, Dong Li, Weigao Sun, and Yiran Zhong. Hgrn2: Gated linear rnns with state expansion. arXiv preprint arXiv:2404.07904, 2024. [47] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [48] Lixiang Ru, Yibing Zhan, Baosheng Yu, and Bo Du. Learning affinity from attention: Endto-end weakly-supervised semantic segmentation with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16846-16855, 2022. [49] Lixiang Ru, Heliang Zheng, Yibing Zhan, and Bo Du. Token contrast for weakly-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3093-3102, 2023. [50] Jimmy TH Smith, Andrew Warrington, and Scott W Linderman. Simplified state space layers for sequence modeling. arXiv preprint arXiv:2208.04933, 2022. [51] Julian Spravil, Sebastian Houben, and Sven Behnke. Hyenapixel: Global image context with convolutions.\n```\n\n#### 5. Scalable MatMul-free Language Modeling (Avg. Score: 0.90)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.90)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n\n\n---\n## Found 10 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: event-based computation neural networks, hierarchical state management\n\nConsidering refining your search by improving the query keywords input.\n\n### 6 related papers from Semantic Scholar\n\n#### 1. Event Detection: Gate Diversity and Syntactic Importance Scores for Graph Convolution Neural Networks\n\n*From Search Query: event-based computation neural networks*\n\n*Viet Dac Lai, Tuan Ngo Nguyen, Thien Huu Nguyen*\n\n**TL;DR:** This study proposes a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate and introduces novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED.\n\n**Abstract:** Recent studies on event detection (ED) have shown that the syntactic dependency graph can be employed in graph convolution neural networks (GCN) to achieve state-of-the-art performance. However, the computation of the hidden vectors in such graph-based models is agnostic to the trigger candidate words, potentially leaving irrelevant information for the trigger candidate for event prediction. In addition, the current models for ED fail to exploit the overall contextual importance scores of the words, which can be obtained via the dependency tree, to boost the performance. In this study, we propose a novel gating mechanism to filter noisy information in the hidden vectors of the GCN models for ED based on the information from the trigger candidate. We also introduce novel mechanisms to achieve the contextual diversity for the gates and the importance score consistency for the graphs and models in ED. The experiments show that the proposed model achieves state-of-the-art performance on two ED datasets.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 89  (*Influential: 9*)\n\n#### 2. Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks\n\n*From Search Query: event-based computation neural networks*\n\n*Wenrui Zhang, Peng Li*\n\n**TL;DR:** A novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs is presented, which breaks down error back Propagation across two types of inter-neuron and intra-Neuron dependencies and leads to improved temporal learning precision.\n\n**Abstract:** Spiking neural networks (SNNs) are well suited for spatio-temporal learning and implementations on energy-efficient event-driven neuromorphic processors. However, existing SNN error backpropagation (BP) methods lack proper handling of spiking discontinuities and suffer from low performance compared with the BP methods for traditional artificial neural networks. In addition, a large number of time steps are typically required to achieve decent performance, leading to high latency and rendering spike-based computation unscalable to deep architectures. We present a novel Temporal Spike Sequence Learning Backpropagation (TSSL-BP) method for training deep SNNs, which breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision. It captures inter-neuron dependencies through presynaptic firing times by considering the all-or-none characteristics of firing activities and captures intra-neuron dependencies by handling the internal evolution of each neuronal state in time. TSSL-BP efficiently trains deep SNNs within a much shortened temporal window of a few steps while improving the accuracy for various image classification datasets including CIFAR10.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 196  (*Influential: 27*)\n\n#### 3. Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning\n\n*From Search Query: event-based computation neural networks*\n\n*Mingqing Xiao, Yixin Zhu, D.K. He, Zhouchen Lin*\n\n**TL;DR:** It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation in SNNs, revealing that SNNs are proficient in executing (knowledge) graph reasoning.\n\n**Abstract:** Spiking neural networks (SNNs) are investigated as biologically inspired models of neural computation, distinguished by their computational capability and energy efficiency due to precise spiking times and sparse spikes with event-driven computation. A significant question is how SNNs can emulate human-like graph-based reasoning of concepts and relations, especially leveraging the temporal domain optimally. This paper reveals that SNNs, when amalgamated with synaptic delay and temporal coding, are proficient in executing (knowledge) graph reasoning. It is elucidated that spiking time can function as an additional dimension to encode relation properties via a neural-generalized path formulation. Empirical results highlight the efficacy of temporal delay in relation processing and showcase exemplary performance in diverse graph reasoning tasks. The spiking model is theoretically estimated to achieve $20\\times$ energy savings compared to non-spiking counterparts, deepening insights into the capabilities and potential of biologically inspired SNNs for efficient reasoning. The code is available at https://github.com/pkuxmq/GRSNN.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 4. Multi-Agent Reinforcement Learning with Hierarchical Coordination for Emergency Responder Stationing\n\n*From Search Query: hierarchical state management*\n\n*Amutheezan Sivagnanam, Ava Pettet, Hunter Lee, Ayan Mukhopadhyay, Abhishek Dubey, Aron Laszka*\n\n**TL;DR:** This work addresses the issue of long decision times by introducing a novel reinforcement learning (RL) approach, based on the same hierarchical decomposition, but replacing online search with learning, which reduces computation time per decision by three orders of magnitude.\n\n**Abstract:** An emergency responder management (ERM) system dispatches responders, such as ambulances, when it receives requests for medical aid. ERM systems can also proactively reposition responders between predesignated waiting locations to cover any gaps that arise due to the prior dispatch of responders or significant changes in the distribution of anticipated requests. Optimal repositioning is computationally challenging due to the exponential number of ways to allocate responders between locations and the uncertainty in future requests. The state-of-the-art approach in proactive repositioning is a hierarchical approach based on spatial decomposition and online Monte Carlo tree search, which may require minutes of computation for each decision in a domain where seconds can save lives. We address the issue of long decision times by introducing a novel reinforcement learning (RL) approach, based on the same hierarchical decomposition, but replacing online search with learning. To address the computational challenges posed by large, variable-dimensional, and discrete state and action spaces, we propose: (1) actor-critic based agents that incorporate transformers to handle variable-dimensional states and actions, (2) projections to fixed-dimensional observations to handle complex states, and (3) combinatorial techniques to map continuous actions to discrete allocations. We evaluate our approach using real-world data from two U.S. cities, Nashville, TN and Seattle, WA. Our experiments show that compared to the state of the art, our approach reduces computation time per decision by three orders of magnitude, while also slightly reducing average ambulance response time by 5 seconds.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 0  (*Influential: 0*)\n\n#### 5. HUMAN: Hierarchical Universal Modular ANnotator\n\n*From Search Query: hierarchical state management*\n\n*Moritz Wolf, Dana Ruiter, Ashwin Geet D'Sa, Liane Reiners, J. Alexandersson, D. Klakow*\n\n**TL;DR:** HUMAN is a novel web-based annotation tool that addresses the above problems by covering a variety of annotation tasks on both textual and image data, and the usage of an internal deterministic state machine, allowing the researcher to chain different annotation tasks in an interdependent manner.\n\n**Abstract:** A lot of real-world phenomena are complex and cannot be captured by single task annotations. This causes a need for subsequent annotations, with interdependent questions and answers describing the nature of the subject at hand. Even in the case a phenomenon is easily captured by a single task, the high specialisation of most annotation tools can result in having to switch to another tool if the task only slightly changes. We introduce HUMAN, a novel web-based annotation tool that addresses the above problems by a) covering a variety of annotation tasks on both textual and image data, and b) the usage of an internal deterministic state machine, allowing the researcher to chain different annotation tasks in an interdependent manner. Further, the modular nature of the tool makes it easy to define new annotation tasks and integrate machine learning algorithms e.g., for active learning. HUMAN comes with an easy-to-use graphical user interface that simplifies the annotation task and management.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2020\n\n**Citations:** 4  (*Influential: 0*)\n\n#### 6. Winning the L2RPN Challenge: Power Grid Management via Semi-Markov Afterstate Actor-Critic\n\n*From Search Query: hierarchical state management*\n\n*Deunsol Yoon, Sunghoon Hong, Byung-Jun Lee, Kee-Eung Kim*\n\n**TL;DR:** This paper presents an off-policy actor-critic approach that effectively tackles the unique challenges in power grid management by RL, adopting the hierarchical policy together with the afterstate representation.\n\n**Abstract:** Safe and reliable electricity transmission in power grids is crucial for modern society. It is thus quite natural that there has been a growing interest in the automatic management of power grids, exemplified by the Learning to Run a Power Network Challenge (L2RPN), modeling the problem as a reinforcement learning (RL) task. However, it is highly challenging to manage a real-world scale power grid, mostly due to the massive scale of its state and action space. In this paper, we present an off-policy actor-critic approach that effectively tackles the unique challenges in power grid management by RL, adopting the hierarchical policy together with the afterstate representation. Our agent ranked first in the latest challenge (L2RPN WCCI 2020), being able to avoid disastrous situations while maintaining the highest level of operational efficiency in every test scenario. This paper provides a formal description of the algorithmic aspect of our approach, as well as further experimental studies on diverse power grids.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 37  (*Influential: 8*)\n\n### 4 related papers from Papers with Code\n\n#### 1. Temporal Graph Networks for Deep Learning on Dynamic Graphs\n\n*From Search Query: event-based computation neural networks*\n\n*Emanuele Rossi, Federico Monti, Fabrizio Frasca, Ben Chamberlain, Michael Bronstein, Davide Eynard*\n\n**Abstract:** Graph Neural Networks (GNNs) have recently become increasingly popular due to their ability to learn complex systems of relations or interactions arising in a broad spectrum of problems ranging from biology and particle physics to social networks and recommendation systems. Despite the plethora of different models for deep learning on graphs, few approaches have been proposed thus far for dealing with graphs that present some sort of dynamic nature (e.g. evolving features or connectivity over time). In this paper, we present Temporal Graph Networks (TGNs), a generic, efficient framework for deep learning on dynamic graphs represented as sequences of timed events. Thanks to a novel combination of memory modules and graph-based operators, TGNs are able to significantly outperform previous approaches being at the same time more computationally efficient. We furthermore show that several previous models for learning on dynamic graphs can be cast as specific instances of our framework. We perform a detailed ablation study of different components of our framework and devise the best configuration that achieves state-of-the-art performance on several transductive and inductive prediction tasks for dynamic graphs.\n\n**Published:** 2020-06-18\n\n\n\n#### 2. Event-based Vision: A Survey\n\n*From Search Query: event-based computation neural networks*\n\n*Kostas Daniilidis, Guillermo Gallego, Garrick Orchard, Davide Scaramuzza, Tobi Delbruck, Stefan Leutenegger, Joerg Conradt, Chiara Bartolozzi, Brian Taba, Andrew Davison, Andrea Censi*\n\n**Abstract:** Event cameras are bio-inspired sensors that differ from conventional frame cameras: Instead of capturing images at a fixed rate, they asynchronously measure per-pixel brightness changes, and output a stream of events that encode the time, location and sign of the brightness changes. Event cameras offer attractive properties compared to traditional cameras: high temporal resolution (in the order of microseconds), very high dynamic range (140 dB vs. 60 dB), low power consumption, and high pixel bandwidth (on the order of kHz) resulting in reduced motion blur. Hence, event cameras have a large potential for robotics and computer vision in challenging scenarios for traditional cameras, such as low-latency, high speed, and high dynamic range. However, novel methods are required to process the unconventional output of these sensors in order to unlock their potential. This paper provides a comprehensive overview of the emerging field of event-based vision, with a focus on the applications and the algorithms developed to unlock the outstanding properties of event cameras. We present event cameras from their working principle, the actual sensors that are available and the tasks that they have been used for, from low-level vision (feature detection and tracking, optic flow, etc.) to high-level vision (reconstruction, segmentation, recognition). We also discuss the techniques developed to process events, including learning-based techniques, as well as specialized processors for these novel sensors, such as spiking neural networks. Additionally, we highlight the challenges that remain to be tackled and the opportunities that lie ahead in the search for a more efficient, bio-inspired way for machines to perceive and interact with the world.\n\n**Published:** 2019-04-17\n\n\n\n#### 3. Data Interpreter: An LLM Agent For Data Science\n\n*From Search Query: hierarchical state management*\n\n*Zhibin Gou, Taicheng Guo, Lingyao Zhang, Jiaqi Chen, Danyang Li, Chenxing Wei, Ceyao Zhang, Wei Tao, Binhao Wu, Bang Liu, Xiawu Zheng, Min Yang, Li Zhang, Chenglin Wu, Zongze Xu, Yuheng Cheng, Yaying Fei, Xinbing Liang, Xiangtao Lu, Xiangru Tang, Tuo Zhou, Mingchen Zhuge, Jinlin Wang, Jiayi Zhang, Bangbang Liu, Yizhang Lin, Sirui Hong*\n\n**Abstract:** Large Language Model (LLM)-based agents have shown effectiveness across many applications. However, their use in data science scenarios requiring solving long-term interconnected tasks, dynamic data adjustments and domain expertise remains challenging. Previous approaches primarily focus on individual tasks, making it difficult to assess the complete data science workflow. Moreover, they struggle to handle real-time changes in intermediate data and fail to adapt dynamically to evolving task dependencies inherent to data science problems. In this paper, we present Data Interpreter, an LLM-based agent designed to automatically solve various data science problems end-to-end. Our Data Interpreter incorporates two key modules: 1) Hierarchical Graph Modeling, which breaks down complex problems into manageable subproblems, enabling dynamic node generation and graph optimization; and 2) Programmable Node Generation, a technique that refines and verifies each subproblem to iteratively improve code generation results and robustness. Extensive experiments consistently demonstrate the superiority of Data Interpreter. On InfiAgent-DABench, it achieves a 25% performance boost, raising accuracy from 75.9% to 94.9%. For machine learning and open-ended tasks, it improves performance from 88% to 95%, and from 60% to 97%, respectively. Moreover, on the MATH dataset, Data Interpreter achieves remarkable performance with a 26% improvement compared to state-of-the-art baselines. The code is available at https://github.com/geekan/MetaGPT.\n\n**Published:** 2024-02-28\n\n\n\n#### 4. More ConvNets in the 2020s: Scaling up Kernels Beyond 51x51 using Sparsity\n\n*From Search Query: hierarchical state management*\n\n*Zhangyang Wang, Decebal Mocanu, Mykola Pechenizkiy, Tommi K\u00e4rkk\u00e4inen, Boqian Wu, Qiao Xiao, Xuxi Chen, Xiaohan Chen, Tianlong Chen, Shiwei Liu*\n\n**Abstract:** Transformers have quickly shined in the computer vision world since the emergence of Vision Transformers (ViTs). The dominant role of convolutional neural networks (CNNs) seems to be challenged by increasingly effective transformer-based models. Very recently, a couple of advanced convolutional models strike back with large kernels motivated by the local-window attention mechanism, showing appealing performance and efficiency. While one of them, i.e. RepLKNet, impressively manages to scale the kernel size to 31x31 with improved performance, the performance starts to saturate as the kernel size continues growing, compared to the scaling trend of advanced ViTs such as Swin Transformer. In this paper, we explore the possibility of training extreme convolutions larger than 31x31 and test whether the performance gap can be eliminated by strategically enlarging convolutions. This study ends up with a recipe for applying extremely large kernels from the perspective of sparsity, which can smoothly scale up kernels to 61x61 with better performance. Built on this recipe, we propose Sparse Large Kernel Network (SLaK), a pure CNN architecture equipped with sparse factorized 51x51 kernels that can perform on par with or better than state-of-the-art hierarchical Transformers and modern ConvNet architectures like ConvNeXt and RepLKNet, on ImageNet classification as well as a wide range of downstream tasks including semantic segmentation on ADE20K, object detection on PASCAL VOC 2007, and object detection/segmentation on MS COCO.\n\n**Published:** 2022-07-07\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models, particularly focusing on event-based computation, hierarchical state management, and efficient implementations, here are some key findings and references that align with your goals:\n\n## Event-Based Computation in Neural Networks\n\n### Sparse and Efficient Computation\n- The concept of event-based computation is well-explored in the context of spiking neural networks (SNNs) and sparse recurrent neural networks. For instance, the \"SparseProp\" method reduces the computational cost of both forward and backward pass operations from O(N) to O(log(N)) per network spike, enabling efficient simulations of large spiking networks Analysis Note.\n- The paper on \"A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequences\" introduces SPikE-SSM, which incorporates event-driven neuronal dynamics with parallel computing, facilitating sparse and efficient training and inference for long-range dependencies learning. This approach leverages a max-min boundary compression (PMBC) strategy to accelerate neuronal dynamics.\n\n## Hierarchical State Management Techniques\n\n### Hierarchical State Integration\n- The \"DenseMamba\" model introduces a hierarchical approach by selectively integrating shallow-layer hidden states into deeper layers. This method retains fine-grained information crucial for the final output, improving information flow and efficiency Analysis Note.\n- Hierarchical state management is also discussed in the context of sequence-to-sequence models, where models like DenseMamba use state space models with dense hidden connections to manage and propagate information efficiently along the sequence length dimension Analysis Note.\n\n## Combining Event-Based and Hierarchical Approaches\n\n### Integrating Event-Based Computation with State Space Models\n- The integration of event-based computation with state space models is a promising direction. For example, SPikE-SSM combines event-driven neuronal dynamics with state space models, enabling efficient and parallel training for long sequence modeling. This approach incorporates a refractory neuron model to adjust the membrane potential with dynamic reset, achieving high accuracy and low spiking rates.\n- The \"Mamba\" paper suggests that making SSM parameters functions of the input addresses can allow the model to selectively propagate or forget information along the sequence length dimension, which can be combined with event-based computation principles to enhance efficiency Analysis Note.\n\n## Practical Implementations and Efficiency Considerations\n\n### Hardware-Efficient Implementation\n- Implementations like \"FlashAttention\" provide IO-aware principles that can be adapted to reduce computational complexity and improve memory efficiency. This is crucial for achieving cache-friendly memory access patterns, which are important for the overall efficiency of the model Analysis Note.\n- The use of sparse factorizations in models like the Sparse Transformer reduces computational complexity from O(n\u00b2) to O(n\u221an), allowing for faster processing times and real-time applications. This is particularly beneficial for handling large input sizes efficiently.\n\n### Memory Optimization\n- Techniques such as Neurocache's compressed state storage and MIMONets' computation in superposition offer significant efficiency gains. These methods balance compression against computational overhead, ensuring that the model remains efficient while maintaining performance Analysis Note.\n\nBy combining these approaches, researchers can develop autoregressive language models that leverage event-based computation for efficiency, hierarchical state management for effective information flow, and hardware-efficient implementations to optimize performance and scalability.\n",
            "analysis": "Analysis Note: Second Round Research on LM Block Design\n\n1. Summary of Analysis\n\nThe second round of research reveals promising directions for enhancing RWKV6's state management and computational efficiency:\n\na) Selective State Processing:\n- Content-based state updates from Mamba show potential for enhancing DDLerpLinear\n- SparseProp demonstrates benefits of event-based computation\n- Neural Wave Machines offer insights into spatiotemporal structure\n\nb) Memory and State Compression:\n- Neurocache's compressed state storage provides efficiency gains\n- MIMONets show benefits of computation in superposition\n- Hierarchical approaches from DenseMamba improve information flow\n\nc) Hardware-Efficient Implementation:\n- FlashAttention's IO-aware principles can be adapted\n- SparseProp's efficient state updates reduce computational complexity\n- Need for cache-friendly memory access patterns\n\n2. Useful References with Excerpts\n\nFrom Mamba paper:\n\"We identify that a key weakness of such models is their inability to perform content-based reasoning [...] simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension\"\n\nFrom SparseProp:\n\"reduces the computational cost of both the forward and backward pass operations from O(N) to O(log(N)) per network spike, thereby enabling numerically exact simulations of large spiking networks\"\n\nFrom Neural Wave Machines:\n\"uses complex spatiotemporal structure such as traveling waves to encode observed transformations [...] both more parameter efficient, and is able to forecast future trajectories\"\n\nFrom DenseMamba:\n\"selectively integrating shallow-layer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output\"\n\n3. Key Insights and Analysis\n\na) State Management Innovations:\n- Content-based state updates could significantly improve RWKV6's ability to selectively process information\n- Event-based computation principles from SparseProp could enhance efficiency\n- Spatiotemporal structure from Neural Wave Machines could improve sequence modeling\n\nb) Memory Optimization:\n- Hierarchical state integration from DenseMamba could enhance information flow\n- Computation in superposition from MIMONets offers efficiency gains\n- Need to balance compression vs computational overhead\n\nc) Implementation Considerations:\n- IO-aware algorithms crucial for performance\n- Event-based updates could reduce computational complexity\n- Cache-friendly access patterns important for efficiency\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Techniques for combining event-based computation with state space models\n2. Methods for hierarchical state management in recurrent architectures\n3. Approaches for hardware-efficient implementation of selective computation\n\n5. References\n\n[1] Engelken, R. (2023). SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks. Neural Information Processing Systems.\n\n[2] Keller, T.A., & Welling, M. (2023). Neural Wave Machines: Learning Spatiotemporally Structured Representations with Locally Coupled Oscillatory Recurrent Neural Networks. International Conference on Machine Learning.\n\n[3] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv.org.\n\n[4] Menet, N., et al. (2023). MIMONets: Multiple-Input-Multiple-Output Neural Networks Exploiting Computation in Superposition. Neural Information Processing Systems.\n\n[5] Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems."
        },
        {
            "ready": false,
            "query": "efficient sparse operations neural networks, dynamic routing hierarchical networks",
            "detail": "Find papers discussing:\n1. Efficient implementations of sparse neural network operations\n2. Dynamic routing mechanisms in hierarchical neural architectures\n3. Hardware-efficient sparse computation techniques\nFocus on practical implementations and performance considerations.",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nFind papers discussing:\n1. Efficient implementations of sparse neural network operations\n2. Dynamic routing mechanisms in hierarchical neural architectures\n3. Hardware-efficient sparse computation techniques\nFocus on practical implementations and performance considerations.\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. An Attention Free Transformer (Avg. Score: 0.87)\n\n*Shuangfei Zhai, Walter A. Talbott, Nitish Srivastava, Chen Huang, Hanlin Goh, Ruixiang Zhang, J. Susskind*\n\n**Published in:** arXiv.org (2021)\t**Cited by** 90  (*Influential: 10*)\n\n**TL;DR:** Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention, is introduced and demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n**Abstract:** We introduce Attention Free Transformer (AFT), an efficient variant of Transformers that eliminates the need for dot product self attention. In an AFT layer, the key and value are first combined with a set of learned position biases, the result of which is multiplied with the query in an element-wise fashion. This new operation has a memory complexity linear w.r.t. both the context size and the dimension of features, making it compatible to both large input and model sizes. We also introduce AFT-local and AFT-conv, two model variants that take advantage of the idea of locality and spatial weight sharing while maintaining global connectivity. We conduct extensive experiments on two autoregressive modeling tasks (CIFAR10 and Enwik8) as well as an image recognition task (ImageNet-1K classification). We show that AFT demonstrates competitive performance on all the benchmarks, while providing excellent efficiency at the same time.\n\n##### *Relevant Chunk: No. 19/28 (Score: 0.87)*\n\n```\nIn $A C L, 2019$. [24] Aurko Roy, M. Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. ArXiv, abs/2003.05997, 2020. [25] Felix Wu, Angela Fan, Alexei Baevski, Yann Dauphin, and M. Auli. Pay less attention with lightweight and dynamic convolutions. ArXiv, abs/1901.10430, 2019. [26] Yi Tay, Dara Bahri, L. Yang, Donald Metzler, and D. Juan. Sparse sinkhorn attention. ArXiv, abs/2002.11296, 2020. [27] Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision, 2021.\n```\n\n#### 2. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.75)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 22/46 (Score: 0.75)*\n\n```\nIn International Conference on Machine Learning, pages 5547-5569. PMLR, 2022 . [34] Yuli Eidelman and Israel Gohberg. On a new class of structured matrices. Integral Equations and Operator Theory, 34(3):293-324, 1999. [35] Murali Emani, Venkatram Vishwanath, Corey Adams, Michael E Papka, Rick Stevens, Laura Florescu, Sumti Jairath, William Liu, Tejas Nama, and Arvind Sujeeth. Accelerating scientific applications with sambanova reconfigurable dataflow architecture. Computing in Science 8 Engineering, 23(2):114-119, 2021 . [36] Yassir Fathullah, Chunyang Wu, Yuan Shangguan, Junteng Jia, Wenhan Xiong, Jay Mahadeokar, Chunxi Liu, Yangyang Shi, Ozlem Kalinli, Mike Seltzer, et al. Multi-head state space model for speech recognition. arXiv preprint arXiv:2305.12498, 2023. [37] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. The Journal of Machine Learning Research, 23(1):5232-5270, 2022 . [38] Quentin Fournier, Ga\u00e9tan Marceau Caron, and Daniel Aloise. A practical survey on faster and lighter transformers. ACM Computing Surveys, 2021. [39] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018. [40] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. Linear mode connectivity and the lottery ticket hypothesis.\n```\n\n#### 3. MoA: Mixture of Sparse Attention for Automatic Large Language Model Compression (Avg. Score: 0.51)\n\n*Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, Guohao Dai, Huazhong Yang, Yu Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** The Mixture of Attention (MoA) is proposed, which automatically tailors distinct sparse attention configurations to different heads and layers, and narrows the capability gaps between sparse and dense models.\n\n**Abstract:** Sparse attention can effectively mitigate the significant memory and throughput demands of Large Language Models (LLMs) in long contexts. Existing methods typically employ a uniform sparse attention mask, applying the same sparse pattern across different attention heads and input lengths. However, this uniform approach fails to capture the diverse attention patterns inherent in LLMs, ignoring their distinct accuracy-latency trade-offs. To address this challenge, we propose the Mixture of Attention (MoA), which automatically tailors distinct sparse attention configurations to different heads and layers. MoA constructs and navigates a search space of various attention patterns and their scaling rules relative to input sequence lengths. It profiles the model, evaluates potential configurations, and pinpoints the optimal sparse attention compression plan. MoA adapts to varying input sizes, revealing that some attention heads expand their focus to accommodate longer sequences, while other heads consistently concentrate on fixed-length local contexts. Experiments show that MoA increases the effective context length by $3.9\\times$ with the same average attention span, boosting retrieval accuracy by $1.5-7.1\\times$ over the uniform-attention baseline across Vicuna-7B, Vicuna-13B, and Llama3-8B models. Moreover, MoA narrows the capability gaps between sparse and dense models, reducing the maximum relative performance drop from $9\\%-36\\%$ to within $5\\%$ across two long-context understanding benchmarks. MoA achieves a $1.2-1.4\\times$ GPU memory reduction and boosts decode throughput by $5.5-6.7 \\times$ for 7B and 13B dense models on a single GPU, with minimal impact on performance.\n\n##### *Relevant Chunk: No. 26/38 (Score: 0.51)*\n\n```\nAssociation for Computing Machinery. [53] Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53-68, 2021. [54] Ying Sheng, Lianmin Zheng, Binhang Yuan, Zhuohan Li, Max Ryabinin, Daniel Y. Fu, Zhiqiang Xie, Beidi Chen, Clark W. Barrett, Joseph Gonzalez, Percy Liang, Christopher R\u00e9, Ion Stoica, and Ce Zhang. High-throughput generative inference of large language models with a single gpu. In International Conference on Machine Learning, 2023. [55] Han Shi, Jiahui Gao, Xiaozhe Ren, Hang Xu, Xiaodan Liang, Zhenguo Li, and James Tin-Yau Kwok. Sparsebert: Rethinking the importance analysis in self-attention.\n```\n\n#### 4. Big Bird: Transformers for Longer Sequences (Avg. Score: 0.26)\n\n*M. Zaheer, Guru Guruganesh, Kumar Avinava Dubey, J. Ainslie, Chris Alberti, Santiago Onta\u00f1\u00f3n, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, Amr Ahmed*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 1631  (*Influential: 238*)\n\n**TL;DR:** It is shown that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model.\n\n**Abstract:** Transformers-based models, such as BERT, have been one of the most successful deep learning models for NLP. Unfortunately, one of their core limitations is the quadratic dependency (mainly in terms of memory) on the sequence length due to their full attention mechanism. To remedy this, we propose, BigBird, a sparse attention mechanism that reduces this quadratic dependency to linear. We show that BigBird is a universal approximator of sequence functions and is Turing complete, thereby preserving these properties of the quadratic, full attention model. Along the way, our theoretical analysis reveals some of the benefits of having $O(1)$ global tokens (such as CLS), that attend to the entire sequence as part of the sparse attention mechanism. The proposed sparse attention can handle sequences of length up to 8x of what was previously possible using similar hardware. As a consequence of the capability to handle longer context, BigBird drastically improves performance on various NLP tasks such as question answering and summarization. We also propose novel applications to genomics data.\n\n##### *Relevant Chunk: No. 70/94 (Score: 0.26)*\n\n```\nIn Advances in neural information processing systems, pages 5754-5764, 2019. [102] Z. Yao, S. Cao, W. Xiao, C. Zhang, and L. Nie. Balanced sparsity for efficient dnn inference on gpu. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages $5676-5683,2019$. [103] Z. Ye, Q. Guo, Q. Gan, X.\n```\n\n#### 5. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.23)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.23)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n\n\n---\n## Found 9 related papers from 2 external sources\n\n\n\nYour 2 raw search queries input to the search frame: efficient sparse operations neural networks, dynamic routing hierarchical networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related papers from Semantic Scholar\n\n#### 1. SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks\n\n*From Search Query: efficient sparse operations neural networks*\n\n*Rainer Engelken*\n\n**TL;DR:** This work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.\n\n**Abstract:** Spiking Neural Networks (SNNs) are biologically-inspired models that are capable of processing information in streams of action potentials. However, simulating and training SNNs is computationally expensive due to the need to solve large systems of coupled differential equations. In this paper, we introduce SparseProp, a novel event-based algorithm for simulating and training sparse SNNs. Our algorithm reduces the computational cost of both the forward and backward pass operations from O(N) to O(log(N)) per network spike, thereby enabling numerically exact simulations of large spiking networks and their efficient training using backpropagation through time. By leveraging the sparsity of the network, SparseProp eliminates the need to iterate through all neurons at each spike, employing efficient state updates instead. We demonstrate the efficacy of SparseProp across several classical integrate-and-fire neuron models, including a simulation of a sparse SNN with one million LIF neurons. This results in a speed-up exceeding four orders of magnitude relative to previous event-based implementations. Our work provides an efficient and exact solution for training large-scale spiking neural networks and opens up new possibilities for building more sophisticated brain-inspired models.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 2. An Empirical Study of Adder Neural Networks for Object Detection\n\n*From Search Query: efficient sparse operations neural networks*\n\n*Xinghao Chen, Chang Xu, Minjing Dong, Chunjing Xu, Yunhe Wang*\n\n**TL;DR:** The proposed Adder FCOS achieves a 37.8\\% AP on the COCO val set, demonstrating comparable performance to that of the convolutional counterpart with an about $1.4\\times$ energy reduction.\n\n**Abstract:** Adder neural networks (AdderNets) have shown impressive performance on image classification with only addition operations, which are more energy efficient than traditional convolutional neural networks built with multiplications. Compared with classification, there is a strong demand on reducing the energy consumption of modern object detectors via AdderNets for real-world applications such as autonomous driving and face detection. In this paper, we present an empirical study of AdderNets for object detection. We first reveal that the batch normalization statistics in the pre-trained adder backbone should not be frozen, since the relatively large feature variance of AdderNets. Moreover, we insert more shortcut connections in the neck part and design a new feature fusion architecture for avoiding the sparse features of adder layers. We present extensive ablation studies to explore several design choices of adder detectors. Comparisons with state-of-the-arts are conducted on COCO and PASCAL VOC benchmarks. Specifically, the proposed Adder FCOS achieves a 37.8\\% AP on the COCO val set, demonstrating comparable performance to that of the convolutional counterpart with an about $1.4\\times$ energy reduction.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 14  (*Influential: 0*)\n\n#### 3. S3: Sign-Sparse-Shift Reparametrization for Effective Training of Low-bit Shift Networks\n\n*From Search Query: efficient sparse operations neural networks*\n\n*Xinlin Li, Bang Liu, Yaoliang Yu, Wulong Liu, Chunjing Xu, V. Nia*\n\n**TL;DR:** The proposed training method pushes the boundaries of shift neural networks and shows 3-bit shift networks out-performs their full-precision counterparts in terms of top-1 accuracy on ImageNet.\n\n**Abstract:** Shift neural networks reduce computation complexity by removing expensive multiplication operations and quantizing continuous weights into low-bit discrete values, which are fast and energy efficient compared to conventional neural networks. However, existing shift networks are sensitive to the weight initialization, and also yield a degraded performance caused by vanishing gradient and weight sign freezing problem. To address these issues, we propose S low-bit re-parameterization, a novel technique for training low-bit shift networks. Our method decomposes a discrete parameter in a sign-sparse-shift 3-fold manner. In this way, it efficiently learns a low-bit network with a weight dynamics similar to full-precision networks and insensitive to weight initialization. Our proposed training method pushes the boundaries of shift neural networks and shows 3-bit shift networks out-performs their full-precision counterparts in terms of top-1 accuracy on ImageNet.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 5  (*Influential: 1*)\n\n#### 4. Mapping State Space using Landmarks for Universal Goal Reaching\n\n*From Search Query: dynamic routing hierarchical networks*\n\n*Zhiao Huang, Fangchen Liu, Hao Su*\n\n**TL;DR:** The method explicitly models the environment in a hierarchical manner, with a high-level dynamic landmark-based map abstracting the visited state space, and a low-level value network to derive precise local decisions that enable the agent to reach long-range goals at the early training stage.\n\n**Abstract:** An agent that has well understood the environment should be able to apply its skills for any given goals, leading to the fundamental problem of learning the Universal Value Function Approximator (UVFA). A UVFA learns to predict the cumulative rewards between all state-goal pairs. However, empirically, the value function for long-range goals is always hard to estimate and may consequently result in failed policy. This has presented challenges to the learning process and the capability of neural networks. We propose a method to address this issue in large MDPs with sparse rewards, in which exploration and routing across remote states are both extremely challenging. Our method explicitly models the environment in a hierarchical manner, with a high-level dynamic landmark-based map abstracting the visited state space, and a low-level value network to derive precise local decisions. We use farthest point sampling to select landmark states from past experience, which has improved exploration compared with simple uniform sampling. Experimentally we showed that our method enables the agent to reach long-range goals at the early training stage, and achieve better performance than standard RL algorithms for a number of challenging tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 57  (*Influential: 9*)\n\n#### 5. Dynamic Routing Transformer Network for Multimodal Sarcasm Detection\n\n*From Search Query: dynamic routing hierarchical networks*\n\n*Yuan Tian, Nan Xu, Ruike Zhang, W. Mao*\n\n**TL;DR:** This work model the dynamic mechanism in multimodal sarcasm detection and proposes the Dynamic Routing Transformer Network (DynRT-Net), which utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity.\n\n**Abstract:** Multimodal sarcasm detection is an important research topic in natural language processing and multimedia computing, and benefits a wide range of applications in multiple domains. Most existing studies regard the incongruity between image and text as the indicative clue in identifying multimodal sarcasm. To capture cross-modal incongruity, previous methods rely on fixed architectures in network design, which restricts the model from dynamically adjusting to diverse image-text pairs. Inspired by routing-based dynamic network, we model the dynamic mechanism in multimodal sarcasm detection and propose the Dynamic Routing Transformer Network (DynRT-Net). Our method utilizes dynamic paths to activate different routing transformer modules with hierarchical co-attention adapting to cross-modal incongruity. Experimental results on a public dataset demonstrate the effectiveness of our method compared to the state-of-the-art methods. Our codes are available at https://github.com/TIAN-viola/DynRT.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2023\n\n**Citations:** 18  (*Influential: 0*)\n\n### 4 related papers from Papers with Code\n\n#### 1. CogDL: A Comprehensive Library for Graph Deep Learning\n\n*From Search Query: efficient sparse operations neural networks*\n\n*Yu Wang, Guohao Dai, Peng Zhang, Yuxiao Dong, Shiguang Guo, Xingcheng Yao, Hengrui Zhang, Zhongming Yu, Yang Yang, Jie Tang, Hongxia Yang, Chang Zhou, Aohan Zeng, Yizhen Luo, Qibin Chen, Yan Wang, Zhenyu Hou, Yukuo Cen*\n\n**Abstract:** Graph neural networks (GNNs) have attracted tremendous attention from the graph learning community in recent years. It has been widely adopted in various real-world applications from diverse domains, such as social networks and biological graphs. The research and applications of graph deep learning present new challenges, including the sparse nature of graph data, complicated training of GNNs, and non-standard evaluation of graph tasks. To tackle the issues, we present CogDL, a comprehensive library for graph deep learning that allows researchers and practitioners to conduct experiments, compare methods, and build applications with ease and efficiency. In CogDL, we propose a unified design for the training and evaluation of GNN models for various graph tasks, making it unique among existing graph learning libraries. By utilizing this unified trainer, CogDL can optimize the GNN training loop with several training techniques, such as mixed precision training. Moreover, we develop efficient sparse operators for CogDL, enabling it to become the most competitive graph library for efficiency. Another important CogDL feature is its focus on ease of use with the aim of facilitating open and reproducible research of graph learning. We leverage CogDL to report and maintain benchmark results on fundamental graph tasks, which can be reproduced and directly used by the community.\n\n**Published:** 2021-03-01\n\n\n\n#### 2. Symbolic Discovery of Optimization Algorithms\n\n*From Search Query: efficient sparse operations neural networks*\n\n*Quoc V. Le, Yifeng Lu, Cho-Jui Hsieh, Thang Luong, Xuanyi Dong, Hieu Pham, Yao Liu, Kaiyuan Wang, Esteban Real, Da Huang, Chen Liang, Xiangning Chen*\n\n**Abstract:** We present a method to formulate algorithm discovery as program search, and apply it to discover optimization algorithms for deep neural network training. We leverage efficient search techniques to explore an infinite and sparse program space. To bridge the large generalization gap between proxy and target tasks, we also introduce program selection and simplification strategies. Our method discovers a simple and effective optimization algorithm, $\\textbf{Lion}$ ($\\textit{Evo$\\textbf{L}$ved S$\\textbf{i}$gn M$\\textbf{o}$me$\\textbf{n}$tum}$). It is more memory-efficient than Adam as it only keeps track of the momentum. Different from adaptive optimizers, its update has the same magnitude for each parameter calculated through the sign operation. We compare Lion with widely used optimizers, such as Adam and Adafactor, for training a variety of models on different tasks. On image classification, Lion boosts the accuracy of ViT by up to 2% on ImageNet and saves up to 5x the pre-training compute on JFT. On vision-language contrastive learning, we achieve 88.3% $\\textit{zero-shot}$ and 91.1% $\\textit{fine-tuning}$ accuracy on ImageNet, surpassing the previous best results by 2% and 0.1%, respectively. On diffusion models, Lion outperforms Adam by achieving a better FID score and reducing the training compute by up to 2.3x. For autoregressive, masked language modeling, and fine-tuning, Lion exhibits a similar or better performance compared to Adam. Our analysis of Lion reveals that its performance gain grows with the training batch size. It also requires a smaller learning rate than Adam due to the larger norm of the update produced by the sign function. Additionally, we examine the limitations of Lion and identify scenarios where its improvements are small or not statistically significant. Lion is also successfully deployed in production systems such as Google search ads CTR model.\n\n**Conference:** symbolic-discovery-of-optimization-algorithms\n\n**Published:** 2023-02-13\n\n\n\n#### 3. Joint Slot Filling and Intent Detection via Capsule Neural Networks\n\n*From Search Query: dynamic routing hierarchical networks*\n\n*Yaliang Li, Philip S. Yu, Nan Du, Chenwei Zhang, Wei Fan*\n\n**Abstract:** Being able to recognize words as slots and detect the intent of an utterance has been a keen issue in natural language understanding. The existing works either treat slot filling and intent detection separately in a pipeline manner, or adopt joint models which sequentially label slots while summarizing the utterance-level intent without explicitly preserving the hierarchical relationship among words, slots, and intents. To exploit the semantic hierarchy for effective modeling, we propose a capsule-based neural network model which accomplishes slot filling and intent detection via a dynamic routing-by-agreement schema. A re-routing schema is proposed to further synergize the slot filling performance using the inferred intent representation. Experiments on two real-world datasets show the effectiveness of our model when compared with other alternative model architectures, as well as existing natural language understanding services.\n\n**Conference:** joint-slot-filling-and-intent-detection-via-1\n\n**Published:** 2018-12-22\n\n\n\n#### 4. Robust Neural Routing Through Space Partitions for Camera Relocalization in Dynamic Indoor Environments\n\n*From Search Query: dynamic routing hierarchical networks*\n\n*Leonidas Guibas, Baoquan Chen, Thomas Funkhouser, Li Yi, Ji Shi, He Wang, Qingnan Fan, Siyan Dong*\n\n**Abstract:** Localizing the camera in a known indoor environment is a key building block for scene mapping, robot navigation, AR, etc. Recent advances estimate the camera pose via optimization over the 2D/3D-3D correspondences established between the coordinates in 2D/3D camera space and 3D world space. Such a mapping is estimated with either a convolution neural network or a decision tree using only the static input image sequence, which makes these approaches vulnerable to dynamic indoor environments that are quite common yet challenging in the real world. To address the aforementioned issues, in this paper, we propose a novel outlier-aware neural tree which bridges the two worlds, deep learning and decision tree approaches. It builds on three important blocks: (a) a hierarchical space partition over the indoor scene to construct the decision tree; (b) a neural routing function, implemented as a deep classification network, employed for better 3D scene understanding; and (c) an outlier rejection module used to filter out dynamic points during the hierarchical routing process. Our proposed algorithm is evaluated on the RIO-10 benchmark developed for camera relocalization in dynamic indoor environments. It achieves robust neural routing through space partitions and outperforms the state-of-the-art approaches by around 30% on camera pose accuracy, while running comparably fast for evaluation.\n\n**Proceeding:** cvpr-2021-1\n\n**Published:** 2020-12-08\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model, particularly focusing on efficient sparse operations, dynamic routing in hierarchical networks, and hardware-efficient sparse computation techniques, here are some relevant findings and references:\n\n## Efficient Implementations of Sparse Neural Network Operations\n\n1. **Sorted Weight Sectioning (SWS):**\n   The paper on \"Sorted Weight Sectioning for Energy-Efficient Unstructured Sparse DNNs\" introduces a weight allocation algorithm that efficiently addresses unstructured sparse models. SWS places sorted DNN weight sections on compute-in-memory (CIM) crossbars to reduce analog-to-digital converter (ADC) energy consumption. This approach leverages the sparsity of weights and their distribution to minimize energy costs, which is crucial for efficient sparse operations.\n\n2. **SPikE-SSM:**\n   The \"SPikE-SSM: A Sparse, Precise, and Efficient Spiking State Space Model for Long Sequences Learning\" paper proposes a framework for spiking neural networks (SNNs) that incorporates sparse and efficient state space models. The boundary compression strategy (PMBC) and the integration of trainable thresholds and refractory magnitudes help in maintaining high sparsity while achieving high accuracy, which is beneficial for efficient sparse operations in autoregressive models.\n\n## Dynamic Routing Mechanisms in Hierarchical Neural Architectures\n\n1. **Hierarchical State Management and Dynamic Routing:**\n   The analysis note highlights the importance of hierarchical state management and dynamic routing of information. While the provided references do not explicitly discuss dynamic routing in the context of hierarchical neural architectures, the concept of selective state integration across layers and dynamic routing can be inferred from the need to balance sparsity with model expressiveness and the efficient flow of information between levels. This suggests that mechanisms like those used in SNNs, such as the reset-refractory mechanism in SPikE-SSM, could be adapted for dynamic routing in hierarchical structures.\n\n## Hardware-Efficient Sparse Computation Techniques\n\n1. **Compute-in-Memory Crossbars:**\n   The SWS algorithm mentioned earlier is specifically designed for compute-in-memory (CIM) crossbars, which are hardware-efficient for sparse computations. By leveraging the sparsity of weights, SWS reduces the energy consumption associated with data conversions, making it a practical implementation for hardware-efficient sparse operations.\n\n2. **Photonic Neural Cellular Automata (PNCA):**\n   The PNCA architecture uses photonic hardware with sparse connections, which requires fewer parameters compared to fully connected networks. This architecture can tolerate noisy hardware and maintain high accuracy with fewer parameters, making it a hardware-efficient approach for sparse computations. The local update rule and the use of fixed weights after training further enhance its efficiency.\n\n## Practical Implementations and Performance Considerations\n\n1. **IO-Aware Computation Strategies:**\n   The analysis note emphasizes the importance of IO-aware computation strategies and cache-friendly memory patterns for performance. Techniques like SWS and the sparse, precise, and efficient spiking SSM framework (SPikE-SSM) are designed with these considerations in mind, ensuring that the implementations are not only efficient in terms of computation but also in terms of memory and IO operations.\n\n2. **Efficient Alternatives to Matrix Multiplication:**\n   The reference to \"Scalable MatMul-free Language Modeling\" suggests that eliminating matrix multiplication operations can significantly reduce memory usage and improve performance. This approach can be integrated with sparse operations and hierarchical state management to enhance the overall efficiency of the autoregressive language model[Analysis Note].\n\nBy combining these insights, researchers can develop an autoregressive language model that incorporates efficient sparse operations, dynamic routing mechanisms, and hardware-efficient computation techniques, leading to improved performance, scalability, and efficiency.\n",
            "analysis": "Analysis Note: Third Round Research on LM Block Design\n\n1. Summary of Analysis\n\nAfter three rounds of research, several promising directions have emerged for improving RWKV6:\n\na) Event-Based Computation:\n- Potential to incorporate event-driven updates from SNNs\n- Opportunities for sparse and selective computation\n- Integration with state space models for efficiency\n\nb) Hierarchical State Management:\n- Multi-level state organization\n- Selective state integration across layers\n- Dynamic routing of information\n\nc) Implementation Efficiency:\n- IO-aware computation strategies\n- Cache-friendly memory patterns\n- Hardware-efficient sparse operations\n\n2. Useful References with Excerpts\n\nFrom \"Temporal Spike Sequence Learning\":\n\"breaks down error backpropagation across two types of inter-neuron and intra-neuron dependencies and leads to improved temporal learning precision [...] captures inter-neuron dependencies through presynaptic firing times\"\n\nThis suggests potential for temporal dependency modeling in state updates.\n\nFrom \"Scalable MatMul-free Language Modeling\":\n\"MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales [...] reduces memory usage by up to 61% over an unoptimized baseline during training\"\n\nShows possibility of efficient alternatives to matrix multiplication.\n\nFrom \"LightNet\":\n\"identifies the inefficiency caused by a multiplicative linear recurrence and proposes an efficient alternative additive linear recurrence [...] can handle multi-dimensional data within a single scan\"\n\nDemonstrates benefits of additive vs multiplicative operations.\n\n3. Key Insights and Analysis\n\na) Event-Based State Updates:\n- Can reduce unnecessary computations by updating states only when significant changes occur\n- Potential to combine with RWKV6's DDLerpLinear for more efficient processing\n- Need to balance sparsity with model expressiveness\n\nb) Hierarchical Organization:\n- Multi-level state representation could improve information flow\n- Selective integration across layers can enhance efficiency\n- Need mechanism to route information between levels\n\nc) Implementation Considerations:\n- IO-aware algorithms crucial for performance\n- Cache-friendly patterns can significantly improve speed\n- Hardware-efficient operations important for scaling\n\n4. Future Search Plan\n\nNeed to investigate:\n1. Specific techniques for combining event-based updates with linear attention\n2. Methods for efficient hierarchical state organization\n3. Hardware-efficient implementations of sparse operations\n4. Approaches for dynamic routing in hierarchical structures\n\n5. References\n\n[1] Zhang, W., & Li, P. (2020). Temporal Spike Sequence Learning via Backpropagation for Deep Spiking Neural Networks. Neural Information Processing Systems.\n\n[2] Zhu, R.J., et al. (2024). Scalable MatMul-free Language Modeling. arXiv.org.\n\n[3] Qin, Z., et al. (2024). You Only Scan Once: Efficient Multi-dimension Sequential Modeling with LightNet. arXiv.org.\n\n[4] Xiao, M., et al. (2024). Temporal Spiking Neural Networks with Synaptic Delay for Graph Reasoning. International Conference on Machine Learning.\n\n[5] Zimerman, I., et al. (2024). A Unified Implicit Attention Formulation for Gated-Linear Recurrent Sequence Models. arXiv.org."
        },
        {
            "ready": false,
            "query": "event driven training stability, hierarchical neural compression, hardware efficient sparse computation",
            "detail": "Search for papers discussing:\n1. Event-driven training methods and stability:\n   - Training dynamics in event-based networks\n   - Stability analysis of selective updates\n   - Convergence guarantees for event-driven systems\n\n2. Hierarchical compression in neural networks:\n   - Multi-level state representation\n   - Adaptive compression techniques\n   - Information-preserving hierarchical structures\n\n3. Hardware-efficient sparse computation:\n   - Memory access optimization\n   - Sparse tensor operations\n   - Hardware-aware sparsity patterns",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Event-driven training methods and stability:\n   - Training dynamics in event-based networks\n   - Stability analysis of selective updates\n   - Convergence guarantees for event-driven systems\n\n2. Hierarchical compression in neural networks:\n   - Multi-level state representation\n   - Adaptive compression techniques\n   - Information-preserving hierarchical structures\n\n3. Hardware-efficient sparse computation:\n   - Memory access optimization\n   - Sparse tensor operations\n   - Hardware-aware sparsity patterns\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.01)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.01)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 2. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.01)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 20/43 (Score: 0.01)*\n\n```\nKnowledge-Based Systems, 295:111780, 2024. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Gpt3. int8 (): 8-bit matrix multiplication for transformers at scale. Advances in Neural Information Processing Systems (NeurIPS), 35:30318-30332, 2022a. Tim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. Llm. int8 (): 8-bit matrix multiplication for transformers at scale. arXiv preprint arXiv:2208.07339, 2022b. Payal Dhar. The carbon impact of artificial intelligence. Nature Machine Intelligence, 2:423-5, 2020. Peter U Diehl, Guido Zarrella, Andrew Cassidy, Bruno U Pedroni, and Emre Neftci. Conversion of artificial recurrent neural networks to spiking neural networks for low-power neuromorphic hardware. In IEEE International Conference on Rebooting Computing (ICRC), pp. 1-8, 2016. Jason K Eshraghian and Wei D Lu. The fine line between dead neurons and sparsity in binarized spiking neural networks. arXiv preprint arXiv:2201.11915, 2022. Jason K Eshraghian, Max Ward, Emre Neftci, Xinxin Wang, Gregor Lenz, Girish Dwivedi, Mohammed Bennamoun, Doo Seok Jeong, and Wei D Lu. Training spiking neural networks using lessons from deep learning. arXiv preprint arXiv:2109.12894, 2021. Jason K Eshraghian, Xinxin Wang, and Wei D Lu. Memristor-based binarized spiking neural networks: Challenges and applications. IEEE Nanotechnology Magazine, 16(2):14-23, 2022. Wei Fang, Yanqi Chen, Jianhao Ding, Ding Chen, Zhaofei Yu, Huihui Zhou, Timoth\u00e9e Masquelier, Yonghong Tian, and other contributors. Spikingjelly. https://github.com/fangwei123456/spikingjelly, 2020. Accessed: 2022-05-21. Wei Fang, Zhaofei Yu, Yanqi Chen, Tiejun Huang, Timoth\u00e9e Masquelier, and Yonghong Tian. Deep residual learning in spiking neural networks.\n```\n\n#### 3. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.01)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.01)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 4. Latent Attention for Linear Time Transformers (Avg. Score: 0.01)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.01)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectifier Neural Networks. In JMLR Workshop and Conference Proceedings, pp. 315-323, 2011. Gokaslan, A. and Cohen, V. OpenWebText Corpus, 2019. URL http://Skylion007.github.io/ OpenWebTextCorpus. Gu, A., Goel, K., and R\u00e9, C. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Hutter, M. The Human Knowledge Compression Prize, 2002. URL https:// www.kurzweilai.net/hutter-prizefor-lossless-compression-of-humanknowledge. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning, pp. 4651-4664. PMLR, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Khan, S., Naseer, M., Hayat, M., Zamir, S.\n```\n\n#### 5. Learning Fast Algorithms for Linear Transforms Using Butterfly Factorizations (Avg. Score: 0.01)\n\n*Tri Dao, Albert Gu, Matthew Eichhorn, A. Rudra, C. R\u00e9*\n\n**Published in:** International Conference on Machine Learning (2019)\t**Cited by** 84  (*Influential: 13*)\n\n**TL;DR:** This work introduces a parameterization of divide-and-conquer methods that can automatically learn an efficient algorithm for many important transforms, and can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations.\n\n**Abstract:** Fast linear transforms are ubiquitous in machine learning, including the discrete Fourier transform, discrete cosine transform, and other structured transformations such as convolutions. All of these transforms can be represented by dense matrix-vector multiplication, yet each has a specialized and highly efficient (subquadratic) algorithm. We ask to what extent hand-crafting these algorithms and implementations is necessary, what structural priors they encode, and how much knowledge is required to automatically learn a fast algorithm for a provided structured transform. Motivated by a characterization of fast matrix-vector multiplication as products of sparse matrices, we introduce a parameterization of divide-and-conquer methods that is capable of representing a large class of transforms. This generic formulation can automatically learn an efficient algorithm for many important transforms; for example, it recovers the O(N log N) Cooley-Tukey FFT algorithm to machine precision, for dimensions N up to 1024. Furthermore, our method can be incorporated as a lightweight replacement of generic matrices in machine learning pipelines to learn efficient and compressible transformations. On a standard task of compressing a single hidden-layer network, our method exceeds the classification accuracy of unconstrained matrices on CIFAR-10 by 3.9 points-the first time a structured approach has done so-with 4\u00d7 faster inference speed and 40\u00d7 fewer parameters.\n\n##### *Relevant Chunk: No. 11/35 (Score: 0.01)*\n\n```\nIn Advances in Neural Information Processing Systems, pp. 190-196, 1999. [2] Bello, I., Pham, H., Le, Q. V., Norouzi, M., and Bengio, S. Neural combinatorial optimization with reinforcement learning. 2016. [3] B\u00fcrgisser, P., Clausen, M., and Shokrollahi, M. A. Algebraic complexity theory, volume 315. Springer Science \\& Business Media, 2013. [4] Cand\u00e8s, E. J., Li, X., Ma, Y., and Wright, J. Robust principal component analysis? Journal of the $A C M(J A C M), 58(3): 11,2011$. [5] Chen, W., Wilson, J., Tyree, S., Weinberger, K., and Chen, Y. Compressing neural networks with the hashing trick. In Bach, F. and Blei, D. (eds.), Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pp. 2285-2294, Lille, France, 07-09 Jul 2015. PMLR. URL http://proceedings.mlr.press/v37/chenc15.html. [6] Cheng, Y., Yu, F.\n```\n\n\n\n---\n## Found 12 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: event driven training stability, hierarchical neural compression, hardware efficient sparse computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 7 related papers from Semantic Scholar\n\n#### 1. Training Spiking Neural Networks with Event-driven Backpropagation\n\n*From Search Query: event driven training stability*\n\n*Yaoyu Zhu, Zhaofei Yu, Wei Fang, Xiaodong Xie, Tiejun Huang, T. Masquelier*\n\n**TL;DR:** This paper analyzes the commonly used temporal backpropagation training approach and proves that the sum of gradients remains unchanged between fully-connected and convolutional layers, and proposes a backward kernel that can solve the reverse gradient problem for time-based gradients and keep the property of the invariable sum ofGradients.\n\n**Abstract:** Spiking Neural networks (SNNs) represent and transmit information by spatiotemporal spike patterns, which bring two major advantages: biological plausibility and suitability for ultralow-power neuromorphic implementation. Despite this, the binary firing characteristic makes training SNNs more challenging. To learn the parameters of deep SNNs in an event-driven fashion as in inference of SNNs, back-propagation with respect to spike timing is proposed. Although this event-driven learning has the advantages of lower computational cost and memory occupation, the accuracy is far below the recurrent neural network-like learning approaches. In this paper, we first analyze the commonly used temporal backpropagation training approach and prove that the sum of gradients remains unchanged between fully-connected and convolutional layers. Secondly, we show that the max pooling layer meets the above invariance rule, while the average pooling layer does not, which will suffer the gradient vanishing problem but can be revised to meet the requirement. Thirdly, we point out the reverse gradient problem for time-based gradients and propose a backward kernel that can solve this problem and keep the property of the invariable sum of gradients. The experimental results show that the proposed approach achieves state-of-the-art performance on CIFAR10 among time-based training methods. Also, this is the first time that the time-based backpropagation approach successfully trains SNN on the CIFAR100 dataset. Our code is available at https://github.com/zhuyaoyu/SNN-event-driven-learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 2*)\n\n#### 2. Event-Driven Emotion Cause Extraction with Corpus Construction\n\n*From Search Query: event driven training stability*\n\n*Lin Gui, Dongyin Wu, Ruifeng Xu, Q. Lu, Yu Zhou*\n\n**TL;DR:** A new event-driven emotion cause extraction method using multi-kernel SVMs where a syntactical tree based approach is used to represent events in text, and a 7-tuple de\ufb01nition to describe emotion cause events is proposed.\n\n**Abstract:** In this paper, we present our work in emotion cause extraction. Since there is no open dataset available, the lack of annotated resources has limited the research in this area. Thus, we \ufb01rst present a dataset we built using SINA city news. The annotation is based on the scheme of the W3C Emotion Markup Language. Second, we propose a 7-tuple de\ufb01nition to describe emotion cause events. Based on this general de\ufb01nition, we propose a new event-driven emotion cause extraction method using multi-kernel SVMs where a syntactical tree based approach is used to represent events in text. A convolution kernel based multi-kernel SVM are used to extract emotion causes. Because traditional convolution kernels do not use lexical information at the terminal nodes of syntactic trees, we modify the kernel function with a synonym based improvement. Even with very limited training data, we can still extract suf\ufb01cient features for the task. Evaluations show that our approach achieves 11.6% higher F-measure compared to referenced methods. The contributions of our work include resource construction, concept de\ufb01nition and algorithm development.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2016\n\n**Citations:** 180  (*Influential: 41*)\n\n#### 3. Temporal Efficient Training of Spiking Neural Network via Gradient Re-weighting\n\n*From Search Query: event driven training stability*\n\n*Shi-Wee Deng, Yuhang Li, Shanghang Zhang, Shi Gu*\n\n**TL;DR:** This paper analyzes why the current direct training approach with surrogate gradient results in SNNs with poor generalizability, and introduces the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalIZability.\n\n**Abstract:** Recently, brain-inspired spiking neuron networks (SNNs) have attracted widespread research interest because of their event-driven and energy-efficient characteristics. Still, it is difficult to efficiently train deep SNNs due to the non-differentiability of its activation function, which disables the typically used gradient descent approaches for traditional artificial neural networks (ANNs). Although the adoption of surrogate gradient (SG) formally allows for the back-propagation of losses, the discrete spiking mechanism actually differentiates the loss landscape of SNNs from that of ANNs, failing the surrogate gradient methods to achieve comparable accuracy as for ANNs. In this paper, we first analyze why the current direct training approach with surrogate gradient results in SNNs with poor generalizability. Then we introduce the temporal efficient training (TET) approach to compensate for the loss of momentum in the gradient descent with SG so that the training process can converge into flatter minima with better generalizability. Meanwhile, we demonstrate that TET improves the temporal scalability of SNN and induces a temporal inheritable training for acceleration. Our method consistently outperforms the SOTA on all reported mainstream datasets, including CIFAR-10/100 and ImageNet. Remarkably on DVS-CIFAR10, we obtained 83$\\%$ top-1 accuracy, over 10$\\%$ improvement compared to existing state of the art. Codes are available at \\url{https://github.com/Gus-Lab/temporal_efficient_training}.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2022\n\n**Citations:** 196  (*Influential: 48*)\n\n#### 4. Joint Autoregressive and Hierarchical Priors for Learned Image Compression\n\n*From Search Query: hierarchical neural compression*\n\n*David C. Minnen, J. Ball\u00e9, G. Toderici*\n\n**TL;DR:** It is found that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models.\n\n**Abstract:** Recent models for learned image compression are based on autoencoders that learn approximately invertible mappings from pixels to a quantized latent representation. The transforms are combined with an entropy model, which is a prior on the latent representation that can be used with standard arithmetic coding algorithms to generate a compressed bitstream. Recently, hierarchical entropy models were introduced as a way to exploit more structure in the latents than previous fully factorized priors, improving compression performance while maintaining end-to-end optimization. Inspired by the success of autoregressive priors in probabilistic generative models, we examine autoregressive, hierarchical, and combined priors as alternatives, weighing their costs and benefits in the context of image compression. While it is well known that autoregressive models can incur a significant computational penalty, we find that in terms of compression performance, autoregressive and hierarchical priors are complementary and can be combined to exploit the probabilistic structure in the latents better than all previous learned models. The combined model yields state-of-the-art rate\u2013distortion performance and generates smaller files than existing methods: 15.8% rate reductions over the baseline hierarchical model and 59.8%, 35%, and 8.4% savings over JPEG, JPEG2000, and BPG, respectively. To the best of our knowledge, our model is the first learning-based method to outperform the top standard image codec (BPG) on both the PSNR and MS-SSIM distortion metrics.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 1082  (*Influential: 275*)\n\n#### 5. Accelerated Sparse Neural Training: A Provable and Efficient Method to Find N: M Transposable Masks\n\n*From Search Query: hardware efficient sparse computation*\n\n*Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, S. Naor, Daniel Soudry*\n\n**TL;DR:** A new measure called mask-diversity is suggested which correlates with the expected accuracy of the different types of structural pruning, and a novel transposable fine-grained sparsity mask is suggested, which guarantees that both the weight matrix and its transpose follow the same sparsity pattern.\n\n**Abstract:** Unstructured pruning reduces the memory footprint in deep neural networks (DNNs). Recently, researchers proposed different types of structural pruning intending to reduce also the computation complexity. In this work, we first suggest a new measure called mask-diversity which correlates with the expected accuracy of the different types of structural pruning. We focus on the recently suggested N:M fine-grained block sparsity mask, in which for each block of M weights, we have at least N zeros. While N:M fine-grained block sparsity allows acceleration in actual modern hardware, it can be used only to accelerate the inference phase. In order to allow for similar accelerations in the training phase, we suggest a novel transposable fine-grained sparsity mask, where the same mask can be used for both forward and backward passes. Our transposable mask guarantees that both the weight matrix and its transpose follow the same sparsity pattern; thus, the matrix multiplication required for passing the error backward can also be accelerated. We formulate the problem of finding the optimal transposable-mask as a minimum-cost flow problem. Additionally, to speed up the minimum-cost flow computation, we also introduce a fast linear-time approximation that can be used when the masks dynamically change during training. Our experiments suggest a 2x speed-up in the matrix multiplications with no accuracy degradation over vision and language models. Finally, to solve the problem of switching between different structure constraints, we suggest a method to convert a pre-trained model with unstructured sparsity to an N:M fine-grained block sparsity model with little to no training. A reference implementation can be found at https://github.com/papers-submission/structured_transposable_masks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 95  (*Influential: 17*)\n\n#### 6. Sparse Weight Activation Training\n\n*From Search Query: hardware efficient sparse computation*\n\n*Md Aamir Raihan, Tor M. Aamodt*\n\n**TL;DR:** Sarse Weight Activation Training (SWAT), an algorithm that embodies these observations, is proposed that reduces computations by 50% to 80% with better accuracy at a given level of sparsity versus the Dynamic Sparse Graph algorithm.\n\n**Abstract:** Neural network training is computationally and memory intensive. Sparse training can reduce the burden on emerging hardware platforms designed to accelerate sparse computations, but it can affect network convergence. In this work, we propose a novel CNN training algorithm Sparse Weight Activation Training (SWAT). SWAT is more computation and memory-efficient than conventional training. SWAT modifies back-propagation based on the empirical insight that convergence during training tends to be robust to the elimination of (i) small magnitude weights during the forward pass and (ii) both small magnitude weights and activations during the backward pass. We evaluate SWAT on recent CNN architectures such as ResNet, VGG, DenseNet and WideResNet using CIFAR-10, CIFAR-100 and ImageNet datasets. For ResNet-50 on ImageNet SWAT reduces total floating-point operations (FLOPS) during training by 80% resulting in a 3.3$\\times$ training speedup when run on a simulated sparse learning accelerator representative of emerging platforms while incurring only 1.63% reduction in validation accuracy. Moreover, SWAT reduces memory footprint during the backward pass by 23% to 50% for activations and 50% to 90% for weights.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 68  (*Influential: 10*)\n\n#### 7. PENNI: Pruned Kernel Sharing for Efficient CNN Inference\n\n*From Search Query: hardware efficient sparse computation*\n\n*Shiyu Li, Edward Hanson, H. Li, Yiran Chen*\n\n**TL;DR:** PENNI is proposed, a CNN model compression framework that is able to achieve model compactness and hardware efficiency simultaneously by implementing kernel sharing in convolution layers via a small number of basis kernels and alternately adjusting bases and coefficients with sparse constraints.\n\n**Abstract:** Although state-of-the-art (SOTA) CNNs achieve outstanding performance on various tasks, their high computation demand and massive number of parameters make it difficult to deploy these SOTA CNNs onto resource-constrained devices. Previous works on CNN acceleration utilize low-rank approximation of the original convolution layers to reduce computation cost. However, these methods are very difficult to conduct upon sparse models, which limits execution speedup since redundancies within the CNN model are not fully exploited. We argue that kernel granularity decomposition can be conducted with low-rank assumption while exploiting the redundancy within the remaining compact coefficients. Based on this observation, we propose PENNI, a CNN model compression framework that is able to achieve model compactness and hardware efficiency simultaneously by (1) implementing kernel sharing in convolution layers via a small number of basis kernels and (2) alternately adjusting bases and coefficients with sparse constraints. Experiments show that we can prune 97% parameters and 92% FLOPs on ResNet18 CIFAR10 with no accuracy loss, and achieve 44% reduction in run-time memory consumption and a 53% reduction in inference latency.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2020\n\n**Citations:** 13  (*Influential: 3*)\n\n### 5 related papers from Papers with Code\n\n#### 1. Fusion of rain radar images and wind forecasts in a deep learning model applied to rain nowcasting\n\n*From Search Query: event driven training stability*\n\n*Arthur Filoche, Anastase Charantonis, Julien Brajard, Dominique B\u00e9r\u00e9ziat, Vincent Bouget*\n\n**Abstract:** Short- or mid-term rainfall forecasting is a major task with several environmental applications such as agricultural management or flood risk monitoring. Existing data-driven approaches, especially deep learning models, have shown significant skill at this task, using only rainfall radar images as inputs. In order to determine whether using other meteorological parameters such as wind would improve forecasts, we trained a deep learning model on a fusion of rainfall radar images and wind velocity produced by a weather forecast model. The network was compared to a similar architecture trained only on radar data, to a basic persistence model and to an approach based on optical flow. Our network outperforms by 8% the F1-score calculated for the optical flow on moderate and higher rain events for forecasts at a horizon time of 30 min. Furthermore, it outperforms by 7% the same architecture trained using only rainfall radar images. Merging rain and wind data has also proven to stabilize the training process and enabled significant improvement especially on the difficult-to-predict high precipitation rainfalls.\n\n**Published:** 2020-12-09\n\n\n\n#### 2. HiNeRV: Video Compression with Hierarchical Encoding-based Neural Representation\n\n*From Search Query: hierarchical neural compression*\n\n*David Bull, Andrew Gower, Fan Zhang, Ge Gao, Ho Man Kwan*\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Conference:** hinerv-video-compression-with-hierarchical\n\n**Published:** 2023-06-16\n\n\n\n#### 3. Hierarchical Autoregressive Modeling for Neural Video Compression\n\n*From Search Query: hierarchical neural compression*\n\n*Stephan Mandt, Joseph Marino, Yibo Yang, Ruihan Yang*\n\n**Abstract:** Recent work by Marino et al. (2020) showed improved performance in sequential density estimation by combining masked autoregressive flows with hierarchical latent variable models. We draw a connection between such autoregressive generative models and the task of lossy video compression. Specifically, we view recent neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal autoregressive transform, and propose avenues for enhancement based on this insight. Comprehensive evaluations on large-scale video data show improved rate-distortion performance over both state-of-the-art neural and conventional video compression methods.\n\n**Conference:** hierarchical-autoregressive-modeling-for\n\n**Published:** 2020-10-19\n\n\n\n#### 4. DeepSpeed Inference: Enabling Efficient Inference of Transformer Models at Unprecedented Scale\n\n*From Search Query: hardware efficient sparse computation*\n\n*Yuxiong He, Olatunji Ruwase, Shaden Smith, Jeff Rasley, Elton Zheng, Du Li, Cheng Li, Ammar Ahmad Awan, Minjia Zhang, Samyam Rajbhandari, Reza Yazdani Aminabadi*\n\n**Abstract:** The past several years have witnessed the success of transformer-based models, and their scale and application scenarios continue to grow aggressively. The current landscape of transformer models is increasingly diverse: the model size varies drastically with the largest being of hundred-billion parameters; the model characteristics differ due to the sparsity introduced by the Mixture-of-Experts; the target application scenarios can be latency-critical or throughput-oriented; the deployment hardware could be single- or multi-GPU systems with different types of memory and storage, etc. With such increasing diversity and the fast-evolving pace of transformer models, designing a highly performant and efficient inference system is extremely challenging. In this paper, we present DeepSpeed Inference, a comprehensive system solution for transformer model inference to address the above-mentioned challenges. DeepSpeed Inference consists of (1) a multi-GPU inference solution to minimize latency while maximizing the throughput of both dense and sparse transformer models when they fit in aggregate GPU memory, and (2) a heterogeneous inference solution that leverages CPU and NVMe memory in addition to the GPU memory and compute to enable high inference throughput with large models which do not fit in aggregate GPU memory. DeepSpeed Inference reduces latency by up to 7.3X over the state-of-the-art for latency-oriented scenarios and increases throughput by over 1.5x for throughput-oriented scenarios. Moreover, it enables trillion parameter scale inference under real-time latency constraints by leveraging hundreds of GPUs, an unprecedented scale for inference. It can inference 25x larger models than with GPU-only solutions, while delivering a high throughput of 84 TFLOPS (over $50\\%$ of A6000 peak).\n\n**Published:** 2022-06-30\n\n\n\n#### 5. Scalable and Efficient MoE Training for Multitask Multilingual Models\n\n*From Search Query: hardware efficient sparse computation*\n\n*Hany Hassan Awadalla, Yuxiong He, Samyam Rajbhandari, Amr Hendy, Liyang Lu, Andres Felipe Cruz Salinas, Alexandre Muzio, Ammar Ahmad Awan, Young Jin Kim*\n\n**Abstract:** The Mixture of Experts (MoE) models are an emerging class of sparsely activated deep learning models that have sublinear compute costs with respect to their parameters. In contrast with dense models, the sparse architecture of MoE offers opportunities for drastically growing model size with significant accuracy gain while consuming much lower compute budget. However, supporting large scale MoE training also has its own set of system and modeling challenges. To overcome the challenges and embrace the opportunities of MoE, we first develop a system capable of scaling MoE models efficiently to trillions of parameters. It combines multi-dimensional parallelism and heterogeneous memory technologies harmoniously with MoE to empower 8x larger models on the same hardware compared with existing work. Besides boosting system efficiency, we also present new training methods to improve MoE sample efficiency and leverage expert pruning strategy to improve inference time efficiency. By combining the efficient system and training methods, we are able to significantly scale up large multitask multilingual models for language generation which results in a great improvement in model accuracy. A model trained with 10 billion parameters on 50 languages can achieve state-of-the-art performance in Machine Translation (MT) and multilingual natural language generation tasks. The system support of efficient MoE training has been implemented and open-sourced with the DeepSpeed library.\n\n**Published:** 2021-09-22\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the autoregressive language model design focusing on event-driven training methods, hierarchical compression, and hardware-efficient sparse computation, here are some relevant insights and references:\n\n## Event-Driven Training Methods and Stability\n\n### Training Dynamics and Stability Analysis\n- The concept of event-driven training can be inspired by spiking neural networks (SNNs) and other event-based systems. For instance, the use of memristive autapse and temporal distillation in SNNs highlights the potential for efficient and stable training in event-driven networks. Although the primary focus of SNNs is on low power consumption and asynchronous event-driven processing, the stability and training dynamics can provide valuable insights for other event-driven systems.\n\n### Convergence Guarantees\n- For convergence guarantees, the work on reinforcement learning in event-driven queuing simulations can offer some insights. The use of work-conserving policies and behavior cloning in reinforcement learning frameworks demonstrates how stability can be achieved through structured policy designs. This approach can be analogously applied to ensure convergence in event-driven training methods by imposing similar structural constraints.\n\n## Hierarchical Compression in Neural Networks\n\n### Multi-Level State Representation\n- Hierarchical state representation is a key area of interest. The \"Vision Mamba\" paper, which uses a bidirectional state space model, shows significant improvements in computation and memory efficiency. This hierarchical approach can be adapted for autoregressive language models to achieve efficient state representations [Analysis Note].\n\n### Adaptive Compression Techniques\n- Adaptive compression techniques can be explored through the lens of sequential models. The \"Associative Memory Augmented\" paper proposes a memory addressing mechanism that updates latent states only when events occur, which is highly relevant for hierarchical and adaptive compression in autoregressive models [Analysis Note].\n\n### Information-Preserving Hierarchical Structures\n- The \"Memory-Efficient Learning of Stable Linear Dynamical Systems\" paper provides theoretical bounds on memory complexity and achieves significant improvements in reconstruction error. This work supports the feasibility of hierarchical and adaptive compression techniques that preserve information efficiently [Analysis Note].\n\n## Hardware-Efficient Sparse Computation\n\n### Memory Access Optimization\n- For hardware-efficient sparse computation, optimizing memory access patterns is crucial. The \"FlashFFTConv\" paper discusses efficient convolutions for long sequences using tensor cores, which can be adapted for sparse tensor operations in autoregressive models. This approach focuses on IO-aware design and block-based processing to improve hardware utilization [Analysis Note].\n\n### Sparse Tensor Operations\n- Efficient sparse tensor operations can be achieved through techniques like fixed-to-fixed encoding for model compression. The \"Encoding Weights of Irregular Sparsity\" paper explores this area, showing how structured sparsity can be efficiently implemented, which is beneficial for hardware-aware sparsity patterns [Analysis Note].\n\n### Hardware-Aware Sparsity Patterns\n- The \"DenseMamba\" paper, although focused on dense hidden connections, discusses state space models that can be optimized for hardware efficiency. By adapting these models to incorporate sparse connections and hardware-aware sparsity patterns, significant improvements in efficiency can be achieved [Analysis Note].\n\nIn summary, the researchers can leverage insights from:\n\n- **Event-driven training**: By studying the stability and training dynamics of SNNs and event-driven queuing simulations to ensure stable and convergent training.\n- **Hierarchical compression**: Using hierarchical state representations, adaptive compression techniques, and information-preserving structures as seen in papers like \"Vision Mamba\" and \"Associative Memory Augmented.\"\n- **Hardware-efficient sparse computation**: Optimizing memory access, using sparse tensor operations, and incorporating hardware-aware sparsity patterns as discussed in papers like \"FlashFFTConv\" and \"Encoding Weights of Irregular Sparsity.\"\n\nThese approaches can help in designing more efficient, stable, and scalable autoregressive language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal several promising directions for the EventDDLerpLinear proposal, particularly in:\n- Memory-efficient state representations using hierarchical and adaptive approaches\n- Event-driven computation mechanisms inspired by spiking neural networks\n- Efficient compression techniques for sequential models\n- Hardware-aware optimization strategies\n\n2. Useful References with Excerpts:\n\na) From Vision Mamba:\n\"Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency [...] saves 86.8% GPU memory\"\n- Validates efficiency claims of state space models\n- Demonstrates practical benefits of bidirectional processing\n\nb) From \"Memory-Efficient Learning of Stable Linear Dynamical Systems\":\n\"achieves an orders-of-magnitude improvement in reconstruction error [...] with an O(n^2) space complexity compared to O(n^4)\"\n- Supports feasibility of efficient state representations\n- Provides theoretical bounds on memory complexity\n\nc) From \"Associative Memory Augmented\":\n\"A memory addressing mechanism is proposed to store and retrieve the latent states only where these events occur and update them only when they occur\"\n- Validates event-driven approach\n- Shows benefits of selective state updates\n\n3. Key Insights:\n\na) State Representation:\n- Hierarchical approaches show strong promise for efficiency\n- Event-driven updates can significantly reduce computation\n- Memory addressing mechanisms crucial for performance\n\nb) Compression Techniques:\n- Fixed-to-fixed encoding shows promise for structured sparsity\n- Sequential compression can approach theoretical limits\n- Block-based processing enables efficient memory usage\n\nc) Hardware Considerations:\n- IO-aware design critical for practical efficiency\n- Block-based processing can improve hardware utilization\n- Memory access patterns crucial for performance\n\n4. Future Search Plan:\n\nNeed additional investigation in:\na) Event-driven training stability\nb) Hierarchical compression techniques\nc) Hardware-efficient implementation strategies\n\nNext search queries should focus on:\n1. Event-driven training methods and stability\n2. Hierarchical state compression in neural networks\n3. Hardware-efficient sparse computation\n\n5. References:\n\n[1] Zhu, L., et al. (2024). Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model. International Conference on Machine Learning.\n\n[2] Mamakoukas, G., et al. (2020). Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control. Neural Information Processing Systems.\n\n[3] Kamal, U., et al. (2023). Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception. International Conference on Learning Representations.\n\n[4] Park, B., et al. (2021). Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression. International Conference on Learning Representations.\n\n[5] Fu, D.Y., et al. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[6] He, W., et al. (2024). DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models. arXiv preprint."
        }
    ],
    "review_search_stack": [
        {
            "ready": false,
            "query": "event driven neural networks, hierarchical state compression, adaptive computation",
            "detail": "Search for papers discussing:\n1. Event-driven computation mechanisms in neural networks, particularly for sequential processing\n2. Hierarchical and adaptive compression techniques for state representation\n3. Methods for selective or adaptive computation based on input importance\n4. Implementations of sparse and efficient state updates\n5. Hardware-efficient approaches to dynamic computation\n\nInclude specific focus on:\n- Threshold adaptation mechanisms\n- Memory-computation trade-offs\n- Training stability with selective updates\n- Impact on model quality\n- Hardware efficiency considerations",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Event-driven computation mechanisms in neural networks, particularly for sequential processing\n2. Hierarchical and adaptive compression techniques for state representation\n3. Methods for selective or adaptive computation based on input importance\n4. Implementations of sparse and efficient state updates\n5. Hardware-efficient approaches to dynamic computation\n\nInclude specific focus on:\n- Threshold adaptation mechanisms\n- Memory-computation trade-offs\n- Training stability with selective updates\n- Impact on model quality\n- Hardware efficiency considerations\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. Mamba: Linear-Time Sequence Modeling with Selective State Spaces (Avg. Score: 0.58)\n\n*Albert Gu, Tri Dao*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 662  (*Influential: 204*)\n\n**TL;DR:** This work identifies that a key weakness of subquadratic-time models based on Transformer architecture is their inability to perform content-based reasoning, and integrates selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba).\n\n**Abstract:** Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.\n\n##### *Relevant Chunk: No. 6/74 (Score: 0.58)*\n\n```\nLi et al. 2023; Orvieto et al. 2023; Poli et al. 2023), and clarify nuances when necessary. SSM Architectures. SSMs are standalone sequence transformations that can be incorporated into end-to-end neural network architectures. (We also sometimes call SSM architectures SSNNs, which are to SSM layers as CNNs are to linear convolution layers.) We discuss some of the most well-known SSM architectures, many of which will also serve as our primary baselines. - Linear attention (Katharopoulos et al. 2020) is an approximation of self-attention involving a recurrence which can be viewed as a degenerate linear SSM. - H3 (Dao, Fu, Saab, et al. 2023) generalized this recurrence to use S4; it can be viewed as an architecture with an SSM sandwiched by two gated connections (Figure 3). H3 also inserts a standard local convolution, which they frame as a shift-SSM, before the main SSM layer. - Hyena (Poli et al. 2023) uses the same architecture as H3 but replaces the S4 layer with an MLP-parameterized global convolution (Romero et al. 2021). - RetNet (Y. Sun et al. 2023) adds an additional gate to the architecture and uses a simpler SSM, allowing an alternative parallelizable computation path, using a variant of multi-head attention (MHA) instead of convolutions. - RWKV (B. Peng et al. 2023) is a recent RNN designed for language modeling based on another linear attention approximation, the attention-free Transformer (S. Zhai et al. 2021). Its main \"WKV\" mechanism involves LTI recurrences and can be viewed as the ratio of two SSMs. Other closely related SSMs and architectures are discussed further in an extended related work (Appendix B). We highlight in particular S5 (Smith, Warrington, and Linderman 2023), QRNN (Bradbury et al. 2016), and SRU (Lei et al. 2017), which we view as the most closely related methods to our core selective SSM. ## 3 Selective State Space Models\n\nWe motivate our selection mechanism using intuition from synthetic tasks (Section 3.1), then explain how to incorporate this mechanism into state space models (Section 3.2). The resulting time-varying SSMs cannot use convolutions, presenting a technical challenge of how to compute them efficiently. We overcome this with a hardware-aware algorithm that exploits the memory hierarchy on modern hardware (Section 3.3). We then describe a simple SSM architecture without attention or even MLP blocks (Section 3.4). Finally, we discuss some additional properties of selection mechanisms (Section 3.5). ### 3.1 Motivation: Selection as a Means of Compression\n\nWe argue that a fundamental problem of sequence modeling is compressing context into a smaller state. In fact, we can view the tradeoffs of popular sequence models from this point of view. For example, attention is both effective and inefficient because it explicitly does not compress context at all. This can be seen from the fact that autoregressive inference requires explicitly storing the entire context (i.e. the KV cache), which directly causes the slow linear-time inference and quadratic-time training of Transformers. On the other hand, recurrent models are efficient because they have a finite state, implying constant-time inference and linear-time training.\n```\n\n#### 2. FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (Avg. Score: 0.32)\n\n*Tri Dao, Daniel Y. Fu, Stefano Ermon, A. Rudra, Christopher R'e*\n\n**Published in:** Neural Information Processing Systems (2022)\t**Cited by** 1034  (*Influential: 98*)\n\n**TL;DR:** This work proposes FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM, and is optimal for a range of SRAM sizes.\n\n**Abstract:** Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).\n\n##### *Relevant Chunk: No. 22/53 (Score: 0.32)*\n\n```\nIn Advances in neural information processing systems (NeurIPS), 2020. [36] Albert Gu, Isys Johnson, Karan Goel, Khaled Saab, Tri Dao, Atri Rudra, and Christopher R\u00e9. Combining recurrent, convolutional, and continuous-time models with linear state space layers. Advances in Neural Information Processing Systems, 34, 2021. [37] Albert Gu, Karan Goel, and Christopher R\u00e9. Efficiently modeling long sequences with structured state spaces. In The International Conference on Learning Representations (ICLR), 2022. [38] Song Han, Jeff Pool, John Tran, and William J Dally. Learning both weights and connections for efficient neural networks. arXiv preprint arXiv:1506.02626, 2015. [39] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. In International Conference on Learning Representations, 2016. [40] John Hennessy and David Patterson. Memory hierarchy design. Computer Architecture: A Quantitative Approach, pages 390-525, 2003. [41] Sara Hooker. The hardware lottery. arXiv preprint arXiv:2009.06489, 2020. [42] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V Le. Transformer quality in linear time. arXiv preprint arXiv:2202.10447, 2022. [43] Andrei Ivanov, Nikoli Dryden, Tal Ben-Nun, Shigang Li, and Torsten Hoefler. Data movement is all you need: A case study on optimizing transformers.\n```\n\n#### 3. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.22)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 3/43 (Score: 0.22)*\n\n```\nnet/forum? id=gcf1anBL9e\n\n\n#### Abstract\n\nAs the size of large language models continue to scale, so does the computational resources required to run them. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and until now, SNNs have yet to succeed at language generation on large-scale datasets. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement 'SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 46 M and 216 M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model when released, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self-attention to reduce quadratic computational complexity $\\mathcal{O}\\left(T^{2}\\right)$ to linear complexity $\\mathcal{O}(T)$ with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining $32.2 \\times$ fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ ridgerchu/SpikeGPT\n\n\n## 1 Introduction\n\nArtificial Neural Networks (ANNs) have recently achieved widespread, public-facing impact in Natural Language Processing (NLP), but with a significant computational and energy consumption burden across training and deployment.\n```\n\n#### 4. Sparse Modular Activation for Efficient Sequence Modeling (Avg. Score: 0.18)\n\n*Liliang Ren, Yang Liu, Shuo Wang, Yichong Xu, Chenguang Zhu, Chengxiang Zhai*\n\n**Published in:** Neural Information Processing Systems (2023)\t**Cited by** 7  (*Influential: 0*)\n\n**TL;DR:** A novel neural architecture, SeqBoat, is designed, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM, and can achieve linear inference complexity with theoretically infinite attention span and provide substantially better quality-efficiency trade-off than the chunking-based models.\n\n**Abstract:** Linear State Space Models (SSMs) have demonstrated strong performance in a variety of sequence modeling tasks due to their efficient encoding of the recurrent structure. However, in more comprehensive tasks like language modeling and machine translation, self-attention-based models still outperform SSMs. Hybrid models employing both SSM and self-attention generally show promising performance, but current approaches apply attention modules statically and uniformly to all elements in the input sequences, leading to sub-optimal quality-efficiency trade-offs. In this work, we introduce Sparse Modular Activation (SMA), a general mechanism enabling neural networks to sparsely and dynamically activate sub-modules for sequence elements in a differentiable manner. Through allowing each element to skip non-activated sub-modules, SMA reduces computation and memory consumption at both training and inference stages of sequence modeling. As a specific instantiation of SMA, we design a novel neural architecture, SeqBoat, which employs SMA to sparsely activate a Gated Attention Unit (GAU) based on the state representations learned from an SSM. By constraining the GAU to only conduct local attention on the activated inputs, SeqBoat can achieve linear inference complexity with theoretically infinite attention span, and provide substantially better quality-efficiency trade-off than the chunking-based models. With experiments on a wide range of tasks, including language modeling, speech classification and long-range arena, SeqBoat brings new state-of-the-art results among hybrid models with linear complexity and reveals the amount of attention needed for each task through the learned sparse activation patterns.\n\n##### *Relevant Chunk: No. 16/32 (Score: 0.18)*\n\n```\nIn Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7275-7286, Dublin, Ireland, may 2022. Association for Computational Linguistics. [Gra16] A. Graves. Adaptive computation time for recurrent neural networks. ARXIV.ORG, 2016. [GZYE20] Trevor Gale, M. Zaharia, C. Young, and Erich Elsen. Sparse gpu kernels for deep learning. International Conference For High Performance Computing, Networking, Storage And Analysis, 2020. [HDLL22] Weizhe Hua, Zihang Dai, Hanxiao Liu, and Quoc V. Le. Transformer quality in linear time. International Conference On Machine Learning, 2022. [HLW ${ }^{+}$22] Ramin Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, and Daniela Rus. Liquid structural state-space models. arXiv preprint arXiv:2209.12951, 2022. [Hut06] Marcus Hutter. The human knowledge compression contest. http://prize.hutter1.net/, 2006. [JGB ${ }^{+}$21] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. International Conference On Machine Learning, 2021. [JGP17] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbelsoftmax.\n```\n\n#### 5. Liquid Structural State-Space Models (Avg. Score: 0.08)\n\n*Ramin M. Hasani, Mathias Lechner, Tsun-Hsuan Wang, Makram Chahine, Alexander Amini, Daniela Rus*\n\n**Published in:** International Conference on Learning Representations (2022)\t**Cited by** 55  (*Influential: 8*)\n\n**TL;DR:** The LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark.\n\n**Abstract:** A proper parametrization of state transition matrices of linear state-space models (SSMs) followed by standard nonlinearities enables them to efficiently learn representations from sequential data, establishing the state-of-the-art on a large series of long-range sequence modeling benchmarks. In this paper, we show that we can improve further when the structural SSM such as S4 is given by a linear liquid time-constant (LTC) state-space model. LTC neural networks are causal continuous-time neural networks with an input-dependent state transition module, which makes them learn to adapt to incoming inputs at inference. We show that by using a diagonal plus low-rank decomposition of the state transition matrix introduced in S4, and a few simplifications, the LTC-based structural state-space model, dubbed Liquid-S4, achieves the new state-of-the-art generalization across sequence modeling tasks with long-term dependencies such as image, text, audio, and medical time-series, with an average performance of 87.32% on the Long-Range Arena benchmark. On the full raw Speech Command recognition, dataset Liquid-S4 achieves 96.78% accuracy with a 30% reduction in parameter counts compared to S4. The additional gain in performance is the direct result of the Liquid-S4's kernel structure that takes into account the similarities of the input sequence samples during training and inference.\n\n##### *Relevant Chunk: No. 28/54 (Score: 0.08)*\n\n```\nJ. J. Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):2554-2558, 1982. J. Jia and A. R. Benson. Neural jump stochastic differential equations. In Advances in Neural Information Processing Systems, pages 9843-9854, 2019. L. Jing, C. Gulcehre, J. Peurifoy, Y. Shen, M. Tegmark, M. Soljacic, and Y. Bengio. Gated orthogonal recurrent units: On learning to forget. Neural computation, 31(4):765-783, 2019. A. Kag, Z. Zhang, and V. Saligrama. Rnns incrementally evolving on an equilibrium manifold: A panacea for vanishing and exploding gradients? In International Conference on Learning Representations, 2019. R. KALMAN. A new approach to linear filtering and prediction problems.\n```\n\n\n\n---\n## Found 16 related papers from 3 external sources\n\n\n\nYour 3 raw search queries input to the search frame: event driven neural networks, hierarchical state compression, adaptive computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 8 related papers from Semantic Scholar\n\n#### 1. Training Spiking Neural Networks with Event-driven Backpropagation\n\n*From Search Query: event driven neural networks*\n\n*Yaoyu Zhu, Zhaofei Yu, Wei Fang, Xiaodong Xie, Tiejun Huang, T. Masquelier*\n\n**TL;DR:** This paper analyzes the commonly used temporal backpropagation training approach and proves that the sum of gradients remains unchanged between fully-connected and convolutional layers, and proposes a backward kernel that can solve the reverse gradient problem for time-based gradients and keep the property of the invariable sum ofGradients.\n\n**Abstract:** Spiking Neural networks (SNNs) represent and transmit information by spatiotemporal spike patterns, which bring two major advantages: biological plausibility and suitability for ultralow-power neuromorphic implementation. Despite this, the binary firing characteristic makes training SNNs more challenging. To learn the parameters of deep SNNs in an event-driven fashion as in inference of SNNs, back-propagation with respect to spike timing is proposed. Although this event-driven learning has the advantages of lower computational cost and memory occupation, the accuracy is far below the recurrent neural network-like learning approaches. In this paper, we first analyze the commonly used temporal backpropagation training approach and prove that the sum of gradients remains unchanged between fully-connected and convolutional layers. Secondly, we show that the max pooling layer meets the above invariance rule, while the average pooling layer does not, which will suffer the gradient vanishing problem but can be revised to meet the requirement. Thirdly, we point out the reverse gradient problem for time-based gradients and propose a backward kernel that can solve this problem and keep the property of the invariable sum of gradients. The experimental results show that the proposed approach achieves state-of-the-art performance on CIFAR10 among time-based training methods. Also, this is the first time that the time-based backpropagation approach successfully trains SNN on the CIFAR100 dataset. Our code is available at https://github.com/zhuyaoyu/SNN-event-driven-learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 30  (*Influential: 2*)\n\n#### 2. Temporal Effective Batch Normalization in Spiking Neural Networks\n\n*From Search Query: event driven neural networks*\n\n*Chaoteng Duan, Jianhao Ding, Shiyan Chen, Zhaofei Yu, Tiejun Huang*\n\n**TL;DR:** Experimental results show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Abstract:** Spiking Neural Networks (SNNs) are promising in neuromorphic hardware owing to utilizing spatio-temporal information and sparse event-driven signal processing. However, it is challenging to train SNNs due to the non-differentiable nature of the binary firing function. The surrogate gradients alleviate the training problem and make SNNs obtain comparable performance as Artificial Neural Networks (ANNs) with the same structure. Unfortunately, batch normalization, contributing to the success of ANNs, does not play a prominent role in SNNs because of the additional temporal dimension. To this end, we propose an effective normalization method called temporal effective batch normalization (TEBN). By rescaling the presynaptic inputs with different weights at every time-step, temporal distributions become smoother and uniform. Theoretical analysis shows that TEBN can be viewed as a smoother of SNN\u2019s optimization landscape and could help stabilize the gradient norm. Experimental results on both static and neuromorphic datasets show that SNNs with TEBN outperform the state-of-the-art accuracy with fewer time-steps, and achieve better robustness to hyper-parameters than other normalizations.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 62  (*Influential: 7*)\n\n#### 3. Exploring Loss Functions for Time-based Training Strategy in Spiking Neural Networks\n\n*From Search Query: event driven neural networks*\n\n*Yaoyu Zhu, Wei Fang, Xiaodong Xie, Tiejun Huang, Zhaofei Yu*\n\n**TL;DR:** This work first map rate-based loss functions to time-based counterparts and explain why they are also applicable to the time-based training scheme, and infer that loss functions providing adequate positive overall gradients help training by theoretical analysis.\n\n**Abstract:** Spiking Neural Networks (SNNs) are considered promising brain-inspired energy-efficient models due to their event-driven computing paradigm. The spatiotemporal spike patterns used to convey information in SNNs consist of both rate coding and temporal coding, where the temporal coding is crucial to biological-plausible learning rules such as spike-timing-dependent-plasticity. The time-based training strategy is proposed to better utilize the temporal information in SNNs and learn in an asynchronous fashion. However, some recent works train SNNs by the time-based scheme with rate-coding-dominated loss functions. In this paper, we first map rate-based loss functions to time-based counterparts and explain why they are also applicable to the time-based training scheme. After that, we infer that loss functions providing adequate positive overall gradients help training by theoretical analysis. Based on this, we propose the enhanced counting loss to replace the commonly used mean square counting loss. In addition, we transfer the training of scale factor in weight standardization into thresholds. Experiments show that our approach outperforms previous time-based training methods in most datasets. Our work provides insights for training SNNs with time-based schemes and offers a fresh perspective on the correlation between rate coding and temporal coding. Our code is available at https://github.com/zhuyaoyu/SNN-temporal-training-losses.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 8  (*Influential: 0*)\n\n#### 4. HiNeRV: Video Compression with Hierarchical Encoding based Neural Representation\n\n*From Search Query: hierarchical state compression*\n\n*Ho Man Kwan, Ge Gao, Fan Zhang, Andrew Gower, David R. Bull*\n\n**TL;DR:** HiNeRV is an INR that combines light weight layers with novel hierarchical positional encodings and employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity, which offers higher performance and flexibility than existing methods.\n\n**Abstract:** Learning-based video compression is currently a popular research topic, offering the potential to compete with conventional standard video codecs. In this context, Implicit Neural Representations (INRs) have previously been used to represent and compress image and video content, demonstrating relatively high decoding speed compared to other methods. However, existing INR-based methods have failed to deliver rate quality performance comparable with the state of the art in video compression. This is mainly due to the simplicity of the employed network architectures, which limit their representation capability. In this paper, we propose HiNeRV, an INR that combines light weight layers with novel hierarchical positional encodings. We employs depth-wise convolutional, MLP and interpolation layers to build the deep and wide network architecture with high capacity. HiNeRV is also a unified representation encoding videos in both frames and patches at the same time, which offers higher performance and flexibility than existing methods. We further build a video codec based on HiNeRV and a refined pipeline for training, pruning and quantization that can better preserve HiNeRV's performance during lossy model compression. The proposed method has been evaluated on both UVG and MCL-JCV datasets for video compression, demonstrating significant improvement over all existing INRs baselines and competitive performance when compared to learning-based codecs (72.3% overall bit rate saving over HNeRV and 43.4% over DCVC on the UVG dataset, measured in PSNR).\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 23  (*Influential: 2*)\n\n#### 5. Hierarchical Autoregressive Modeling for Neural Video Compression\n\n*From Search Query: hierarchical state compression*\n\n*Ruihan Yang, Yibo Yang, Joseph Marino, S. Mandt*\n\n**TL;DR:** This work views recent neural video compression methods as instances of a generalized stochastic temporal autoregressive trans-form, and proposes avenues for enhancement based on this insight.\n\n**Abstract:** Recent work by Marino et al. (2020) showed improved performance in sequential density estimation by combining masked autoregressive flows with hierarchical latent variable models. We draw a connection between such autoregressive generative models and the task of lossy video compression. Specifically, we view recent neural video compression methods (Lu et al., 2019; Yang et al., 2020b; Agustssonet al., 2020) as instances of a generalized stochastic temporal autoregressive trans-form, and propose avenues for enhancement based on this insight. Comprehensive evaluations on large-scale video data show improved rate-distortion performance over both state-of-the-art neural and conventional video compression methods.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2020\n\n**Citations:** 39  (*Influential: 3*)\n\n#### 6. HiLLoC: Lossless Image Compression with Hierarchical Latent Variable Models\n\n*From Search Query: hierarchical state compression*\n\n*James Townsend, Thomas Bird, Julius Kunze, D. Barber*\n\n**TL;DR:** Full convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model, achieving state of the art for compression of full size ImageNet images.\n\n**Abstract:** We make the following striking observation: fully convolutional VAE models trained on 32x32 ImageNet can generalize well, not just to 64x64 but also to far larger photographs, with no changes to the model. We use this property, applying fully convolutional models to lossless compression, demonstrating a method to scale the VAE-based 'Bits-Back with ANS' algorithm for lossless compression to large color photographs, and achieving state of the art for compression of full size ImageNet images. We release Craystack, an open source library for convenient prototyping of lossless compression using probabilistic models, along with full implementations of all of our compression results.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2019\n\n**Citations:** 53  (*Influential: 13*)\n\n#### 7. Scalable Adaptive Computation for Iterative Generation\n\n*From Search Query: adaptive computation*\n\n*A. Jabri, David J. Fleet, Ting Chen*\n\n**TL;DR:** The Recurrent Interface Networks (RINs) are proposed, an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data.\n\n**Abstract:** Natural data is redundant yet predominant architectures tile computation uniformly across their input and output space. We propose the Recurrent Interface Networks (RINs), an attention-based architecture that decouples its core computation from the dimensionality of the data, enabling adaptive computation for more scalable generation of high-dimensional data. RINs focus the bulk of computation (i.e. global self-attention) on a set of latent tokens, using cross-attention to read and write (i.e. route) information between latent and data tokens. Stacking RIN blocks allows bottom-up (data to latent) and top-down (latent to data) feedback, leading to deeper and more expressive routing. While this routing introduces challenges, this is less problematic in recurrent computation settings where the task (and routing problem) changes gradually, such as iterative generation with diffusion models. We show how to leverage recurrence by conditioning the latent tokens at each forward pass of the reverse diffusion process with those from prior computation, i.e. latent self-conditioning. RINs yield state-of-the-art pixel diffusion models for image and video generation, scaling to 1024X1024 images without cascades or guidance, while being domain-agnostic and up to 10X more efficient than 2D and 3D U-Nets.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 83  (*Influential: 12*)\n\n#### 8. AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for Efficient Neural Machine Translation\n\n*From Search Query: adaptive computation*\n\n*Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, L. Lakshmanan, A. Awadallah, S\u00e9bastien Bubeck, Jianfeng Gao*\n\n**TL;DR:** AutoMoE is developed -- a framework for designing heterogeneous MoE's under computational constraints and leverages Neural Architecture Search to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, on aggregate over benchmark datasets for NMT.\n\n**Abstract:** Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a homogeneous design where the same number of experts of the same size are placed uniformly throughout the network. Furthermore, existing MoE works do not consider computational constraints (e.g., FLOPs, latency) to guide their design. To this end, we develop AutoMoE -- a framework for designing heterogeneous MoE's under computational constraints. AutoMoE leverages Neural Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with 4x inference speedup (CPU) and FLOPs reduction over manually designed Transformers, with parity in BLEU score over dense Transformer and within 1 BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for NMT. Heterogeneous search space with dense and sparsely activated Transformer modules (e.g., how many experts? where to place them? what should be their sizes?) allows for adaptive compute -- where different amounts of computations are used for different tokens in the input. Adaptivity comes naturally from routing decisions which send tokens to experts of different sizes. AutoMoE code, data, and trained models are available at https://aka.ms/AutoMoE.\n\n**Venue:** Annual Meeting of the Association for Computational Linguistics\n\n**Year:** 2022\n\n**Citations:** 5  (*Influential: 0*)\n\n### 2 related papers from ArXiv\n\n#### 1. Duo-LLM: A Framework for Studying Adaptive Computation in Large Language\n  Models\n\n*From Search Query: adaptive computation*\n\n*Keivan Alizadeh, Iman Mirzadeh, Hooman Shahrokhi, Dmitry Belenko, Frank Sun, Minsik Cho, Mohammad Hossein Sekhavat, Moin Nabi, Mehrdad Farajtabar*\n\n**Abstract:** Large Language Models (LLMs) typically generate outputs token by token using\na fixed compute budget, leading to inefficient resource utilization. To address\nthis shortcoming, recent advancements in mixture of expert (MoE) models,\nspeculative decoding, and early exit strategies leverage the insight that\ncomputational demands can vary significantly based on the complexity and nature\nof the input. However, identifying optimal routing patterns for dynamic\nexecution remains an open challenge, limiting the full potential of these\nadaptive methods. To address this need, we study adaptive computation in LLMs\nmore systematically. We propose a novel framework that integrates smaller\nauxiliary modules within each Feed-Forward Network layer of the LLM. This\ndesign enables dynamic routing of tokens based on task complexity: tokens can\nbe processed by either the small or big modules at each layer, or even bypass\ncertain layers entirely. This allows us to introduce a novel notion of a\ntoken's difficulty, defined by its potential to benefit from additional\ncomputational resources. Importantly, by employing oracles to identify optimal\npatterns of adaptive computations, we gain valuable insights into the internal\nworkings of LLMs and the routing processes in a simplified heterogeneous MoE\nsetup. We show that trained routers operate differently from oracles and often\nyield suboptimal solutions. Notably, activating a large module in just one\nlayer outperforms models that use large modules across all layers, underscoring\nthe gap between practical implementations of routing in MoE models and\ntheoretical optima for adaptive computation.\n\n**Published:** 2024-10-01T16:10:21Z  (*Updated: 2024-10-01T16:10:21Z*)\n\n\n\n#### 2. CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs\n\n*From Search Query: adaptive computation*\n\n*Weijie Lv, Xuan Xia, Sheng-Jun Huang*\n\n**Abstract:** Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.\n\n**Published:** 2024-08-05T02:38:48Z  (*Updated: 2024-08-05T02:38:48Z*)\n\n\n\n### 6 related papers from Papers with Code\n\n#### 1. Optimising Event-Driven Spiking Neural Network with Regularisation and Cutoff\n\n*From Search Query: event driven neural networks*\n\n*Xiaowei Huang, Xinping Yi, Han Yu, Gaojie Jin, Dengyu Wu*\n\n**Abstract:** Spiking neural network (SNN), next generation of artificial neural network (ANN) that more closely mimic natural neural networks offers promising improvements in computational efficiency. However, current SNN training methodologies predominantly employ a fixed timestep approach, overlooking the potential of dynamic inference in SNN. In this paper, we strengthen the marriage between SNN and event-driven processing with a proposal to consider cutoff in SNN, which can terminate SNN anytime during the inference to achieve efficient inference. Two novel optimisation techniques are presented to achieve inference efficient SNN: a Top-K cutoff and a regularisation. The Top-K cutoff technique optimises the inference of SNN, and the regularisation are proposed to affect the training and construct SNN with optimised performance for cutoff. We conduct an extensive set of experiments on multiple benchmark frame-based datsets, such as Cifar10/100, Tiny-ImageNet and event-based datasets, including CIFAR10-DVS, N-Caltech101 and DVS128 Gesture. The experimental results demonstrate the effectiveness of our techniques in both ANN-to-SNN conversion and direct training, affirming their compatibility and potential benefits in enhancing accuracy and reducing inference timestep when integrated with existing methods. Code available: https://github.com/Dengyu-Wu/SNN-Regularisation-Cutoff\n\n**Published:** 2023-01-23\n\n\n\n#### 2. RMP-SNN: Residual Membrane Potential Neuron for Enabling Deeper High-Accuracy and Low-Latency Spiking Neural Network\n\n*From Search Query: event driven neural networks*\n\n*Gopalakrishnan Srinivasan, Kaushik Roy, Bing Han*\n\n**Abstract:** Spiking Neural Networks (SNNs) have recently attracted significant research interest as the third generation of artificial neural networks that can enable low-power event-driven data analytics. The best performing SNNs for image recognition tasks are obtained by converting a trained Analog Neural Network (ANN), consisting of Rectified Linear Units (ReLU), to SNN composed of integrate-and-fire neurons with \"proper\" firing thresholds. The converted SNNs typically incur loss in accuracy compared to that provided by the original ANN and require sizable number of inference time-steps to achieve the best accuracy. We find that performance degradation in the converted SNN stems from using \"hard reset\" spiking neuron that is driven to fixed reset potential once its membrane potential exceeds the firing threshold, leading to information loss during SNN inference. We propose ANN-SNN conversion using \"soft reset\" spiking neuron model, referred to as Residual Membrane Potential (RMP) spiking neuron, which retains the \"residual\" membrane potential above threshold at the firing instants. We demonstrate near loss-less ANN-SNN conversion using RMP neurons for VGG-16, ResNet-20, and ResNet-34 SNNs on challenging datasets including CIFAR-10 (93.63% top-1), CIFAR-100 (70.93% top-1), and ImageNet (73.09% top-1 accuracy). Our results also show that RMP-SNN surpasses the best inference accuracy provided by the converted SNN with \"hard reset\" spiking neurons using 2-8 times fewer inference time-steps across network architectures and datasets.\n\n**Conference:** rmp-snn-residual-membrane-potential-neuron\n\n**Published:** 2020-02-25\n\n\n\n#### 3. Generating Diverse High-Fidelity Images with VQ-VAE-2\n\n*From Search Query: hierarchical state compression*\n\n*Ali Razavi, Oriol Vinyals, Aaron van den Oord*\n\n**Abstract:** We explore the use of Vector Quantized Variational AutoEncoder (VQ-VAE) models for large scale image generation. To this end, we scale and enhance the autoregressive priors used in VQ-VAE to generate synthetic samples of much higher coherence and fidelity than possible before. We use simple feed-forward encoder and decoder networks, making our model an attractive candidate for applications where the encoding and/or decoding speed is critical. Additionally, VQ-VAE requires sampling an autoregressive model only in the compressed latent space, which is an order of magnitude faster than sampling in the pixel space, especially for large images. We demonstrate that a multi-scale hierarchical organization of VQ-VAE, augmented with powerful priors over the latent codes, is able to generate samples with quality that rivals that of state of the art Generative Adversarial Networks on multifaceted datasets such as ImageNet, while not suffering from GAN's known shortcomings such as mode collapse and lack of diversity.\n\n**Conference:** generating-diverse-high-fidelity-images-with\n\n**Published:** 2019-06-02\n\n\n\n#### 4. Joint Autoregressive and Hierarchical Priors for Learned Image Compression\n\n*From Search Query: hierarchical state compression*\n\n*Johannes Ball\u00e9, David Minnen, George Toderici*\n\n**Abstract:** Recent models for learned image compression are based on autoencoders,\nlearning approximately invertible mappings from pixels to a quantized latent\nrepresentation. These are combined with an entropy model, a prior on the latent\nrepresentation that can be used with standard arithmetic coding algorithms to\nyield a compressed bitstream. Recently, hierarchical entropy models have been\nintroduced as a way to exploit more structure in the latents than simple fully\nfactorized priors, improving compression performance while maintaining\nend-to-end optimization. Inspired by the success of autoregressive priors in\nprobabilistic generative models, we examine autoregressive, hierarchical, as\nwell as combined priors as alternatives, weighing their costs and benefits in\nthe context of image compression. While it is well known that autoregressive\nmodels come with a significant computational penalty, we find that in terms of\ncompression performance, autoregressive and hierarchical priors are\ncomplementary and, together, exploit the probabilistic structure in the latents\nbetter than all previous learned models. The combined model yields\nstate-of-the-art rate--distortion performance, providing a 15.8% average\nreduction in file size over the previous state-of-the-art method based on deep\nlearning, which corresponds to a 59.8% size reduction over JPEG, more than 35%\nreduction compared to WebP and JPEG2000, and bitstreams 8.4% smaller than BPG,\nthe current state-of-the-art image codec. To the best of our knowledge, our\nmodel is the first learning-based method to outperform BPG on both PSNR and\nMS-SSIM distortion metrics.\n\n**Conference:** joint-autoregressive-and-hierarchical-priors-1\n\n**Published:** 2018-09-08\n\n\n\n#### 5. Adaptive Computation Time for Recurrent Neural Networks\n\n*From Search Query: adaptive computation*\n\n*Alex Graves*\n\n**Abstract:** This paper introduces Adaptive Computation Time (ACT), an algorithm that\nallows recurrent neural networks to learn how many computational steps to take\nbetween receiving an input and emitting an output. ACT requires minimal changes\nto the network architecture, is deterministic and differentiable, and does not\nadd any noise to the parameter gradients. Experimental results are provided for\nfour synthetic problems: determining the parity of binary vectors, applying\nbinary logic operations, adding integers, and sorting real numbers. Overall,\nperformance is dramatically improved by the use of ACT, which successfully\nadapts the number of computational steps to the requirements of the problem. We\nalso present character-level language modelling results on the Hutter prize\nWikipedia dataset. In this case ACT does not yield large gains in performance;\nhowever it does provide intriguing insight into the structure of the data, with\nmore computation allocated to harder-to-predict transitions, such as spaces\nbetween words and ends of sentences. This suggests that ACT or other adaptive\ncomputation methods could provide a generic method for inferring segment\nboundaries in sequence data.\n\n**Published:** 2016-03-29\n\n\n\n#### 6. Adaptive Computation with Elastic Input Sequence\n\n*From Search Query: adaptive computation*\n\n*Yang You, Mostafa Dehghani, Neil Houlsby, Anurag Arnab, Valerii Likhosherstov, Fuzhao Xue*\n\n**Abstract:** Humans have the ability to adapt the type of information they use, the procedure they employ, and the amount of time they spend when solving problems. However, most standard neural networks have a fixed function type and computation budget regardless of the sample's nature or difficulty. Adaptivity is a powerful paradigm as it not only imbues practitioners with flexibility pertaining to the downstream usage of these models but can also serve as a powerful inductive bias for solving certain challenging classes of problems. In this work, we introduce a new approach called AdaTape, which allows for dynamic computation in neural networks through adaptive tape tokens. AdaTape utilizes an elastic input sequence by equipping an architecture with a dynamic read-and-write tape. Specifically, we adaptively generate input sequences using tape tokens obtained from a tape bank which can be either trainable or derived from input data. We examine the challenges and requirements to obtain dynamic sequence content and length, and propose the Adaptive Tape Reading (ATR) algorithm to achieve both goals. Through extensive experiments on image recognition tasks, we show that AdaTape can achieve better performance while maintaining the computational cost. To facilitate further research, we have released code at https://github.com/google-research/scenic.\n\n**Published:** 2023-01-30\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of an autoregressive language model incorporating event-driven computation, hierarchical state compression, and adaptive computation, here are some key points and references that align with the researcher's goals:\n\n## Event-Driven Computation Mechanisms\n\n- **Spiking Neural Networks (SNNs):** SNNs are a prime example of event-driven computation, where information is transmitted as binary spikes between synapses. This approach is sparse and efficient, particularly for handling long-range dependencies. The paper on SPikE-SSM discusses how spiking state space models can be used for efficient long-sequence modeling, including dynamic hidden states and parallel training mechanisms.\n- **Threshold Adaptation:** The SPikE-SSM model incorporates a refractory neuron with dynamic reset to adjust the membrane potential, which involves threshold adaptation. This method helps in achieving high accuracy and low spiking rates, thus enhancing efficiency.\n\n## Hierarchical and Adaptive Compression Techniques\n\n- **Neural History Compressor:** This concept involves an unsupervised stack of RNNs where each higher-level RNN compresses the representation of the information from the lower-level RNN. This hierarchical approach minimizes the description length of the data and can be particularly useful for language models by focusing on unpredictable inputs.\n- **State Compression in Language Models:** Hierarchical state compression can be achieved by distilling the RNN hierarchy into a \"conscious\" chunker and a \"subconscious\" automatizer. The chunker predicts and compresses unpredictable inputs, while the automatizer learns to predict the hidden units of the chunker, making long-term memories more predictable.\n\n## Selective or Adaptive Computation\n\n- **LSTM and Gated Mechanisms:** Long Short-Term Memory (LSTM) cells use gates to selectively store or delete information based on its importance. This adaptive computation mechanism is crucial for balancing computational savings with model expressivity.\n- **Computation Routing Strategies:** In the context of transformers, adaptive computation can be implemented by routing computation based on input importance. For example, techniques like sparse attention or dynamic pruning can be used to selectively allocate computational resources.\n\n## Sparse and Efficient State Updates\n\n- **Spiking State Space Models (SSMs):** The SPikE-SSM model is designed to handle long sequences efficiently by using sparse and event-driven computation. It employs a max-min boundary compression (PMBC) strategy to facilitate efficient inference and parallel training, significantly reducing the number of required iterations.\n- **Parallel Computing Mechanisms:** The PMBC method in SPikE-SSM allows for parallel processing of all tokens in a sequence simultaneously, which is much more efficient than serial computation methods. This approach is particularly beneficial for long-sequence tasks.\n\n## Hardware-Efficient Approaches\n\n- **Spiking Neural Networks:** SNNs are inherently hardware-efficient due to their sparse and event-driven nature. They can exploit the temporal dimension and dynamic hidden states, making them compatible with parallel computing architectures.\n- **Sparse Operations:** The use of sparse operations in models like SPikE-SSM can lead to significant computational efficiency gains. These models are designed to minimize the number of required computations, making them more hardware-friendly.\n\n### Key Considerations\n\n- **Threshold Adaptation Mechanisms:** The dynamic reset and threshold adaptation in SPikE-SSM are crucial for balancing accuracy and sparsity. These mechanisms ensure that the model can adapt to different input patterns efficiently.\n- **Memory-Computation Trade-offs:** Hierarchical compression techniques, such as the neural history compressor, need to balance the trade-off between memory usage and computational efficiency. This balance is critical for maintaining model performance while reducing computational overhead.\n- **Training Stability with Selective Updates:** Models using selective or adaptive computation need careful tuning to ensure training stability. Techniques like backpropagation through time (BPTT) and gradient clipping can help mitigate issues like vanishing gradients.\n- **Impact on Model Quality:** The impact of compression and selective computation on model quality must be carefully evaluated. Hierarchical compression and adaptive computation can potentially reduce model performance if not implemented correctly, so thorough testing is necessary.\n\nBy integrating these concepts, the researcher can develop an autoregressive language model that leverages event-driven computation, hierarchical state compression, and adaptive computation to achieve significant efficiency gains while maintaining or improving model quality.\n",
            "analysis": "Initial Analysis of EventDDLerpLinear Proposal:\n\n1. Key Aspects Requiring Investigation:\n\na) Event-Driven Updates:\n- How does the event detection mechanism compare to existing approaches in spiking neural networks and event-based computing?\n- What are the trade-offs between computational savings and potential information loss?\n- How to determine optimal thresholds for event detection?\n\nb) Hierarchical State Compression:\n- What are existing approaches to adaptive/hierarchical compression in language models?\n- How does compression affect model performance and training stability?\n- What are the memory-performance trade-offs?\n\nc) Selective Computation:\n- How do existing models implement adaptive computation?\n- What are effective strategies for routing computation based on input importance?\n- How to balance computational savings with model expressivity?\n\n2. Comparison with Previous Designs:\n\nThe proposal appears distinct from its siblings:\n- rwkv_lightning focused on attention mechanisms and retrospection\n- rwkv6_sc emphasized scan-based operations and IO-aware algorithms\n- EventDDLerpLinear introduces unique event-driven and hierarchical compression concepts\n\n3. Initial Assessment:\n\nStrengths:\n- Novel combination of event-driven updates with hierarchical compression\n- Potential for significant computational efficiency gains\n- Hardware-friendly sparse operations\n- Clear theoretical foundation\n\nConcerns:\n- Complexity of event detection mechanism\n- Impact of compression on model quality\n- Training stability with selective updates\n- Overhead from routing decisions\n\n4. Investigation Plan:\n\nPhase 1: Event-Driven Mechanisms\n- Research spiking neural networks in language models\n- Investigate event detection in sequential models\n- Study threshold adaptation techniques\n\nPhase 2: Hierarchical Compression\n- Examine state compression in language models\n- Research adaptive compression techniques\n- Study impact on model performance\n\nPhase 3: Selective Computation\n- Investigate adaptive computation in transformers\n- Research computation routing strategies\n- Study efficiency-accuracy trade-offs\n\n5. Relevant References Found:\n\nFrom the initial search:\n- SMR: State Memory Replay paper provides insights into state management and sampling adaptation\n- Neurocache paper offers relevant approaches to efficient vector retrieval\n- RWKV-6 (Eagle) paper gives baseline architecture details\n\n6. Next Steps:\n\nNeed to investigate:\n1. Event-driven computation in neural networks\n2. Hierarchical compression techniques\n3. Adaptive computation mechanisms\n4. State management strategies\n5. Hardware efficiency considerations\n\nThe proposal shows promise in improving efficiency while maintaining model expressivity, but requires careful investigation of its components and their interactions."
        },
        {
            "ready": false,
            "query": "dynamic threshold neural networks, hybrid continuous discrete networks, sparse training stability",
            "detail": "Search for papers discussing:\n1. Dynamic threshold adaptation mechanisms in neural networks, particularly:\n   - Adaptive thresholding techniques\n   - Learning-based threshold adjustment\n   - Applications in language models\n\n2. Hybrid architectures combining continuous and discrete computations:\n   - Integration of spiking and traditional neural networks\n   - Conversion between continuous and discrete representations\n   - Gradient propagation through hybrid systems\n\n3. Training stability and convergence in sparse/selective computation:\n   - Gradient flow in sparse networks\n   - Stability analysis of selective updates\n   - Convergence guarantees for adaptive computation",
            "search_ret": "\n---\n## Found 4 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Dynamic threshold adaptation mechanisms in neural networks, particularly:\n   - Adaptive thresholding techniques\n   - Learning-based threshold adjustment\n   - Applications in language models\n\n2. Hybrid architectures combining continuous and discrete computations:\n   - Integration of spiking and traditional neural networks\n   - Conversion between continuous and discrete representations\n   - Gradient propagation through hybrid systems\n\n3. Training stability and convergence in sparse/selective computation:\n   - Gradient flow in sparse networks\n   - Stability analysis of selective updates\n   - Convergence guarantees for adaptive computation\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 4 papers in Internal Library\n\n#### 1. Efficient Content-Based Sparse Attention with Routing Transformers (Avg. Score: 0.14)\n\n*Aurko Roy, M. Saffar, Ashish Vaswani, David Grangier*\n\n**Published in:** Transactions of the Association for Computational Linguistics (2020)\t**Cited by** 478  (*Influential: 45*)\n\n**TL;DR:** This work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest, and shows that this model outperforms comparable sparse attention models on language modeling on Wikitext-103, as well as on image generation on ImageNet-64 while using fewer self-attention layers.\n\n**Abstract:** Self-attention has recently been adopted for a wide range of sequence modeling problems. Despite its effectiveness, self-attention suffers from quadratic computation and memory requirements with respect to sequence length. Successful approaches to reduce this complexity focused on attending to local sliding windows or a small set of locations independent of content. Our work proposes to learn dynamic sparse attention patterns that avoid allocating computation and memory to attend to content unrelated to the query of interest. This work builds upon two lines of research: It combines the modeling flexibility of prior work on content-based sparse attention with the efficiency gains from approaches based on local, temporal sparse attention. Our model, the Routing Transformer, endows self-attention with a sparse routing module based on online k-means while reducing the overall complexity of attention to O(n1.5d) from O(n2d) for sequence length n and hidden dimension d. We show that our model outperforms comparable sparse attention models on language modeling on Wikitext-103 (15.8 vs 18.3 perplexity), as well as on image generation on ImageNet-64 (3.43 vs 3.44 bits/dim) while using fewer self-attention layers. Additionally, we set a new state-of-the-art on the newly released PG-19 data-set, obtaining a test perplexity of 33.2 with a 22 layer Routing Transformer model trained on sequences of length 8192. We open-source the code for Routing Transformer in Tensorflow.1\n\n##### *Relevant Chunk: No. 13/50 (Score: 0.14)*\n\n```\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450. Alexei Baevski and Michael Auli. 2019. Adaptive input representations for neural language modeling. In International Conference on Learning Representations. Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015. Neural machine translation by jointly learning to align and translate. In 3rd International Conference on Learning Representations, ICLR 2015. Arindam Banerjee and Joydeep Ghosh. 2004. Frequency-sensitive competitive learning for scalable balanced clustering on high-dimensional hyperspheres. IEEE Transactions on Neural Networks, 15(3):702-719. Yoshua Bengio, Nicholas L\u00e9onard, and Aaron Courville. 2013. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432. Mathieu Blondel, Andr\u00e9 F. T. Martins, and Vlad Niculae. 2019. Learning classifiers with fenchelyoung losses: Generalized entropies, margins, and algorithms. In The 22nd International Conference on Artificial Intelligence and Statistics, AISTATS 2019, 16-18 April 2019, Naha, Okinawa, Japan, pages 606-615.\n```\n\n#### 2. Scalable MatMul-free Language Modeling (Avg. Score: 0.06)\n\n*Rui-Jie Zhu, Yu Zhang, Ethan Sifferman, Tyler Sheaves, Yiqiao Wang, Dustin Richmond, Peng Zhou, J. Eshraghian*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 3  (*Influential: 0*)\n\n**TL;DR:** This work shows that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales and points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs.\n\n**Abstract:** Matrix multiplication (MatMul) typically dominates the overall computational cost of large language models (LLMs). This cost only grows as LLMs scale to larger embedding dimensions and context lengths. In this work, we show that MatMul operations can be completely eliminated from LLMs while maintaining strong performance at billion-parameter scales. Our experiments show that our proposed MatMul-free models achieve performance on-par with state-of-the-art Transformers that require far more memory during inference at a scale up to at least 2.7B parameters. We investigate the scaling laws and find that the performance gap between our MatMul-free models and full precision Transformers narrows as the model size increases. We also provide a GPU-efficient implementation of this model which reduces memory usage by up to 61% over an unoptimized baseline during training. By utilizing an optimized kernel during inference, our model's memory consumption can be reduced by more than 10x compared to unoptimized models. To properly quantify the efficiency of our architecture, we build a custom hardware solution on an FPGA which exploits lightweight operations beyond what GPUs are capable of. We processed billion-parameter scale models at 13W beyond human readable throughput, moving LLMs closer to brain-like efficiency. This work not only shows how far LLMs can be stripped back while still performing effectively, but also points at the types of operations future accelerators should be optimized for in processing the next generation of lightweight LLMs. Our code implementation is available at https://github.com/ridgerchu/matmulfreellm.\n\n##### *Relevant Chunk: No. 10/27 (Score: 0.07)*\n\n```\narXiv preprint arXiv:1602.02830, 2016. [8] Sreyes Venkatesh, Razvan Marinescu, and Jason K Eshraghian. Squat: Stateful quantizationaware training in recurrent spiking neural networks. arXiv preprint arXiv:2404.19668, 2024. [9] Jason K Eshraghian, Xinxin Wang, and Wei D Lu. Memristor-based binarized spiking neural networks: Challenges and applications. IEEE Nanotechnology Magazine, 16(2):14-23, 2022. [10] Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Huaijie Wang, Lingxiao Ma, Fan Yang, Ruiping Wang, Yi Wu, and Furu Wei. Bitnet: Scaling 1-bit transformers for large language models. arXiv preprint arXiv:2310.11453, 2023. [11] Shuming Ma, Hongyu Wang, Lingxiao Ma, Lei Wang, Wenhui Wang, Shaohan Huang, Li Dong, Ruiping Wang, Jilong Xue, and Furu Wei. The era of 1-bit llms: All large language models are in 1.58 bits.\n```\n\n##### *Relevant Chunk: No. 19/27 (Score: 0.05)*\n\n```\nIn International Conference on Machine Learning, pages 38087-38099. PMLR, 2023. [34] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural computation, $9(8): 1735-1780,1997$. [35] Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pages 26670-26698. PMLR, 2023. [36] Soham De, Samuel L Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. [37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. [38] Zhen Qin, Songlin Yang, and Yiran Zhong. Hierarchically gated recurrent neural network for sequence modeling. Advances in Neural Information Processing Systems, 36, 2024. [39] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [40] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [41] AI@Meta. Llama 3 model card. 2024. [42] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [43] Yoshua Bengio, Nicholas L\u00e9onard, and Aaron C. Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. CoRR, abs/1308.3432, 2013. [44] Yichi Zhang, Ankush Garg, Yuan Cao, Lukasz Lew, Behrooz Ghorbani, Zhiru Zhang, and Orhan Firat. Binarized neural machine translation. Advances in Neural Information Processing Systems, 36, 2024. [45] Zechun Liu, Barlas Oguz, Aasish Pappu, Yangyang Shi, and Raghuraman Krishnamoorthi. Binary and ternary natural language generation. arXiv preprint arXiv:2306.01841, 2023. [46] Zhen Qin, Dong Li, Weigao Sun, Weixuan Sun, Xuyang Shen, Xiaodong Han, Yunshen Wei, Baohong Lv, Fei Yuan, Xiao Luo, et al. Scaling transnormer to 175 billion parameters.\n```\n\n#### 3. Leave No Context Behind: Efficient Infinite Context Transformers with Infini-attention (Avg. Score: 0.05)\n\n*Tsendsuren Munkhdalai, Manaal Faruqui, Siddharth Gopal*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 34  (*Influential: 3*)\n\n**TL;DR:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation and introduces a new attention technique dubbed Infini-attention.\n\n**Abstract:** This work introduces an efficient method to scale Transformer-based Large Language Models (LLMs) to infinitely long inputs with bounded memory and computation. A key component in our proposed approach is a new attention technique dubbed Infini-attention. The Infini-attention incorporates a compressive memory into the vanilla attention mechanism and builds in both masked local attention and long-term linear attention mechanisms in a single Transformer block. We demonstrate the effectiveness of our approach on long-context language modeling benchmarks, 1M sequence length passkey context block retrieval and 500K length book summarization tasks with 1B and 8B LLMs. Our approach introduces minimal bounded memory parameters and enables fast streaming inference for LLMs.\n\n##### *Relevant Chunk: No. 17/24 (Score: 0.05)*\n\n```\narXiv preprint arXiv:1910.13461, 2019. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for near-infinite context. arXiv preprint arXiv:2310.01889, 2023. Nelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. Lost in the middle: How language models use long contexts. Transactions of the Association for Computational Linguistics, 12:157-173, 2024. Wolfgang Maass, Thomas Natschl\u00e4ger, and Henry Markram. Real-time computing without stable states: A new framework for neural computation based on perturbations. Neural computation, 14(11):2531-2560, 2002. Thomas Miconi, Kenneth Stanley, and Jeff Clune. Differentiable plasticity: training plastic neural networks with backpropagation. In International Conference on Machine Learning, pp. 3559-3568. PMLR, 2018. Amirkeivan Mohtashami and Martin Jaggi. Random-access infinite context length for transformers.\n```\n\n#### 4. SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks (Avg. Score: 0.05)\n\n*Rui Zhu, Qihang Zhao, J. Eshraghian*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 54  (*Influential: 2*)\n\n**TL;DR:** This paper successfully implements `SpikeGPT', a generative language model with binary, event-driven spiking activation units, and is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language.\n\n**Abstract:** As the size of large language models continue to scale, so does the computational resources required to run it. Spiking Neural Networks (SNNs) have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead associated with model inference. While they have become competitive with non-spiking models on many computer vision tasks, SNNs have also proven to be more challenging to train. As a result, their performance lags behind modern deep learning, and we are yet to see the effectiveness of SNNs in language generation. In this paper, inspired by the Receptance Weighted Key Value (RWKV) language model, we successfully implement `SpikeGPT', a generative language model with binary, event-driven spiking activation units. We train the proposed model on two model variants: 45M and 216M parameters. To the best of our knowledge, SpikeGPT is the largest backpropagation-trained SNN model to date, rendering it suitable for both the generation and comprehension of natural language. We achieve this by modifying the transformer block to replace multi-head self attention to reduce quadratic computational complexity O(N^2) to linear complexity O(N) with increasing sequence length. Input tokens are instead streamed in sequentially to our attention mechanism (as with typical SNNs). Our preliminary experiments show that SpikeGPT remains competitive with non-spiking models on tested benchmarks, while maintaining 20x fewer operations when processed on neuromorphic hardware that can leverage sparse, event-driven activations. Our code implementation is available at https://github.com/ridgerchu/SpikeGPT.\n\n##### *Relevant Chunk: No. 31/43 (Score: 0.05)*\n\n```\nAdvances in Neural Information Processing Systems (NeurIPS), 32, 2019. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Michael Pfeiffer and Thomas Pfeil. Deep learning with spiking neurons: Opportunities and challenges. Frontiers in Neuroscience, 12:774, 2018. Xuerui Qiu, Rui-Jie Zhu, Yuhong Chou, Zhaorui Wang, Liang-jian Deng, and Guoqi Li. Gated attention coding for training high-performance and efficient spiking neural networks. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 601-610, 2024. Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language understanding by generative pre-training, 2018. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Nitin Rathi and Kaushik Roy. Diet-snn: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization. IEEE Transactions on Neural Networks and Learning Systems, 2021. Frank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychological Rev., 65(6):386, 1958. Kaushik Roy, Akhilesh Jaiswal, and Priyadarshini Panda. Towards spike-based machine intelligence with neuromorphic computing. Nature, 575(7784):607-617, 2019. Noam Shazeer. GLU variants improve transformer. CoRR, abs/2002.05202, 2020. URL https://arxiv. org/abs/2002.05202\n\nGuobin Shen, Dongcheng Zhao, Tenglong Li, Jindong Li, and Yi Zeng. Is conventional snn really efficient? a perspective from network quantization. arXiv preprint arXiv:2311.10802, 2023. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D Manning, Andrew Y Ng, and Christopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: dynamic threshold neural networks, hybrid continuous discrete networks, sparse training stability\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Biologically Inspired Dynamic Thresholds for Spiking Neural Networks\n\n*From Search Query: dynamic threshold neural networks*\n\n*Jianchuan Ding, B. Dong, Felix Heide, Yufei Ding, Yunduo Zhou, Baocai Yin, Xin Yang*\n\n**TL;DR:** This work finds that the BDETT outperforms existing static and heuristic threshold approaches by significant margins in all tested conditions, and confirms that the proposed bioinspired dynamic threshold scheme offers homeostasis to SNNs in complex real-world tasks.\n\n**Abstract:** The dynamic membrane potential threshold, as one of the essential properties of a biological neuron, is a spontaneous regulation mechanism that maintains neuronal homeostasis, i.e., the constant overall spiking firing rate of a neuron. As such, the neuron firing rate is regulated by a dynamic spiking threshold, which has been extensively studied in biology. Existing work in the machine learning community does not employ bioinspired spiking threshold schemes. This work aims at bridging this gap by introducing a novel bioinspired dynamic energy-temporal threshold (BDETT) scheme for spiking neural networks (SNNs). The proposed BDETT scheme mirrors two bioplausible observations: a dynamic threshold has 1) a positive correlation with the average membrane potential and 2) a negative correlation with the preceding rate of depolarization. We validate the effectiveness of the proposed BDETT on robot obstacle avoidance and continuous control tasks under both normal conditions and various degraded conditions, including noisy observations, weights, and dynamic environments. We find that the BDETT outperforms existing static and heuristic threshold approaches by significant margins in all tested conditions, and we confirm that the proposed bioinspired dynamic threshold scheme offers homeostasis to SNNs in complex real-world tasks.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 19  (*Influential: 1*)\n\n#### 2. State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks\n\n*From Search Query: dynamic threshold neural networks*\n\n*Yanqing Chen, Zhaofei Yu, Wei Fang, Zhengyu Ma, Tiejun Huang, Yonghong Tian*\n\n**TL;DR:** This work organizes SNN pruning techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights, which yields sparse deep networks on the large-scale dataset while maintaining state-of-the-art low performance loss.\n\n**Abstract:** Spiking Neural Networks (SNNs) are considered a promising alternative to Arti\ufb01cial Neural Networks (ANNs) for their event-driven computing paradigm when deployed on energy-ef\ufb01cient neuromorphic hardware. Recently, deep SNNs have shown breathtaking performance improvement through cutting-edge training strategy and \ufb02exi-ble structure, which also scales up the number of parameters and computational burdens in a single network. Inspired by the state transition of dendritic spines in the \ufb01lopodial model of spinogen-esis, we model different states of SNN weights, facilitating weight optimization for pruning. Furthermore, the pruning speed can be regulated by using different functions describing the growing threshold of state transition. We organize these techniques as a dynamic pruning algorithm based on nonlinear reparameterization mapping from spine size to SNN weights. Our approach yields sparse deep networks on the large-scale dataset (SEW ResNet18 on ImageNet) while maintaining state-of-the-art low performance loss ( \u223c 3% at 88.8% sparsity) compared to existing pruning methods on directly trained SNNs. Moreover, we \ufb01nd out pruning speed regulation while learning is crucial to avoiding disastrous performance degradation at the \ufb01nal stages of training, which may shed light on future work on SNN pruning.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 32  (*Influential: 2*)\n\n#### 3. Bipartite expander Hopfield networks as self-decoding high-capacity error correcting codes\n\n*From Search Query: dynamic threshold neural networks*\n\n*Rishidev Chaudhuri, I. Fiete*\n\n**TL;DR:** It is proved that it is possible to construct an associative content-addressable network that combines the properties of strong error correcting codes and Hopfield networks: it simultaneously possesses exponentially many stable states, and can be correctly recovered despite errors in a finite fraction of all nodes.\n\n**Abstract:** Neural network models of memory and error correction famously include the Hopfield network, which can directly store---and error-correct through its dynamics---arbitrary N-bit patterns, but only for ~N such patterns. On the other end of the spectrum, Shannon's coding theory established that it is possible to represent exponentially many states (~e^N) using N symbols in such a way that an optimal decoder could correct all noise upto a threshold. We prove that it is possible to construct an associative content-addressable network that combines the properties of strong error correcting codes and Hopfield networks: it simultaneously possesses exponentially many stable states, these states are robust enough, with large enough basins of attraction that they can be correctly recovered despite errors in a finite fraction of all nodes, and the errors are intrinsically corrected by the network\u2019s own dynamics. The network is a two-layer Boltzmann machine with simple neural dynamics, low dynamic-range (binary) pairwise synaptic connections, and sparse expander graph connectivity. Thus, quasi-random sparse structures---characteristic of important error-correcting codes---may provide for high-performance computation in artificial neural networks and the brain.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2019\n\n**Citations:** 17  (*Influential: 0*)\n\n#### 4. A theory of continuous generative flow networks\n\n*From Search Query: hybrid continuous discrete networks*\n\n*Salem Lahlou, T. Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernandez-Garcia, L'ena N'ehale Ezzine, Y. Bengio, Nikolay Malkin*\n\n**TL;DR:** This work presents a theory for generalized GFlowNets, which encompasses both existing discrete G FlowNets and ones with continuous or hybrid state spaces, and performs experiments to illustrate critical points of the theory and the importance of various assumptions.\n\n**Abstract:** Generative flow networks (GFlowNets) are amortized variational inference algorithms that are trained to sample from unnormalized target distributions over compositional objects. A key limitation of GFlowNets until this time has been that they are restricted to discrete spaces. We present a theory for generalized GFlowNets, which encompasses both existing discrete GFlowNets and ones with continuous or hybrid state spaces, and perform experiments with two goals in mind. First, we illustrate critical points of the theory and the importance of various assumptions. Second, we empirically demonstrate how observations about discrete GFlowNets transfer to the continuous case and show strong results compared to non-GFlowNet baselines on several previously studied tasks. This work greatly widens the perspectives for the application of GFlowNets in probabilistic inference and various modeling settings.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2023\n\n**Citations:** 63  (*Influential: 3*)\n\n#### 5. Continuous vs. Discrete Optimization of Deep Neural Networks\n\n*From Search Query: hybrid continuous discrete networks*\n\n*Omer Elkabetz, Nadav Cohen*\n\n**TL;DR:** It is hypothesized that the theory of gradient flows will unravel mysteries behind deep learning, and it is shown that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent.\n\n**Abstract:** Existing analyses of optimization in deep learning are either continuous, focusing on (variants of) gradient flow, or discrete, directly treating (variants of) gradient descent. Gradient flow is amenable to theoretical analysis, but is stylized and disregards computational efficiency. The extent to which it represents gradient descent is an open question in the theory of deep learning. The current paper studies this question. Viewing gradient descent as an approximate numerical solution to the initial value problem of gradient flow, we find that the degree of approximation depends on the curvature around the gradient flow trajectory. We then show that over deep neural networks with homogeneous activations, gradient flow trajectories enjoy favorable curvature, suggesting they are well approximated by gradient descent. This finding allows us to translate an analysis of gradient flow over deep linear neural networks into a guarantee that gradient descent efficiently converges to global minimum almost surely under random initialization. Experiments suggest that over simple deep neural networks, gradient descent with conventional step size is indeed close to gradient flow. We hypothesize that the theory of gradient flows will unravel mysteries behind deep learning.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2021\n\n**Citations:** 36  (*Influential: 6*)\n\n#### 6. HyAR: Addressing Discrete-Continuous Action Reinforcement Learning via Hybrid Action Representation\n\n*From Search Query: hybrid continuous discrete networks*\n\n*Boyan Li, Hongyao Tang, Yan Zheng, Jianye Hao, Pengyi Li, Zhen Wang, Zhaopeng Meng, Li Wang*\n\n**TL;DR:** This paper proposes Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space and demonstrates the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.\n\n**Abstract:** Discrete-continuous hybrid action space is a natural setting in many practical problems, such as robot control and game AI. However, most previous Reinforcement Learning (RL) works only demonstrate the success in controlling with either discrete or continuous action space, while seldom take into account the hybrid action space. One naive way to address hybrid action RL is to convert the hybrid action space into a unified homogeneous action space by discretization or continualization, so that conventional RL algorithms can be applied. However, this ignores the underlying structure of hybrid action space and also induces the scalability issue and additional approximation difficulties, thus leading to degenerated results. In this paper, we propose Hybrid Action Representation (HyAR) to learn a compact and decodable latent representation space for the original hybrid action space. HyAR constructs the latent space and embeds the dependence between discrete action and continuous parameter via an embedding table and conditional Variantional Auto-Encoder (VAE). To further improve the effectiveness, the action representation is trained to be semantically smooth through unsupervised environmental dynamics prediction. Finally, the agent then learns its policy with conventional DRL algorithms in the learned representation space and interacts with the environment by decoding the hybrid action embeddings to the original action space. We evaluate HyAR in a variety of environments with discrete-continuous action space. The results demonstrate the superiority of HyAR when compared with previous baselines, especially for high-dimensional action spaces.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 29  (*Influential: 4*)\n\n#### 7. NISPA: Neuro-Inspired Stability-Plasticity Adaptation for Continual Learning in Sparse Networks\n\n*From Search Query: sparse training stability*\n\n*Mustafa Burak Gurbuz, C. Dovrolis*\n\n**TL;DR:** This work proposes the Neuro-Inspired Stability-Plasticity Adaptation (NISPA) architecture, a sparse neural network with fixed density that significantly outperforms representative state-of-the-art continual learning baselines, and makes the case that sparsity is an essential ingredient for continual learning.\n\n**Abstract:** The goal of continual learning (CL) is to learn different tasks over time. The main desiderata associated with CL are to maintain performance on older tasks, leverage the latter to improve learning of future tasks, and to introduce minimal overhead in the training process (for instance, to not require a growing model or retraining). We propose the Neuro-Inspired Stability-Plasticity Adaptation (NISPA) architecture that addresses these desiderata through a sparse neural network with fixed density. NISPA forms stable paths to preserve learned knowledge from older tasks. Also, NISPA uses connection rewiring to create new plastic paths that reuse existing knowledge on novel tasks. Our extensive evaluation on EMNIST, FashionMNIST, CIFAR10, and CIFAR100 datasets shows that NISPA significantly outperforms representative state-of-the-art continual learning baselines, and it uses up to ten times fewer learnable parameters compared to baselines. We also make the case that sparsity is an essential ingredient for continual learning. The NISPA code is available at https://github.com/BurakGurbuz97/NISPA.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 34  (*Influential: 4*)\n\n#### 8. Sparse Attention with Linear Units\n\n*From Search Query: sparse training stability*\n\n*Biao Zhang, Ivan Titov, Rico Sennrich*\n\n**TL;DR:** This work introduces a novel, simple method for achieving sparsity in attention: it replaces the softmax activation with a ReLU, and shows that sparsity naturally emerges from such a formulation.\n\n**Abstract:** Recently, it has been argued that encoder-decoder models can be made more interpretable by replacing the softmax function in the attention with its sparse variants. In this work, we introduce a novel, simple method for achieving sparsity in attention: we replace the softmax activation with a ReLU, and show that sparsity naturally emerges from such a formulation. Training stability is achieved with layer normalization with either a specialized initialization or an additional gating function. Our model, which we call Rectified Linear Attention (ReLA), is easy to implement and more efficient than previously proposed sparse attention mechanisms. We apply ReLA to the Transformer and conduct experiments on five machine translation tasks. ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention. Our analysis shows that ReLA delivers high sparsity rate and head diversity, and the induced cross attention achieves better accuracy with respect to source-target word alignment than recent sparsified softmax-based models. Intriguingly, ReLA heads also learn to attend to nothing (i.e. \u2018switch off\u2019) for some queries, which is not possible with sparsified softmax alternatives.\n\n**Venue:** Conference on Empirical Methods in Natural Language Processing\n\n**Year:** 2021\n\n**Citations:** 33  (*Influential: 4*)\n\n#### 9. Accelerating Transformer Pre-Training with 2: 4 Sparsity\n\n*From Search Query: sparse training stability*\n\n*Yuezhou Hu, Kang Zhao, Weiyu Huang, Jianfei Chen, Jun Zhu*\n\n**TL;DR:** This work comprehensively investigates the feasibility of accelerating feed-forward networks of transformers in pre-training and devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss.\n\n**Abstract:** Training large transformers is slow, but recent innovations on GPU architecture give us an advantage. NVIDIA Ampere GPUs can execute a fine-grained 2:4 sparse matrix multiplication twice as fast as its dense equivalent. In the light of this property, we comprehensively investigate the feasibility of accelerating feed-forward networks (FFNs) of transformers in pre-training. First, we define a ``flip rate'' to monitor the stability of a 2:4 training process. Utilizing this metric, we propose three techniques to preserve accuracy: to modify the sparse-refined straight-through estimator by applying the masked decay term on gradients, to determine a feasible decay factor in warm-up stage, and to enhance the model's quality by a dense fine-tuning procedure near the end of pre-training. Besides, we devise two techniques to practically accelerate training: to calculate transposable 2:4 masks by convolution, and to accelerate gated activation functions by reducing GPU L2 cache miss. Experiments show that our 2:4 sparse training algorithm achieves similar convergence to dense training algorithms on several transformer pre-training tasks, while actual acceleration can be observed on different shapes of transformer block apparently. Our toolkit is available at https://github.com/huyz2023/2by4-pretrain.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 4  (*Influential: 2*)\n\n### 6 related papers from Papers with Code\n\n#### 1. DenoDet: Attention as Deformable Multi-Subspace Feature Denoising for Target Detection in SAR Images\n\n*From Search Query: dynamic threshold neural networks*\n\n*Jian Yang, Kang Ni, Xiang Li, YuXuan Li, Minrui Zou, Yimian Dai*\n\n**Abstract:** Synthetic Aperture Radar (SAR) target detection has long been impeded by inherent speckle noise and the prevalence of diminutive, ambiguous targets. While deep neural networks have advanced SAR target detection, their intrinsic low-frequency bias and static post-training weights falter with coherent noise and preserving subtle details across heterogeneous terrains. Motivated by traditional SAR image denoising, we propose DenoDet, a network aided by explicit frequency domain transform to calibrate convolutional biases and pay more attention to high-frequencies, forming a natural multi-scale subspace representation to detect targets from the perspective of multi-subspace denoising. We design TransDeno, a dynamic frequency domain attention module that performs as a transform domain soft thresholding operation, dynamically denoising across subspaces by preserving salient target signals and attenuating noise. To adaptively adjust the granularity of subspace processing, we also propose a deformable group fully-connected layer (DeGroFC) that dynamically varies the group conditioned on the input features. Without bells and whistles, our plug-and-play TransDeno sets state-of-the-art scores on multiple SAR target detection datasets. The code is available at https://github.com/GrokCV/GrokSAR.\n\n**Published:** 2024-06-05\n\n\n\n#### 2. Floor-SP: Inverse CAD for Floorplans by Sequential Room-wise Shortest Path\n\n*From Search Query: dynamic threshold neural networks*\n\n*Yasutaka Furukawa, Jiacheng Chen, Jiaye Wu, Chen Liu*\n\n**Abstract:** This paper proposes a new approach for automated floorplan reconstruction from RGBD scans, a major milestone in indoor mapping research. The approach, dubbed Floor-SP, formulates a novel optimization problem, where room-wise coordinate descent sequentially solves dynamic programming to optimize the floorplan graph structure. The objective function consists of data terms guided by deep neural networks, consistency terms encouraging adjacent rooms to share corners and walls, and the model complexity term. The approach does not require corner/edge detection with thresholds, unlike most other methods. We have evaluated our system on production-quality RGBD scans of 527 apartments or houses, including many units with non-Manhattan structures. Qualitative and quantitative evaluations demonstrate a significant performance boost over the current state-of-the-art. Please refer to our project website http://jcchen.me/floor-sp/ for code and data.\n\n**Conference:** floor-sp-inverse-cad-for-floorplans-by-1\n\n**Published:** 2019-08-19\n\n\n\n#### 3. Parametrized Deep Q-Networks Learning: Reinforcement Learning with Discrete-Continuous Hybrid Action Space\n\n*From Search Query: hybrid continuous discrete networks*\n\n*Peng Sun, Jiechao Xiong, Ji Liu, Han Liu, Zhuoran Yang, Yang Zheng, Tong Zhang, Qing Wang, Lei Han, Haobo Fu*\n\n**Abstract:** Most existing deep reinforcement learning (DRL) frameworks consider either\ndiscrete action space or continuous action space solely. Motivated by\napplications in computer games, we consider the scenario with\ndiscrete-continuous hybrid action space. To handle hybrid action space,\nprevious works either approximate the hybrid space by discretization, or relax\nit into a continuous set. In this paper, we propose a parametrized deep\nQ-network (P- DQN) framework for the hybrid action space without approximation\nor relaxation. Our algorithm combines the spirits of both DQN (dealing with\ndiscrete action space) and DDPG (dealing with continuous action space) by\nseamlessly integrating them. Empirical results on a simulation example, scoring\na goal in simulated RoboCup soccer and the solo mode in game King of Glory\n(KOG) validate the efficiency and effectiveness of our method.\n\n**Published:** 2018-10-10\n\n\n\n#### 4. PARAMETRIZED DEEP Q-NETWORKS LEARNING: PLAYING ONLINE BATTLE ARENA WITH DISCRETE-CONTINUOUS HYBRID ACTION SPACE\n\n*From Search Query: hybrid continuous discrete networks*\n\n*Emmanuel Ekwedike, Haichuan Yang, Haobo Fu, Peng Sun, Carson Eisenach, Ji Liu, Jiechao Xiong, Han Liu, Bei Peng, Zhuoran Yang, Yang Zheng, Xiangru Lian, Tong Zhang, Qing Wang, Lei Han, Haoyue Gao*\n\n**Abstract:** Most existing deep reinforcement learning (DRL) frameworks consider action spaces that are either\ndiscrete or continuous space. Motivated by the project of design Game AI for King of Glory\n(KOG), one the world\u2019s most popular mobile game, we consider the scenario with the discrete-continuous\nhybrid action space. To directly apply existing DLR frameworks, existing approaches\neither approximate the hybrid space by a discrete set or relaxing it into a continuous set, which is\nusually less efficient and robust. In this paper, we propose a parametrized deep Q-network (P-DQN)\nfor the hybrid action space without approximation or relaxation. Our algorithm combines DQN and\nDDPG and can be viewed as an extension of the DQN to hybrid actions. The empirical study on the\ngame KOG validates the efficiency and effectiveness of our method.\n\n**Proceeding:** iclr-2018-1\n\n**Published:** 2018-01-01\n\n\n\n#### 5. Sparse Mixers: Combining MoE and Mixing to build a more efficient BERT\n\n*From Search Query: sparse training stability*\n\n*Joshua Ainslie, James Lee-Thorp*\n\n**Abstract:** We combine the capacity of sparsely gated Mixture-of-Experts (MoE) with the speed and stability of linear, mixing transformations to design the Sparse Mixer encoder model. Sparse Mixer slightly outperforms (<1%) BERT on GLUE and SuperGLUE, but more importantly trains 65% faster and runs inference 61% faster. We also present a faster variant, prosaically named Fast Sparse Mixer, that marginally underperforms BERT on SuperGLUE, but trains and runs nearly twice as fast. We justify the design of these two models by carefully ablating through various mixing mechanisms, MoE configurations and hyperparameters. Sparse Mixer overcomes many of the latency and stability concerns of MoE models and offers the prospect of serving sparse student models, without resorting to distilling them to dense variants.\n\n**Published:** 2022-05-24\n\n\n\n#### 6. AdaMV-MoE: Adaptive Multi-Task Vision Mixture-of-Experts\n\n*From Search Query: sparse training stability*\n\n*Yeqing Li, Zhangyang Wang, Huizhong Chen, Fan Yang, Abdullah Rashwan, Xianzhi Du, Xuxi Chen, Tianlong Chen*\n\n**Abstract:**     Sparsely activated Mixture-of-Experts (MoE) is becoming a promising paradigm for multi-task learning (MTL). Instead of compressing multiple tasks' knowledge into a single model, MoE separates the parameter space and only utilizes the relevant model pieces given task type and its input, which provides stabilized MTL training and ultra-efficient inference. However, current MoE approaches adopt a fixed network capacity (e.g., two experts in usual) for all tasks. It potentially results in the over-fitting of simple tasks or the under-fitting of challenging scenarios, especially when tasks are significantly distinctive in their complexity. In this paper, we propose an adaptive MoE framework for multi-task vision recognition, dubbed AdaMV-MoE. Based on the training dynamics, it automatically determines the number of activated experts for each task, avoiding the laborious manual tuning of optimal model size. To validate our proposal, we benchmark it on ImageNet classification and COCO object detection & instance segmentation which are notoriously difficult to learn in concert, due to their discrepancy. Extensive experiments across a variety of vision transformers demonstrate a superior performance of AdaMV-MoE, compared to MTL with a shared backbone and the recent state-of-the-art (SoTA) MTL MoE approach. Codes are available online: https://github.com/google-research/google-research/tree/master/moe_mtl.    \n\n**Proceeding:** iccv-2023-1\n\n**Published:** 2023-01-01\n\n\n\n\n\n---\n## Web search results\n\n To help the researchers improve the design of autoregressive language models using the specified keywords and details, here are some relevant findings and discussions:\n\n## Dynamic Threshold Adaptation Mechanisms\n\n### Adaptive Thresholding Techniques\n- The concept of dynamic threshold adaptation is closely related to the mechanisms used in Spiking Neural Networks (SNNs). For instance, the Multiple Threshold (MT) approach introduced in uses multiple thresholds in either parallel or cascade configurations to enhance the precision of SNNs. This approach can be adapted to other neural networks, including language models, to dynamically adjust thresholds based on the input or the state of the network.\n- In SNNs, techniques like Spike Layer Error Reassignment (SLAYER) and Spatio-temporal backpropagation (STBP) address the nondifferentiable issue of spike functions by using temporal credit assignment policies and derivative approximations, respectively. These methods can inspire dynamic threshold adaptation in continuous-valued networks.\n\n### Learning-Based Threshold Adjustment\n- The MT-SNN approach in demonstrates how learning-based threshold adjustment can improve network performance. By applying multiple thresholds, the network can learn to optimize the firing rates of neurons, which could be translated to adjusting activation thresholds in traditional neural networks.\n- Hybrid macro/micro level backpropagation (HM2-BP) and Spike-train level recurrent SNNs backpropagation (ST-RSBP) also involve learning-based adjustments that could be adapted for dynamic thresholding in language models.\n\n### Applications in Language Models\n- While the direct application of these techniques to language models is not explicitly covered in the sources, the principles of dynamic threshold adaptation can be integrated into autoregressive models. For example, using adaptive thresholds to control the flow of information or to selectively propagate or forget information along the sequence length dimension, as hinted in the Mamba paper, could enhance the efficiency and performance of language models.\n\n## Hybrid Architectures Combining Continuous and Discrete Computations\n\n### Integration of Spiking and Traditional Neural Networks\n- The MT-SNN approach in shows how SNNs can be enhanced to match or surpass the performance of traditional ANNs. This integration can be extended to hybrid architectures where spiking neurons are used in conjunction with traditional neurons, potentially leveraging the energy efficiency and event-driven nature of SNNs.\n- The use of hybrid macro/micro level backpropagation (HM2-BP) and other techniques in provides insights into how continuous and discrete computations can be integrated, which could be beneficial for designing hybrid language models.\n\n### Conversion Between Continuous and Discrete Representations\n- The conversion between continuous and discrete representations is crucial in hybrid architectures. Techniques like those described in, where multiple thresholds are used to enhance precision, can help in converting continuous activations to discrete spike representations and vice versa.\n- The work on Spiking Neural Networks with lateral interactions (LISNN) and other SNN models in also addresses how to handle the conversion and integration of continuous and discrete signals.\n\n### Gradient Propagation Through Hybrid Systems\n- Gradient propagation in hybrid systems is a key challenge. Techniques such as SLAYER, STBP, and HM2-BP in provide methods to handle the nondifferentiable nature of spike functions, which can be adapted for gradient propagation in hybrid continuous-discrete networks.\n- The MT-SNN approach in demonstrates how to train hybrid networks efficiently, maintaining compatibility with both traditional and neuromorphic hardware.\n\n## Training Stability and Convergence in Sparse/Selective Computation\n\n### Gradient Flow in Sparse Networks\n- The concept of sparse modular activation (SMA) discussed in the context of sequence modeling highlights how sparse operations can reduce computation and memory consumption. Ensuring stable gradient flow in such sparse networks is crucial, and techniques like those used in SNNs (e.g., SLAYER, ST-RSBP) can provide insights into managing sparse updates.\n- The Mamba paper's selective state space mechanism also addresses how to selectively propagate or forget information, which can be related to managing gradient flow in sparse networks.\n\n### Stability Analysis of Selective Updates\n- Stability analysis in selective updates is essential for ensuring that the network converges to optimal parameters. The work on SNNs, such as the use of pseudo derivatives and temporal credit assignment policies, provides a framework for analyzing stability in sparse and selective computation.\n- The discussion on the performance of MT-SNNs in also touches on the stability and convergence of these networks, which can be applied to the analysis of selective updates in language models.\n\n### Convergence Guarantees for Adaptive Computation\n- Convergence guarantees are critical for adaptive computation strategies. The use of energy functions and dynamical equations in Hopfield networks, as described in, can provide a theoretical framework for proving convergence in adaptive computation systems.\n- The work on adaptive computation in large language models, such as Duo-LLM, also provides insights into how routing strategies and hardware considerations can impact convergence and stability.\n\nIn summary, the integration of dynamic threshold adaptation mechanisms, hybrid continuous-discrete architectures, and stable sparse computation techniques can significantly enhance the performance, efficiency, and scalability of autoregressive language models. By leveraging insights from SNNs and other adaptive computation strategies, researchers can develop more robust and efficient language models.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe initial search reveals several promising directions for the EventDDLerpLinear proposal, particularly in:\n- Event-driven computation mechanisms from spiking neural networks\n- Hierarchical state compression techniques\n- Adaptive computation strategies\n- Hardware-efficient implementations\n\nThe proposal appears novel in combining these elements specifically for DDLerpLinear, though more investigation is needed into similar approaches in language models.\n\n2. Useful References with Excerpts:\n\na) From Mamba paper:\n\"First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token.\"\n- Relevant for selective computation mechanism\n\nb) From SpikeGPT:\n\"SNNs have emerged as an energy-efficient approach to deep learning that leverage sparse and event-driven activations to reduce the computational overhead\"\n- Validates event-driven approach\n\nc) From \"Sparse Modular Activation\":\n\"SMA reduces computation and memory consumption at both training and inference stages of sequence modeling\"\n- Supports efficiency claims\n\n3. Key Insights:\n\na) Event-Driven Mechanisms:\n- SNNs provide proven frameworks for event-based computation\n- Need to investigate adaptation to continuous-valued networks\n- Threshold mechanisms crucial for performance\n\nb) Hierarchical Compression:\n- Multiple approaches exist (VQ-VAE, HiLLoC)\n- Trade-offs between compression and model quality\n- Need more research on dynamic compression\n\nc) Adaptive Computation:\n- Several successful implementations (AutoMoE, Duo-LLM)\n- Routing strategies critical for performance\n- Hardware considerations important\n\n4. Future Search Plan:\n\nNeed additional investigation in:\na) Dynamic threshold adaptation mechanisms\nb) Integration of event-driven and continuous networks\nc) Hardware-efficient implementations of sparse operations\nd) Training stability with selective updates\ne) Specific applications in language models\n\nNext search queries should focus on:\n1. Dynamic threshold mechanisms in neural networks\n2. Hybrid continuous-discrete neural architectures\n3. Training stability in sparse neural networks\n\n5. References:\n\n[1] Gu, A., & Dao, T. (2023). Mamba: Linear-Time Sequence Modeling with Selective State Spaces. arXiv:2312.00752.\n\n[2] Zhu, R., Zhao, Q., & Eshraghian, J. (2023). SpikeGPT: Generative Pre-trained Language Model with Spiking Neural Networks. arXiv:2304.12210.\n\n[3] Ren, L., Liu, Y., Wang, S., Xu, Y., Zhu, C., Zhai, C., & Zhou, B. (2023). Sparse Modular Activation for Efficient Sequence Modeling. Neural Information Processing Systems.\n\n[4] Jabri, A., Fleet, D.J., & Chen, T. (2022). Scalable Adaptive Computation for Iterative Generation. International Conference on Machine Learning.\n\n[5] Alizadeh, K., et al. (2024). Duo-LLM: A Framework for Studying Adaptive Computation in Large Language Models. arXiv:2402.09668."
        },
        {
            "ready": false,
            "query": "memory efficient state representation, sequential model compression, sparse gradient optimization",
            "detail": "Search for papers discussing:\n1. Memory-efficient state representations in neural networks:\n   - Compact state encoding techniques\n   - Memory management in sequential models\n   - Efficient state update mechanisms\n\n2. Compression methods for sequential models:\n   - State compression in language models\n   - Dynamic and adaptive compression\n   - Information-preserving compression\n\n3. Gradient optimization techniques:\n   - Sparse gradient propagation\n   - Efficient backpropagation methods\n   - Stability in sparse networks",
            "search_ret": "\n---\n## Found 5 related chunks from 1 internal sources\n\nYour raw search query input to the search frame: \n\nSearch for papers discussing:\n1. Memory-efficient state representations in neural networks:\n   - Compact state encoding techniques\n   - Memory management in sequential models\n   - Efficient state update mechanisms\n\n2. Compression methods for sequential models:\n   - State compression in language models\n   - Dynamic and adaptive compression\n   - Information-preserving compression\n\n3. Gradient optimization techniques:\n   - Sparse gradient propagation\n   - Efficient backpropagation methods\n   - Stability in sparse networks\n\nConsidering refining your search by improving the query keywords input.\n\n### 5 related chunks from 5 papers in Internal Library\n\n#### 1. There is HOPE to Avoid HiPPOs for Long-memory State Space Models (Avg. Score: 0.61)\n\n*Annan Yu, Michael W. Mahoney, N. Benjamin Erichson*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 1  (*Influential: 0*)\n\n**TL;DR:** A new parameterization scheme, called HOPE, is developed for LTI systems that utilizes Markov parameters within Hankel operators, which allows for random initializations of the LTI systems and helps to improve training stability, while also providing the SSMs with non-decaying memory capabilities.\n\n**Abstract:** State-space models (SSMs) that utilize linear, time-invariant (LTI) systems are known for their effectiveness in learning long sequences. However, these models typically face several challenges: (i) they require specifically designed initializations of the system matrices to achieve state-of-the-art performance, (ii) they require training of state matrices on a logarithmic scale with very small learning rates to prevent instabilities, and (iii) they require the model to have exponentially decaying memory in order to ensure an asymptotically stable LTI system. To address these issues, we view SSMs through the lens of Hankel operator theory, which provides us with a unified theory for the initialization and training of SSMs. Building on this theory, we develop a new parameterization scheme, called HOPE, for LTI systems that utilizes Markov parameters within Hankel operators. This approach allows for random initializations of the LTI systems and helps to improve training stability, while also provides the SSMs with non-decaying memory capabilities. Our model efficiently implements these innovations by nonuniformly sampling the transfer functions of LTI systems, and it requires fewer parameters compared to canonical SSMs. When benchmarked against HiPPO-initialized models such as S4 and S4D, an SSM parameterized by Hankel operators demonstrates improved performance on Long-Range Arena (LRA) tasks. Moreover, we use a sequential CIFAR-10 task with padded noise to empirically corroborate our SSM's long memory capacity.\n\n##### *Relevant Chunk: No. 23/31 (Score: 0.61)*\n\n```\n[27] Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman. Simplified state space layers for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023. [28] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efficient transformers. International Conference in Learning Representations, 2021. [29] Aaron Voelker, Ivana Kaji\u0107, and Chris Eliasmith. Legendre memory units: Continuoustime representation in recurrent neural networks. Advances in neural information processing systems, 32, 2019. [30] Shida Wang and Qianxiao Li. Stablessm: Alleviating the curse of memory in state-space models through stable reparameterization.\n```\n\n#### 2. Latent Attention for Linear Time Transformers (Avg. Score: 0.54)\n\n*Rares Dolga, Marius Cobzarenco, David Barber*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 0  (*Influential: 0*)\n\n**TL;DR:** A method to reduce the time complexity of the standard attention mechanism in a transformer to linear scaling with time, based on defining attention via latent vectors is introduced, which allows scaling to context windows much larger than practical in standard attention.\n\n**Abstract:** The time complexity of the standard attention mechanism in a transformer scales quadratically with the length of the sequence. We introduce a method to reduce this to linear scaling with time, based on defining attention via latent vectors. The method is readily usable as a drop-in replacement for the standard attention mechanism. Our\"Latte Transformer\"model can be implemented for both bidirectional and unidirectional tasks, with the causal version allowing a recurrent implementation which is memory and time-efficient during inference of language generation tasks. Whilst next token prediction scales linearly with the sequence length for a standard transformer, a Latte Transformer requires constant time to compute the next token. The empirical performance of our method is comparable to standard attention, yet allows scaling to context windows much larger than practical in standard attention.\n\n##### *Relevant Chunk: No. 10/21 (Score: 0.54)*\n\n```\narXiv preprint arXiv:2212.14052, 2022. Glorot, X., Bordes, A., and Bengio, Y. Deep Sparse Rectifier Neural Networks. In JMLR Workshop and Conference Proceedings, pp. 315-323, 2011. Gokaslan, A. and Cohen, V. OpenWebText Corpus, 2019. URL http://Skylion007.github.io/ OpenWebTextCorpus. Gu, A., Goel, K., and R\u00e9, C. Efficiently Modeling Long Sequences with Structured State Spaces. arXiv preprint arXiv:2111.00396, 2021. Hutter, M. The Human Knowledge Compression Prize, 2002. URL https:// www.kurzweilai.net/hutter-prizefor-lossless-compression-of-humanknowledge. Jaegle, A., Gimeno, F., Brock, A., Vinyals, O., Zisserman, A., and Carreira, J. Perceiver: General Perception with Iterative Attention. In International Conference on Machine Learning, pp. 4651-4664. PMLR, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, pp. 5156-5165. PMLR, 2020. Khan, S., Naseer, M., Hayat, M., Zamir, S.\n```\n\n#### 3. HiPPO: Recurrent Memory with Optimal Polynomial Projections (Avg. Score: 0.51)\n\n*Albert Gu, Tri Dao, Stefano Ermon, A. Rudra, C. R\u00e9*\n\n**Published in:** Neural Information Processing Systems (2020)\t**Cited by** 255  (*Influential: 36*)\n\n**TL;DR:** This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale and enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients.\n\n**Abstract:** A central problem in learning from sequential data is representing cumulative history in an incremental fashion as more data is processed. We introduce a general framework (HiPPO) for the online compression of continuous signals and discrete time series by projection onto polynomial bases. Given a measure that specifies the importance of each time step in the past, HiPPO produces an optimal solution to a natural online function approximation problem. As special cases, our framework yields a short derivation of the recent Legendre Memory Unit (LMU) from first principles, and generalizes the ubiquitous gating mechanism of recurrent neural networks such as GRUs. This formal framework yields a new memory update mechanism (HiPPO-LegS) that scales through time to remember all history, avoiding priors on the timescale. HiPPO-LegS enjoys the theoretical benefits of timescale robustness, fast updates, and bounded gradients. By incorporating the memory dynamics into recurrent neural networks, HiPPO RNNs can empirically capture complex temporal dependencies. On the benchmark permuted MNIST dataset, HiPPO-LegS sets a new state-of-the-art accuracy of 98.3%. Finally, on a novel trajectory classification task testing robustness to out-of-distribution timescales and missing data, HiPPO-LegS outperforms RNN and neural ODE baselines by 25-40% accuracy.\n\n##### *Relevant Chunk: No. 31/54 (Score: 0.51)*\n\n```\n## A. 2 Memory in Machine Learning\n\nMemory in sequence models Sequential or temporal data in areas such as language, reinforcement learning, and continual learning can involve increasingly long dependencies. However, direct parametric modeling cannot handle inputs of unknown and potentially unbounded lengths. Many modern solutions such as attention [70] and dilated convolutions [5], are functions on finite windows, thus sidestepping the need for an explicit memory representation. While this suffices for certain tasks, these approaches can only process a finite context window instead of an entire sequence. Naively increasing the window length poses significant compute and memory challenges. This has spurred various approaches to extend this fixed context window subjected to compute and storage constraints [6, 15, 18, 42, 59, 60, 64, 74]. We instead focus on the core problem of online processing and memorization of continuous and discrete signals, and anticipate that the study of this foundational problem will be useful in improving a variety of models. Recurrent memory Recurrent neural networks are a natural tool for modeling sequential data online, with the appealing property of having unbounded context; in other words they can summarize history indefinitely. However, due to difficulties in the optimization process (vanishing/exploding gradients [56]), particular care must be paid to endow them with longer memory. The ubiquitous LSTM 34 and simplifications such as the GRU [17] control the update with gates to smooth the optimization process. With more careful parametrization, the addition of gates alone make RNNs significantly more robust and able to address long-term dependencies [31. Tallec and Ollivier [66] show that gates are in fact fundamental for recurrent dynamics by allowing time dilations. Many other approaches to endowing RNNs with better memory exist, such as noise injection [32] or non-saturating gates [9], which can suffer from instability issues.\n```\n\n#### 4. DenseMamba: State Space Models with Dense Hidden Connection for Efficient Large Language Models (Avg. Score: 0.37)\n\n*Wei He, Kai Han, Yehui Tang, Chengcheng Wang, Yujie Yang, Tianyu Guo, Yunhe Wang*\n\n**Published in:** arXiv.org (2024)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** DenseSSM is introduced, a novel approach to enhance the flow of hidden information between layers in SSMs by selectively integrating shallowlayer hidden states into deeper layers, and retains fine-grained information crucial for the final output.\n\n**Abstract:** Large language models (LLMs) face a daunting challenge due to the excessive computational and memory requirements of the commonly used Transformer architecture. While state space model (SSM) is a new type of foundational network architecture offering lower computational complexity, their performance has yet to fully rival that of Transformers. This paper introduces DenseSSM, a novel approach to enhance the flow of hidden information between layers in SSMs. By selectively integrating shallowlayer hidden states into deeper layers, DenseSSM retains fine-grained information crucial for the final output. Dense connections enhanced DenseSSM still maintains the training parallelizability and inference efficiency. The proposed method can be widely applicable to various SSM types like RetNet and Mamba. With similar model size, DenseSSM achieves significant improvements, exemplified by DenseRetNet outperforming the original RetNet with up to 5% accuracy improvement on public benchmarks. code is avalaible at https://github.com/WailordHe/DenseSSM\n\n##### *Relevant Chunk: No. 3/21 (Score: 0.37)*\n\n```\n## 2. Related Works\n\n### 2.1. Large Language Models\n\nLarge language models (LLMs) have seen transformative advancements, enabling them to excel in a diverse array of natural language processing (NLP) tasks, including machine translation, text summarization, and emergent abilities like incontext learning, which were previously unattainable by earlier language models (Devlin et al., 2019; Raffel et al., 2023). The evolution of LLMs has been marked by a monumental shift in scale, exemplified by models like GPT3 (Brown et al., 2020), with its 175 billion parameters, and the even more expansive PaLM (Chowdhery et al., 2022), packing in a astounding 540 billion parameters. These models have empirically validated the scaling law (Kaplan et al., 2020), which posits that increasing model size leads to improved performance. The rapid expansion in model size has underscored the critical need for the development of efficient Transformer algorithms, where FlashAttention (Dao et al., 2022; Dao, 2023) has emerged as a significant innovation. This approach enhances the pivotal attention mechanism within Transformers by optimizing softmax computations using a technique known as tiling. By minimizing memory transactions between the GPU's HBM and on-chip SRAM, FlashAttention compute exact attention with fewer memory accesses, result- ing in both faster execution and a lower memory footprint compared to standard attention implementations. ### 2.2. State Space Models\n\nWhile the Transformer is currently the de facto architecture for large language models (LLMs), providing efficient parallel GPU training, the inference time for single-token inference increases significantly with longer sequence lengths, posing challenges for deployment due to the $\\mathrm{O}(\\mathrm{N})$ complexity per step even with accelerating algorithms like FlashAttention (Dao et al., 2022; Dao, 2023). Efforts have been dedicated to researching the Transformer-Next architecture, aiming to achieve state-of-the-art (SOTA) performance with efficient parallel training and effective inference, particularly for long sequence lengths. State Space Sequence Models (SSMs) have recently emerged as promising architectures for sequence modeling. HiPPO (Gu et al., 2020) streamlines sequence modeling by compressing lengthy inputs into a dynamic, polynomialbased representation using orthogonal polynomials. S4 (Gu et al., 2021) introduced a novel parameterization through the application of a low-rank structured correction, enabling stable diagonalization and simplifying the process into Cauchy kernel operations. S5 (Smith et al., 2023) further simplifies the S 4 layer by employing a single multi-input, multi-output SSM and introducing efficient parallel scan algorithms into the S4 layers. H3 (Fu et al., 2023) narrows the performance gap between SSMs and Transformer language models by designing three projections $(\\mathrm{Q}, \\mathrm{K}, \\mathrm{V})$ to simulate the attention mechanism and adopting a fast Fourier transform (FFT) to reduce computation and memory consumption further. GSS (Mehta et al., 2022) was the first gated neural network architecture incorporating SSMs, it builds upon (Hua et al., 2022) and introducing a compact SSM architecture that contracts model dimensions. Unlike GSS, which emphasizes compressing context into a smaller state, Mamba (Gu \\& Dao, 2023) diverges by focusing on enhancing the selectivity of the state representation, aiming to balance the tradeoff between efficiency and effectiveness without compromising the model's ability to capture essential information from the context.\n```\n\n#### 5. FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores (Avg. Score: 0.27)\n\n*Daniel Y. Fu, Hermann Kumbong, Eric N. D. Nguyen, Christopher R'e*\n\n**Published in:** arXiv.org (2023)\t**Cited by** 14  (*Influential: 1*)\n\n**TL;DR:** Partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n**Abstract:** Convolution models with long filters have demonstrated state-of-the-art reasoning abilities in many long-sequence tasks but lag behind the most optimized Transformers in wall-clock time. A major bottleneck is the Fast Fourier Transform (FFT)--which allows long convolutions to run in $O(N logN)$ time in sequence length $N$ but has poor hardware utilization. In this paper, we study how to optimize the FFT convolution. We find two key bottlenecks: the FFT does not effectively use specialized matrix multiply units, and it incurs expensive I/O between layers of the memory hierarchy. In response, we propose FlashFFTConv. FlashFFTConv uses a matrix decomposition that computes the FFT using matrix multiply units and enables kernel fusion for long sequences, reducing I/O. We also present two sparse convolution algorithms--1) partial convolutions and 2) frequency-sparse convolutions--which can be implemented simply by skipping blocks in the matrix decomposition, enabling further opportunities for memory and compute savings. FlashFFTConv speeds up exact FFT convolutions by up to 7.93$\\times$ over PyTorch and achieves up to 4.4$\\times$ speedup end-to-end. Given the same compute budget, FlashFFTConv allows Hyena-GPT-s to achieve 2.3 points better perplexity on the PILE and M2-BERT-base to achieve 3.3 points higher GLUE score--matching models with twice the parameter count. FlashFFTConv also achieves 96.1% accuracy on Path-512, a high-resolution vision task where no model had previously achieved better than 50%. Furthermore, partial convolutions enable longer-sequence models--yielding the first DNA model that can process the longest human genes (2.3M base pairs)--and frequency-sparse convolutions speed up pretrained models while maintaining or improving model quality.\n\n##### *Relevant Chunk: No. 30/46 (Score: 0.27)*\n\n```\nAdvances in neural information processing systems, 32, 2019 . [65] Mitsuru Kusumoto, Takuya Inoue, Gentaro Watanabe, Takuya Akiba, and Masanori Koyama. A graph theoretic framework of recomputation algorithms for memory-efficient backpropagation. Advances in Neural Information Processing Systems, 32, 2019. [66] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [67] Kushal Lakhotia, Eugene Kharitonov, Wei-Ning Hsu, Yossi Adi, Adam Polyak, Benjamin Bolte, Tu-Anh Nguyen, Jade Copet, Alexei Baevski, Abdelrahman Mohamed, et al. On generative spoken language modeling from raw audio. Transactions of the Association for Computational Linguistics, 9:1336-1354, 2021 . [68] Adam Lavely. Powering extreme-scale hpc with cerebras wafer-scale accelerators. Cerebras White Paper, 2022 . [69] Binrui Li, Shenggan Cheng, and James Lin. tcfft: Accelerating half-precision fft through tensor cores.\n```\n\n\n\n---\n## Found 15 related papers from 2 external sources\n\n\n\nYour 3 raw search queries input to the search frame: memory efficient state representation, sequential model compression, sparse gradient optimization\n\nConsidering refining your search by improving the query keywords input.\n\n### 9 related papers from Semantic Scholar\n\n#### 1. Vision Mamba: Efficient Visual Representation Learning with Bidirectional State Space Model\n\n*From Search Query: memory efficient state representation*\n\n*Lianghui Zhu, Bencheng Liao, Qian Zhang, Xinlong Wang, Wenyu Liu, Xinggang Wang*\n\n**TL;DR:** This paper proposes a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models and has great potential to be the next-generation backbone for vision foundation models.\n\n**Abstract:** Recently the state space models (SSMs) with efficient hardware-aware designs, i.e., the Mamba deep learning model, have shown great potential for long sequence modeling. Meanwhile building efficient and generic vision backbones purely upon SSMs is an appealing direction. However, representing visual data is challenging for SSMs due to the position-sensitivity of visual data and the requirement of global context for visual understanding. In this paper, we show that the reliance on self-attention for visual representation learning is not necessary and propose a new generic vision backbone with bidirectional Mamba blocks (Vim), which marks the image sequences with position embeddings and compresses the visual representation with bidirectional state space models. On ImageNet classification, COCO object detection, and ADE20k semantic segmentation tasks, Vim achieves higher performance compared to well-established vision transformers like DeiT, while also demonstrating significantly improved computation&memory efficiency. For example, Vim is 2.8$\\times$ faster than DeiT and saves 86.8% GPU memory when performing batch inference to extract features on images with a resolution of 1248$\\times$1248. The results demonstrate that Vim is capable of overcoming the computation&memory constraints on performing Transformer-style understanding for high-resolution images and it has great potential to be the next-generation backbone for vision foundation models. Code is available at https://github.com/hustvl/Vim.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2024\n\n**Citations:** 351  (*Influential: 57*)\n\n#### 2. Memory-Efficient Learning of Stable Linear Dynamical Systems for Prediction and Control\n\n*From Search Query: memory efficient state representation*\n\n*Giorgos Mamakoukas, Orest Xherija, T. Murphey*\n\n**TL;DR:** This work presents an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived, and achieves an orders-of-magnitude improvement in reconstruction error and superior results in terms of control performance.\n\n**Abstract:** Learning a stable Linear Dynamical System (LDS) from data involves creating models that both minimize reconstruction error and enforce stability of the learned representation. We propose a novel algorithm for learning stable LDSs. Using a recent characterization of stable matrices, we present an optimization method that ensures stability at every step and iteratively improves the reconstruction error using gradient directions derived in this paper. When applied to LDSs with inputs, our approach---in contrast to current methods for learning stable LDSs---updates both the state and control matrices, expanding the solution space and allowing for models with lower reconstruction error. We apply our algorithm in simulations and experiments to a variety of problems, including learning dynamic textures from image sequences and controlling a robotic manipulator. Compared to existing approaches, our proposed method achieves an orders-of-magnitude improvement in reconstruction error and superior results in terms of control performance. In addition, it is provably more memory-efficient, with an O(n^2) space complexity compared to O(n^4) of competing alternatives, thus scaling to higher-dimensional systems when the other methods fail.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2020\n\n**Citations:** 18  (*Influential: 1*)\n\n#### 3. Associative Memory Augmented Asynchronous Spatiotemporal Representation Learning for Event-based Perception\n\n*From Search Query: memory efficient state representation*\n\n*U. Kamal, Saurabh Dash, S. Mukhopadhyay*\n\n**TL;DR:** EventFormer treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism and achieves 0.5% and 9% better accuracy with 30000 \u00d7 and 200 \u00d7 less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets.\n\n**Abstract:** We propose EventFormer \u2013 a computationally ef\ufb01cient event-based representation learning framework for asynchronously processing event camera data. Event-Former treats sparse input events as a spatially unordered set and models their spatial interactions using self-attention mechanism. An associative memory-augmented recurrent module is used to correlate with the stored representation computed from past events. A memory addressing mechanism is proposed to store and retrieve the latent states only where these events occur and update them only when they occur. The representation learning shift from input space to the latent memory space resulting in reduced computation cost for processing each event. We show that EventFormer achieves 0 . 5% and 9% better accuracy with 30000 \u00d7 and 200 \u00d7 less computation compared to the state-of-the-art dense and event-based method, respectively, on event-based object recognition datasets.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2023\n\n**Citations:** 2  (*Influential: 0*)\n\n#### 4. Encoding Weights of Irregular Sparsity for Fixed-to-Fixed Model Compression\n\n*From Search Query: sequential model compression*\n\n*Baeseong Park, S. Kwon, Daehwan Oh, Byeongwook Kim, Dongsoo Lee*\n\n**TL;DR:** A sequential fixed-to-fixed (lossless) encoding architecture/algorithm is studied to support fine-grained pruning methods such that sparse neural networks can be stored in a highly regular structure and it is demonstrated that the proposed compression scheme achieves almost the maximum compression ratio for the Transformer and ResNet-50 pruned by various fine- grained pruned methods.\n\n**Abstract:** Even though fine-grained pruning techniques achieve a high compression ratio, conventional sparsity representations (such as CSR) associated with irregular sparsity degrade parallelism significantly. Practical pruning methods, thus, usually lower pruning rates (by structured pruning) to improve parallelism. In this paper, we study fixed-to-fixed (lossless) encoding architecture/algorithm to support fine-grained pruning methods such that sparse neural networks can be stored in a highly regular structure. We first estimate the maximum compression ratio of encoding-based compression using entropy. Then, as an effort to push the compression ratio to the theoretical maximum (by entropy), we propose a sequential fixed-to-fixed encoding scheme. We demonstrate that our proposed compression scheme achieves almost the maximum compression ratio for the Transformer and ResNet-50 pruned by various fine-grained pruning methods.\n\n**Venue:** International Conference on Learning Representations\n\n**Year:** 2021\n\n**Citations:** 2  (*Influential: 1*)\n\n#### 5. Towards Efficient Image Compression Without Autoregressive Models\n\n*From Search Query: sequential model compression*\n\n*Muhammad Salman Ali, Yeong-Gwan Kim, Maryam Qamar, Sung-Chang Lim, Donghyun Kim, Chaoning Zhang, Sung-Ho Bae, Hui Yong Kim*\n\n**TL;DR:** A novel alternative to the AR-based approach that can provide a significantly better trade-off between performance and complexity and is proved to act as a general plug-in for the hyperprior (HP) based learned image compression methods.\n\n**Abstract:** Recently, learned image compression (LIC) has garnered increasing interest with its rapidly improving performance surpassing conventional codecs. A key ingredient of LIC is a hyperprior-based entropy model, where the underlying joint probability of the latent image features is modeled as a product of Gaussian distributions from each latent element. Since latents from the actual images are not spatially independent, autoregressive (AR) context based entropy models were proposed to handle the discrepancy between the assumed distribution and the actual distribution. Though the AR-based models have proven effective, the computational complexity is significantly increased due to the inherent sequential nature of the algorithm. In this paper, we present a novel alternative to the AR-based approach that can provide a significantly better trade-off between performance and complexity. To minimize the discrepancy, we introduce a correlation loss that forces the latents to be spatially decorrelated and better fitted to the independent probability model. Our correlation loss is proved to act as a general plug-in for the hyperprior (HP) based learned image compression methods. The performance gain from our correlation loss is \u2018free\u2019 in terms of computation complexity for both inference time and decoding time. To our knowledge, our method gives the best trade-off between the complexity and performance: combined with the Checkerboard-CM, it attains 90% and when combined with ChARM-CM, it attains 98% of the AR-based BD-Rate gains yet is around 50 times and 30 times faster than AR-based methods respectively.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2023\n\n**Citations:** 3  (*Influential: 1*)\n\n#### 6. Deep Generative Video Compression\n\n*From Search Query: sequential model compression*\n\n*Salvator Lombardo, Jun Han, Christopher Schroers, S. Mandt*\n\n**TL;DR:** This work proposes an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video that builds upon variational autoencoder models for sequential data and combines them with recent work on neural image compression.\n\n**Abstract:** The usage of deep generative models for image compression has led to impressive performance gains over classical codecs while neural video compression is still in its infancy. Here, we propose an end-to-end, deep generative modeling approach to compress temporal sequences with a focus on video. Our approach builds upon variational autoencoder (VAE) models for sequential data and combines them with recent work on neural image compression. The approach jointly learns to transform the original sequence into a lower-dimensional representation as well as to discretize and entropy code this representation according to predictions of the sequential VAE. Rate-distortion evaluations on small videos from public data sets with varying complexity and diversity show that our model yields competitive results when trained on generic video content. Extreme compression performance is achieved when training the model on specialized content.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2018\n\n**Citations:** 55  (*Influential: 2*)\n\n#### 7. Get More at Once: Alternating Sparse Training with Gradient Correction\n\n*From Search Query: sparse gradient optimization*\n\n*Li Yang, Jian Meng, J.-s. Seo, Deliang Fan*\n\n**TL;DR:** This work proposes a novel alternating sparse training (AST) scheme to train multiple sparse sub-nets for dynamic inference without extra training cost compared to the case of training a single sparse model from scratch, and pro-poses gradient correction within the inner-group iterations to mitigate the interference of weight update among sub-nets.\n\n**Abstract:** Recently, a new trend of exploring training sparsity has emerged, which removes parameters during training, leading to both training and inference efficiency improvement. This line of works primarily aims to obtain a single sparse model under a pre-defined large sparsity ratio. It leads to a static/fixed sparse inference model that is not capable of adjusting or re-configuring its computation complexity (i.e., inference structure, latency) after training for real-world varying and dynamic hardware resource availability. To enable such run-time or post-training network morphing, the concept of \u2018dynamic inference\u2019 or \u2018training-once-for-all\u2019 has been proposed to train a single network consisting of multiple sub-nets once, but each sub-net could perform the same inference function with different computing complexity. However, the traditional dynamic inference training method requires a joint training scheme with multi-objective optimization, which suffers from very large training overhead. In this work, for the first time, we propose a novel alternating sparse training (AST) scheme to train multiple sparse sub-nets for dynamic inference without extra training cost compared to the case of training a single sparse model from scratch. Furthermore, to mitigate the interference of weight update among sub-nets without losing the generalization of optimization, we pro-pose gradient correction within the inner-group iterations to reduce their weight update interference. We validate the proposed AST on multiple datasets against state-of-the-art sparse training methods, which shows that AST achieves similar or better accuracy, but only needs to train once to get multiple sparse sub-nets with different sparsity ratios. More importantly, comparing with the traditional joint training based dynamic inference training methodology, the large training overhead is completely eliminated without affecting the accuracy of each sub-net. Code is available at https://github.com/mengjian0502/AST .\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 1  (*Influential: 0*)\n\n#### 8. Adaptive Random Walk Gradient Descent for Decentralized Optimization\n\n*From Search Query: sparse gradient optimization*\n\n*Tao Sun, Dongsheng Li, Bao Wang*\n\n**TL;DR:** It is proved that adaptive random walk algorithms perform as well as the non-adaptive method for dependent data in general cases but achieve acceleration when the stochastic gradients are \u201csparse\u201d.\n\n**Abstract:** In this paper, we study the adaptive step size random walk gradient descent with momentum for decentralized optimization, in which the training samples are drawn dependently with each other. We establish theoretical convergence rates of the adaptive step size random walk gradient descent with momentum for both convex and nonconvex settings. In particular, we prove that adaptive random walk algorithms perform as well as the non-adaptive method for dependent data in general cases but achieve acceleration when the stochastic gradients are \u201csparse\u201d. Moreover, we study the zeroth-order version of adaptive random walk gradient descent and provide corresponding convergence results. All assumptions used in this paper are mild and general, making our results applicable to many machine learning problems.\n\n**Venue:** International Conference on Machine Learning\n\n**Year:** 2022\n\n**Citations:** 14  (*Influential: 2*)\n\n#### 9. Local Linear Convergence of Gradient Methods for Subspace Optimization via Strict Complementarity\n\n*From Search Query: sparse gradient optimization*\n\n*Ron Fisher, D. Garber*\n\n**TL;DR:** A main result is a proof that a natural nonconvex gradient method which is SVD-free and requires only a single QR-factorization of an $n\\times k$ matrix per iteration, converges locally with a linear rate.\n\n**Abstract:** We consider optimization problems in which the goal is find a $k$-dimensional subspace of $\\mathbb{R}^n$, $k<<n$, which minimizes a convex and smooth loss. Such problems generalize the fundamental task of principal component analysis (PCA) to include robust and sparse counterparts, and logistic PCA for binary data, among others. This problem could be approached either via nonconvex gradient methods with highly-efficient iterations, but for which arguing about fast convergence to a global minimizer is difficult or, via a convex relaxation for which arguing about convergence to a global minimizer is straightforward, but the corresponding methods are often inefficient in high dimensions. In this work we bridge these two approaches under a strict complementarity assumption, which in particular implies that the optimal solution to the convex relaxation is unique and is also the optimal solution to the original nonconvex problem. Our main result is a proof that a natural nonconvex gradient method which is \\textit{SVD-free} and requires only a single QR-factorization of an $n\\times k$ matrix per iteration, converges locally with a linear rate. We also establish linear convergence results for the nonconvex projected gradient method, and the Frank-Wolfe method when applied to the convex relaxation.\n\n**Venue:** Neural Information Processing Systems\n\n**Year:** 2022\n\n**Citations:** 0  (*Influential: 0*)\n\n### 6 related papers from Papers with Code\n\n#### 1. Fully Convolutional Mesh Autoencoder using Efficient Spatially Varying Kernels\n\n*From Search Query: memory efficient state representation*\n\n*Yuting Ye, Zimo Li, Jason Saragih, Chen Cao, Chenglei Wu, Yi Zhou, Yaser Sheikh, Hao Li*\n\n**Abstract:** Learning latent representations of registered meshes is useful for many 3D tasks. Techniques have recently shifted to neural mesh autoencoders. Although they demonstrate higher precision than traditional methods, they remain unable to capture fine-grained deformations. Furthermore, these methods can only be applied to a template-specific surface mesh, and is not applicable to more general meshes, like tetrahedrons and non-manifold meshes. While more general graph convolution methods can be employed, they lack performance in reconstruction precision and require higher memory usage. In this paper, we propose a non-template-specific fully convolutional mesh autoencoder for arbitrary registered mesh data. It is enabled by our novel convolution and (un)pooling operators learned with globally shared weights and locally varying coefficients which can efficiently capture the spatially varying contents presented by irregular mesh connections. Our model outperforms state-of-the-art methods on reconstruction accuracy. In addition, the latent codes of our network are fully localized thanks to the fully convolutional structure, and thus have much higher interpolation capability than many traditional 3D mesh generation models.\n\n**Proceeding:** neurips-2020-12\n\n**Published:** 2020-06-08\n\n\n\n#### 2. Dual Path Networks\n\n*From Search Query: memory efficient state representation*\n\n*Jianan Li, Xiaojie Jin, Huaxin Xiao, Yunpeng Chen, Jiashi Feng, Shuicheng Yan*\n\n**Abstract:** In this work, we present a simple, highly efficient and modularized Dual Path\nNetwork (DPN) for image classification which presents a new topology of\nconnection paths internally. By revealing the equivalence of the\nstate-of-the-art Residual Network (ResNet) and Densely Convolutional Network\n(DenseNet) within the HORNN framework, we find that ResNet enables feature\nre-usage while DenseNet enables new features exploration which are both\nimportant for learning good representations. To enjoy the benefits from both\npath topologies, our proposed Dual Path Network shares common features while\nmaintaining the flexibility to explore new features through dual path\narchitectures. Extensive experiments on three benchmark datasets, ImagNet-1k,\nPlaces365 and PASCAL VOC, clearly demonstrate superior performance of the\nproposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset,\na shallow DPN surpasses the best ResNeXt-101(64x4d) with 26% smaller model\nsize, 25% less computational cost and 8% lower memory consumption, and a deeper\nDPN (DPN-131) further pushes the state-of-the-art single model performance with\nabout 2 times faster training speed. Experiments on the Places365 large-scale\nscene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation\ndataset also demonstrate its consistently better performance than DenseNet,\nResNet and the latest ResNeXt model over various applications.\n\n**Conference:** dual-path-networks-1\n\n**Published:** 2017-07-06\n\n\n\n#### 3. Well-Read Students Learn Better: On the Importance of Pre-training Compact Models\n\n*From Search Query: sequential model compression*\n\n*Ming-Wei Chang, Kenton Lee, Iulia Turc, Kristina Toutanova*\n\n**Abstract:** Recent developments in natural language representations have been accompanied by large and expensive models that leverage vast amounts of general-domain text through self-supervised pre-training. Due to the cost of applying such models to down-stream tasks, several model compression techniques on pre-trained language representations have been proposed (Sun et al., 2019; Sanh, 2019). However, surprisingly, the simple baseline of just pre-training and fine-tuning compact models has been overlooked. In this paper, we first show that pre-training remains important in the context of smaller architectures, and fine-tuning pre-trained compact models can be competitive to more elaborate methods proposed in concurrent work. Starting with pre-trained compact models, we then explore transferring task knowledge from large fine-tuned models through standard knowledge distillation. The resulting simple, yet effective and general algorithm, Pre-trained Distillation, brings further improvements. Through extensive experiments, we more generally explore the interaction between pre-training and distillation under two variables that have been under-studied: model size and properties of unlabeled task data. One surprising observation is that they have a compound effect even when sequentially applied on the same data. To accelerate future research, we will make our 24 pre-trained miniature BERT models publicly available.\n\n**Conference:** well-read-students-learn-better-on-the\n\n**Published:** 2019-08-23\n\n\n\n#### 4. VideoComposer: Compositional Video Synthesis with Motion Controllability\n\n*From Search Query: sequential model compression*\n\n*Jingren Zhou, Deli Zhao, Yujun Shen, Yingya Zhang, Jiuniu Wang, Dayou Chen, Shiwei Zhang, Hangjie Yuan, Xiang Wang*\n\n**Abstract:** The pursuit of controllability as a higher standard of visual content creation has yielded remarkable progress in customizable image synthesis. However, achieving controllable video synthesis remains challenging due to the large variation of temporal dynamics and the requirement of cross-frame temporal consistency. Based on the paradigm of compositional generation, this work presents VideoComposer that allows users to flexibly compose a video with textual conditions, spatial conditions, and more importantly temporal conditions. Specifically, considering the characteristic of video data, we introduce the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics. In addition, we develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency. Extensive experimental results suggest that VideoComposer is able to control the spatial and temporal patterns simultaneously within a synthesized video in various forms, such as text description, sketch sequence, reference video, or even simply hand-crafted motions. The code and models will be publicly available at https://videocomposer.github.io.\n\n**Proceeding:** neurips-2023-11\n\n**Published:** 2023-06-03\n\n\n\n#### 5. Zeroth-Order Regularized Optimization (ZORO): Approximately Sparse Gradients and Adaptive Sampling\n\n*From Search Query: sparse gradient optimization*\n\n*Daniel Mckenzie, Zhenliang Zhang, Wotao Yin, HanQin Cai*\n\n**Abstract:** We consider the problem of minimizing a high-dimensional objective function, which may include a regularization term, using (possibly noisy) evaluations of the function. Such optimization is also called derivative-free, zeroth-order, or black-box optimization. We propose a new $\\textbf{Z}$eroth-$\\textbf{O}$rder $\\textbf{R}$egularized $\\textbf{O}$ptimization method, dubbed ZORO. When the underlying gradient is approximately sparse at an iterate, ZORO needs very few objective function evaluations to obtain a new iterate that decreases the objective function. We achieve this with an adaptive, randomized gradient estimator, followed by an inexact proximal-gradient scheme. Under a novel approximately sparse gradient assumption and various different convex settings, we show the (theoretical and empirical) convergence rate of ZORO is only logarithmically dependent on the problem dimension. Numerical experiments show that ZORO outperforms the existing methods with similar assumptions, on both synthetic and real datasets.\n\n**Published:** 2020-03-29\n\n\n\n#### 6. Sparse Optimization on Measures with Over-parameterized Gradient Descent\n\n*From Search Query: sparse gradient optimization*\n\n*Lenaic Chizat*\n\n**Abstract:** Minimizing a convex function of a measure with a sparsity-inducing penalty is a typical problem arising, e.g., in sparse spikes deconvolution or two-layer neural networks training. We show that this problem can be solved by discretizing the measure and running non-convex gradient descent on the positions and weights of the particles. For measures on a $d$-dimensional manifold and under some non-degeneracy assumptions, this leads to a global optimization algorithm with a complexity scaling as $\\log(1/\\epsilon)$ in the desired accuracy $\\epsilon$, instead of $\\epsilon^{-d}$ for convex methods. The key theoretical tools are a local convergence analysis in Wasserstein space and an analysis of a perturbed mirror descent in the space of measures. Our bounds involve quantities that are exponential in $d$ which is unavoidable under our assumptions.\n\n**Published:** 2019-07-24\n\n\n\n\n\n---\n## Web search results\n\n To improve the design of autoregressive language models with a focus on memory-efficient state representations, sequential model compression, and sparse gradient optimization, here are some key insights and relevant sources:\n\n## Memory-Efficient State Representations\n\n### Compact State Encoding Techniques\n- **LSTM Networks**: Long Short-Term Memory (LSTM) networks are designed to mitigate the vanishing gradient problem and can efficiently manage long-term dependencies. LSTMs use a cell and three gates (input, output, and forget gates) to regulate the flow of information, which helps in maintaining useful long-term dependencies.\n- **Neural ODEs**: Using neural Ordinary Differential Equations (ODEs) with reversible integrators can help in reducing memory requirements during backpropagation. This approach avoids storing intermediate states, making it more memory-efficient for deep networks.\n\n### Memory Management in Sequential Models\n- **Reversible Neural Networks**: The use of reversible neural networks based on higher-order numerical methods can ensure accurate gradient computations without high memory requirements. These networks reconstruct intermediate states precisely during backward integration, which is crucial for memory efficiency.\n\n### Efficient State Update Mechanisms\n- **Rate-Based Backpropagation**: For Spiking Neural Networks (SNNs), rate-based backpropagation can streamline the computational graph, reducing memory and computational demands. This method focuses on averaged dynamics rather than detailed temporal derivatives, which can be beneficial for efficient state updates.\n\n## Compression Methods for Sequential Models\n\n### State Compression in Language Models\n- **Neural History Compressor**: This approach involves an unsupervised stack of RNNs where each higher-level RNN studies a compressed representation of the information from the RNN below. This method minimizes the description length of the data, effectively compressing the state representation.\n\n### Dynamic and Adaptive Compression\n- **Adaptive Time Stepping**: Using higher-order reversible methods with adaptive time stepping can efficiently compress and process time-series data at irregular intervals. This approach ensures that the model can handle variable time steps without compromising accuracy.\n\n### Information-Preserving Compression\n- **Hierarchical Approaches**: Hierarchical models, such as those using multiple timescales (MTRNN), can segment continuous sequences into reusable primitives, ensuring that important information is preserved while compressing the state representation.\n\n## Gradient Optimization Techniques\n\n### Sparse Gradient Propagation\n- **Rate-Based Backpropagation**: This method for SNNs minimizes reliance on detailed temporal derivatives, focusing on averaged dynamics. This approach reduces the complexity of backpropagation through time (BPTT), making it more efficient for sparse networks.\n\n### Efficient Backpropagation Methods\n- **Reversible Integrators**: Using reversible integrators in neural ODEs ensures that gradient computations are accurate and memory-efficient. This method avoids the need to store intermediate states during forward propagation, which is beneficial for efficient backpropagation.\n\n### Stability in Sparse Networks\n- **Stability-Plasticity Adaptation**: Techniques like NISPA (Neuro-Inspired Stability-Plasticity Adaptation) are designed to maintain stability in sparse networks. These methods ensure that learned knowledge is preserved while adapting to new tasks, which is crucial for the stability of sparse networks.\n\nBy incorporating these techniques, researchers can develop autoregressive language models that are more memory-efficient, capable of compressing sequential data effectively, and optimized for sparse gradient propagation, ultimately leading to better scalability and performance.\n",
            "analysis": "Analysis Note:\n\n1. Summary of Analysis:\nThe search results reveal important insights into three key aspects of the EventDDLerpLinear proposal:\n\na) Event-driven mechanisms:\n- Biologically inspired dynamic thresholds show promise for efficient computation\n- Spiking Neural Networks provide frameworks for event-based processing\n- State transition mechanisms can improve sparse network learning\n\nb) Hybrid architectures:\n- Continuous-discrete integration methods exist in various forms\n- Gradient propagation through hybrid systems requires careful consideration\n- Conversion between representations needs efficient mechanisms\n\nc) Training stability:\n- Sparse networks can maintain stability with proper mechanisms\n- Adaptive computation requires careful convergence guarantees\n- Memory-computation trade-offs are crucial for efficiency\n\n2. Useful References with Excerpts:\n\nFrom \"Biologically Inspired Dynamic Thresholds\":\n\"The dynamic membrane potential threshold...is a spontaneous regulation mechanism that maintains neuronal homeostasis\" - Provides biological motivation for event detection\n\nFrom \"State Transition of Dendritic Spines\":\n\"We model different states of SNN weights, facilitating weight optimization for pruning\" - Relevant for state compression\n\nFrom \"NISPA\":\n\"NISPA forms stable paths to preserve learned knowledge from older tasks\" - Important for stability\n\nFrom \"Sparse Attention with Linear Units\":\n\"ReLA achieves translation performance comparable to several strong baselines, with training and decoding speed similar to that of the vanilla attention\" - Validates efficiency claims\n\n3. Key Insights:\n\na) Event Detection:\n- Dynamic thresholds should adapt to input characteristics\n- Biological inspiration can improve efficiency\n- State transitions need careful management\n\nb) Compression:\n- Hierarchical approaches show promise\n- Need balance between compression and information preservation\n- State representation crucial for performance\n\nc) Training Considerations:\n- Stability mechanisms essential for sparse networks\n- Gradient flow needs careful handling\n- Memory efficiency crucial for scalability\n\n4. Future Search Plan:\n\nNeed additional investigation in:\n1. Memory management techniques in sparse networks\n2. Efficient state compression methods\n3. Gradient flow optimization in hybrid systems\n\nNext search queries should focus on:\n- Memory-efficient state representations\n- Compression techniques for sequential models\n- Gradient optimization in sparse networks\n\n5. References:\n\n[1] Ding, J., Dong, B., Heide, F., Ding, Y., Zhou, Y., Yin, B., & Yang, X. (2022). Biologically Inspired Dynamic Thresholds for Spiking Neural Networks. Neural Information Processing Systems.\n\n[2] Chen, Y., Yu, Z., Fang, W., Ma, Z., Huang, T., & Tian, Y. (2022). State Transition of Dendritic Spines Improves Learning of Sparse Spiking Neural Networks. International Conference on Machine Learning.\n\n[3] Gurbuz, M.B., & Dovrolis, C. (2022). NISPA: Neuro-Inspired Stability-Plasticity Adaptation for Continual Learning in Sparse Networks. International Conference on Machine Learning.\n\n[4] Zhang, B., Titov, I., & Sennrich, R. (2021). Sparse Attention with Linear Units. Conference on Empirical Methods in Natural Language Processing.\n\n[5] Hu, Y., Zhao, K., Huang, W., Chen, J., & Zhu, J. (2024). Accelerating Transformer Pre-Training with 2:4 Sparsity. International Conference on Machine Learning."
        }
    ],
    "rating": 4.2,
    "instructions": null,
    "changes": null,
    "search_references": null,
    "passed": true,
    "costs": {
        "DESIGN_PROPOSER": 0.693099,
        "IMPLEMENTATION_PLANNER": 0,
        "IMPLEMENTATION_CODER": 0,
        "PROPOSAL_REVIEWER": 0.6966300000000001,
        "IMPLEMENTATION_OBSERVER": 0,
        "SEARCH_ASSISTANT": 0
    },
    "search_report": null,
    "reflection": null,
    "selection": "DDLerpLinear",
    "proposal": "# EventDDLerpLinear: Event-Driven Dynamic State Updates for Efficient Language Modeling\n\n## 1. Motivation\n\nCurrent language models face significant challenges in computational efficiency and memory usage, particularly for long sequences. While RWKV6's DDLerpLinear provides an effective foundation for dynamic depth, it still performs uniform computation regardless of input importance. This leads to:\n- Unnecessary computations for less relevant inputs\n- Inefficient memory usage due to uniform state storage\n- Limited ability to adapt computation to content importance\n\n## 2. Problem Analysis\n\nThe DDLerpLinear component in RWKV6 has several limitations:\n1. **Uniform Processing**: Computes full state updates regardless of input significance\n2. **Memory Inefficiency**: Stores full states without compression\n3. **Fixed Computation**: Uses same computational depth for all inputs\n4. **Limited Adaptivity**: Cannot selectively focus computation on important inputs\n\n## 3. Core Idea and Philosophy\n\nEventDDLerpLinear introduces three key innovations:\n1. **Event-Driven Updates**: Content-dependent state updates inspired by spiking neural networks\n2. **Hierarchical State Compression**: Multi-level state representation with adaptive compression\n3. **Selective Computation**: Dynamic routing of computation based on input importance\n\n## 4. Design Plan\n\n### 4.1 Architecture Overview\n\nThe EventDDLerpLinear consists of three main components:\n\n1. **Event Detection Module**:\n```python\nclass EventDetector(nn.Module):\n    def __init__(self, dim, threshold=0.1):\n        super().__init__()\n        self.gate = nn.Linear(dim, 1)\n        self.threshold = threshold\n        \n    def forward(self, x, delta):\n        # Compute importance score\n        importance = torch.sigmoid(self.gate(x))\n        # Generate event mask\n        event_mask = (importance > self.threshold) | (torch.abs(delta) > self.threshold)\n        return event_mask, importance\n```\n\n2. **Hierarchical State Compressor**:\n```python\nclass StateCompressor(nn.Module):\n    def __init__(self, dim, levels=3):\n        super().__init__()\n        self.projections = nn.ModuleList([\n            nn.Linear(dim, dim // (2**i))\n            for i in range(levels)\n        ])\n        \n    def forward(self, x, importance):\n        # Select compression level based on importance\n        level = torch.argmin(importance > torch.tensor([0.7, 0.3, 0.1]))\n        return self.projections[level](x)\n```\n\n3. **Selective Computation Router**:\n```python\nclass ComputationRouter(nn.Module):\n    def __init__(self, dim, low_rank_dim):\n        super().__init__()\n        self.router = nn.Linear(dim, 2)\n        self.lora_light = LoRA(dim, low_rank_dim//2)\n        self.lora_heavy = LoRA(dim, low_rank_dim)\n        \n    def forward(self, x, importance):\n        route = torch.argmax(self.router(x), dim=-1)\n        return torch.where(\n            route.unsqueeze(-1),\n            self.lora_heavy(x),\n            self.lora_light(x)\n        )\n```\n\n### 4.2 Mathematical Formulation\n\n1. Event Detection:\n   \\[\n   e(x, \u03b4) = \u03c3(W_g x + b_g) > \u03c4 \u2228 \\|\u03b4\\| > \u03c4\n   \\]\n   where \u03c4 is the event threshold\n\n2. State Compression:\n   \\[\n   c_l(x) = W_l x\n   \\]\n   where l is the compression level\n\n3. Selective Computation:\n   \\[\n   y = r(x) ? h(x) : l(x)\n   \\]\n   where h(x) and l(x) are heavy and light computations\n\n### 4.3 Implementation Guidelines\n\n1. Initialization:\n```python\nclass EventDDLerpLinear(nn.Module):\n    def __init__(self, dim, low_rank_dim):\n        super().__init__()\n        self.event_detector = EventDetector(dim)\n        self.state_compressor = StateCompressor(dim)\n        self.computation_router = ComputationRouter(dim, low_rank_dim)\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n```\n\n2. Forward Pass:\n```python\ndef forward(self, x, mu=None, delta=None):\n    if delta is None:\n        shifted = self.time_shift(x)\n        delta = shifted - x\n        \n    # Detect events\n    event_mask, importance = self.event_detector(x, delta)\n    \n    # Compress states\n    compressed_state = self.state_compressor(x, importance)\n    \n    # Route computation\n    if event_mask:\n        output = self.computation_router(compressed_state, importance)\n    else:\n        output = x\n        \n    return output\n```\n\n## 5. Theoretical Analysis\n\n1. Computational Complexity:\n- Best case: O(n) when few events detected\n- Average case: O(n log n) with hierarchical compression\n- Worst case: O(n * d) where d is maximum depth\n\n2. Memory Usage:\n- Reduced by factor of (1 - s) where s is compression ratio\n- Additional overhead from event detection: O(n)\n- Hierarchical state storage: O(n * log d)\n\n3. Expected Benefits:\n- Reduced computation through selective processing\n- Improved memory efficiency via compression\n- Better handling of varying input importance\n\n## 6. Research Summary\n\nKey findings from research:\n1. Event-based computation reduces unnecessary operations\n2. Hierarchical compression improves memory efficiency\n3. Selective routing enhances computational focus\n4. Hardware-efficient sparse operations crucial for performance\n\n## 7. Evolution of Design\n\nThe design evolved through several stages:\n1. Initial focus on event-driven updates\n2. Addition of hierarchical compression\n3. Integration of selective computation\n4. Final refinement with hardware considerations\n\n## 8. Implementation Details\n\nKey components:\n1. Event detection network\n2. State compression module\n3. Computation routing system\n4. Hierarchical state management\n\n## 9. Advantages and Trade-offs\n\nAdvantages:\n- Reduced computational cost\n- Improved memory efficiency\n- Better handling of varying importance\n- Hardware-friendly implementation\n\nTrade-offs:\n- Additional routing overhead\n- Compression/decompression cost\n- Increased implementation complexity\n\n## 10. References\n\n[1] Engelken, R. (2023). SparseProp: Efficient Event-Based Simulation and Training of Sparse Recurrent Spiking Neural Networks. Neural Information Processing Systems.\n\n[2] Dao, T., et al. (2022). FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness. Neural Information Processing Systems.\n\n[3] Fu, D.Y., et al. (2023). FlashFFTConv: Efficient Convolutions for Long Sequences with Tensor Cores. arXiv preprint.\n\n[4] Peng, B., et al. (2024). Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence. arXiv preprint.\n\n[5] Qi, B., et al. (2024). SMR: State Memory Replay for Long Sequence Modeling. arXiv preprint.",
    "design_cfg": {
        "max_attemps": {
            "post_refinement": 0,
            "max_search_rounds": 3,
            "implementation_debug": 7,
            "design_proposal": 10
        },
        "threshold": {
            "proposal_rating": 4.0,
            "implementation_rating": 3.0
        },
        "use_unlimited_prompt": true,
        "mutation_no_tree": true,
        "agent_types": {
            "DESIGN_PROPOSER": "hybrid",
            "IMPLEMENTATION_PLANNER": "hybrid",
            "IMPLEMENTATION_CODER": "hybrid",
            "PROPOSAL_REVIEWER": "hybrid",
            "IMPLEMENTATION_OBSERVER": "hybrid",
            "SEARCH_ASSISTANT": "None"
        },
        "running_mode": "Proposal + Implementation",
        "unittest_pass_required": false,
        "crossover_no_ref": true,
        "scratch_no_tree": true,
        "_agent_types": {
            "DESIGN_PROPOSER": "claude3.5_sonnet",
            "IMPLEMENTATION_PLANNER": "o1_mini",
            "IMPLEMENTATION_CODER": "o1_mini",
            "PROPOSAL_REVIEWER": "claude3.5_sonnet",
            "IMPLEMENTATION_OBSERVER": "o1_mini",
            "SEARCH_ASSISTANT": "None"
        },
        "termination": {
            "max_debug_budget": 0,
            "max_failed_rounds": 3,
            "max_total_budget": 0
        },
        "agent_weights": {
            "DESIGN_PROPOSER": [
                0.05,
                0.0,
                0.6000000000000001,
                0.2,
                0.15
            ],
            "IMPLEMENTATION_PLANNER": [
                0.05000000000000002,
                0.0,
                0.44999999999999996,
                0.3,
                0.20000000000000007
            ],
            "IMPLEMENTATION_CODER": [
                0.0,
                0.0,
                0.3,
                0.4999999999999996,
                0.2
            ],
            "PROPOSAL_REVIEWER": [
                0.10000000000000002,
                0.0,
                0.5499999999999999,
                0.2,
                0.15000000000000002
            ],
            "IMPLEMENTATION_OBSERVER": [
                0.05,
                0.0,
                0.15000000000000002,
                0.15000000000000002,
                0.6499999999999999,
                0.0
            ]
        },
        "num_samples": {
            "implementation": 1,
            "rerank_method": "rating",
            "proposal": 1
        },
        "search_settings": {
            "proposal_search": true,
            "proposal_review_search": true,
            "search_for_papers_num": 10
        },
        "max_attempts": {
            "post_refinement": 0,
            "max_search_rounds": 4,
            "implementation_debug": 5,
            "design_proposal": 5
        }
    },
    "abstract": "We propose EventDDLerpLinear, an event-driven enhancement to DDLerpLinear that incorporates selective state updates and hierarchical compression. The design leverages principles from spiking neural networks to perform content-dependent state updates, reducing unnecessary computations while maintaining model expressivity. A novel hierarchical state compression mechanism further improves memory efficiency while preserving important temporal dependencies.",
    "ideation": null,
    "modelname": "rwkv6_event",
    "suggestions": "1. Implementation Details:\n- Provide more detailed analysis of memory-computation trade-offs\n- Include concrete examples of event detection thresholds\n- Add guidelines for hierarchical compression ratios\n- Specify initialization strategies for different components\n\n2. Theoretical Analysis:\n- Expand on stability analysis for training\n- Include formal proof of complexity bounds\n- Provide more detailed analysis of memory scaling\n- Add theoretical guarantees for convergence\n\n3. Integration Guidelines:\n- Add specific guidance for distributed training\n- Include strategies for gradient handling\n- Provide more detailed cache optimization guidelines\n- Specify requirements for different model scales\n\n4. Performance Analysis:\n- Include expected performance metrics\n- Add ablation study recommendations\n- Provide benchmarking guidelines\n- Specify memory-performance trade-offs\n\n5. Documentation:\n- Add more detailed API specifications\n- Include example configurations\n- Provide debugging guidelines\n- Add performance optimization tips",
    "user_input": ""
}