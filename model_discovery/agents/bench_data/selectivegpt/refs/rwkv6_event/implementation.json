{
    "implementation": {
        "review": null,
        "root": "RWKV6",
        "proposal": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: this https URL Training code at: this https URL Inference code at: this https URL Time-parallel training code at: this https URL",
        "units": {
            "DDLerpLinear": {
                "review": "```rating 3.0\n```\n\n### Strengths of the Implementation\n\n1. **Modular and Hierarchical Design**: The implementation maintains a clear modular structure by utilizing nested GAUs (`LoRA`, `LerpLinear`, etc.), which aligns well with the proposed framework. This modularity facilitates easier debugging, testing, and future extensions.\n\n2. **Comprehensive Documentation**: The `DDLerpLinear` class includes detailed docstrings that explain its functionality, mathematical formulation, arguments, inputs, outputs, and usage examples. This level of documentation enhances code readability and maintainability, making it easier for team members to understand and work with the codebase.\n\n3. **Conditional LoRA Integration**: The implementation correctly integrates the `LoRA` module conditionally based on the `low_rank_dim` parameter. This flexibility allows for varying model complexities and can lead to better parameter efficiency when needed.\n\n4. **Proper Parameter Initialization**: The use of `nn.Parameter` for `mu` ensures that it is registered as a learnable parameter within the model, enabling it to receive gradients during training. This is crucial for the dynamic depth adjustment intended by the `DDLerpLinear` design.\n\n5. **Adherence to Base Class Contracts**: The `DDLerpLinear` class correctly inherits from `GAUBase` and implements the required `_forward` method, maintaining consistency with the GAU framework. This adherence ensures that the GAU integrates seamlessly within the larger model architecture.\n\n### Areas for Improvement and Specific Suggestions\n\n1. **Addressing CHILDREN_DECLARATIONS Warning**:\n\n   - **Issue**: The format checker issues a warning stating that no `CHILDREN_DECLARATIONS` are found in the `DDLerpLinear` GAU. This occurs despite `LoRA` being conditionally integrated into `DDLerpLinear`.\n\n   - **Analysis**: The absence of `CHILDREN_DECLARATIONS` means that the parser assumes `DDLerpLinear` has no child GAUs. However, when `low_rank_dim` is specified, `DDLerpLinear` utilizes `LoRA` as a child unit. Not declaring `LoRA` as a child leads to inconsistency between the declared architecture and the actual usage.\n\n   - **Solution**:\n     - **Declare CHILDREN_DECLARATIONS Appropriately**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is used. This can be achieved by conditionally appending to the `CHILDREN_DECLARATIONS` based on the presence of `low_rank_dim`.\n     \n     - **Implementation Example**:\n       \n       ```python\n       CHILDREN_DECLARATIONS = []\n       if self.low_rank_dim is not None:\n           CHILDREN_DECLARATIONS = [\n               UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n           ]\n       ```\n     \n     - **Alternatively**, declare all possible child GAUs and manage their usage within the class. This ensures that the hierarchy is explicitly defined, preventing parser warnings.\n     \n       ```python\n       CHILDREN_DECLARATIONS = [\n           UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n       ]\n       ```\n     \n     - **Adjust Reformatter Settings**: Modify the reformatter to preserve `CHILDREN_DECLARATIONS`. This might involve adding specific markers or comments that instruct the reformatter not to alter these critical lines.\n     \n     - **Implement Post-Formatting Scripts**: Develop scripts that automatically reinsert or correct `CHILDREN_DECLARATIONS` after the code is processed by the reformatter.\n\n2. **Optimizing Training Efficiency**:\n\n   - **Issue**: While the functionality checker now passes, previous iterations indicated efficiency concerns. Ensuring that the model remains efficient is crucial for scalability.\n   \n   - **Suggestions**:\n     - **Vectorize Operations**: Replace explicit Python loops in methods like `naive_chunk_rwkv6` with vectorized PyTorch operations to leverage optimized backend implementations and GPU acceleration.\n     \n     - **Profile the Model**: Utilize profiling tools such as PyTorch Profiler to identify bottlenecks in the computation graph and optimize accordingly.\n     \n     - **Simplify LoRA Integration**: Assess whether the current integration of `LoRA` introduces significant computational overhead. If so, explore more efficient architectures or parameter-sharing mechanisms.\n     \n     - **Batch Operations**: Ensure that tensor operations are batched appropriately to maximize parallelism and hardware utilization.\n\n3. **Code Organization and Maintainability**:\n\n   - **Issue**: The `gab.py` file contains multiple GAU class definitions, which can lead to confusion and maintenance challenges.\n   \n   - **Suggestions**:\n     - **Separate GAUs into Individual Files**: Organize each GAU into its own module or file. This separation enhances code clarity and makes it easier to navigate and manage individual components.\n     \n     - **Remove Duplicated Class Definitions**: Ensure that each GAU class is defined uniquely without duplications to prevent namespace conflicts and unexpected behaviors.\n     \n     - **Implement Consistent Naming Conventions**: Adopt consistent and descriptive naming conventions for classes and files to facilitate easier identification and usage.\n\n4. **Ensuring Comprehensive Testing**:\n\n   - **Issue**: While unit tests passed, ensuring comprehensive coverage is vital to prevent future regressions.\n   \n   - **Suggestions**:\n     - **Implement Gradient Flow Tests**: Beyond basic forward passes, include tests that verify whether gradients flow correctly through all learnable parameters, especially `mu`.\n     \n     - **Efficiency Benchmarking**: Regularly benchmark the model\u2019s training and inference times against established baselines to monitor and address any performance regressions promptly.\n     \n     - **Causality Tests**: Ensure that the model maintains causality, especially after modifications that introduce dynamic depth adjustments.\n\n5. **Documentation Enhancements**:\n\n   - **Issue**: Although the docstrings are comprehensive, ensuring that all classes and their relationships are well-documented is essential.\n   \n   - **Suggestions**:\n     - **Document Conditional Logic**: Clearly document the conditions under which certain child GAUs (like `LoRA`) are used. This transparency aids in understanding the model\u2019s architecture and behavior.\n     \n     - **Provide Usage Examples**: While examples are provided in the docstrings, including more diverse examples covering different configurations can enhance understanding.\n\n### Comments on Innovation and Potential Impact\n\nThe implementation of `DDLerpLinear` introduces a dynamic depth mechanism through the learnable parameter `mu`, aligning well with the proposal's objective of adaptable computation based on input significance. The conditional integration of `LoRA` further enhances parameter efficiency, allowing the model to scale more effectively without a proportional increase in computational resources.\n\n**Potential Impact**:\n\n- **Enhanced Computational Efficiency**: By dynamically adjusting computational depth, the model can allocate resources more effectively, potentially reducing unnecessary computations for less important inputs.\n\n- **Improved Scalability**: The combination of dynamic depth and low-rank adaptations facilitates better scalability, enabling the model to handle larger datasets and more complex tasks without exorbitant increases in computational or memory overhead.\n\n- **Flexibility and Adaptability**: The ability to conditionally integrate components like `LoRA` based on configuration parameters allows for greater flexibility in model design, catering to diverse application requirements.\n\n**Concerns**:\n\n- **Integration Complexity**: The conditional usage of `LoRA` introduces additional layers of complexity in managing dependencies and ensuring consistent behavior across different configurations.\n\n- **Maintenance Overhead**: Ensuring that `CHILDREN_DECLARATIONS` accurately reflect the model\u2019s architecture requires disciplined coding practices and possibly additional tooling to automate verification.\n\n### Detailed Analysis of Failed Checks\n\n#### Format Checker Issues\n\n- **Warning**: *\"Code block 1 of DDLerpLinear: Warning: No CHILDREN_DECLARATIONS found in the GAU. Will assume there is no children.\"*\n\n  **Analysis**:\n\n  - The `DDLerpLinear` GAU conditionally integrates the `LoRA` module based on the `low_rank_dim` parameter. However, the current implementation does not declare `LoRA` in `CHILDREN_DECLARATIONS`, leading the format checker to assume that there are no child GAUs.\n\n  - This discrepancy between actual usage and declarations can lead to issues in the model's structural integrity, as the parser relies on `CHILDREN_DECLARATIONS` to understand GAU hierarchies and dependencies.\n\n  **Recommendations**:\n\n  - **Declare Child GAUs Appropriately**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is utilized. This ensures that the parser accurately recognizes and manages all child GAUs.\n\n    ```python\n    class DDLerpLinear(GAUBase):\n        ...\n        CHILDREN_DECLARATIONS = []\n        if low_rank_dim is not None:\n            CHILDREN_DECLARATIONS = [\n                UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n            ]\n        ...\n    ```\n    \n  - **Avoid Self-Declaration**: Ensure that `DDLerpLinear` does not list itself as a child, which can cause recursive parsing issues.\n\n  - **Consistent Declaration Practices**: Adopt a consistent approach across all GAUs in declaring children. Whether or not a child GAU is used, its declaration should reflect the actual dependencies to prevent format checker warnings.\n\n  - **Adjust Reformatter Settings**: Modify the code reformatter to preserve critical declarations like `CHILDREN_DECLARATIONS`. This might involve marking these lines with specific comments or patterns that instruct the reformatter to retain them.\n\n  - **Post-Formatting Validation**: Implement scripts or pre-commit hooks that verify the presence and correctness of `CHILDREN_DECLARATIONS` after formatting processes.\n\n#### Functionality Checker Overview\n\n- **Status**: Functionality check passed.\n\n  **Note**: Unlike previous iterations where differentiability tests failed due to `mu` parameters lacking gradients, the current implementation passes the functionality checker. This indicates that issues related to parameter gradient flow have been addressed, leading to successful forward and backward passes without causality violations.\n\n### Recommendations for the Coder\n\n1. **Immediately Address CHILDREN_DECLARATIONS Warning**:\n\n   - **Action**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is used. This alignment is crucial to satisfy the format checker and ensure that the model's hierarchical structure is accurately represented.\n   \n   - **Implementation Example**:\n     \n     ```python\n     CHILDREN_DECLARATIONS = []\n     if self.low_rank_dim is not None:\n         CHILDREN_DECLARATIONS = [\n             UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n         ]\n     ```\n   \n   - **Alternatively**, if conditional declarations are not feasible within the class, always declare `LoRA` as a potential child GAU, and manage its usage within the class logic.\n   \n     ```python\n     CHILDREN_DECLARATIONS = [\n         UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n     ]\n     ```\n\n2. **Enhance Training Efficiency**:\n\n   - **Action**: Optimize the `naive_chunk_rwkv6` method by replacing explicit Python loops with vectorized PyTorch operations. This change can significantly reduce training time and improve scalability.\n   \n   - **Implementation Suggestion**:\n     \n     - **Vectorize Attention Computations**: Utilize batch matrix operations and PyTorch's optimized tensor functions to handle chunk processing without Python-level loops.\n     \n     - **Example Refactoring**:\n       \n       ```python\n       def optimized_chunk_rwkv6(self, q, k, v, w, u, chunk_size=32):\n           # Example of vectorizing chunk processing\n           # Placeholder for optimized implementation\n           pass\n       ```\n   \n   - **Profile and Benchmark**: Regularly use profiling tools to identify and address bottlenecks in the model's computation graph.\n\n3. **Improve Code Organization**:\n\n   - **Action**: Refactor the codebase to separate each GAU into individual modules or files. This separation enhances maintainability and reduces the risk of class duplication or namespace conflicts.\n   \n   - **Implementation Example**:\n     \n     ```bash\n     model_discovery/\n         model/\n             utils/\n                 modules.py\n             gars/\n                 DDLerpLinear.py\n                 LoRA.py\n                 LerpLinear.py\n                 RWKV6.py\n                 RWKV6Attention.py\n                 RWKV6FeedForward.py\n                 ...\n     ```\n   \n   - **Benefits**:\n     - **Enhanced Clarity**: Clear separation of components makes the codebase easier to navigate.\n     - **Simplified Maintenance**: Isolated modules allow for targeted updates and bug fixes without affecting unrelated components.\n\n4. **Ensure Comprehensive Testing**:\n\n   - **Action**: Implement additional unit tests that specifically verify gradient flow through all learnable parameters, especially `mu`.\n   \n   - **Implementation Example**:\n     \n     ```python\n     def test_mu_gradient():\n         embed_dim = 128\n         block_loc = (0, 1)\n         output_dim = 64\n         unit = DDLerpLinear(embed_dim, block_loc, {}, output_dim)\n         X = torch.randn(2, 10, embed_dim, requires_grad=True)\n         mu = torch.randn(embed_dim, requires_grad=True)\n         Y, Z = unit(X, mu=mu)\n         loss = Y.sum()\n         loss.backward()\n         assert unit.mu.grad is not None, \"Gradients not flowing to 'mu'.\"\n     \n     test_mu_gradient()\n     ```\n   \n   - **Benefits**:\n     - **Early Detection of Issues**: Ensures that all parameters are correctly integrated into the computation graph.\n     - **Increased Confidence**: Validates the model's learnability and effectiveness during training.\n\n5. **Maintain CHILDREN_DECLARATIONS Integrity**:\n\n   - **Action**: Ensure that `CHILDREN_DECLARATIONS` are consistently and accurately maintained across all GAUs. Implement mechanisms to protect these declarations from being altered by automated tools like code reformatters.\n   \n   - **Implementation Suggestions**:\n     - **Use Preservation Comments**: Add comments that instruct reformatters to ignore specific sections.\n       \n       ```python\n       # fmt: off\n       CHILDREN_DECLARATIONS = [\n           UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n       ]\n       # fmt: on\n       ```\n     \n     - **Post-Formatting Scripts**: Develop scripts that check and restore `CHILDREN_DECLARATIONS` after code is formatted.\n   \n   - **Benefits**:\n     - **Consistent Architecture**: Prevents discrepancies between declared and actual GAU hierarchies.\n     - **Automation Safety**: Reduces manual intervention by ensuring critical declarations are preserved automatically.\n\n6. **Enhance Documentation Further**:\n\n   - **Action**: Expand documentation to include more diverse usage examples, especially showcasing different configurations of `DDLerpLinear` with and without `LoRA`.\n   \n   - **Implementation Example**:\n     \n     ```python\n     \"\"\"\n     **Example with LoRA**:\n     \n         >>> X = torch.randn(2, 10, 128)\n         >>> mu = torch.randn(128)\n         >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64, low_rank_dim=32)\n         >>> Y, Z = unit(X, mu=mu)\n     \n     **Example without LoRA**:\n     \n         >>> X = torch.randn(2, 10, 128)\n         >>> mu = torch.randn(128)\n         >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=128)\n         >>> Y, Z = unit(X, mu=mu)\n     \"\"\"\n     ```\n   \n   - **Benefits**:\n     - **Improved Clarity**: Helps users understand how to configure and utilize the GAU in different scenarios.\n     - **Facilitates Adoption**: Clear examples encourage correct and efficient usage of the GAU components.\n\n7. **Review and Clean Up Code Duplication**:\n\n   - **Action**: Ensure that GAU classes are not duplicated across different sections or files. Centralize common functionalities to avoid redundancy.\n   \n   - **Implementation Example**:\n     \n     - **Abstract Common Components**: If multiple GAUs share similar structures or components, abstract these into base classes or helper functions.\n     \n     - **Example**:\n       \n       ```python\n       class BaseLinearGAU(GAUBase):\n           def __init__(self, embed_dim, block_loc, kwarg_all, output_dim, low_rank_dim=None, device=None, dtype=None, **kwargs):\n               super().__init__(embed_dim, block_loc, kwarg_all)\n               self.output_dim = output_dim\n               self.low_rank_dim = low_rank_dim\n               self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n               if self.low_rank_dim is None:\n                   self.linear = nn.Linear(embed_dim, output_dim, bias=False, device=device, dtype=dtype)\n               else:\n                   kwarg_all['output_dim'] = output_dim\n                   kwarg_all['low_rank_dim'] = low_rank_dim\n                   self.linear = LoRA(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n       ```\n   \n   - **Benefits**:\n     - **Reduced Redundancy**: Centralizing common functionalities minimizes code duplication.\n     - **Enhanced Maintainability**: Changes to shared components need to be made only once, propagating across all dependent GAUs.\n\n### General Recommendations\n\n1. **Implement Comprehensive Testing**:\n   \n   - Beyond passing current functionality checks, implement more exhaustive tests covering various configurations and potential edge cases. This ensures robustness and reliability of the GAUs under diverse scenarios.\n\n2. **Maintain Consistent Coding Standards**:\n   \n   - Adhere to consistent coding conventions and standards across all GAU implementations. This uniformity enhances readability and facilitates smoother collaboration within the team.\n\n3. **Regularly Review and Refactor Code**:\n   \n   - Periodically assess the codebase for potential optimizations, refactorings, and improvements. This proactive approach helps in sustaining code quality and performance over time.\n\n4. **Enhance Collaboration and Communication**:\n   \n   - Document the architectural decisions, especially around conditional GAU integrations and hierarchical structures. Clear communication ensures that all team members are aligned and understand the model's design principles.\n\n### Conclusion\n\nThe current implementation of `DDLerpLinear` demonstrates significant progress with its modular design, comprehensive documentation, and conditional integration of `LoRA`. Addressing the `CHILDREN_DECLARATIONS` warning is essential to ensure that the GAU hierarchy is accurately represented and that the model functions as intended. By implementing the recommended improvements, particularly around declaration accuracy and training efficiency, the `DDLerpLinear` GAU can effectively contribute to the robustness, scalability, and efficiency of the overall language model architecture.",
                "requirements": "N/A",
                "reuse_from": null,
                "desc": null,
                "gautests": {
                    "test_ddlerp_linear": "@gau_test\ndef test_DDLerpLinear_test_ddlerp_linear(device=None, dtype=None) ->None:\n    embed_dim = 128\n    output_dim = 64\n    batch_size = 2\n    seq_len = 5\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=\n        dtype, requires_grad=True)\n    mu = torch.randn(embed_dim, device=device, dtype=dtype, requires_grad=True)\n    unit = DDLerpLinear(embed_dim=embed_dim, block_loc=(0, 1), kwarg_all={},\n        output_dim=output_dim, device=device, dtype=dtype)\n    Y, Z = unit(X, mu=mu)\n    assert Y.shape == X.shape, f'Expected Y.shape to be {X.shape}, got {Y.shape}'\n    assert 'o' in Z, \"Output 'o' not found in Z\"\n    assert Z['o'].shape == (batch_size, seq_len, output_dim\n        ), f\"Expected Z['o'].shape to be {batch_size, seq_len, output_dim}, got {Z['o'].shape}\"\n    loss = Z['o'].sum()\n    loss.backward()\n    assert mu.grad is not None, 'Gradient did not flow back to mu'\n    assert mu.grad.shape == mu.shape, f'Expected mu.grad.shape to be {mu.shape}, got {mu.grad.shape}'\n    print('test_ddlerp_linear without low_rank_dim passed')\n    mu.grad.zero_()\n    low_rank_dim = 16\n    unit_lora = DDLerpLinear(embed_dim=embed_dim, block_loc=(0, 1),\n        kwarg_all={}, output_dim=output_dim, low_rank_dim=low_rank_dim,\n        device=device, dtype=dtype)\n    Y_lora, Z_lora = unit_lora(X, mu=mu)\n    assert Y_lora.shape == X.shape, f'Expected Y.shape to be {X.shape}, got {Y_lora.shape}'\n    assert 'o' in Z_lora, \"Output 'o' not found in Z\"\n    assert Z_lora['o'].shape == (batch_size, seq_len, output_dim\n        ), f\"Expected Z['o'].shape to be {batch_size, seq_len, output_dim}, got {Z_lora['o'].shape}\"\n    loss = Z_lora['o'].sum()\n    loss.backward()\n    assert mu.grad is not None, 'Gradient did not flow back to mu in LoRA case'\n    assert mu.grad.shape == mu.shape, f'Expected mu.grad.shape to be {mu.shape}, got {mu.grad.shape}'\n    print('test_ddlerp_linear with low_rank_dim passed')\n"
                },
                "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional\n\n\nclass DDLerpLinear(GAUBase):\n    \"\"\"\n    DDLerpLinear (Dynamic Depth Lerp Linear) applies a linear transformation to the input `X` after adjusting it with a delta scaled by an input parameter `mu`.\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = X, \\\\\n        o = \text{Linear}(X + \\\\mu \\\\cdot \\\\delta)\n\n    Where:\n        - `X` is the input tensor of shape (B, L, D).\n        - `\\\\delta` is the difference between shifted and current input.\n        - `\\\\mu` is an input tensor that scales the delta.\n\n    If `low_rank_dim` is specified, it uses a low-rank approximation through LoRA.\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        output_dim (int): Dimension of the output.\n        low_rank_dim (Optional[int], optional): Low-rank dimension for LoRA. Defaults to None.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq_len, embed_dim).\n        - **mu**: Scaling parameter tensor of shape (embed_dim).\n        - **delta** (optional): Delta tensor of shape (batch, seq_len, embed_dim). If None, it is computed internally.\n\n    **Outputs:**\n\n        - **Y**: Output tensor of same shape as input X.\n        - **Z'['o']**: Output of the linear transformation.\n\n    **Example:**\n\n        >>> X = torch.randn(2, 10, 128)\n        >>> mu = torch.randn(128)\n        >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64)\n        >>> Y, Z = unit(X, mu=mu)\n\n    **Note:**\n\n        - The unit can optionally use a low-rank approximation through LoRA if `low_rank_dim` is specified.\n        - LoRA is an existing GAU and is reused directly without declaring it as a child.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: Optional[int]=None, device=None,\n        dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if self.low_rank_dim is None:\n            self.linear = nn.Linear(embed_dim, output_dim, bias=False,\n                device=device, dtype=dtype)\n        else:\n            kwarg_all['output_dim'] = output_dim\n            kwarg_all['low_rank_dim'] = low_rank_dim\n            self.linear = LoRA(embed_dim=embed_dim, block_loc=self.\n                block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **\n                kwarg_all)\n\n    def _forward(self, X: torch.Tensor, mu: torch.Tensor, delta: Optional[\n        torch.Tensor]=None, **Z):\n        if delta is None:\n            shifted = self.time_shift(X)\n            delta = shifted - X\n        adjusted_input = X + delta * mu\n        if self.low_rank_dim is None:\n            o = self.linear(adjusted_input)\n        else:\n            Y_lora, Z_lora = self.linear(adjusted_input)\n            o = Z_lora['o']\n        return X, {'o': o}\n",
                "rating": 3.0,
                "spec": "{\"unitname\":\"DDLerpLinear\",\"document\":\"DDLerpLinear (Dynamic Depth Lerp Linear) applies a linear transformation to the input `X` after adjusting it with a delta scaled by an input parameter `mu`.\\n\\n**Mathematical Formulation:**\\n\\n.. math::\\n\\n    Y = X, \\\\\\n    o =     ext{Linear}(X + \\\\mu \\\\cdot \\\\delta)\\n\\nWhere:\\n    - `X` is the input tensor of shape (B, L, D).\\n    - `\\\\delta` is the difference between shifted and current input.\\n    - `\\\\mu` is an input tensor that scales the delta.\\n\\nIf `low_rank_dim` is specified, it uses a low-rank approximation through LoRA.\\n\\n**Args:**\\n\\n    embed_dim (int): Embedding dimension of the input and output.\\n    block_loc (tuple): Location of the block within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments.\\n    output_dim (int): Dimension of the output.\\n    low_rank_dim (Optional[int], optional): Low-rank dimension for LoRA. Defaults to None.\\n    device (optional): Device to place the model on.\\n    dtype (optional): Data type of the model parameters.\\n\\n**Inputs:**\\n\\n    - **X**: Input tensor of shape (batch, seq_len, embed_dim).\\n    - **mu**: Scaling parameter tensor of shape (embed_dim).\\n    - **delta** (optional): Delta tensor of shape (batch, seq_len, embed_dim). If None, it is computed internally.\\n\\n**Outputs:**\\n\\n    - **Y**: Output tensor of same shape as input X.\\n    - **Z'['o']**: Output of the linear transformation.\\n\\n**Example:**\\n\\n    >>> X = torch.randn(2, 10, 128)\\n    >>> mu = torch.randn(128)\\n    >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64)\\n    >>> Y, Z = unit(X, mu=mu)\\n\\n**Note:**\\n\\n    - The unit can optionally use a low-rank approximation through LoRA if `low_rank_dim` is specified.\\n    - LoRA is an existing GAU and is reused directly without declaring it as a child.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {
                    "low_rank_dim": null,
                    "output_dim": null
                },
                "design_traces": null
            },
            "LoRA": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_lora": "@gau_test\ndef test_LoRA_test_lora(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    lora = LoRA(embed_dim, block_loc, kwarg_all, output_dim=128,\n        low_rank_dim=32, device=device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y, _ = lora(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Optional\n\n\nclass LoRA(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: int, bias: Optional[bool]=True,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.bias = bias\n        self.lora = nn.Sequential(nn.Linear(embed_dim, low_rank_dim, bias=\n            False, device=device, dtype=dtype), nn.Tanh(), nn.Linear(\n            low_rank_dim, output_dim, bias=bias, device=device, dtype=dtype))\n\n    def __repr__(self) ->str:\n        s = f'{self.__class__.__name__}('\n        s += (\n            f'input_dim={self.input_dim}, low_rank_dim={self.low_rank_dim}, output_dim={self.output_dim}'\n            )\n        if not self.bias:\n            s += f', bias={self.bias}'\n        s += ')'\n        return s\n\n    def _forward(self, X, **Z):\n        return X, {'o': self.lora(X)}\n\n\nCHILDREN_DECLARATIONS = []\n",
                "rating": null,
                "spec": "{\"unitname\":\"LoRA\",\"document\":\"\\nLoRA\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "LerpLinear": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_lerplinear": "@gau_test\ndef test_LerpLinear_test_lerplinear(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    lerplinear = LerpLinear(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype, **kwarg_all)\n    X = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = lerplinear(X)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Optional\n\n\nclass LerpLinear(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: Optional[int]=None, device=None,\n        dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if self.low_rank_dim is None:\n            self.linear = nn.Linear(embed_dim, output_dim, bias=False,\n                device=device, dtype=dtype)\n        else:\n            kwarg_all['output_dim'] = output_dim\n            kwarg_all['low_rank_dim'] = low_rank_dim\n            self.linear = LoRA(embed_dim=self.embed_dim, block_loc=self.\n                block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n                **self.kwarg_all)\n        self.mu = nn.Parameter(torch.zeros(embed_dim, device=device, dtype=\n            dtype))\n\n    def __repr__(self) ->str:\n        s = f'{self.__class__.__name__}({self.input_dim}, {self.output_dim}'\n        if self.low_rank_dim is not None:\n            s += f', low_rank_dim={self.low_rank_dim}'\n        s += ')'\n        return s\n\n    def _forward(self, X: torch.Tensor, delta: Optional[torch.Tensor]=None\n        ) ->torch.Tensor:\n        if delta is None:\n            shifted = self.time_shift(X)\n            if len(shifted.shape) == 2:\n                shifted = shifted.unsqueeze(1)\n            delta = shifted - X\n        if self.low_rank_dim is None:\n            o = self.linear(X + delta * self.mu)\n        else:\n            o = self.linear(X + delta * self.mu)[1]['o']\n        return X, {'o': o}\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LoRA', requirements='', inputs=\n    ['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"LerpLinear\",\"document\":\"\\nLerpLinear\\n\",\"inputs\":[\"X\",\"delta\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "LoRA"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "RWKV6FeedForward": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rwkv6feedforward": "@gau_test\ndef test_RWKV6FeedForward_test_rwkv6feedforward(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6feedforward = RWKV6FeedForward(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rwkv6feedforward(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RWKV6FeedForward(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        hidden_ratio = 3.5\n        intermediate_size = int(embed_dim * hidden_ratio)\n        intermediate_size = 32 * ((intermediate_size + 32 - 1) // 32)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        kwarg_all['output_dim'] = intermediate_size\n        self.key = LerpLinear(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.value = nn.Linear(intermediate_size, embed_dim, bias=False,\n            device=device, dtype=dtype)\n        kwarg_all['output_dim'] = embed_dim\n        self.receptance = LerpLinear(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.relu = nn.ReLU()\n\n    def _forward(self, X, **Z):\n        shifted = self.time_shift(X)\n        delta = shifted - X\n        _key = self.key(X, **{'delta': delta})[1]['o']\n        r = self.relu(_key)\n        key = r * r\n        value = self.value(key)\n        receptance = self.receptance(X, **{'delta': delta})[1]['o']\n        return receptance.sigmoid() * value\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LerpLinear', requirements='',\n    inputs=['X', 'delta'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"RWKV6FeedForward\",\"document\":\"\\nRWKV6FeedForward\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "LerpLinear"
                ],
                "suggestions": null,
                "args": {},
                "design_traces": null
            },
            "RWKV6": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rwkv6": "@gau_test\ndef test_RWKV6_test_rwkv6(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6 = RWKV6(embed_dim, block_loc, kwarg_all, device=device, dtype=\n        dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rwkv6(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RWKV6(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        norm_eps: float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=\n            norm_eps, **self.factory_kwargs)\n        self.attn = RWKV6Attention(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=\n            norm_eps, **self.factory_kwargs)\n        self.ffn = RWKV6FeedForward(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, _ = self.attn(self.attn_norm(X), **Z)\n        X = X1 + X\n        X2, _ = self.ffn(self.ffn_norm(X), **Z)\n        X = X2 + X\n        return X\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RWKV6Attention', requirements=\n    '', inputs=['X'], outputs=['Y']), UnitDecl(unitname='RWKV6FeedForward',\n    requirements='', inputs=['X'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"RWKV6\",\"document\":\"\\nRWKV6\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "RWKV6Attention",
                    "RWKV6FeedForward"
                ],
                "suggestions": null,
                "args": {
                    "norm_eps": 1e-05
                },
                "design_traces": null
            },
            "RWKV6Attention": {
                "review": null,
                "requirements": null,
                "reuse_from": null,
                "desc": "\n",
                "gautests": {
                    "test_rwkv6attention": "@gau_test\ndef test_RWKV6Attention_test_rwkv6attention(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6attention = RWKV6Attention(embed_dim, block_loc, kwarg_all, device\n        =device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y, _ = rwkv6attention(x)\n    assert y.shape == (1, 100, 128)\n"
                },
                "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom einops import rearrange\nfrom transformers.activations import ACT2FN\nfrom typing import Optional\n\n\nclass RWKV6Attention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        num_heads: int=4, gate_fn: str='swish', proj_low_rank_dim: int=32,\n        gate_low_rank_dim: int=64, elementwise_affine: Optional[bool]=True,\n        norm_eps: float=1e-05, chunk_size: int=32, device=None, dtype=None,\n        **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.num_heads = num_heads\n        self.proj_low_rank_dim = proj_low_rank_dim\n        self.gate_low_rank_dim = gate_low_rank_dim\n        self.chunk_size = chunk_size\n        self.key_dim = embed_dim // 2\n        self.value_dim = embed_dim\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        kwarg_all['output_dim'] = proj_low_rank_dim * 5\n        self.x_proj = nn.Sequential(LerpLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all), nn.Tanh(), nn.Linear(\n            proj_low_rank_dim * 5, embed_dim, bias=False, device=device,\n            dtype=dtype))\n        self.x_bias = nn.Parameter(torch.zeros(5, embed_dim, device=device,\n            dtype=dtype))\n        kwarg_all['output_dim'] = self.key_dim\n        self.r_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['low_rank_dim'] = gate_low_rank_dim\n        self.w_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all.pop('low_rank_dim')\n        self.k_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['output_dim'] = self.value_dim\n        self.v_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['low_rank_dim'] = gate_low_rank_dim\n        self.g_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.bonus = nn.Parameter(torch.zeros(num_heads, self.head_qk_dim,\n            device=device, dtype=dtype))\n        self.g_norm = nn.LayerNorm(self.value_dim, elementwise_affine=\n            elementwise_affine, eps=norm_eps, device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, embed_dim, bias=False,\n            device=device, dtype=dtype)\n        self.gate_fn = ACT2FN[gate_fn]\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.Parameter):\n            nn.init.xavier_uniform_(module, gain=2 ** -2.5)\n        module._is_hf_initialized = True\n\n    def naive_chunk_rwkv6(self, q: torch.Tensor, k: torch.Tensor, v: torch.\n        Tensor, w: torch.Tensor, u: torch.Tensor, chunk_size: int=32):\n        assert q.shape[-2] % chunk_size == 0\n        orig_dtype = q.dtype\n        num_chunk = q.shape[-2] // chunk_size\n        u = u.unsqueeze(0)\n        q, k, v, w = map(lambda x: rearrange(x, 'b h (n c) d -> b h n c d',\n            c=chunk_size).float(), (q, k, v, w))\n        w_cumsum = w.cumsum(-2)\n        kw = k * (w_cumsum[..., -1, None, :] - w_cumsum).exp()\n        wkv = kw.transpose(-1, -2) @ v\n        wkv_new = torch.zeros_like(wkv)\n        for i in range(num_chunk - 1):\n            wkv_new[:, :, i + 1] = wkv_new[:, :, i].clone() * w_cumsum[:, :,\n                i, -1, :, None].exp() + wkv[:, :, i]\n        o_inter = torch.einsum('b h n d p, b h n c d -> b h n c p', wkv_new,\n            q * (w_cumsum - w).exp())\n        o_intra = torch.zeros_like(o_inter)\n        for i in range(chunk_size):\n            attn = (q[:, :, :, i, None] * k * (w_cumsum[:, :, :, i, None] -\n                w[:, :, :, i, None] - w_cumsum).exp()).sum(-1)\n            mask = (torch.arange(0, chunk_size) < i).to(attn.device)\n            attn.masked_fill_(~mask, 0)\n            intra_inter_o = (attn.unsqueeze(-1) * v).sum(-2)\n            intra_intra_o = (q[:, :, :, i] * u.unsqueeze(2) * k[:, :, :, i]\n                ).sum(-1).unsqueeze(-1) * v[:, :, :, i]\n            o_intra[:, :, :, i] = intra_inter_o + intra_intra_o\n        o = o_inter + o_intra\n        return rearrange(o, 'b h n c d -> b h (n c) d').to(orig_dtype)\n\n    def pad_input(self, X):\n        _seq_len = X.shape[-2]\n        pad_len = (X.shape[-2] + self.chunk_size - 1\n            ) // self.chunk_size * self.chunk_size - X.shape[-2]\n        return F.pad(X, (0, 0, 0, pad_len)), _seq_len\n\n    def _forward(self, X: torch.Tensor):\n        X, _seq_len = self.pad_input(X)\n        batch_size, seq_len, hidden_size = X.shape\n        last_state = None\n        if X.shape[1] == 1 and last_state is not None:\n            shifted = last_state[0].unsqueeze(1)\n        else:\n            shifted = self.time_shift(X)\n            if last_state is not None:\n                shifted[:, 0] = last_state[0]\n        delta = shifted - X\n        x = self.x_proj[0](X, **{'delta': delta})[1]['o'].view(batch_size,\n            seq_len, -1, self.proj_low_rank_dim)\n        x = torch.einsum('b l n r, h n r-> b l n h', self.x_proj[1](x),\n            self.x_proj[2].weight.view(hidden_size, 5, -1))\n        r, w, k, v, g = x.add_(self.x_bias).unbind(-2)\n        r = self.r_proj(X, **{'mu': r, 'delta': delta})[1]['o']\n        w = self.w_proj(X, **{'mu': w, 'delta': delta})[1]['o']\n        k = self.k_proj(X, **{'mu': k, 'delta': delta})[1]['o']\n        v = self.v_proj(X, **{'mu': v, 'delta': delta})[1]['o']\n        g = self.g_proj(X, **{'mu': g, 'delta': delta})[1]['o']\n        r, w, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=\n            self.num_heads), (r, w, k, v))\n        w = -torch.exp(w)\n        u = self.bonus\n        o = self.naive_chunk_rwkv6(r, k, v, w, u, chunk_size=self.chunk_size)\n        o = rearrange(o, 'b h l d -> b l (h d)')\n        o = self.g_norm(o)\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        o = o[:, :_seq_len]\n        return o\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LerpLinear', requirements='',\n    inputs=['X', 'delta'], outputs=['Y']), UnitDecl(unitname='DDLerpLinear',\n    requirements='', inputs=['X', 'mu', 'delta'], outputs=['Y'])]\n",
                "rating": null,
                "spec": "{\"unitname\":\"RWKV6Attention\",\"document\":\"\\nRWKV6Attention\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                "children": [
                    "LerpLinear",
                    "DDLerpLinear"
                ],
                "suggestions": null,
                "args": {
                    "proj_low_rank_dim": 32,
                    "gate_low_rank_dim": 64,
                    "elementwise_affine": true,
                    "gate_fn": "swish",
                    "num_heads": 4,
                    "chunk_size": 32
                },
                "design_traces": null
            }
        },
        "rating": null,
        "declares": {
            "DDLerpLinear": "{\"unitname\":\"DDLerpLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
        },
        "proposal_traces": [],
        "suggestions": null,
        "name": "rwkv6_event"
    },
    "status": "implemented",
    "history": [
        {
            "tree": {
                "review": null,
                "root": "RWKV6",
                "proposal": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: this https URL Training code at: this https URL Inference code at: this https URL Time-parallel training code at: this https URL",
                "units": {
                    "DDLerpLinear": {
                        "review": "```rating 3.0\n```\n\n### Strengths of the Implementation\n\n1. **Modular and Hierarchical Design**: The implementation maintains a clear modular structure by utilizing nested GAUs (`LoRA`, `LerpLinear`, etc.), which aligns well with the proposed framework. This modularity facilitates easier debugging, testing, and future extensions.\n\n2. **Comprehensive Documentation**: The `DDLerpLinear` class includes detailed docstrings that explain its functionality, mathematical formulation, arguments, inputs, outputs, and usage examples. This level of documentation enhances code readability and maintainability, making it easier for team members to understand and work with the codebase.\n\n3. **Conditional LoRA Integration**: The implementation correctly integrates the `LoRA` module conditionally based on the `low_rank_dim` parameter. This flexibility allows for varying model complexities and can lead to better parameter efficiency when needed.\n\n4. **Proper Parameter Initialization**: The use of `nn.Parameter` for `mu` ensures that it is registered as a learnable parameter within the model, enabling it to receive gradients during training. This is crucial for the dynamic depth adjustment intended by the `DDLerpLinear` design.\n\n5. **Adherence to Base Class Contracts**: The `DDLerpLinear` class correctly inherits from `GAUBase` and implements the required `_forward` method, maintaining consistency with the GAU framework. This adherence ensures that the GAU integrates seamlessly within the larger model architecture.\n\n### Areas for Improvement and Specific Suggestions\n\n1. **Addressing CHILDREN_DECLARATIONS Warning**:\n\n   - **Issue**: The format checker issues a warning stating that no `CHILDREN_DECLARATIONS` are found in the `DDLerpLinear` GAU. This occurs despite `LoRA` being conditionally integrated into `DDLerpLinear`.\n\n   - **Analysis**: The absence of `CHILDREN_DECLARATIONS` means that the parser assumes `DDLerpLinear` has no child GAUs. However, when `low_rank_dim` is specified, `DDLerpLinear` utilizes `LoRA` as a child unit. Not declaring `LoRA` as a child leads to inconsistency between the declared architecture and the actual usage.\n\n   - **Solution**:\n     - **Declare CHILDREN_DECLARATIONS Appropriately**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is used. This can be achieved by conditionally appending to the `CHILDREN_DECLARATIONS` based on the presence of `low_rank_dim`.\n     \n     - **Implementation Example**:\n       \n       ```python\n       CHILDREN_DECLARATIONS = []\n       if self.low_rank_dim is not None:\n           CHILDREN_DECLARATIONS = [\n               UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n           ]\n       ```\n     \n     - **Alternatively**, declare all possible child GAUs and manage their usage within the class. This ensures that the hierarchy is explicitly defined, preventing parser warnings.\n     \n       ```python\n       CHILDREN_DECLARATIONS = [\n           UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n       ]\n       ```\n     \n     - **Adjust Reformatter Settings**: Modify the reformatter to preserve `CHILDREN_DECLARATIONS`. This might involve adding specific markers or comments that instruct the reformatter not to alter these critical lines.\n     \n     - **Implement Post-Formatting Scripts**: Develop scripts that automatically reinsert or correct `CHILDREN_DECLARATIONS` after the code is processed by the reformatter.\n\n2. **Optimizing Training Efficiency**:\n\n   - **Issue**: While the functionality checker now passes, previous iterations indicated efficiency concerns. Ensuring that the model remains efficient is crucial for scalability.\n   \n   - **Suggestions**:\n     - **Vectorize Operations**: Replace explicit Python loops in methods like `naive_chunk_rwkv6` with vectorized PyTorch operations to leverage optimized backend implementations and GPU acceleration.\n     \n     - **Profile the Model**: Utilize profiling tools such as PyTorch Profiler to identify bottlenecks in the computation graph and optimize accordingly.\n     \n     - **Simplify LoRA Integration**: Assess whether the current integration of `LoRA` introduces significant computational overhead. If so, explore more efficient architectures or parameter-sharing mechanisms.\n     \n     - **Batch Operations**: Ensure that tensor operations are batched appropriately to maximize parallelism and hardware utilization.\n\n3. **Code Organization and Maintainability**:\n\n   - **Issue**: The `gab.py` file contains multiple GAU class definitions, which can lead to confusion and maintenance challenges.\n   \n   - **Suggestions**:\n     - **Separate GAUs into Individual Files**: Organize each GAU into its own module or file. This separation enhances code clarity and makes it easier to navigate and manage individual components.\n     \n     - **Remove Duplicated Class Definitions**: Ensure that each GAU class is defined uniquely without duplications to prevent namespace conflicts and unexpected behaviors.\n     \n     - **Implement Consistent Naming Conventions**: Adopt consistent and descriptive naming conventions for classes and files to facilitate easier identification and usage.\n\n4. **Ensuring Comprehensive Testing**:\n\n   - **Issue**: While unit tests passed, ensuring comprehensive coverage is vital to prevent future regressions.\n   \n   - **Suggestions**:\n     - **Implement Gradient Flow Tests**: Beyond basic forward passes, include tests that verify whether gradients flow correctly through all learnable parameters, especially `mu`.\n     \n     - **Efficiency Benchmarking**: Regularly benchmark the model\u2019s training and inference times against established baselines to monitor and address any performance regressions promptly.\n     \n     - **Causality Tests**: Ensure that the model maintains causality, especially after modifications that introduce dynamic depth adjustments.\n\n5. **Documentation Enhancements**:\n\n   - **Issue**: Although the docstrings are comprehensive, ensuring that all classes and their relationships are well-documented is essential.\n   \n   - **Suggestions**:\n     - **Document Conditional Logic**: Clearly document the conditions under which certain child GAUs (like `LoRA`) are used. This transparency aids in understanding the model\u2019s architecture and behavior.\n     \n     - **Provide Usage Examples**: While examples are provided in the docstrings, including more diverse examples covering different configurations can enhance understanding.\n\n### Comments on Innovation and Potential Impact\n\nThe implementation of `DDLerpLinear` introduces a dynamic depth mechanism through the learnable parameter `mu`, aligning well with the proposal's objective of adaptable computation based on input significance. The conditional integration of `LoRA` further enhances parameter efficiency, allowing the model to scale more effectively without a proportional increase in computational resources.\n\n**Potential Impact**:\n\n- **Enhanced Computational Efficiency**: By dynamically adjusting computational depth, the model can allocate resources more effectively, potentially reducing unnecessary computations for less important inputs.\n\n- **Improved Scalability**: The combination of dynamic depth and low-rank adaptations facilitates better scalability, enabling the model to handle larger datasets and more complex tasks without exorbitant increases in computational or memory overhead.\n\n- **Flexibility and Adaptability**: The ability to conditionally integrate components like `LoRA` based on configuration parameters allows for greater flexibility in model design, catering to diverse application requirements.\n\n**Concerns**:\n\n- **Integration Complexity**: The conditional usage of `LoRA` introduces additional layers of complexity in managing dependencies and ensuring consistent behavior across different configurations.\n\n- **Maintenance Overhead**: Ensuring that `CHILDREN_DECLARATIONS` accurately reflect the model\u2019s architecture requires disciplined coding practices and possibly additional tooling to automate verification.\n\n### Detailed Analysis of Failed Checks\n\n#### Format Checker Issues\n\n- **Warning**: *\"Code block 1 of DDLerpLinear: Warning: No CHILDREN_DECLARATIONS found in the GAU. Will assume there is no children.\"*\n\n  **Analysis**:\n\n  - The `DDLerpLinear` GAU conditionally integrates the `LoRA` module based on the `low_rank_dim` parameter. However, the current implementation does not declare `LoRA` in `CHILDREN_DECLARATIONS`, leading the format checker to assume that there are no child GAUs.\n\n  - This discrepancy between actual usage and declarations can lead to issues in the model's structural integrity, as the parser relies on `CHILDREN_DECLARATIONS` to understand GAU hierarchies and dependencies.\n\n  **Recommendations**:\n\n  - **Declare Child GAUs Appropriately**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is utilized. This ensures that the parser accurately recognizes and manages all child GAUs.\n\n    ```python\n    class DDLerpLinear(GAUBase):\n        ...\n        CHILDREN_DECLARATIONS = []\n        if low_rank_dim is not None:\n            CHILDREN_DECLARATIONS = [\n                UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n            ]\n        ...\n    ```\n    \n  - **Avoid Self-Declaration**: Ensure that `DDLerpLinear` does not list itself as a child, which can cause recursive parsing issues.\n\n  - **Consistent Declaration Practices**: Adopt a consistent approach across all GAUs in declaring children. Whether or not a child GAU is used, its declaration should reflect the actual dependencies to prevent format checker warnings.\n\n  - **Adjust Reformatter Settings**: Modify the code reformatter to preserve critical declarations like `CHILDREN_DECLARATIONS`. This might involve marking these lines with specific comments or patterns that instruct the reformatter to retain them.\n\n  - **Post-Formatting Validation**: Implement scripts or pre-commit hooks that verify the presence and correctness of `CHILDREN_DECLARATIONS` after formatting processes.\n\n#### Functionality Checker Overview\n\n- **Status**: Functionality check passed.\n\n  **Note**: Unlike previous iterations where differentiability tests failed due to `mu` parameters lacking gradients, the current implementation passes the functionality checker. This indicates that issues related to parameter gradient flow have been addressed, leading to successful forward and backward passes without causality violations.\n\n### Recommendations for the Coder\n\n1. **Immediately Address CHILDREN_DECLARATIONS Warning**:\n\n   - **Action**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is used. This alignment is crucial to satisfy the format checker and ensure that the model's hierarchical structure is accurately represented.\n   \n   - **Implementation Example**:\n     \n     ```python\n     CHILDREN_DECLARATIONS = []\n     if self.low_rank_dim is not None:\n         CHILDREN_DECLARATIONS = [\n             UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n         ]\n     ```\n   \n   - **Alternatively**, if conditional declarations are not feasible within the class, always declare `LoRA` as a potential child GAU, and manage its usage within the class logic.\n   \n     ```python\n     CHILDREN_DECLARATIONS = [\n         UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n     ]\n     ```\n\n2. **Enhance Training Efficiency**:\n\n   - **Action**: Optimize the `naive_chunk_rwkv6` method by replacing explicit Python loops with vectorized PyTorch operations. This change can significantly reduce training time and improve scalability.\n   \n   - **Implementation Suggestion**:\n     \n     - **Vectorize Attention Computations**: Utilize batch matrix operations and PyTorch's optimized tensor functions to handle chunk processing without Python-level loops.\n     \n     - **Example Refactoring**:\n       \n       ```python\n       def optimized_chunk_rwkv6(self, q, k, v, w, u, chunk_size=32):\n           # Example of vectorizing chunk processing\n           # Placeholder for optimized implementation\n           pass\n       ```\n   \n   - **Profile and Benchmark**: Regularly use profiling tools to identify and address bottlenecks in the model's computation graph.\n\n3. **Improve Code Organization**:\n\n   - **Action**: Refactor the codebase to separate each GAU into individual modules or files. This separation enhances maintainability and reduces the risk of class duplication or namespace conflicts.\n   \n   - **Implementation Example**:\n     \n     ```bash\n     model_discovery/\n         model/\n             utils/\n                 modules.py\n             gars/\n                 DDLerpLinear.py\n                 LoRA.py\n                 LerpLinear.py\n                 RWKV6.py\n                 RWKV6Attention.py\n                 RWKV6FeedForward.py\n                 ...\n     ```\n   \n   - **Benefits**:\n     - **Enhanced Clarity**: Clear separation of components makes the codebase easier to navigate.\n     - **Simplified Maintenance**: Isolated modules allow for targeted updates and bug fixes without affecting unrelated components.\n\n4. **Ensure Comprehensive Testing**:\n\n   - **Action**: Implement additional unit tests that specifically verify gradient flow through all learnable parameters, especially `mu`.\n   \n   - **Implementation Example**:\n     \n     ```python\n     def test_mu_gradient():\n         embed_dim = 128\n         block_loc = (0, 1)\n         output_dim = 64\n         unit = DDLerpLinear(embed_dim, block_loc, {}, output_dim)\n         X = torch.randn(2, 10, embed_dim, requires_grad=True)\n         mu = torch.randn(embed_dim, requires_grad=True)\n         Y, Z = unit(X, mu=mu)\n         loss = Y.sum()\n         loss.backward()\n         assert unit.mu.grad is not None, \"Gradients not flowing to 'mu'.\"\n     \n     test_mu_gradient()\n     ```\n   \n   - **Benefits**:\n     - **Early Detection of Issues**: Ensures that all parameters are correctly integrated into the computation graph.\n     - **Increased Confidence**: Validates the model's learnability and effectiveness during training.\n\n5. **Maintain CHILDREN_DECLARATIONS Integrity**:\n\n   - **Action**: Ensure that `CHILDREN_DECLARATIONS` are consistently and accurately maintained across all GAUs. Implement mechanisms to protect these declarations from being altered by automated tools like code reformatters.\n   \n   - **Implementation Suggestions**:\n     - **Use Preservation Comments**: Add comments that instruct reformatters to ignore specific sections.\n       \n       ```python\n       # fmt: off\n       CHILDREN_DECLARATIONS = [\n           UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n       ]\n       # fmt: on\n       ```\n     \n     - **Post-Formatting Scripts**: Develop scripts that check and restore `CHILDREN_DECLARATIONS` after code is formatted.\n   \n   - **Benefits**:\n     - **Consistent Architecture**: Prevents discrepancies between declared and actual GAU hierarchies.\n     - **Automation Safety**: Reduces manual intervention by ensuring critical declarations are preserved automatically.\n\n6. **Enhance Documentation Further**:\n\n   - **Action**: Expand documentation to include more diverse usage examples, especially showcasing different configurations of `DDLerpLinear` with and without `LoRA`.\n   \n   - **Implementation Example**:\n     \n     ```python\n     \"\"\"\n     **Example with LoRA**:\n     \n         >>> X = torch.randn(2, 10, 128)\n         >>> mu = torch.randn(128)\n         >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64, low_rank_dim=32)\n         >>> Y, Z = unit(X, mu=mu)\n     \n     **Example without LoRA**:\n     \n         >>> X = torch.randn(2, 10, 128)\n         >>> mu = torch.randn(128)\n         >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=128)\n         >>> Y, Z = unit(X, mu=mu)\n     \"\"\"\n     ```\n   \n   - **Benefits**:\n     - **Improved Clarity**: Helps users understand how to configure and utilize the GAU in different scenarios.\n     - **Facilitates Adoption**: Clear examples encourage correct and efficient usage of the GAU components.\n\n7. **Review and Clean Up Code Duplication**:\n\n   - **Action**: Ensure that GAU classes are not duplicated across different sections or files. Centralize common functionalities to avoid redundancy.\n   \n   - **Implementation Example**:\n     \n     - **Abstract Common Components**: If multiple GAUs share similar structures or components, abstract these into base classes or helper functions.\n     \n     - **Example**:\n       \n       ```python\n       class BaseLinearGAU(GAUBase):\n           def __init__(self, embed_dim, block_loc, kwarg_all, output_dim, low_rank_dim=None, device=None, dtype=None, **kwargs):\n               super().__init__(embed_dim, block_loc, kwarg_all)\n               self.output_dim = output_dim\n               self.low_rank_dim = low_rank_dim\n               self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n               if self.low_rank_dim is None:\n                   self.linear = nn.Linear(embed_dim, output_dim, bias=False, device=device, dtype=dtype)\n               else:\n                   kwarg_all['output_dim'] = output_dim\n                   kwarg_all['low_rank_dim'] = low_rank_dim\n                   self.linear = LoRA(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n       ```\n   \n   - **Benefits**:\n     - **Reduced Redundancy**: Centralizing common functionalities minimizes code duplication.\n     - **Enhanced Maintainability**: Changes to shared components need to be made only once, propagating across all dependent GAUs.\n\n### General Recommendations\n\n1. **Implement Comprehensive Testing**:\n   \n   - Beyond passing current functionality checks, implement more exhaustive tests covering various configurations and potential edge cases. This ensures robustness and reliability of the GAUs under diverse scenarios.\n\n2. **Maintain Consistent Coding Standards**:\n   \n   - Adhere to consistent coding conventions and standards across all GAU implementations. This uniformity enhances readability and facilitates smoother collaboration within the team.\n\n3. **Regularly Review and Refactor Code**:\n   \n   - Periodically assess the codebase for potential optimizations, refactorings, and improvements. This proactive approach helps in sustaining code quality and performance over time.\n\n4. **Enhance Collaboration and Communication**:\n   \n   - Document the architectural decisions, especially around conditional GAU integrations and hierarchical structures. Clear communication ensures that all team members are aligned and understand the model's design principles.\n\n### Conclusion\n\nThe current implementation of `DDLerpLinear` demonstrates significant progress with its modular design, comprehensive documentation, and conditional integration of `LoRA`. Addressing the `CHILDREN_DECLARATIONS` warning is essential to ensure that the GAU hierarchy is accurately represented and that the model functions as intended. By implementing the recommended improvements, particularly around declaration accuracy and training efficiency, the `DDLerpLinear` GAU can effectively contribute to the robustness, scalability, and efficiency of the overall language model architecture.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_ddlerp_linear": "@gau_test\ndef test_DDLerpLinear_test_ddlerp_linear(device=None, dtype=None) ->None:\n    embed_dim = 128\n    output_dim = 64\n    batch_size = 2\n    seq_len = 5\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=\n        dtype, requires_grad=True)\n    mu = torch.randn(embed_dim, device=device, dtype=dtype, requires_grad=True)\n    unit = DDLerpLinear(embed_dim=embed_dim, block_loc=(0, 1), kwarg_all={},\n        output_dim=output_dim, device=device, dtype=dtype)\n    Y, Z = unit(X, mu=mu)\n    assert Y.shape == X.shape, f'Expected Y.shape to be {X.shape}, got {Y.shape}'\n    assert 'o' in Z, \"Output 'o' not found in Z\"\n    assert Z['o'].shape == (batch_size, seq_len, output_dim\n        ), f\"Expected Z['o'].shape to be {batch_size, seq_len, output_dim}, got {Z['o'].shape}\"\n    loss = Z['o'].sum()\n    loss.backward()\n    assert mu.grad is not None, 'Gradient did not flow back to mu'\n    assert mu.grad.shape == mu.shape, f'Expected mu.grad.shape to be {mu.shape}, got {mu.grad.shape}'\n    print('test_ddlerp_linear without low_rank_dim passed')\n    mu.grad.zero_()\n    low_rank_dim = 16\n    unit_lora = DDLerpLinear(embed_dim=embed_dim, block_loc=(0, 1),\n        kwarg_all={}, output_dim=output_dim, low_rank_dim=low_rank_dim,\n        device=device, dtype=dtype)\n    Y_lora, Z_lora = unit_lora(X, mu=mu)\n    assert Y_lora.shape == X.shape, f'Expected Y.shape to be {X.shape}, got {Y_lora.shape}'\n    assert 'o' in Z_lora, \"Output 'o' not found in Z\"\n    assert Z_lora['o'].shape == (batch_size, seq_len, output_dim\n        ), f\"Expected Z['o'].shape to be {batch_size, seq_len, output_dim}, got {Z_lora['o'].shape}\"\n    loss = Z_lora['o'].sum()\n    loss.backward()\n    assert mu.grad is not None, 'Gradient did not flow back to mu in LoRA case'\n    assert mu.grad.shape == mu.shape, f'Expected mu.grad.shape to be {mu.shape}, got {mu.grad.shape}'\n    print('test_ddlerp_linear with low_rank_dim passed')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional\n\n\nclass DDLerpLinear(GAUBase):\n    \"\"\"\n    DDLerpLinear (Dynamic Depth Lerp Linear) applies a linear transformation to the input `X` after adjusting it with a delta scaled by an input parameter `mu`.\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = X, \\\\\n        o = \text{Linear}(X + \\\\mu \\\\cdot \\\\delta)\n\n    Where:\n        - `X` is the input tensor of shape (B, L, D).\n        - `\\\\delta` is the difference between shifted and current input.\n        - `\\\\mu` is an input tensor that scales the delta.\n\n    If `low_rank_dim` is specified, it uses a low-rank approximation through LoRA.\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        output_dim (int): Dimension of the output.\n        low_rank_dim (Optional[int], optional): Low-rank dimension for LoRA. Defaults to None.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq_len, embed_dim).\n        - **mu**: Scaling parameter tensor of shape (embed_dim).\n        - **delta** (optional): Delta tensor of shape (batch, seq_len, embed_dim). If None, it is computed internally.\n\n    **Outputs:**\n\n        - **Y**: Output tensor of same shape as input X.\n        - **Z'['o']**: Output of the linear transformation.\n\n    **Example:**\n\n        >>> X = torch.randn(2, 10, 128)\n        >>> mu = torch.randn(128)\n        >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64)\n        >>> Y, Z = unit(X, mu=mu)\n\n    **Note:**\n\n        - The unit can optionally use a low-rank approximation through LoRA if `low_rank_dim` is specified.\n        - LoRA is an existing GAU and is reused directly without declaring it as a child.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: Optional[int]=None, device=None,\n        dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if self.low_rank_dim is None:\n            self.linear = nn.Linear(embed_dim, output_dim, bias=False,\n                device=device, dtype=dtype)\n        else:\n            kwarg_all['output_dim'] = output_dim\n            kwarg_all['low_rank_dim'] = low_rank_dim\n            self.linear = LoRA(embed_dim=embed_dim, block_loc=self.\n                block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **\n                kwarg_all)\n\n    def _forward(self, X: torch.Tensor, mu: torch.Tensor, delta: Optional[\n        torch.Tensor]=None, **Z):\n        if delta is None:\n            shifted = self.time_shift(X)\n            delta = shifted - X\n        adjusted_input = X + delta * mu\n        if self.low_rank_dim is None:\n            o = self.linear(adjusted_input)\n        else:\n            Y_lora, Z_lora = self.linear(adjusted_input)\n            o = Z_lora['o']\n        return X, {'o': o}\n",
                        "rating": 3.0,
                        "spec": "{\"unitname\":\"DDLerpLinear\",\"document\":\"DDLerpLinear (Dynamic Depth Lerp Linear) applies a linear transformation to the input `X` after adjusting it with a delta scaled by an input parameter `mu`.\\n\\n**Mathematical Formulation:**\\n\\n.. math::\\n\\n    Y = X, \\\\\\n    o =     ext{Linear}(X + \\\\mu \\\\cdot \\\\delta)\\n\\nWhere:\\n    - `X` is the input tensor of shape (B, L, D).\\n    - `\\\\delta` is the difference between shifted and current input.\\n    - `\\\\mu` is an input tensor that scales the delta.\\n\\nIf `low_rank_dim` is specified, it uses a low-rank approximation through LoRA.\\n\\n**Args:**\\n\\n    embed_dim (int): Embedding dimension of the input and output.\\n    block_loc (tuple): Location of the block within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments.\\n    output_dim (int): Dimension of the output.\\n    low_rank_dim (Optional[int], optional): Low-rank dimension for LoRA. Defaults to None.\\n    device (optional): Device to place the model on.\\n    dtype (optional): Data type of the model parameters.\\n\\n**Inputs:**\\n\\n    - **X**: Input tensor of shape (batch, seq_len, embed_dim).\\n    - **mu**: Scaling parameter tensor of shape (embed_dim).\\n    - **delta** (optional): Delta tensor of shape (batch, seq_len, embed_dim). If None, it is computed internally.\\n\\n**Outputs:**\\n\\n    - **Y**: Output tensor of same shape as input X.\\n    - **Z'['o']**: Output of the linear transformation.\\n\\n**Example:**\\n\\n    >>> X = torch.randn(2, 10, 128)\\n    >>> mu = torch.randn(128)\\n    >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64)\\n    >>> Y, Z = unit(X, mu=mu)\\n\\n**Note:**\\n\\n    - The unit can optionally use a low-rank approximation through LoRA if `low_rank_dim` is specified.\\n    - LoRA is an existing GAU and is reused directly without declaring it as a child.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "low_rank_dim": null,
                            "output_dim": null
                        },
                        "design_traces": null
                    },
                    "LoRA": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_lora": "@gau_test\ndef test_LoRA_test_lora(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    lora = LoRA(embed_dim, block_loc, kwarg_all, output_dim=128,\n        low_rank_dim=32, device=device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y, _ = lora(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Optional\n\n\nclass LoRA(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: int, bias: Optional[bool]=True,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.bias = bias\n        self.lora = nn.Sequential(nn.Linear(embed_dim, low_rank_dim, bias=\n            False, device=device, dtype=dtype), nn.Tanh(), nn.Linear(\n            low_rank_dim, output_dim, bias=bias, device=device, dtype=dtype))\n\n    def __repr__(self) ->str:\n        s = f'{self.__class__.__name__}('\n        s += (\n            f'input_dim={self.input_dim}, low_rank_dim={self.low_rank_dim}, output_dim={self.output_dim}'\n            )\n        if not self.bias:\n            s += f', bias={self.bias}'\n        s += ')'\n        return s\n\n    def _forward(self, X, **Z):\n        return X, {'o': self.lora(X)}\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"LoRA\",\"document\":\"\\nLoRA\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "LerpLinear": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_lerplinear": "@gau_test\ndef test_LerpLinear_test_lerplinear(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    lerplinear = LerpLinear(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype, **kwarg_all)\n    X = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = lerplinear(X)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Optional\n\n\nclass LerpLinear(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: Optional[int]=None, device=None,\n        dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if self.low_rank_dim is None:\n            self.linear = nn.Linear(embed_dim, output_dim, bias=False,\n                device=device, dtype=dtype)\n        else:\n            kwarg_all['output_dim'] = output_dim\n            kwarg_all['low_rank_dim'] = low_rank_dim\n            self.linear = LoRA(embed_dim=self.embed_dim, block_loc=self.\n                block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n                **self.kwarg_all)\n        self.mu = nn.Parameter(torch.zeros(embed_dim, device=device, dtype=\n            dtype))\n\n    def __repr__(self) ->str:\n        s = f'{self.__class__.__name__}({self.input_dim}, {self.output_dim}'\n        if self.low_rank_dim is not None:\n            s += f', low_rank_dim={self.low_rank_dim}'\n        s += ')'\n        return s\n\n    def _forward(self, X: torch.Tensor, delta: Optional[torch.Tensor]=None\n        ) ->torch.Tensor:\n        if delta is None:\n            shifted = self.time_shift(X)\n            if len(shifted.shape) == 2:\n                shifted = shifted.unsqueeze(1)\n            delta = shifted - X\n        if self.low_rank_dim is None:\n            o = self.linear(X + delta * self.mu)\n        else:\n            o = self.linear(X + delta * self.mu)[1]['o']\n        return X, {'o': o}\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LoRA', requirements='', inputs=\n    ['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"LerpLinear\",\"document\":\"\\nLerpLinear\\n\",\"inputs\":[\"X\",\"delta\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LoRA"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RWKV6FeedForward": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rwkv6feedforward": "@gau_test\ndef test_RWKV6FeedForward_test_rwkv6feedforward(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6feedforward = RWKV6FeedForward(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rwkv6feedforward(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RWKV6FeedForward(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        hidden_ratio = 3.5\n        intermediate_size = int(embed_dim * hidden_ratio)\n        intermediate_size = 32 * ((intermediate_size + 32 - 1) // 32)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        kwarg_all['output_dim'] = intermediate_size\n        self.key = LerpLinear(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.value = nn.Linear(intermediate_size, embed_dim, bias=False,\n            device=device, dtype=dtype)\n        kwarg_all['output_dim'] = embed_dim\n        self.receptance = LerpLinear(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.relu = nn.ReLU()\n\n    def _forward(self, X, **Z):\n        shifted = self.time_shift(X)\n        delta = shifted - X\n        _key = self.key(X, **{'delta': delta})[1]['o']\n        r = self.relu(_key)\n        key = r * r\n        value = self.value(key)\n        receptance = self.receptance(X, **{'delta': delta})[1]['o']\n        return receptance.sigmoid() * value\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LerpLinear', requirements='',\n    inputs=['X', 'delta'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RWKV6FeedForward\",\"document\":\"\\nRWKV6FeedForward\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LerpLinear"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RWKV6": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rwkv6": "@gau_test\ndef test_RWKV6_test_rwkv6(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6 = RWKV6(embed_dim, block_loc, kwarg_all, device=device, dtype=\n        dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rwkv6(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RWKV6(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        norm_eps: float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=\n            norm_eps, **self.factory_kwargs)\n        self.attn = RWKV6Attention(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=\n            norm_eps, **self.factory_kwargs)\n        self.ffn = RWKV6FeedForward(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, _ = self.attn(self.attn_norm(X), **Z)\n        X = X1 + X\n        X2, _ = self.ffn(self.ffn_norm(X), **Z)\n        X = X2 + X\n        return X\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RWKV6Attention', requirements=\n    '', inputs=['X'], outputs=['Y']), UnitDecl(unitname='RWKV6FeedForward',\n    requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RWKV6\",\"document\":\"\\nRWKV6\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RWKV6Attention",
                            "RWKV6FeedForward"
                        ],
                        "suggestions": null,
                        "args": {
                            "norm_eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "RWKV6Attention": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rwkv6attention": "@gau_test\ndef test_RWKV6Attention_test_rwkv6attention(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6attention = RWKV6Attention(embed_dim, block_loc, kwarg_all, device\n        =device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y, _ = rwkv6attention(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom einops import rearrange\nfrom transformers.activations import ACT2FN\nfrom typing import Optional\n\n\nclass RWKV6Attention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        num_heads: int=4, gate_fn: str='swish', proj_low_rank_dim: int=32,\n        gate_low_rank_dim: int=64, elementwise_affine: Optional[bool]=True,\n        norm_eps: float=1e-05, chunk_size: int=32, device=None, dtype=None,\n        **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.num_heads = num_heads\n        self.proj_low_rank_dim = proj_low_rank_dim\n        self.gate_low_rank_dim = gate_low_rank_dim\n        self.chunk_size = chunk_size\n        self.key_dim = embed_dim // 2\n        self.value_dim = embed_dim\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        kwarg_all['output_dim'] = proj_low_rank_dim * 5\n        self.x_proj = nn.Sequential(LerpLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all), nn.Tanh(), nn.Linear(\n            proj_low_rank_dim * 5, embed_dim, bias=False, device=device,\n            dtype=dtype))\n        self.x_bias = nn.Parameter(torch.zeros(5, embed_dim, device=device,\n            dtype=dtype))\n        kwarg_all['output_dim'] = self.key_dim\n        self.r_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['low_rank_dim'] = gate_low_rank_dim\n        self.w_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all.pop('low_rank_dim')\n        self.k_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['output_dim'] = self.value_dim\n        self.v_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['low_rank_dim'] = gate_low_rank_dim\n        self.g_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.bonus = nn.Parameter(torch.zeros(num_heads, self.head_qk_dim,\n            device=device, dtype=dtype))\n        self.g_norm = nn.LayerNorm(self.value_dim, elementwise_affine=\n            elementwise_affine, eps=norm_eps, device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, embed_dim, bias=False,\n            device=device, dtype=dtype)\n        self.gate_fn = ACT2FN[gate_fn]\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.Parameter):\n            nn.init.xavier_uniform_(module, gain=2 ** -2.5)\n        module._is_hf_initialized = True\n\n    def naive_chunk_rwkv6(self, q: torch.Tensor, k: torch.Tensor, v: torch.\n        Tensor, w: torch.Tensor, u: torch.Tensor, chunk_size: int=32):\n        assert q.shape[-2] % chunk_size == 0\n        orig_dtype = q.dtype\n        num_chunk = q.shape[-2] // chunk_size\n        u = u.unsqueeze(0)\n        q, k, v, w = map(lambda x: rearrange(x, 'b h (n c) d -> b h n c d',\n            c=chunk_size).float(), (q, k, v, w))\n        w_cumsum = w.cumsum(-2)\n        kw = k * (w_cumsum[..., -1, None, :] - w_cumsum).exp()\n        wkv = kw.transpose(-1, -2) @ v\n        wkv_new = torch.zeros_like(wkv)\n        for i in range(num_chunk - 1):\n            wkv_new[:, :, i + 1] = wkv_new[:, :, i].clone() * w_cumsum[:, :,\n                i, -1, :, None].exp() + wkv[:, :, i]\n        o_inter = torch.einsum('b h n d p, b h n c d -> b h n c p', wkv_new,\n            q * (w_cumsum - w).exp())\n        o_intra = torch.zeros_like(o_inter)\n        for i in range(chunk_size):\n            attn = (q[:, :, :, i, None] * k * (w_cumsum[:, :, :, i, None] -\n                w[:, :, :, i, None] - w_cumsum).exp()).sum(-1)\n            mask = (torch.arange(0, chunk_size) < i).to(attn.device)\n            attn.masked_fill_(~mask, 0)\n            intra_inter_o = (attn.unsqueeze(-1) * v).sum(-2)\n            intra_intra_o = (q[:, :, :, i] * u.unsqueeze(2) * k[:, :, :, i]\n                ).sum(-1).unsqueeze(-1) * v[:, :, :, i]\n            o_intra[:, :, :, i] = intra_inter_o + intra_intra_o\n        o = o_inter + o_intra\n        return rearrange(o, 'b h n c d -> b h (n c) d').to(orig_dtype)\n\n    def pad_input(self, X):\n        _seq_len = X.shape[-2]\n        pad_len = (X.shape[-2] + self.chunk_size - 1\n            ) // self.chunk_size * self.chunk_size - X.shape[-2]\n        return F.pad(X, (0, 0, 0, pad_len)), _seq_len\n\n    def _forward(self, X: torch.Tensor):\n        X, _seq_len = self.pad_input(X)\n        batch_size, seq_len, hidden_size = X.shape\n        last_state = None\n        if X.shape[1] == 1 and last_state is not None:\n            shifted = last_state[0].unsqueeze(1)\n        else:\n            shifted = self.time_shift(X)\n            if last_state is not None:\n                shifted[:, 0] = last_state[0]\n        delta = shifted - X\n        x = self.x_proj[0](X, **{'delta': delta})[1]['o'].view(batch_size,\n            seq_len, -1, self.proj_low_rank_dim)\n        x = torch.einsum('b l n r, h n r-> b l n h', self.x_proj[1](x),\n            self.x_proj[2].weight.view(hidden_size, 5, -1))\n        r, w, k, v, g = x.add_(self.x_bias).unbind(-2)\n        r = self.r_proj(X, **{'mu': r, 'delta': delta})[1]['o']\n        w = self.w_proj(X, **{'mu': w, 'delta': delta})[1]['o']\n        k = self.k_proj(X, **{'mu': k, 'delta': delta})[1]['o']\n        v = self.v_proj(X, **{'mu': v, 'delta': delta})[1]['o']\n        g = self.g_proj(X, **{'mu': g, 'delta': delta})[1]['o']\n        r, w, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=\n            self.num_heads), (r, w, k, v))\n        w = -torch.exp(w)\n        u = self.bonus\n        o = self.naive_chunk_rwkv6(r, k, v, w, u, chunk_size=self.chunk_size)\n        o = rearrange(o, 'b h l d -> b l (h d)')\n        o = self.g_norm(o)\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        o = o[:, :_seq_len]\n        return o\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LerpLinear', requirements='',\n    inputs=['X', 'delta'], outputs=['Y']), UnitDecl(unitname='DDLerpLinear',\n    requirements='', inputs=['X', 'mu', 'delta'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RWKV6Attention\",\"document\":\"\\nRWKV6Attention\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LerpLinear",
                            "DDLerpLinear"
                        ],
                        "suggestions": null,
                        "args": {
                            "proj_low_rank_dim": 32,
                            "gate_low_rank_dim": 64,
                            "elementwise_affine": true,
                            "gate_fn": "swish",
                            "num_heads": 4,
                            "chunk_size": 32
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "DDLerpLinear": "{\"unitname\":\"DDLerpLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "rwkv6_event"
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0,
                "IMPLEMENTATION_CODER": 0,
                "PROPOSAL_REVIEWER": 0,
                "SEARCH_ASSISTANT": 0,
                "IMPLEMENTATION_OBSERVER": 0
            },
            "status": "implemented",
            "user_input": "",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "claude3.5_sonnet",
                    "IMPLEMENTATION_CODER": "o1_mini",
                    "PROPOSAL_REVIEWER": "o1_preview",
                    "IMPLEMENTATION_OBSERVER": "o1_mini",
                    "SEARCH_ASSISTANT": "None"
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            }
        },
        {
            "tree": {
                "review": null,
                "root": "RWKV6",
                "proposal": "We present Eagle (RWKV-5) and Finch (RWKV-6), sequence models improving upon the RWKV (RWKV-4) architecture. Our architectural design advancements include multi-headed matrix-valued states and a dynamic recurrence mechanism that improve expressivity while maintaining the inference efficiency characteristics of RNNs. We introduce a new multilingual corpus with 1.12 trillion tokens and a fast tokenizer based on greedy matching for enhanced multilinguality. We trained four Eagle models, ranging from 0.46 to 7.5 billion parameters, and two Finch models with 1.6 and 3.1 billion parameters and find that they achieve competitive performance across a wide variety of benchmarks. We release all our models on HuggingFace under the Apache 2.0 license. Models at: this https URL Training code at: this https URL Inference code at: this https URL Time-parallel training code at: this https URL",
                "units": {
                    "DDLerpLinear": {
                        "review": "```rating 3.0\n```\n\n### Strengths of the Implementation\n\n1. **Modular and Hierarchical Design**: The implementation maintains a clear modular structure by utilizing nested GAUs (`LoRA`, `LerpLinear`, etc.), which aligns well with the proposed framework. This modularity facilitates easier debugging, testing, and future extensions.\n\n2. **Comprehensive Documentation**: The `DDLerpLinear` class includes detailed docstrings that explain its functionality, mathematical formulation, arguments, inputs, outputs, and usage examples. This level of documentation enhances code readability and maintainability, making it easier for team members to understand and work with the codebase.\n\n3. **Conditional LoRA Integration**: The implementation correctly integrates the `LoRA` module conditionally based on the `low_rank_dim` parameter. This flexibility allows for varying model complexities and can lead to better parameter efficiency when needed.\n\n4. **Proper Parameter Initialization**: The use of `nn.Parameter` for `mu` ensures that it is registered as a learnable parameter within the model, enabling it to receive gradients during training. This is crucial for the dynamic depth adjustment intended by the `DDLerpLinear` design.\n\n5. **Adherence to Base Class Contracts**: The `DDLerpLinear` class correctly inherits from `GAUBase` and implements the required `_forward` method, maintaining consistency with the GAU framework. This adherence ensures that the GAU integrates seamlessly within the larger model architecture.\n\n### Areas for Improvement and Specific Suggestions\n\n1. **Addressing CHILDREN_DECLARATIONS Warning**:\n\n   - **Issue**: The format checker issues a warning stating that no `CHILDREN_DECLARATIONS` are found in the `DDLerpLinear` GAU. This occurs despite `LoRA` being conditionally integrated into `DDLerpLinear`.\n\n   - **Analysis**: The absence of `CHILDREN_DECLARATIONS` means that the parser assumes `DDLerpLinear` has no child GAUs. However, when `low_rank_dim` is specified, `DDLerpLinear` utilizes `LoRA` as a child unit. Not declaring `LoRA` as a child leads to inconsistency between the declared architecture and the actual usage.\n\n   - **Solution**:\n     - **Declare CHILDREN_DECLARATIONS Appropriately**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is used. This can be achieved by conditionally appending to the `CHILDREN_DECLARATIONS` based on the presence of `low_rank_dim`.\n     \n     - **Implementation Example**:\n       \n       ```python\n       CHILDREN_DECLARATIONS = []\n       if self.low_rank_dim is not None:\n           CHILDREN_DECLARATIONS = [\n               UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n           ]\n       ```\n     \n     - **Alternatively**, declare all possible child GAUs and manage their usage within the class. This ensures that the hierarchy is explicitly defined, preventing parser warnings.\n     \n       ```python\n       CHILDREN_DECLARATIONS = [\n           UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n       ]\n       ```\n     \n     - **Adjust Reformatter Settings**: Modify the reformatter to preserve `CHILDREN_DECLARATIONS`. This might involve adding specific markers or comments that instruct the reformatter not to alter these critical lines.\n     \n     - **Implement Post-Formatting Scripts**: Develop scripts that automatically reinsert or correct `CHILDREN_DECLARATIONS` after the code is processed by the reformatter.\n\n2. **Optimizing Training Efficiency**:\n\n   - **Issue**: While the functionality checker now passes, previous iterations indicated efficiency concerns. Ensuring that the model remains efficient is crucial for scalability.\n   \n   - **Suggestions**:\n     - **Vectorize Operations**: Replace explicit Python loops in methods like `naive_chunk_rwkv6` with vectorized PyTorch operations to leverage optimized backend implementations and GPU acceleration.\n     \n     - **Profile the Model**: Utilize profiling tools such as PyTorch Profiler to identify bottlenecks in the computation graph and optimize accordingly.\n     \n     - **Simplify LoRA Integration**: Assess whether the current integration of `LoRA` introduces significant computational overhead. If so, explore more efficient architectures or parameter-sharing mechanisms.\n     \n     - **Batch Operations**: Ensure that tensor operations are batched appropriately to maximize parallelism and hardware utilization.\n\n3. **Code Organization and Maintainability**:\n\n   - **Issue**: The `gab.py` file contains multiple GAU class definitions, which can lead to confusion and maintenance challenges.\n   \n   - **Suggestions**:\n     - **Separate GAUs into Individual Files**: Organize each GAU into its own module or file. This separation enhances code clarity and makes it easier to navigate and manage individual components.\n     \n     - **Remove Duplicated Class Definitions**: Ensure that each GAU class is defined uniquely without duplications to prevent namespace conflicts and unexpected behaviors.\n     \n     - **Implement Consistent Naming Conventions**: Adopt consistent and descriptive naming conventions for classes and files to facilitate easier identification and usage.\n\n4. **Ensuring Comprehensive Testing**:\n\n   - **Issue**: While unit tests passed, ensuring comprehensive coverage is vital to prevent future regressions.\n   \n   - **Suggestions**:\n     - **Implement Gradient Flow Tests**: Beyond basic forward passes, include tests that verify whether gradients flow correctly through all learnable parameters, especially `mu`.\n     \n     - **Efficiency Benchmarking**: Regularly benchmark the model\u2019s training and inference times against established baselines to monitor and address any performance regressions promptly.\n     \n     - **Causality Tests**: Ensure that the model maintains causality, especially after modifications that introduce dynamic depth adjustments.\n\n5. **Documentation Enhancements**:\n\n   - **Issue**: Although the docstrings are comprehensive, ensuring that all classes and their relationships are well-documented is essential.\n   \n   - **Suggestions**:\n     - **Document Conditional Logic**: Clearly document the conditions under which certain child GAUs (like `LoRA`) are used. This transparency aids in understanding the model\u2019s architecture and behavior.\n     \n     - **Provide Usage Examples**: While examples are provided in the docstrings, including more diverse examples covering different configurations can enhance understanding.\n\n### Comments on Innovation and Potential Impact\n\nThe implementation of `DDLerpLinear` introduces a dynamic depth mechanism through the learnable parameter `mu`, aligning well with the proposal's objective of adaptable computation based on input significance. The conditional integration of `LoRA` further enhances parameter efficiency, allowing the model to scale more effectively without a proportional increase in computational resources.\n\n**Potential Impact**:\n\n- **Enhanced Computational Efficiency**: By dynamically adjusting computational depth, the model can allocate resources more effectively, potentially reducing unnecessary computations for less important inputs.\n\n- **Improved Scalability**: The combination of dynamic depth and low-rank adaptations facilitates better scalability, enabling the model to handle larger datasets and more complex tasks without exorbitant increases in computational or memory overhead.\n\n- **Flexibility and Adaptability**: The ability to conditionally integrate components like `LoRA` based on configuration parameters allows for greater flexibility in model design, catering to diverse application requirements.\n\n**Concerns**:\n\n- **Integration Complexity**: The conditional usage of `LoRA` introduces additional layers of complexity in managing dependencies and ensuring consistent behavior across different configurations.\n\n- **Maintenance Overhead**: Ensuring that `CHILDREN_DECLARATIONS` accurately reflect the model\u2019s architecture requires disciplined coding practices and possibly additional tooling to automate verification.\n\n### Detailed Analysis of Failed Checks\n\n#### Format Checker Issues\n\n- **Warning**: *\"Code block 1 of DDLerpLinear: Warning: No CHILDREN_DECLARATIONS found in the GAU. Will assume there is no children.\"*\n\n  **Analysis**:\n\n  - The `DDLerpLinear` GAU conditionally integrates the `LoRA` module based on the `low_rank_dim` parameter. However, the current implementation does not declare `LoRA` in `CHILDREN_DECLARATIONS`, leading the format checker to assume that there are no child GAUs.\n\n  - This discrepancy between actual usage and declarations can lead to issues in the model's structural integrity, as the parser relies on `CHILDREN_DECLARATIONS` to understand GAU hierarchies and dependencies.\n\n  **Recommendations**:\n\n  - **Declare Child GAUs Appropriately**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is utilized. This ensures that the parser accurately recognizes and manages all child GAUs.\n\n    ```python\n    class DDLerpLinear(GAUBase):\n        ...\n        CHILDREN_DECLARATIONS = []\n        if low_rank_dim is not None:\n            CHILDREN_DECLARATIONS = [\n                UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n            ]\n        ...\n    ```\n    \n  - **Avoid Self-Declaration**: Ensure that `DDLerpLinear` does not list itself as a child, which can cause recursive parsing issues.\n\n  - **Consistent Declaration Practices**: Adopt a consistent approach across all GAUs in declaring children. Whether or not a child GAU is used, its declaration should reflect the actual dependencies to prevent format checker warnings.\n\n  - **Adjust Reformatter Settings**: Modify the code reformatter to preserve critical declarations like `CHILDREN_DECLARATIONS`. This might involve marking these lines with specific comments or patterns that instruct the reformatter to retain them.\n\n  - **Post-Formatting Validation**: Implement scripts or pre-commit hooks that verify the presence and correctness of `CHILDREN_DECLARATIONS` after formatting processes.\n\n#### Functionality Checker Overview\n\n- **Status**: Functionality check passed.\n\n  **Note**: Unlike previous iterations where differentiability tests failed due to `mu` parameters lacking gradients, the current implementation passes the functionality checker. This indicates that issues related to parameter gradient flow have been addressed, leading to successful forward and backward passes without causality violations.\n\n### Recommendations for the Coder\n\n1. **Immediately Address CHILDREN_DECLARATIONS Warning**:\n\n   - **Action**: Update the `DDLerpLinear` class to include `CHILDREN_DECLARATIONS` when `LoRA` is used. This alignment is crucial to satisfy the format checker and ensure that the model's hierarchical structure is accurately represented.\n   \n   - **Implementation Example**:\n     \n     ```python\n     CHILDREN_DECLARATIONS = []\n     if self.low_rank_dim is not None:\n         CHILDREN_DECLARATIONS = [\n             UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n         ]\n     ```\n   \n   - **Alternatively**, if conditional declarations are not feasible within the class, always declare `LoRA` as a potential child GAU, and manage its usage within the class logic.\n   \n     ```python\n     CHILDREN_DECLARATIONS = [\n         UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n     ]\n     ```\n\n2. **Enhance Training Efficiency**:\n\n   - **Action**: Optimize the `naive_chunk_rwkv6` method by replacing explicit Python loops with vectorized PyTorch operations. This change can significantly reduce training time and improve scalability.\n   \n   - **Implementation Suggestion**:\n     \n     - **Vectorize Attention Computations**: Utilize batch matrix operations and PyTorch's optimized tensor functions to handle chunk processing without Python-level loops.\n     \n     - **Example Refactoring**:\n       \n       ```python\n       def optimized_chunk_rwkv6(self, q, k, v, w, u, chunk_size=32):\n           # Example of vectorizing chunk processing\n           # Placeholder for optimized implementation\n           pass\n       ```\n   \n   - **Profile and Benchmark**: Regularly use profiling tools to identify and address bottlenecks in the model's computation graph.\n\n3. **Improve Code Organization**:\n\n   - **Action**: Refactor the codebase to separate each GAU into individual modules or files. This separation enhances maintainability and reduces the risk of class duplication or namespace conflicts.\n   \n   - **Implementation Example**:\n     \n     ```bash\n     model_discovery/\n         model/\n             utils/\n                 modules.py\n             gars/\n                 DDLerpLinear.py\n                 LoRA.py\n                 LerpLinear.py\n                 RWKV6.py\n                 RWKV6Attention.py\n                 RWKV6FeedForward.py\n                 ...\n     ```\n   \n   - **Benefits**:\n     - **Enhanced Clarity**: Clear separation of components makes the codebase easier to navigate.\n     - **Simplified Maintenance**: Isolated modules allow for targeted updates and bug fixes without affecting unrelated components.\n\n4. **Ensure Comprehensive Testing**:\n\n   - **Action**: Implement additional unit tests that specifically verify gradient flow through all learnable parameters, especially `mu`.\n   \n   - **Implementation Example**:\n     \n     ```python\n     def test_mu_gradient():\n         embed_dim = 128\n         block_loc = (0, 1)\n         output_dim = 64\n         unit = DDLerpLinear(embed_dim, block_loc, {}, output_dim)\n         X = torch.randn(2, 10, embed_dim, requires_grad=True)\n         mu = torch.randn(embed_dim, requires_grad=True)\n         Y, Z = unit(X, mu=mu)\n         loss = Y.sum()\n         loss.backward()\n         assert unit.mu.grad is not None, \"Gradients not flowing to 'mu'.\"\n     \n     test_mu_gradient()\n     ```\n   \n   - **Benefits**:\n     - **Early Detection of Issues**: Ensures that all parameters are correctly integrated into the computation graph.\n     - **Increased Confidence**: Validates the model's learnability and effectiveness during training.\n\n5. **Maintain CHILDREN_DECLARATIONS Integrity**:\n\n   - **Action**: Ensure that `CHILDREN_DECLARATIONS` are consistently and accurately maintained across all GAUs. Implement mechanisms to protect these declarations from being altered by automated tools like code reformatters.\n   \n   - **Implementation Suggestions**:\n     - **Use Preservation Comments**: Add comments that instruct reformatters to ignore specific sections.\n       \n       ```python\n       # fmt: off\n       CHILDREN_DECLARATIONS = [\n           UnitDecl(unitname='LoRA', requirements='', inputs=['X'], outputs=['Y'])\n       ]\n       # fmt: on\n       ```\n     \n     - **Post-Formatting Scripts**: Develop scripts that check and restore `CHILDREN_DECLARATIONS` after code is formatted.\n   \n   - **Benefits**:\n     - **Consistent Architecture**: Prevents discrepancies between declared and actual GAU hierarchies.\n     - **Automation Safety**: Reduces manual intervention by ensuring critical declarations are preserved automatically.\n\n6. **Enhance Documentation Further**:\n\n   - **Action**: Expand documentation to include more diverse usage examples, especially showcasing different configurations of `DDLerpLinear` with and without `LoRA`.\n   \n   - **Implementation Example**:\n     \n     ```python\n     \"\"\"\n     **Example with LoRA**:\n     \n         >>> X = torch.randn(2, 10, 128)\n         >>> mu = torch.randn(128)\n         >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64, low_rank_dim=32)\n         >>> Y, Z = unit(X, mu=mu)\n     \n     **Example without LoRA**:\n     \n         >>> X = torch.randn(2, 10, 128)\n         >>> mu = torch.randn(128)\n         >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=128)\n         >>> Y, Z = unit(X, mu=mu)\n     \"\"\"\n     ```\n   \n   - **Benefits**:\n     - **Improved Clarity**: Helps users understand how to configure and utilize the GAU in different scenarios.\n     - **Facilitates Adoption**: Clear examples encourage correct and efficient usage of the GAU components.\n\n7. **Review and Clean Up Code Duplication**:\n\n   - **Action**: Ensure that GAU classes are not duplicated across different sections or files. Centralize common functionalities to avoid redundancy.\n   \n   - **Implementation Example**:\n     \n     - **Abstract Common Components**: If multiple GAUs share similar structures or components, abstract these into base classes or helper functions.\n     \n     - **Example**:\n       \n       ```python\n       class BaseLinearGAU(GAUBase):\n           def __init__(self, embed_dim, block_loc, kwarg_all, output_dim, low_rank_dim=None, device=None, dtype=None, **kwargs):\n               super().__init__(embed_dim, block_loc, kwarg_all)\n               self.output_dim = output_dim\n               self.low_rank_dim = low_rank_dim\n               self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n               if self.low_rank_dim is None:\n                   self.linear = nn.Linear(embed_dim, output_dim, bias=False, device=device, dtype=dtype)\n               else:\n                   kwarg_all['output_dim'] = output_dim\n                   kwarg_all['low_rank_dim'] = low_rank_dim\n                   self.linear = LoRA(embed_dim=embed_dim, block_loc=block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **kwarg_all)\n       ```\n   \n   - **Benefits**:\n     - **Reduced Redundancy**: Centralizing common functionalities minimizes code duplication.\n     - **Enhanced Maintainability**: Changes to shared components need to be made only once, propagating across all dependent GAUs.\n\n### General Recommendations\n\n1. **Implement Comprehensive Testing**:\n   \n   - Beyond passing current functionality checks, implement more exhaustive tests covering various configurations and potential edge cases. This ensures robustness and reliability of the GAUs under diverse scenarios.\n\n2. **Maintain Consistent Coding Standards**:\n   \n   - Adhere to consistent coding conventions and standards across all GAU implementations. This uniformity enhances readability and facilitates smoother collaboration within the team.\n\n3. **Regularly Review and Refactor Code**:\n   \n   - Periodically assess the codebase for potential optimizations, refactorings, and improvements. This proactive approach helps in sustaining code quality and performance over time.\n\n4. **Enhance Collaboration and Communication**:\n   \n   - Document the architectural decisions, especially around conditional GAU integrations and hierarchical structures. Clear communication ensures that all team members are aligned and understand the model's design principles.\n\n### Conclusion\n\nThe current implementation of `DDLerpLinear` demonstrates significant progress with its modular design, comprehensive documentation, and conditional integration of `LoRA`. Addressing the `CHILDREN_DECLARATIONS` warning is essential to ensure that the GAU hierarchy is accurately represented and that the model functions as intended. By implementing the recommended improvements, particularly around declaration accuracy and training efficiency, the `DDLerpLinear` GAU can effectively contribute to the robustness, scalability, and efficiency of the overall language model architecture.",
                        "requirements": "N/A",
                        "reuse_from": null,
                        "desc": null,
                        "gautests": {
                            "test_ddlerp_linear": "@gau_test\ndef test_DDLerpLinear_test_ddlerp_linear(device=None, dtype=None) ->None:\n    embed_dim = 128\n    output_dim = 64\n    batch_size = 2\n    seq_len = 5\n    X = torch.randn(batch_size, seq_len, embed_dim, device=device, dtype=\n        dtype, requires_grad=True)\n    mu = torch.randn(embed_dim, device=device, dtype=dtype, requires_grad=True)\n    unit = DDLerpLinear(embed_dim=embed_dim, block_loc=(0, 1), kwarg_all={},\n        output_dim=output_dim, device=device, dtype=dtype)\n    Y, Z = unit(X, mu=mu)\n    assert Y.shape == X.shape, f'Expected Y.shape to be {X.shape}, got {Y.shape}'\n    assert 'o' in Z, \"Output 'o' not found in Z\"\n    assert Z['o'].shape == (batch_size, seq_len, output_dim\n        ), f\"Expected Z['o'].shape to be {batch_size, seq_len, output_dim}, got {Z['o'].shape}\"\n    loss = Z['o'].sum()\n    loss.backward()\n    assert mu.grad is not None, 'Gradient did not flow back to mu'\n    assert mu.grad.shape == mu.shape, f'Expected mu.grad.shape to be {mu.shape}, got {mu.grad.shape}'\n    print('test_ddlerp_linear without low_rank_dim passed')\n    mu.grad.zero_()\n    low_rank_dim = 16\n    unit_lora = DDLerpLinear(embed_dim=embed_dim, block_loc=(0, 1),\n        kwarg_all={}, output_dim=output_dim, low_rank_dim=low_rank_dim,\n        device=device, dtype=dtype)\n    Y_lora, Z_lora = unit_lora(X, mu=mu)\n    assert Y_lora.shape == X.shape, f'Expected Y.shape to be {X.shape}, got {Y_lora.shape}'\n    assert 'o' in Z_lora, \"Output 'o' not found in Z\"\n    assert Z_lora['o'].shape == (batch_size, seq_len, output_dim\n        ), f\"Expected Z['o'].shape to be {batch_size, seq_len, output_dim}, got {Z_lora['o'].shape}\"\n    loss = Z_lora['o'].sum()\n    loss.backward()\n    assert mu.grad is not None, 'Gradient did not flow back to mu in LoRA case'\n    assert mu.grad.shape == mu.shape, f'Expected mu.grad.shape to be {mu.shape}, got {mu.grad.shape}'\n    print('test_ddlerp_linear with low_rank_dim passed')\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\nfrom typing import Optional\n\n\nclass DDLerpLinear(GAUBase):\n    \"\"\"\n    DDLerpLinear (Dynamic Depth Lerp Linear) applies a linear transformation to the input `X` after adjusting it with a delta scaled by an input parameter `mu`.\n\n    **Mathematical Formulation:**\n\n    .. math::\n\n        Y = X, \\\\\n        o = \text{Linear}(X + \\\\mu \\\\cdot \\\\delta)\n\n    Where:\n        - `X` is the input tensor of shape (B, L, D).\n        - `\\\\delta` is the difference between shifted and current input.\n        - `\\\\mu` is an input tensor that scales the delta.\n\n    If `low_rank_dim` is specified, it uses a low-rank approximation through LoRA.\n\n    **Args:**\n\n        embed_dim (int): Embedding dimension of the input and output.\n        block_loc (tuple): Location of the block within the network.\n        kwarg_all (dict): Dictionary of all keyword arguments.\n        output_dim (int): Dimension of the output.\n        low_rank_dim (Optional[int], optional): Low-rank dimension for LoRA. Defaults to None.\n        device (optional): Device to place the model on.\n        dtype (optional): Data type of the model parameters.\n\n    **Inputs:**\n\n        - **X**: Input tensor of shape (batch, seq_len, embed_dim).\n        - **mu**: Scaling parameter tensor of shape (embed_dim).\n        - **delta** (optional): Delta tensor of shape (batch, seq_len, embed_dim). If None, it is computed internally.\n\n    **Outputs:**\n\n        - **Y**: Output tensor of same shape as input X.\n        - **Z'['o']**: Output of the linear transformation.\n\n    **Example:**\n\n        >>> X = torch.randn(2, 10, 128)\n        >>> mu = torch.randn(128)\n        >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64)\n        >>> Y, Z = unit(X, mu=mu)\n\n    **Note:**\n\n        - The unit can optionally use a low-rank approximation through LoRA if `low_rank_dim` is specified.\n        - LoRA is an existing GAU and is reused directly without declaring it as a child.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: Optional[int]=None, device=None,\n        dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if self.low_rank_dim is None:\n            self.linear = nn.Linear(embed_dim, output_dim, bias=False,\n                device=device, dtype=dtype)\n        else:\n            kwarg_all['output_dim'] = output_dim\n            kwarg_all['low_rank_dim'] = low_rank_dim\n            self.linear = LoRA(embed_dim=embed_dim, block_loc=self.\n                block_loc, kwarg_all=kwarg_all, **self.factory_kwargs, **\n                kwarg_all)\n\n    def _forward(self, X: torch.Tensor, mu: torch.Tensor, delta: Optional[\n        torch.Tensor]=None, **Z):\n        if delta is None:\n            shifted = self.time_shift(X)\n            delta = shifted - X\n        adjusted_input = X + delta * mu\n        if self.low_rank_dim is None:\n            o = self.linear(adjusted_input)\n        else:\n            Y_lora, Z_lora = self.linear(adjusted_input)\n            o = Z_lora['o']\n        return X, {'o': o}\n",
                        "rating": 3.0,
                        "spec": "{\"unitname\":\"DDLerpLinear\",\"document\":\"DDLerpLinear (Dynamic Depth Lerp Linear) applies a linear transformation to the input `X` after adjusting it with a delta scaled by an input parameter `mu`.\\n\\n**Mathematical Formulation:**\\n\\n.. math::\\n\\n    Y = X, \\\\\\n    o =     ext{Linear}(X + \\\\mu \\\\cdot \\\\delta)\\n\\nWhere:\\n    - `X` is the input tensor of shape (B, L, D).\\n    - `\\\\delta` is the difference between shifted and current input.\\n    - `\\\\mu` is an input tensor that scales the delta.\\n\\nIf `low_rank_dim` is specified, it uses a low-rank approximation through LoRA.\\n\\n**Args:**\\n\\n    embed_dim (int): Embedding dimension of the input and output.\\n    block_loc (tuple): Location of the block within the network.\\n    kwarg_all (dict): Dictionary of all keyword arguments.\\n    output_dim (int): Dimension of the output.\\n    low_rank_dim (Optional[int], optional): Low-rank dimension for LoRA. Defaults to None.\\n    device (optional): Device to place the model on.\\n    dtype (optional): Data type of the model parameters.\\n\\n**Inputs:**\\n\\n    - **X**: Input tensor of shape (batch, seq_len, embed_dim).\\n    - **mu**: Scaling parameter tensor of shape (embed_dim).\\n    - **delta** (optional): Delta tensor of shape (batch, seq_len, embed_dim). If None, it is computed internally.\\n\\n**Outputs:**\\n\\n    - **Y**: Output tensor of same shape as input X.\\n    - **Z'['o']**: Output of the linear transformation.\\n\\n**Example:**\\n\\n    >>> X = torch.randn(2, 10, 128)\\n    >>> mu = torch.randn(128)\\n    >>> unit = DDLerpLinear(128, (0, 1), {}, output_dim=64)\\n    >>> Y, Z = unit(X, mu=mu)\\n\\n**Note:**\\n\\n    - The unit can optionally use a low-rank approximation through LoRA if `low_rank_dim` is specified.\\n    - LoRA is an existing GAU and is reused directly without declaring it as a child.\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {
                            "low_rank_dim": null,
                            "output_dim": null
                        },
                        "design_traces": null
                    },
                    "LoRA": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_lora": "@gau_test\ndef test_LoRA_test_lora(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    lora = LoRA(embed_dim, block_loc, kwarg_all, output_dim=128,\n        low_rank_dim=32, device=device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y, _ = lora(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Optional\n\n\nclass LoRA(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: int, bias: Optional[bool]=True,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.bias = bias\n        self.lora = nn.Sequential(nn.Linear(embed_dim, low_rank_dim, bias=\n            False, device=device, dtype=dtype), nn.Tanh(), nn.Linear(\n            low_rank_dim, output_dim, bias=bias, device=device, dtype=dtype))\n\n    def __repr__(self) ->str:\n        s = f'{self.__class__.__name__}('\n        s += (\n            f'input_dim={self.input_dim}, low_rank_dim={self.low_rank_dim}, output_dim={self.output_dim}'\n            )\n        if not self.bias:\n            s += f', bias={self.bias}'\n        s += ')'\n        return s\n\n    def _forward(self, X, **Z):\n        return X, {'o': self.lora(X)}\n\n\nCHILDREN_DECLARATIONS = []\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"LoRA\",\"document\":\"\\nLoRA\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "LerpLinear": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_lerplinear": "@gau_test\ndef test_LerpLinear_test_lerplinear(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    lerplinear = LerpLinear(embed_dim, block_loc, kwarg_all, device=device,\n        dtype=dtype, **kwarg_all)\n    X = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = lerplinear(X)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom typing import Optional\n\n\nclass LerpLinear(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        output_dim: int, low_rank_dim: Optional[int]=None, device=None,\n        dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.input_dim = embed_dim\n        self.output_dim = output_dim\n        self.low_rank_dim = low_rank_dim\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        if self.low_rank_dim is None:\n            self.linear = nn.Linear(embed_dim, output_dim, bias=False,\n                device=device, dtype=dtype)\n        else:\n            kwarg_all['output_dim'] = output_dim\n            kwarg_all['low_rank_dim'] = low_rank_dim\n            self.linear = LoRA(embed_dim=self.embed_dim, block_loc=self.\n                block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n                **self.kwarg_all)\n        self.mu = nn.Parameter(torch.zeros(embed_dim, device=device, dtype=\n            dtype))\n\n    def __repr__(self) ->str:\n        s = f'{self.__class__.__name__}({self.input_dim}, {self.output_dim}'\n        if self.low_rank_dim is not None:\n            s += f', low_rank_dim={self.low_rank_dim}'\n        s += ')'\n        return s\n\n    def _forward(self, X: torch.Tensor, delta: Optional[torch.Tensor]=None\n        ) ->torch.Tensor:\n        if delta is None:\n            shifted = self.time_shift(X)\n            if len(shifted.shape) == 2:\n                shifted = shifted.unsqueeze(1)\n            delta = shifted - X\n        if self.low_rank_dim is None:\n            o = self.linear(X + delta * self.mu)\n        else:\n            o = self.linear(X + delta * self.mu)[1]['o']\n        return X, {'o': o}\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LoRA', requirements='', inputs=\n    ['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"LerpLinear\",\"document\":\"\\nLerpLinear\\n\",\"inputs\":[\"X\",\"delta\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LoRA"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RWKV6FeedForward": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rwkv6feedforward": "@gau_test\ndef test_RWKV6FeedForward_test_rwkv6feedforward(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6feedforward = RWKV6FeedForward(embed_dim, block_loc, kwarg_all,\n        device=device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y = rwkv6feedforward(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RWKV6FeedForward(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        hidden_ratio = 3.5\n        intermediate_size = int(embed_dim * hidden_ratio)\n        intermediate_size = 32 * ((intermediate_size + 32 - 1) // 32)\n        self.hidden_ratio = hidden_ratio\n        self.intermediate_size = intermediate_size\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        kwarg_all['output_dim'] = intermediate_size\n        self.key = LerpLinear(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.value = nn.Linear(intermediate_size, embed_dim, bias=False,\n            device=device, dtype=dtype)\n        kwarg_all['output_dim'] = embed_dim\n        self.receptance = LerpLinear(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n        self.relu = nn.ReLU()\n\n    def _forward(self, X, **Z):\n        shifted = self.time_shift(X)\n        delta = shifted - X\n        _key = self.key(X, **{'delta': delta})[1]['o']\n        r = self.relu(_key)\n        key = r * r\n        value = self.value(key)\n        receptance = self.receptance(X, **{'delta': delta})[1]['o']\n        return receptance.sigmoid() * value\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LerpLinear', requirements='',\n    inputs=['X', 'delta'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RWKV6FeedForward\",\"document\":\"\\nRWKV6FeedForward\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LerpLinear"
                        ],
                        "suggestions": null,
                        "args": {},
                        "design_traces": null
                    },
                    "RWKV6": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rwkv6": "@gau_test\ndef test_RWKV6_test_rwkv6(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6 = RWKV6(embed_dim, block_loc, kwarg_all, device=device, dtype=\n        dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    Z = {}\n    y, Z_ = rwkv6(x, **Z)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\n\n\nclass RWKV6(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        norm_eps: float=1e-05, device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.attn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=\n            norm_eps, **self.factory_kwargs)\n        self.attn = RWKV6Attention(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn_norm = nn.LayerNorm(self.hidden_size, bias=True, eps=\n            norm_eps, **self.factory_kwargs)\n        self.ffn = RWKV6FeedForward(embed_dim=self.embed_dim, block_loc=\n            self.block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs,\n            **self.kwarg_all)\n\n    def _forward(self, X, **Z):\n        X1, _ = self.attn(self.attn_norm(X), **Z)\n        X = X1 + X\n        X2, _ = self.ffn(self.ffn_norm(X), **Z)\n        X = X2 + X\n        return X\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='RWKV6Attention', requirements=\n    '', inputs=['X'], outputs=['Y']), UnitDecl(unitname='RWKV6FeedForward',\n    requirements='', inputs=['X'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RWKV6\",\"document\":\"\\nRWKV6\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "RWKV6Attention",
                            "RWKV6FeedForward"
                        ],
                        "suggestions": null,
                        "args": {
                            "norm_eps": 1e-05
                        },
                        "design_traces": null
                    },
                    "RWKV6Attention": {
                        "review": null,
                        "requirements": null,
                        "reuse_from": null,
                        "desc": "\n",
                        "gautests": {
                            "test_rwkv6attention": "@gau_test\ndef test_RWKV6Attention_test_rwkv6attention(device=None, dtype=None):\n    embed_dim = 128\n    block_loc = 0, 6\n    kwarg_all = {}\n    rwkv6attention = RWKV6Attention(embed_dim, block_loc, kwarg_all, device\n        =device, dtype=dtype, **kwarg_all)\n    x = torch.randn(1, 100, 128).to(device=device, dtype=dtype)\n    y, _ = rwkv6attention(x)\n    assert y.shape == (1, 100, 128)\n"
                        },
                        "code": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nfrom einops import rearrange\nfrom transformers.activations import ACT2FN\nfrom typing import Optional\n\n\nclass RWKV6Attention(GAUBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        num_heads: int=4, gate_fn: str='swish', proj_low_rank_dim: int=32,\n        gate_low_rank_dim: int=64, elementwise_affine: Optional[bool]=True,\n        norm_eps: float=1e-05, chunk_size: int=32, device=None, dtype=None,\n        **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.hidden_size = embed_dim\n        self.num_heads = num_heads\n        self.proj_low_rank_dim = proj_low_rank_dim\n        self.gate_low_rank_dim = gate_low_rank_dim\n        self.chunk_size = chunk_size\n        self.key_dim = embed_dim // 2\n        self.value_dim = embed_dim\n        assert self.key_dim % num_heads == 0, f'key dim must be divisible by num_heads of {num_heads}'\n        assert self.value_dim % num_heads == 0, f'value dim must be divisible by num_heads of {num_heads}'\n        self.head_qk_dim = self.key_dim // num_heads\n        self.head_v_dim = self.value_dim // num_heads\n        self.time_shift = nn.ZeroPad2d((0, 0, 1, -1))\n        kwarg_all['output_dim'] = proj_low_rank_dim * 5\n        self.x_proj = nn.Sequential(LerpLinear(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all), nn.Tanh(), nn.Linear(\n            proj_low_rank_dim * 5, embed_dim, bias=False, device=device,\n            dtype=dtype))\n        self.x_bias = nn.Parameter(torch.zeros(5, embed_dim, device=device,\n            dtype=dtype))\n        kwarg_all['output_dim'] = self.key_dim\n        self.r_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['low_rank_dim'] = gate_low_rank_dim\n        self.w_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all.pop('low_rank_dim')\n        self.k_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['output_dim'] = self.value_dim\n        self.v_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        kwarg_all['low_rank_dim'] = gate_low_rank_dim\n        self.g_proj = DDLerpLinear(embed_dim=self.embed_dim, block_loc=self\n            .block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.bonus = nn.Parameter(torch.zeros(num_heads, self.head_qk_dim,\n            device=device, dtype=dtype))\n        self.g_norm = nn.LayerNorm(self.value_dim, elementwise_affine=\n            elementwise_affine, eps=norm_eps, device=device, dtype=dtype)\n        self.o_proj = nn.Linear(self.value_dim, embed_dim, bias=False,\n            device=device, dtype=dtype)\n        self.gate_fn = ACT2FN[gate_fn]\n        self.apply(self._initialize_weights)\n\n    def _initialize_weights(self, module: nn.Module):\n        if getattr(module, '_is_hf_initialized', False):\n            return\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight, gain=2 ** -2.5)\n            if module.bias is not None:\n                nn.init.zeros_(module.bias)\n        if isinstance(module, nn.Parameter):\n            nn.init.xavier_uniform_(module, gain=2 ** -2.5)\n        module._is_hf_initialized = True\n\n    def naive_chunk_rwkv6(self, q: torch.Tensor, k: torch.Tensor, v: torch.\n        Tensor, w: torch.Tensor, u: torch.Tensor, chunk_size: int=32):\n        assert q.shape[-2] % chunk_size == 0\n        orig_dtype = q.dtype\n        num_chunk = q.shape[-2] // chunk_size\n        u = u.unsqueeze(0)\n        q, k, v, w = map(lambda x: rearrange(x, 'b h (n c) d -> b h n c d',\n            c=chunk_size).float(), (q, k, v, w))\n        w_cumsum = w.cumsum(-2)\n        kw = k * (w_cumsum[..., -1, None, :] - w_cumsum).exp()\n        wkv = kw.transpose(-1, -2) @ v\n        wkv_new = torch.zeros_like(wkv)\n        for i in range(num_chunk - 1):\n            wkv_new[:, :, i + 1] = wkv_new[:, :, i].clone() * w_cumsum[:, :,\n                i, -1, :, None].exp() + wkv[:, :, i]\n        o_inter = torch.einsum('b h n d p, b h n c d -> b h n c p', wkv_new,\n            q * (w_cumsum - w).exp())\n        o_intra = torch.zeros_like(o_inter)\n        for i in range(chunk_size):\n            attn = (q[:, :, :, i, None] * k * (w_cumsum[:, :, :, i, None] -\n                w[:, :, :, i, None] - w_cumsum).exp()).sum(-1)\n            mask = (torch.arange(0, chunk_size) < i).to(attn.device)\n            attn.masked_fill_(~mask, 0)\n            intra_inter_o = (attn.unsqueeze(-1) * v).sum(-2)\n            intra_intra_o = (q[:, :, :, i] * u.unsqueeze(2) * k[:, :, :, i]\n                ).sum(-1).unsqueeze(-1) * v[:, :, :, i]\n            o_intra[:, :, :, i] = intra_inter_o + intra_intra_o\n        o = o_inter + o_intra\n        return rearrange(o, 'b h n c d -> b h (n c) d').to(orig_dtype)\n\n    def pad_input(self, X):\n        _seq_len = X.shape[-2]\n        pad_len = (X.shape[-2] + self.chunk_size - 1\n            ) // self.chunk_size * self.chunk_size - X.shape[-2]\n        return F.pad(X, (0, 0, 0, pad_len)), _seq_len\n\n    def _forward(self, X: torch.Tensor):\n        X, _seq_len = self.pad_input(X)\n        batch_size, seq_len, hidden_size = X.shape\n        last_state = None\n        if X.shape[1] == 1 and last_state is not None:\n            shifted = last_state[0].unsqueeze(1)\n        else:\n            shifted = self.time_shift(X)\n            if last_state is not None:\n                shifted[:, 0] = last_state[0]\n        delta = shifted - X\n        x = self.x_proj[0](X, **{'delta': delta})[1]['o'].view(batch_size,\n            seq_len, -1, self.proj_low_rank_dim)\n        x = torch.einsum('b l n r, h n r-> b l n h', self.x_proj[1](x),\n            self.x_proj[2].weight.view(hidden_size, 5, -1))\n        r, w, k, v, g = x.add_(self.x_bias).unbind(-2)\n        r = self.r_proj(X, **{'mu': r, 'delta': delta})[1]['o']\n        w = self.w_proj(X, **{'mu': w, 'delta': delta})[1]['o']\n        k = self.k_proj(X, **{'mu': k, 'delta': delta})[1]['o']\n        v = self.v_proj(X, **{'mu': v, 'delta': delta})[1]['o']\n        g = self.g_proj(X, **{'mu': g, 'delta': delta})[1]['o']\n        r, w, k, v = map(lambda x: rearrange(x, 'b l (h d) -> b h l d', h=\n            self.num_heads), (r, w, k, v))\n        w = -torch.exp(w)\n        u = self.bonus\n        o = self.naive_chunk_rwkv6(r, k, v, w, u, chunk_size=self.chunk_size)\n        o = rearrange(o, 'b h l d -> b l (h d)')\n        o = self.g_norm(o)\n        o = o * self.gate_fn(g)\n        o = self.o_proj(o)\n        o = o[:, :_seq_len]\n        return o\n\n\nCHILDREN_DECLARATIONS = [UnitDecl(unitname='LerpLinear', requirements='',\n    inputs=['X', 'delta'], outputs=['Y']), UnitDecl(unitname='DDLerpLinear',\n    requirements='', inputs=['X', 'mu', 'delta'], outputs=['Y'])]\n",
                        "rating": null,
                        "spec": "{\"unitname\":\"RWKV6Attention\",\"document\":\"\\nRWKV6Attention\\n\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}",
                        "children": [
                            "LerpLinear",
                            "DDLerpLinear"
                        ],
                        "suggestions": null,
                        "args": {
                            "proj_low_rank_dim": 32,
                            "gate_low_rank_dim": 64,
                            "elementwise_affine": true,
                            "gate_fn": "swish",
                            "num_heads": 4,
                            "chunk_size": 32
                        },
                        "design_traces": null
                    }
                },
                "rating": null,
                "declares": {
                    "DDLerpLinear": "{\"unitname\":\"DDLerpLinear\",\"requirements\":\"N/A\",\"inputs\":[\"X\"],\"outputs\":[\"Y\"]}"
                },
                "proposal_traces": [],
                "suggestions": null,
                "name": "rwkv6_event"
            },
            "user_input": "",
            "status": "implemented",
            "design_cfg": {
                "max_attemps": {
                    "post_refinement": 0,
                    "max_search_rounds": 3,
                    "implementation_debug": 7,
                    "design_proposal": 10
                },
                "threshold": {
                    "proposal_rating": 4.0,
                    "implementation_rating": 3.0
                },
                "use_unlimited_prompt": true,
                "mutation_no_tree": true,
                "agent_types": {
                    "DESIGN_PROPOSER": "hybrid",
                    "IMPLEMENTATION_PLANNER": "hybrid",
                    "IMPLEMENTATION_CODER": "hybrid",
                    "PROPOSAL_REVIEWER": "hybrid",
                    "IMPLEMENTATION_OBSERVER": "hybrid",
                    "SEARCH_ASSISTANT": "None"
                },
                "running_mode": "Proposal + Implementation",
                "unittest_pass_required": false,
                "crossover_no_ref": true,
                "scratch_no_tree": true,
                "_agent_types": {
                    "DESIGN_PROPOSER": "claude3.5_sonnet",
                    "IMPLEMENTATION_PLANNER": "o1_preview",
                    "IMPLEMENTATION_CODER": "o1_preview",
                    "PROPOSAL_REVIEWER": "claude3.5_sonnet",
                    "IMPLEMENTATION_OBSERVER": "o1_mini",
                    "SEARCH_ASSISTANT": "None"
                },
                "termination": {
                    "max_debug_budget": 0,
                    "max_failed_rounds": 3,
                    "max_total_budget": 0
                },
                "agent_weights": {
                    "DESIGN_PROPOSER": [
                        0.05,
                        0.0,
                        0.6000000000000001,
                        0.2,
                        0.15
                    ],
                    "IMPLEMENTATION_PLANNER": [
                        0.05000000000000002,
                        0.0,
                        0.44999999999999996,
                        0.3,
                        0.20000000000000007
                    ],
                    "IMPLEMENTATION_CODER": [
                        0.0,
                        0.0,
                        0.3,
                        0.4999999999999996,
                        0.2
                    ],
                    "PROPOSAL_REVIEWER": [
                        0.10000000000000002,
                        0.0,
                        0.5499999999999999,
                        0.2,
                        0.15000000000000002
                    ],
                    "IMPLEMENTATION_OBSERVER": [
                        0.05,
                        0.0,
                        0.15000000000000002,
                        0.15000000000000002,
                        0.6499999999999999,
                        0.0
                    ]
                },
                "num_samples": {
                    "implementation": 1,
                    "rerank_method": "rating",
                    "proposal": 1
                },
                "search_settings": {
                    "proposal_search": true,
                    "proposal_review_search": true,
                    "search_for_papers_num": 10
                },
                "max_attempts": {
                    "post_refinement": 0,
                    "max_search_rounds": 4,
                    "implementation_debug": 5,
                    "design_proposal": 5
                }
            },
            "costs": {
                "DESIGN_PROPOSER": 0,
                "IMPLEMENTATION_PLANNER": 0.511755,
                "IMPLEMENTATION_CODER": 2.931075,
                "PROPOSAL_REVIEWER": 0,
                "IMPLEMENTATION_OBSERVER": 0.507399,
                "SEARCH_ASSISTANT": 0
            }
        }
    ]
}