{
    "31M": {
        "31M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "760M": {
        "760M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig = {\n    'd_model': 768,\n    'n_block': 60\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "70M": {
        "70M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "1300M": {
        "1300M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig = {\n    'd_model': 1024,\n    'n_block': 59\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "125M": {
        "125M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig = {\n    'd_model': 384,\n    'n_block': 32\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "14M": {
        "14M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig={}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    },
    "350M": {
        "350M": "import torch\nimport torch.nn as nn\nfrom model_discovery.model.utils.modules import GABBase\n\n\nclass GAB(GABBase):\n\n    def __init__(self, embed_dim: int, block_loc: tuple, device=None, dtype\n        =None, **kwargs):\n        factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc)\n        self.root = HybridRWKV(embed_dim=embed_dim, block_loc=block_loc,\n            kwarg_all=kwargs, **factory_kwargs, **kwargs)\n\n    def _forward(self, X, **Z):\n        X, Z = self.root(X, **Z)\n        return X, Z\n\n\nfrom model_discovery.model.utils.modules import GAUBase, gau_test, UnitDecl\nimport torch.nn.functional as F\n\n\nclass HybridRWKV(GAUBase):\n    \"\"\"\n    HybridRWKV: Combines RWKV6's matrix-valued states with FastTTTLinear's\n    test-time adaptation for enhanced performance and efficiency.\n\n    **Description:**\n\n    This GAU implements a hybrid architecture that integrates matrix-valued states\n    from RWKV6 with test-time adaptation mechanisms inspired by FastTTTLinear.\n    It consists of an attention mechanism and a feed-forward network (FFN), both\n    wrapped with RMSNorm layers and connected via residual connections.\n\n    **Key Features:**\n\n    - **Matrix-Valued States:** Enhances expressivity through dynamic state updates.\n    - **Test-Time Adaptation:** Improves adaptation to new inputs during inference.\n    - **Linear Attention:** Utilizes efficient linear attention mechanisms.\n    - **Hardware Optimization:** Designed for efficient computation on modern hardware.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Example:**\n\n        >>> block = HybridRWKV(embed_dim=512, block_loc=(0, 12), kwarg_all={}, device='cuda')\n        >>> x = torch.randn(8, 128, 512).to('cuda')\n        >>> y, z = block(x)\n\n    **References:**\n\n    - Peng, B., et al. (2024). \"Eagle and Finch: RWKV with Matrix-Valued States and Dynamic Recurrence.\"\n    - Yang, S., et al. (2023). \"Gated Linear Attention Transformers with Hardware-Efficient Training.\"\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.norm1 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.attention = HybridAttention(embed_dim=self.embed_dim,\n            block_loc=self.block_loc, kwarg_all=self.kwarg_all, **self.\n            factory_kwargs, **self.kwarg_all)\n        self.norm2 = RMSNorm(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.ffn = HybridFFN(embed_dim=self.embed_dim, block_loc=self.\n            block_loc, kwarg_all=self.kwarg_all, **self.factory_kwargs, **\n            self.kwarg_all)\n        self.use_checkpointing = False\n\n    def _forward(self, X, **Z):\n        X, Z = self._forward_impl(X, Z)\n        return X, Z\n\n    def _forward_impl(self, X, Z):\n        X_norm, Z_norm = self.norm1(X, **Z)\n        attn_out, Z_attn = self.attention(X_norm, **Z_norm)\n        X = X + attn_out\n        Z.update(Z_attn)\n        X_norm, Z_norm = self.norm2(X, **Z)\n        ffn_out, Z_ffn = self.ffn(X_norm, **Z_norm)\n        X = X + ffn_out\n        Z.update(Z_ffn)\n        return X, Z\n\n\nimport torch.nn.functional as F\n\n\nclass HybridFFN(GAUBase):\n    \"\"\"\n    HybridFFN: Feed-forward network with SwiGLU activation.\n\n    **Description:**\n\n    This GAU implements a feed-forward network with SwiGLU activation function,\n    following the design of modern transformer architectures.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        ffn_hidden_size (int, optional): Size of the hidden layer in FFN.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, ffn_hidden_size=None, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.ffn_hidden_size = ffn_hidden_size or embed_dim * 4\n        self.gate_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.up_proj = nn.Linear(embed_dim, self.ffn_hidden_size, **self.\n            factory_kwargs)\n        self.down_proj = nn.Linear(self.ffn_hidden_size, embed_dim, **self.\n            factory_kwargs)\n\n    def _forward(self, X, **Z):\n        gate_out = F.silu(self.gate_proj(X))\n        up_out = self.up_proj(X)\n        out = self.down_proj(gate_out * up_out)\n        return out, {}\n\n\nclass RMSNorm(GAUBase):\n    \"\"\"\n    RMSNorm: Root Mean Square Layer Normalization.\n\n    **Description:**\n\n    This GAU implements RMSNorm, a variant of layer normalization that uses\n    the root mean square of the input rather than the mean and variance.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        eps (float, optional): Epsilon for numerical stability.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, eps=1e-06, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.weight = nn.Parameter(torch.ones(embed_dim, **self.factory_kwargs)\n            )\n        self.eps = eps\n\n    def _forward(self, X, **Z):\n        norm = torch.norm(X, dim=-1, keepdim=True) * X.shape[-1] ** -0.5\n        X_norm = X / (norm + self.eps)\n        return self.weight * X_norm, {}\n\n\nimport torch.nn.functional as F\n\n\nclass HybridAttention(GAUBase):\n    \"\"\"\n    HybridAttention: Implements matrix-valued state attention with test-time adaptation.\n\n    **Description:**\n\n    This GAU implements an attention mechanism that combines matrix-valued states\n    with linear attention mechanisms for efficient computation.\n\n    **Args:**\n        embed_dim (int): Embedding dimension.\n        block_loc (tuple): Location in the model architecture.\n        kwarg_all (dict): Additional keyword arguments.\n        device (torch.device, optional): Device for computations.\n        dtype (torch.dtype, optional): Data type for computations.\n        num_heads (int, optional): Number of attention heads.\n        **kwargs: Additional keyword arguments.\n\n    **Inputs:**\n        - **X** (torch.Tensor): Input tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    **Outputs:**\n        - **Y** (torch.Tensor): Output tensor of shape `(batch_size, seq_len, embed_dim)`.\n\n    \"\"\"\n\n    def __init__(self, embed_dim: int, block_loc: tuple, kwarg_all: dict,\n        device=None, dtype=None, num_heads=8, **kwargs):\n        self.factory_kwargs = {'device': device, 'dtype': dtype}\n        super().__init__(embed_dim, block_loc, kwarg_all)\n        self.num_heads = num_heads\n        assert embed_dim % self.num_heads == 0, 'embed_dim must be divisible by num_heads'\n        self.head_dim = embed_dim // self.num_heads\n        self.q_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.k_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.v_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n        self.o_proj = nn.Linear(embed_dim, embed_dim, **self.factory_kwargs)\n\n    def _forward(self, X, **Z):\n        B, L, D = X.shape\n        H = self.num_heads\n        head_dim = D // H\n        q = self.q_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        k = self.k_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        v = self.v_proj(X).view(B, L, H, head_dim).transpose(1, 2)\n        q = F.elu(q) + 1\n        k = F.elu(k) + 1\n        kv = k * v\n        kv_cumsum = kv.cumsum(dim=2)\n        k_cumsum = k.cumsum(dim=2)\n        attn_out = q * kv_cumsum / (q * k_cumsum + 1e-06)\n        attn_out = attn_out.transpose(1, 2).contiguous().view(B, L, D)\n        attn_out = self.o_proj(attn_out)\n        return attn_out, {}\n\n\ngab_config = {'num_heads': 8, 'eps': 1e-06, 'ffn_hidden_size': None}\n\n\n\nautoconfig = {\n    'd_model': 512,\n    'n_block': 60\n}\nblock_config=gab_config\nblock_config.update(autoconfig)\n\n\nfrom .block_registry import BlockRegister\n\nBlockRegister(\n    name=\"default\",\n    config=block_config\n)(GAB)"
    }
}